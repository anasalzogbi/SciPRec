"doc.id","title","citeulike.id","raw.title","raw.abstract"
1,"the metabolic world of escherichia coli is not small",42,"The metabolic world of Escherichia coli is not small","To elucidate the organizational and evolutionary principles of the metabolism of living organisms, recent studies have addressed the graph-theoretic analysis of large biochemical networks responsible for the synthesis and degradation of cellular building blocks [Jeong, H., Tombor, B., Albert, R., Oltvai, Z. N. \& Barab\{\'a\}si, A. L. (2000) Nature 407, 651-654; Wagner, A. \& Fell, D. A. (2001) Proc. R. Soc. London Ser. B 268, 1803-1810; and Ma, H.-W. \& Zeng, A.-P. (2003) Bioinformatics 19, 270-277]. In such studies, the global properties of the network are computed by considering enzymatic reactions as links between metabolites. However, the pathways computed in this manner do not conserve their structural moieties and therefore do not correspond to biochemical pathways on the traditional metabolic map. In this work, we reassessed earlier results by digitizing carbon atomic traces in metabolic reactions annotated for Escherichia coli. Our analysis revealed that the average path length of its metabolism is much longer than previously thought and that the metabolic world of this organism is not small in terms of biosynthesis and degradation."
2,"reverse engineering of biological complexity",43,"Reverse Engineering of Biological Complexity","Advanced technologies and biology have extremely different physical implementations, but they are far more alike in systems-level organization than is widely appreciated. {C}onvergent evolution in both domains produces modular architectures that are composed of elaborate hierarchies of protocols and layers of feedback regulation, are driven by demand for robustness to uncertain environments, and use often imprecise components. {T}his complexity may be largely hidden in idealized laboratory settings and in normal operation, becoming conspicuous only when contributing to rare cascading failures. {T}hese puzzling and paradoxical features are neither accidental nor artificial, but derive from a deep and necessary interplay between complexity and robustness, modularity, feedback, and fragility. {T}his review describes insights from engineering theory and practice that can shed some light on biological complexity."
3,"early language acquisition cracking the speech code",60,"Early language acquisition: cracking the speech code.","Infants learn language with remarkable speed, but how they do it remains a mystery. New data show that infants use computational strategies to detect the statistical and prosodic patterns in language input, and that this leads to the discovery of phonemes and words. Social interaction with another human being affects speech learning in a way that resembles communicative learning in songbirds. The brain's commitment to the statistical and prosodic patterns that are experienced early in life might help to explain the long-standing puzzle of why infants are better language learners than adults. Successful learning by infants, as well as constraints on that learning, are changing theories of language acquisition."
4,"organization development and function of complex brain networks",61,"Organization, development and function of complex brain networks"," Recent research has revealed general principles in the structural and functional organization of complex networks which are shared by various natural, social and technological systems. This review examines these principles as applied to the organization, development and function of complex brain networks. Specifically, we examine the structural properties of large-scale anatomical and functional brain networks and discuss how they might arise in the course of network growth and rewiring. Moreover, we examine the relationship between the structural substrate of neuroanatomy and more dynamic functional and effective connectivity patterns that underlie human cognition. We suggest that network analysis offers new fundamental insights into global and integrative aspects of brain function, including the origin of flexible and coherent cognitive states within the neural architecture."
5,"motifs in brain networks",62,"Motifs in brain networks.","Complex brains have evolved a highly efficient network architecture whose structural connectivity is capable of generating a large repertoire of functional states. We detect characteristic network building blocks ( structural and functional motifs) in neuroanatomical data sets and identify a small set of structural motifs that occur in significantly increased numbers. Our analysis suggests the hypothesis that brain networks maximize both the number and the diversity of functional motifs, while the repertoire of structural motifs remains small. Using functional motif number as a cost function in an optimization algorithm, we obtain network topologies that resemble real brain networks across a broad spectrum of structural measures, including small-world attributes. These results are consistent with the hypothesis that highly evolved neural architectures are organized to maximize functional repertoires and to support highly efficient integration of information."
6,"topological generalizations of network motifs",98,"Topological Generalizations of network motifs","Biological and technological networks contain patterns, termed network motifs, which occur far more often than in randomized networks. {N}etwork motifs were suggested to be elementary building blocks that carry out key functions in the network. {I}t is of interest to understand how network motifs combine to form larger structures. {T}o address this, we present a systematic approach to define ""motif generalizations"": families of motifs of different sizes that share a common architectural theme. {T}o define motif generalizations, we first define ""roles"" in a subgraph according to structural equivalence. {F}or example, the feedforward loop triad--a motif in transcription, neuronal, and some electronic networks--has three roles: an input node, an output node, and an internal node. {T}he roles are used to define possible generalizations of the motif. {T}he feedforward loop can have three simple generalizations, based on replicating each of the three roles and their connections. {W}e present algorithms for efficiently detecting motif generalizations. {W}e find that the transcription networks of bacteria and yeast display only one of the three generalizations, the multi-output feedforward generalization. {I}n contrast, the neuronal network of {C}. elegans mainly displays the multi-input generalization. {F}orward-logic electronic circuits display a multi-input, multi-output hybrid. {T}hus, networks which share a common motif can have very different generalizations of that motif. {U}sing mathematical modeling, we describe the information processing functions of the different motif generalizations in transcription, neuronal, and electronic networks."
7,"collective dynamics of smallworld networks",99,"Collective dynamics of 'small-world' networks.","Networks of coupled dynamical systems have been used to model biological oscillators1, 2, 3, 4, Josephson junction arrays5,6, excitable media7, neural networks8, 9, 10, spatial games11, genetic control networks12 and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks 'rewired' to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them 'small-world' networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices."
8,"network motifs simple building blocks of complex networks",101,"Network motifs: simple building blocks of complex networks.","Complex networks are studied across many fields of science. {T}o uncover their structural design principles, we defined ""network motifs,"" patterns of interconnections occurring in complex networks at numbers that are significantly higher than those in randomized networks. {W}e found such motifs in networks from biochemistry, neurobiology, ecology, and engineering. {T}he motifs shared by ecological food webs were distinct from the motifs shared by the genetic networks of {E}scherichia coli and {S}accharomyces cerevisiae or from those found in the {W}orld {W}ide {W}eb. {S}imilar motifs were found in networks that perform information processing, even though they describe elements as different as biomolecules within a cell and synaptic connections between neurons in {C}aenorhabditis elegans. {M}otifs may thus define universal classes of networks. {T}his approach may uncover the basic building blocks of most networks."
9,"authoritative sources in a hyperlinked environment",102,"Authoritative sources in a hyperlinked environment","The network structure of a hyperlinked environment can be a rich source of in- formation about the content of the environment, provided we have eective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their eectiveness in a variety of contexts on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of \authoritative"" information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of \hub pages"" that join them together in the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristics for link-based analysis.  We began with the goal of discovering authoritative pages, but our approach in fact identies a more complex pattern of social organization on the www, in which hub pages link densely to a set of thematically related authorities. This equilibrium between hubs and authorities is a phenomenon that recurs in the context of a wide variety of topics on the www. Measures of impact and influence in bibliometrics have typically lacked, and arguably not required, an analogous formulation of the role that hubs play; the www is very dierent from the scientic literature, and our framework seems appropriate as a model of the way in which authority is conferred in an environment such as the Web."
10,"the anatomy of a largescale hypertextual web search engine",114,"The anatomy of a large-scale hypertextual Web search engine","Abstract In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http:// google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical largescale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want."
11,"spectra and eigenvectors of scalefree networks",122,"Spectra and eigenvectors of scale-free networks.","We study the spectra and eigenvectors of the adjacency matrices of scale-free networks when bidirectional interaction is allowed, so that the adjacency matrix is real and symmetric. The spectral density shows an exponential decay around the center, followed by power-law long tails at both spectrum edges. The largest eigenvalue lambda1 depends on system size N as lambda1 approximately N1/4 for large N, and the corresponding eigenfunction is strongly localized at the hub, the vertex with largest degree. The component of the normalized eigenfunction at the hub is of order unity. We also find that the mass gap scales as N(-0.68)."
12,"spectra of random graphs with given expected degrees",123,"Spectra of random graphs with given expected degrees","In the study of the spectra of power-law graphs, there are basically two  competing approaches. One is to prove analogues of Wigner's semicircle law,  whereas the other predicts that the eigenvalues follow a power-law  distribution. Although the semicircle law and the power law have nothing in  common, we will show that both approaches are essentially correct if one  considers the appropriate matrices. We will prove that (under certain mild  conditions) the eigenvalues of the (normalized) Laplacian of a random  power-law graph follow the semicircle law, whereas the spectrum of the  adjacency matrix of a power-law graph obeys the power law. Our results are  based on the analysis of random graphs with given expected degrees and their  relations to several key invariants. Of interest are a number of (new) values  for the exponent β, where phase transitions for eigenvalue distributions  occur. The spectrum distributions have direct implications to numerous graph  algorithms such as, for example, randomized algorithms that involve rapidly  mixing Markov chains."
13,"growing and navigating the small world web by local content",127,"Growing and navigating the small world Web by local content.","Can we model the scale-free distribution of Web hypertext degree under realistic assumptions about the behavior of page authors? Can a Web crawler efficiently locate an unknown relevant page? These questions are receiving much attention due to their potential impact for understanding the structure of the Web and for building better search engines. Here I investigate the connection between the linkage and content topology of Web pages. The relationship between a text-induced distance metric and a link-based neighborhood probability distribution displays a phase transition between a region where linkage is not determined by content and one where linkage decays according to a power law. This relationship is used to propose a Web growth model that is shown to accurately predict the distribution of Web page degree, based on textual content and assuming only local knowledge of degree for existing pages. A qualitatively similar phase transition is found between linkage and semantic distance, with an exponential decay tail. Both relationships suggest that efficient paths can be discovered by decentralized Web navigation algorithms based on textual and/or categorical cues."
14,"the structure of scientific collaboration networks",128,"The structure of scientific collaboration networks","The structure of scientific collaboration networks is investigated. Two scientists are considered connected if they have authored a paper together and explicit networks of such connections are constructed by using data drawn from a number of databases, including MEDLINE (biomedical research), the Los Alamos e-Print Archive (physics), and NCSTRL (computer science). I show that these collaboration networks form âsmall worlds,â in which randomly chosen pairs of scientists are typically separated by only a short path of intermediate acquaintances. I further give results for mean and distribution of numbers of collaborators of authors, demonstrate the presence of clustering in the networks, and highlight a number of apparent differences in the patterns of collaboration between the fields studied."
15,"classes of smallworld networks",129,"Classes of small-world networks","We study the statistical properties of a variety of diverse real-world networks. We present evidence of the occurrence of three classes of small-world networks: (a) scale-free networks, characterized by a vertex connectivity distribution that decays as a power law; (b) broad-scale networks, characterized by a connectivity distribution that has a power law regime followed by a sharp cutoff; and (c) single-scale networks, characterized by a connectivity distribution with a fast decaying tail. Moreover. we note for the classes of broad-scale and single-scale networks that there are constraints limiting the addition of new links. Our results suggest that the nature of such constraints may be the controlling factor for the emergence of different classes of networks."
16,"multistability in the lactose utilization network of escherichia coli",148,"Multistability in the lactose utilization network of Escherichia coli","Multistability, the capacity to achieve multiple internal states in response to a single set of external inputs, is the defining characteristic of a switch. Biological switches are essential for the determination of cell fate in multicellular organisms1, the regulation of cell-cycle oscillations during mitosis2, 3 and the maintenance of epigenetic traits in microbes4. The multistability of several natural1, 2, 3, 4, 5, 6 and synthetic7, 8, 9 systems has been attributed to positive feedback loops in their regulatory networks10. However, feedback alone does not guarantee multistability. The phase diagram of a multistable system, a concise description of internal states as key parameters are varied, reveals the conditions required to produce a functional switch11, 12. Here we present the phase diagram of the bistable lactose utilization network of Escherichia coli13. We use this phase diagram, coupled with a mathematical model of the network, to quantitatively investigate processes such as sugar uptake and transcriptional regulation in vivo. We then show how the hysteretic response of the wild-type system can be converted to an ultrasensitive graded response14, 15. The phase diagram thus serves as a sensitive probe of molecular interactions and as a powerful tool for rational network design."
17,"finding and evaluating community structure in networks",154,"Finding and evaluating community structure in networks","We propose and study a set of algorithms for discovering community structure in networksnatural divisions of network nodes into densely connected subgroups. Our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using any one of a number of possible ""betweenness"" measures, and second, these measures are, crucially, recalculated after each removal. We also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. We demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems.We propose and study a set of algorithms for discovering community structure in networksnatural divisions of network nodes into densely connected subgroups. Our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using any one of a number of possible ""betweenness"" measures, and second, these measures are, crucially, recalculated after each removal. We also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. We demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems."
18,"the structure and function of complex networks",155,"The structure and function of complex networks","Inspired by empirical studies of networked systems such as the Internet, social networks, and bio- logical networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks."
19,"matching words and pictures",156,"Matching words and pictures","We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-moda  and correspondence extensions to Hofmann’s hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data."
20,"an overview of audio information retrieval",175,"An overview of audio information retrieval","Abstract.   The problem of audio information retrieval is familiar to anyone who has returned from vacation to find an answering machine full of messages. While there is not yet an ?AltaVista? for the audio data type, many workers are finding ways to automatically locate, index, and browse audio using recent advances in speech recognition and machine listening. This paper reviews the state of the art in audio information retrieval, and presents recent advances in automatic speech recognition, word spotting, speaker and music identification, and audio similarity with a view towards making audio less ?opaque?. A special section addresses intelligent interfaces for navigating and browsing audio and multimedia documents, using automatically derived information to go beyond the tape recorder metaphor."
21,"programmed population control by cellcell communication and regulated killing",186,"Programmed population control by cell–cell communication and regulated killing","De novo engineering of gene circuits inside cells is extremely difficult1, 2, 3, 4, 5, 6, 7, 8, 9, and efforts to realize predictable and robust performance must deal with noise in gene expression and variation in phenotypes between cells10, 11, 12. Here we demonstrate that by coupling gene expression to cell survival and death using cell–cell communication, we can programme the dynamics of a population despite variability in the behaviour of individual cells. Specifically, we have built and characterized a 'population control' circuit that autonomously regulates the density of an Escherichia coli population. The cell density is broadcasted and detected by elements from a bacterial quorum-sensing system13, 14, which in turn regulate the death rate. As predicted by a simple mathematical model, the circuit can set a stable steady state in terms of cell density and gene expression that is easily tunable by varying the stability of the cell–cell communication signal. This circuit incorporates a mechanism for programmed death in response to changes in the environment, and allows us to probe the design principles of its more complex natural counterparts."
22,"mobility in collaboration",205,"Mobility in collaboration","This paper addresses an issue that has received little attention within CSCW - the requirements to support mobility within collaborative activities. By examining three quite different settings each with differing technological support, we examine the ways in which mobility is critical to collaborative work. We suggest that taking mobility seriously may not only contribute our understanding of current support for collaboration, but raise more general issues concerning the requirements for mobile and other technologies."
23,"the sociocognitive psychology of computermediated communication the present and future of technologybased interactions",214,"The sociocognitive psychology of computer-mediated communication: the present and future of technology-based interactions.","The increased diffusion of the Internet has made computer-mediated communication (CMC) very popular. However, a difficult question arises for psychologists and communication researchers: ""What are the communicative characteristics of CMC?"" According to the ""cues-filtered-out"" approach, CMC lacks the specifically relational features (social cues), which enable the interlocutors to identify correctly the kind of interpersonal situations they find themselves in. This paper counters this vision by integrating in its theoretical frame the different psycho-social approaches available in current literature. In particular, the paper describes the characteristics of the socio-cognitive processes-emotional expression, context definition, and identity creation-used by the interlocutors to make order and create relationships out of the miscommunication processes typical of CMC. Moreover, it presents the emerging forms of CMC-instant messaging, shared hypermedia, weblogs, and graphical chats-and their possible social and communicative effects."
24,"design of artificial cellcell communication using gene and metabolic networks",226,"Design of artificial cell-cell communication using gene and metabolic networks.","Artificial transcriptional networks have been used to achieve novel, nonnative behavior in bacteria. {T}ypically, these artificial circuits are isolated from cellular metabolism and are designed to function without intercellular communication. {T}o attain concerted biological behavior in a population, synchronization through intercellular communication is highly desirable. {H}ere we demonstrate the design and construction of a gene-metabolic circuit that uses a common metabolite to achieve tunable artificial cell-cell communication. {T}his circuit uses a threshold concentration of acetate to induce gene expression by acetate kinase and part of the nitrogen-regulation two-component system. {A}s one application of the cell-cell communication circuit we created an artificial quorum sensor. {E}ngineering of carbon metabolism in {E}scherichia coli made acetate secretion proportional to cell density and independent of oxygen availability. {I}n these cells the circuit induced gene expression in response to a threshold cell density. {T}his threshold can be tuned effectively by controlling {D}eltap{H} over the cell membrane, which determines the partition of acetate between medium and cells. {M}utagenesis of the enhancer sequence of the gln{A}p2 promoter produced variants of the circuit with changed sensitivity demonstrating tunability of the circuit by engineering of its components. {T}he behavior of the circuit shows remarkable predictability based on a mathematical design model."
25,"dynamic conditional random fields",230,"Dynamic conditional random fields","Conditional random fields (CRFs) for sequence modeling have several  advantages over joint models such as HMMs, including the ability to  relax strong independence assumptions made in those models, and the  ability to incorporate arbitrary overlapping features. Previous work has  focused on linear-chain CRFs, which correspond to finite-state machines,  and have efficient exact inference algorithms. Often, however, we wish  to label sequence data in multiple interacting ways---for example,..."
26,"functional and topological characterization of protein interaction networks",239,"Functional and topological characterization of protein interaction networks.","The elucidation of the cell's large-scale organization is a primary challenge for post-genomic biology, and understanding the structure of protein interaction networks offers an important starting point for such studies. We compare four available databases that approximate the protein interaction network of the yeast, Saccharomyces cerevisiae, aiming to uncover the network's generic large-scale properties and the impact of the proteins' function and cellular localization on the network topology. We show how each database supports a scale-free, topology with hierarchical modularity, indicating that these features represent a robust and generic property of the protein interactions network. We also find strong correlations between the network's structure and the functional role and subcellular localization of its protein constituents, concluding that most functional and/or localization classes appear as relatively segregated subnetworks of the full protein interaction network. The uncovered systematic differences between the four protein interaction databases reflect their relative coverage for different functional and localization classes and provide a guide for their utility in various bioinformatics studies."
27,"metabolomics and systems biology making sense of the soup",271,"Metabolomics and systems biology: making sense of the soup.","Novel techniques for acquiring metabolomics data continue to emerge. Such data require proper storage in suitably configured databases, which then permit one to establish the size of microbial metabolomes (hundreds of major metabolites) and allow the nature, organisation and control of metabolic networks to be investigated. A variety of algorithms for metabolic network reconstruction coupled to suitable modelling algorithms are the ground substances for the development of metabolic network and systems biology. Even qualitative models of metabolic networks, when subject to stoichiometric constraints, can prove highly informative, and are the first step to the quantitative models, which alone can allow the true representation of complex biochemical systems."
28,"schemes of flux control in a model of saccharomyces cerevisiae glycolysis",306,"Schemes of flux control in a model of Saccharomyces cerevisiae glycolysis.","We used parameter scanning to emulate changes to the limiting rate for steps in a fitted model of glucose-derepressed yeast glycolysis. Three flux-control regimes were observed, two of which were under the dominant control of hexose transport, in accordance with various experimental studies and other model predictions. A third control regime in which phosphofructokinase exerted dominant glycolytic flux control was also found, but it appeared to be physiologically unreachable by this model, and all realistically obtainable flux control regimes featured hexose transport as a step involving high flux control."
29,"network dynamics and cell physiology",310,"Network dynamics and cell physiology","Complex assemblies of interacting proteins carry out most of the interesting jobs in a cell, such as metabolism, DNA synthesis, movement and information processing. These physiological properties play out as a subtle molecular dance, choreographed by underlying regulatory networks. To understand this dance, a new breed of theoretical molecular biologists reproduces these networks in computers and in the mathematical language of dynamical systems."
30,"a synthetic oscillatory network of transcriptional regulators",311,"A synthetic oscillatory network of transcriptional regulators.","Networks of interacting biomolecules carry out many essential functions in living cells, but the 'design principles' underlying the functioning of such intracellular networks remain poorly understood, despite intensive efforts including quantitative analysis of relatively simple systems. {H}ere we present a complementary approach to this problem: the design and construction of a synthetic network to implement a particular function. {W}e used three transcriptional repressor systems that are not part of any natural biological clock to build an oscillating network, termed the repressilator, in {E}scherichia coli. {T}he network periodically induces the synthesis of green fluorescent protein as a readout of its state in individual cells. {T}he resulting oscillations, with typical periods of hours, are slower than the cell-division cycle, so the state of the oscillator has to be transmitted from generation to generation. {T}his artificial clock displays noisy behaviour, possibly because of stochastic fluctuations of its components. {S}uch 'rational network design may lead both to the engineering of new cellular behaviours and to an improved understanding of naturally occurring networks."
31,"information foraging",448,"Information foraging","Information foraging theory is an approach to understanding how strategies and technologies for information seeking, gathering, and consumption are adapted to the flux of information in the environment. The theory assumes that people, when possible, will modify their strategies or the structure of the environment to maximize their rate of gaining valuable information. The theory is developed by (a) adaptation (rational) analysis of information foraging problems and (b) a detailed process model (adaptive control of thought in information foraging [ACT-IF]). The adaptation analysis develops (a) information patch models, which deal with time allocation and information filtering and enrichment activities in environments in which information is encountered in clusters; (b) information scent models, which address the identification of information value from proximal cues; and (c) information diet models, which address decisions about the selection and pursuit of information items. ACT-IF is instantiated as a production system model of people interacting with complex information technology. Humans actively seek, gather, share, and consume information to a degree unapproached by other organisms. Ours might properly be characterized as a species of informavores (Dennett, 1991). Our adaptive success depends to a large extent on a vast and complex"
32,"design and implementation of microarray gene expression markup language mageml",459,"Design and implementation of microarray gene expression markup language (MAGE-ML).","BACKGROUND: Meaningful exchange of microarray data is currently difficult because it is rare that published data provide sufficient information depth or are even in the same format from one publication to another. Only when data can be easily exchanged will the entire biological community be able to derive the full benefit from such microarray studies. RESULTS: To this end we have developed three key ingredients towards standardizing the storage and exchange of microarray data. First, we have created a minimal information for the annotation of a microarray experiment (MIAME)-compliant conceptualization of microarray experiments modeled using the unified modeling language (UML) named MAGE-OM (microarray gene expression object model). Second, we have translated MAGE-OM into an XML-based data format, MAGE-ML, to facilitate the exchange of data. Third, some of us are now using MAGE (or its progenitors) in data production settings. Finally, we have developed a freely available software tool kit (MAGE-STK) that eases the integration of MAGE-ML into end users' systems. CONCLUSIONS: MAGE will help microarray data producers and users to exchange information by providing a common platform for data exchange, and MAGE-STK will make the adoption of MAGE easier."
33,"a theory of problemsolving behavior",487,"A Theory of Problem-Solving Behavior","In this paper we develop a formal, testable theory of problem-solving behavior with special relevance to individuals and small groups. The theory is consistent with principles drawn from operant behavior and social exchange theories but also incorporates elements of cognitive psychology. Problem solving is defined as a nonroutine activity oriented toward changing an undesirable state of affairs. The focus on change differentiates problem solving from coping, which is oriented toward relieving feelings of stress. A decision-making model is presented, which takes the problem-solving process through its latter stages. The theory is based on two axioms and three theorems pertaining to the process of decision making. These axioms and theorems serve as the foundation for deriving 14 theorems that establish the antecedent conditions affecting decisions relevnat to each of four stages in the problem-solving process. This theory is distinguished from other problem-solving theories in its effort to account for conditions leading to awareness of problems and in its emphasis on generic problem-solving processes rather than on the effectiveness of problem-solving outcomes."
34,"mips a database for genomes and protein sequences",501,"MIPS: a database for genomes and protein sequences","The Munich Information Center for Protein Sequences (MIPS-GSF, Neuherberg, Germany) continues to provide genome-related information in a systematic way. MIPS supports both national and European sequencing and functional analysis projects, develops and maintains automatically generated and manually annotated genome-specific databases, develops systematic classification schemes for the functional annotation of protein sequences, and provides tools for the comprehensive analysis of protein sequences. This report updates the information on the yeast genome (CYGD), the Neurospora crassa genome (MNCDB), the databases for the comprehensive set of genomes (PEDANT genomes), the database of annotated human EST clusters (HIB), the database of complete cDNAs from the DHGP (German Human Genome Project), as well as the project specific databases for the GABI (Genome Analysis in Plants) and HNB (Helmholtz-Netzwerk Bioinformatik) networks. The Arabidospsis thaliana database (MATDB), the database of mitochondrial proteins (MITOP) and our contribution to the PIR International Protein Sequence Database have been described elsewhere [Schoof et al. (2002) Nucleic Acids Res., 30, 91-93; Scharfe et al. (2000) Nucleic Acids Res., 28, 155-158; Barker et al. (2001) Nucleic Acids Res., 29, 29-32]. All databases described, the protein analysis tools provided and the detailed descriptions of our projects can be accessed through the MIPS World Wide Web server (http://mips.gsf.de)."
35,"random networks with tunable degree distribution and clustering",516,"Random Networks with Tunable Degree Distribution and Clustering","We present an algorithm for generating random networks with arbitrary degree distribution and Clustering (frequency of triadic closure). We use this algorithm to generate networks with exponential, power law, and poisson degree distributions with variable levels of clustering. Such networks may be used as models of social networks and as a testable null hypothesis about network structure. Finally, we explore the effects of clustering on the point of the phase transition where a giant component forms in a random network, and on the size of the giant component. Some analysis of these effects is presented."
36,"social structure and opinion formation",644,"Social Structure and Opinion Formation","We present a dynamical theory of opinion formation that takes explicitly into account the structure of the social network in which in- dividuals are embedded. The theory predicts the evolution of a set of opinions through the social network and establishes the existence of a martingale property, i.e. that the expected weighted fraction of the population that holds a given opinion is constant in time. Most importantly, this weighted fraction is not either zero or one, but corresponds to a non-trivial distribution of opinions in the long time limit. This co-existence of opinions within a social network is in agreement with the often observed locality effect, in which an opinion or a fad is localized to given groups without infecting the whole society. We verified these predictions, as well as those concerning the fragility of opinions and the importance of highly connected individuals in opinion formation, by performing computer experiments on a number of social networks."
37,"edutella a pp networking infrastructure based on rdf",722,"EDUTELLA: A P2P Networking Infrastructure Based on RDF","Metadata for the World Wide Web is important, but metadata for Peer-to-Peer (P2P) networks is absolutely crucial. In this paper we discuss the open source project Edutella which builds upon metadata standards defined for the WWW and aims to provide an RDF-based metadata infrastructure for P2P applications, building on the recently announced JXTA Framework. We describe the goals and main services this infrastructure will provide and the architecture to connect Edutella Peers based on exchange of RDF metadata. As the query service is one of the core services of Edutella, upon which other services are built, we specify in detail the Edutella Common Data Model (ECDM) as basis for the Edutella query exchange language (RDF-QEL-i) and format implementing distributed queries over the Edutella network. Finally, we shortly discuss registration and mediation services, and introduce the prototype and application scenario for our current Edutella aware peers."
38,"from kuramoto to crawford exploring the onset of synchronization in populations of coupled oscillators",769,"From Kuramoto to Crawford: exploring the onset of synchronization in populations of coupled oscillators","The Kuramoto model describes a large population of coupled limit-cycle oscillators whose natural frequencies are drawn from some prescribed distribution. If the coupling strength exceeds a certain threshold, the system exhibits a phase transition: some of the oscillators spontaneously synchronize, while others remain incoherent. The mathematical analysis of this bifurcation has proved both problematic and fascinating. We review 25 years of research on the Kuramoto model, highlighting the false turns as well as the successes, but mainly following the trail leading from Kuramoto's work to Crawford's recent contributions. It is a lovely winding road, with excursions through mathematical biology, statistical physics, kinetic theory, bifurcation theory, and plasma physics. (C) 2000 Elsevier Science B.V. All rights reserved."
39,"the systems biology markup language sbml a medium for representation and exchange of biochemical network models",781,"The systems biology markup language (SBML): a medium for representation and exchange of biochemical network models","Motivation: Molecular biotechnology now makes it possible to build elaborate systems models, but the systems biology community needs information standards if models are to be shared, evaluated and developed cooperatively.  Results: We summarize the Systems Biology Markup Language (SBML) Level 1, a free, open, XML-based format for representing biochemical reaction networks. SBML is a software-independent language for describing models common to research in many areas of computational biology, including cell signaling pathways, metabolic pathways, gene regulation, and others.  Availability: The specification of SBML Level 1 is freely available from http://www.sbml.org/  Contact: sysbio-team@caltech.edu 10.1093/bioinformatics/btg015"
40,"introduction to random boolean networks",816,"Introduction to Random Boolean Networks","The goal of this tutorial is to promote interest in the study of random Boolean networks (RBNs). These can be very interesting models, since one does not have to assume any functionality or particular connectivity of the networks to study their generic properties. Like this, RBNs have been used for exploring the configurations where life could emerge. The fact that RBNs are a generalization of cellular automata makes their research a very important topic. The tutorial, intended for a broad audience, presents the state of the art in RBNs, spanning over several lines of research carried out by different groups. We focus on research done within artificial life, as we cannot exhaust the abundant research done over the decades related to RBNs."
41,"robustness in bacterial chemotaxis",841,"Robustness in bacterial chemotaxis.","Networks of interacting proteins orchestrate the responses of living cells to a variety of external stimuli, but how sensitive is the functioning of these protein networks to variations in their biochemical parameters? One possibility is that to achieve appropriate function, the reaction rate constants and enzyme concentrations need to be adjusted in a precise manner, and any deviation from these 'fine-tuned' values ruins the network's performance. An alternative possibility is that key properties of biochemical networks are robust; that is, they are insensitive to the precise values of the biochemical parameters. Here we address this issue in experiments using chemotaxis of Escherichia coli, one of the best-characterized sensory systems. We focus on how response and adaptation to attractant signals vary with systematic changes in the intracellular concentration of the components of the chemotaxis network. We find that some properties, such as steady-state behaviour and adaptation time, show strong variations in response to varying protein concentrations. In contrast, the precision of adaptation is robust and does not vary with the protein concentrations. This is consistent with a recently proposed molecular mechanism for exact adaptation, where robustness is a direct consequence of the network's architecture."
42,"serendipity and information seeking an empirical study",943,"Serendipity and information seeking: an empirical study","""Serendipity"" has both a classical origin in literature and a more modern manifestation where it is found in the descriptions of the problem solving and knowledge acquisition of humanities and science scholars. Studies of information retrieval and information seeking have also discussed the utility of the notion of serendipity. Some have implied that it may be stimulated, or that certain people may ""encounter"" serendipitous information more than others. All to some extent accept the classical definition of serendipity as a ""fortuitous"" accident. The analysis presented here is part of a larger study concerning the information-seeking behaviour of interdisciplinary scholars. This paper considers the nature of serendipity in information-seeking contexts, and reinterprets the notion of serendipity as a phenomenon arising from both conditions and strategies - as both a purposive and a non-purposive component of information seeking and related knowledge acquisition."
43,"inferring quantitative models of regulatory networks from expression data",1019,"Inferring quantitative models of regulatory networks from expression data.","Motivation: Genetic networks regulate key processes in living cells. Various methods have been suggested to reconstruct network architecture from gene expression data. However, most approaches are based on qualitative models that provide only rough approximations of the underlying events, and lack the quantitative aspects that are critical for understanding the proper function of biomolecular systems.  Results: We present fine-grained dynamical models of gene transcription and develop methods for reconstructing them from gene expression data within the framework of a generative probabilistic model. Unlike previous works, we employ quantitative transcription rates, and simultaneously estimate both the kinetic parameters that govern these rates, and the activity levels of unobserved regulators that control them. We apply our approach to expression datasets from yeast and show that we can learn the unknown regulator activity profiles, as well as the binding affinity parameters. We also introduce a novel structure learning algorithm, and demonstrate its power to accurately reconstruct the regulatory network from those datasets. 10.1093/bioinformatics/bth941"
44,"using bayesian networks to analyze expression data",1021,"Using Bayesian networks to analyze expression data.","DNA hybridization arrays simultaneously measure the expression level for thousands of genes. These measurements provide a “snapshot ” of transcription levels within the cell. A major challenge in computational biology is to uncover, from such measurements, gene/protein interactions and key biological features of cellular systems. In this paper, we propose a new framework for discovering interactions between genes based on multiple expression measurements. This framework builds on the use of Bayesian networks for representing statistical dependencies. A Bayesian network is a graph-based model of joint multivariate probability distributions that captures properties of conditional independence between variables. Such models are attractive for their ability to describe complex stochastic processes and because they provide a clear methodology for learning from (noisy) observations. We start by showing how Bayesian networks can describe interactions between genes. We then describe a method for recovering gene interactions from microarray data using tools for learning Bayesian networks. Finally, we demonstrate this method on the S. cerevisiae cell-cycle measurements of Spellman et al. (1998). Key words: gene expression, microarrays, Bayesian methods. 1."
45,"combining location and expression data for principled discovery of genetic regulatory network models",1022,"Combining location and expression data for principled discovery of genetic regulatory network models.","We develop principled methods for the automatic induction (discovery) of genetic regulatory network models from multiple data sources and data modalities. Models of regulatory networks are represented as Bayesian networks, allowing the models to compactly and robustly capture probabilistic multivariate statistical dependencies between the various cellular factors in these networks. We build on previous Bayesian network validation results by extending the validation framework to the context of model induction, leveraging heuristic simulated annealing search algorithms and posterior model averaging. Using expression data in isolation yields results inconsistent with location data so we incorporate genomic location data to guide the model induction process. We combine these two data modalities by allowing location data to influence the model prior and expression data to influence the model likelihood. We demonstrate the utility of this approach by discovering genetic regulatory models of thirty-three variables involved in S. cerevisiae pheromone response. The models we automatically generate are consistent with the current understanding regarding this regulatory network, but also suggest new directions for future experimental investigation. 1"
46,"using graphical models and genomic expression data to statistically validate models of genetic regulatory networks",1023,"Using graphical models and genomic expression data to statistically validate models of genetic regulatory networks.","We propose a model-driven approach for analyzing genomic expression data that permits genetic regulatory networks to be represented in a biologically interpretable computational form. Our models permit latent variables capturing unobserved factors, describe arbitrarily complex (more than pair-wise) relationships at varying levels of refinement, and can be scored rigorously against observational data. The models that we use are based on Bayesian networks and their extensions. As a demonstration of this approach, we utilize 52 genomes worth of Affymetrix GeneChip expression data to correctly differentiate between alternative hypotheses of the galactose regulatory network in S. cerevisiae. When we extend the graph semantics to permit annotated edges, we are able to score models describing relationships at a finer degree of specification. 1 Introduction The vast quantity of data generated by genomic expression arrays affords researchers a significant opportunity to transform biology, medicine, and pharmacology using systematic computational methods. The availability of genomic (and eventually proteomic) expression data promises to have a profound impact on the understanding of basic cellular processes, the diagnosis and treatment of disease, and the efficacy of designing and delivering targeted therapeutics. Particularly relevant to these objectives is the development of a deeper understanding of the various mechanisms by which cells control and regulate the transcription of their genes. In this paper, we present a principled method for using genomic expression data to elucidate these genetic regulatory networks. While the potential utility of expression data is immense, some obstacles"
47,"integrated semanticsyntactic video modeling for search and browsing",1064,"Integrated Semantic-Syntactic Video Modeling for Search and Browsing","Video processing and computer vision communities usually employ shot-based or object-based structural video models and associate low-level (color, texture, shape, and motion) and semantic descriptions (textual annotations) with these structural (syntactic) elements. Database and information retrieval communities, on the other hand, employ entity-relation or object-oriented models to model the semantics of multimedia documents. This paper proposes a new generic integrated semantic-syntactic video model to include all of these elements within a single framework to enable structured video search and browsing combining textual and low-level descriptors. The proposed model includes semantic entities (video objects and events) and the relations between them. We introduce a new ""actor"" entity to enable grouping of object roles in specific events. This context-dependent classification of attributes of an object allows for more efficient browsing and retrieval. The model also allows for decomposition of events into elementary motion units and elementary reaction/interaction units in order to access mid-level semantics and low-level video features. The instantiations of the model are expressed as graphs. Users can formulate flexible queries that can be translated into such graphs. Alternatively, users can input query graphs by editing an abstract model (model template). Search and retrieval is accomplished by matching the query graph with those instantiated models in the database. Examples and experimental results are provided to demonstrate the effectiveness of the proposed integrated modeling and querying framework."
48,"composable and compilable macros you want it when",1164,"Composable and compilable macros:: you want it when?","Many macro systems, especially for Lisp and Scheme, allow macro transformers to perform general computation. Moreover, the language for implementing compile-time macro transformers is usually the same as the language for implementing run-time functions. As a side effect of this sharing, implementations tend to allow the mingling of compile-time values and run-time values, as well as values from separate compilations. Such mingling breaks programming tools that must parse code without executing it. Macro implementors avoid harmful mingling by obeying certain macro-definition protocols and by inserting phase-distinguishing annotations into the code. However, the annotations are fragile, the protocols are not enforced, and programmers can only reason about the result in terms of the compiler's implementation. MzScheme---the language of the PLT Scheme tool suite---addresses the problem through a macro system that separates compilation without sacrificing the expressiveness of macros."
49,"a callbyneed lambda calculus",1380,"A call-by-need lambda calculus","The mismatch between the operational semantics of the lambda calculus and the actual behavior of implementations is a major obstacle for compiler writers. They cannot explain the behavior of their evaluator in terms of source level syntax, and they cannot easily compare distinct implementations of different lazy strategies. In this paper we derive an equational characterization of call-by-need and prove it correct with respect to the original lambda calculus. The theory is a strictly smaller theory than the lambda calculus. Immediate applications of the theory concern the correctness proofs of a number of implementation strategies,  e.g. , the call-by-need continuation passing transformation and the realization of sharing via assignments."
50,"integrated genomic and proteomic analyses of a systematically perturbed metabolic network",2100,"Integrated Genomic and Proteomic Analyses of a Systematically Perturbed Metabolic Network","We demonstrate an integrated approach to build, test, and refine a model of a cellular pathway, in which perturbations to critical pathway components are analyzed using DNA microarrays, quantitative proteomics, and databases of known physical interactions. Using this approach, we identify 997 messenger RNAs responding to 20 systematic perturbations of the yeast galactose-utilization pathway, provide evidence that approximately 15 of 289 detected proteins are regulated posttranscriptionally, and identify explicit physical interactions governing the cellular response to each perturbation. We refine the model through further iterations of perturbation and global measurements, suggesting hypotheses about the regulation of galactose utilization and physical interactions between this and a variety of other metabolic pathways."
51,"network structure and the diffusion of knowledge",2118,"Network structure and the diffusion of knowledge","This paper models knowledge diffusion as a barter process in which agents exchange different types of knowledge. This is intended to capture the observed practice of informal knowledge trading. Agents are located on a network and are directly connected with a small number of other agents. Agents repeatedly meet those with whom direct connections exist and trade if mutually profitable trades exist. In this way knowledge diffuses throughout the economy. We examine the relationship between network architecture and diffusion performance. We consider the space of structures that fall between, at one extreme, a network in which every agent is connected to  n  nearest neighbours, and at the other extreme a network with each agent being connected to, on average,  n  randomly chosen agents. We find that the performance of the system exhibits clear ‘small world’ properties, in that the steady-state level of average knowledge is maximal when the structure is a small world (that is, when most connections are local, but roughly 10 percent of them are long distance). The variance of knowledge levels among agents is maximal in the small world region, whereas the coefficient of variation is minimal. We explain these results as reflecting the dynamics of knowledge transmission as affected by the architecture of connections among agents."
52,"improved monomeric red orange and yellow fluorescent proteins derived from discosoma sp red fluorescent protein",2837,"Improved monomeric red, orange and yellow fluorescent proteins derived from Discosoma sp. red fluorescent protein","Fluorescent proteins are genetically encoded, easily imaged reporters crucial in biology and biotechnology1, 2. When a protein is tagged by fusion to a fluorescent protein, interactions between fluorescent proteins can undesirably disturb targeting or function3. Unfortunately, all wild-type yellow-to-red fluorescent proteins reported so far are obligately tetrameric and often toxic or disruptive4, 5. The first true monomer was mRFP1, derived from the Discosoma sp. fluorescent protein “DsRed” by directed evolution first to increase the speed of maturation6, then to break each subunit interface while restoring fluorescence, which cumulatively required 33 substitutions7. Although mRFP1 has already proven widely useful, several properties could bear improvement and more colors would be welcome. We report the next generation of monomers. The latest red version matures more completely, is more tolerant of N-terminal fusions and is over tenfold more photostable than mRFP1. Three monomers with distinguishable hues from yellow-orange to red-orange have higher quantum efficiencies."
53,"traits composable units of behavior",3123,"Traits: Composable Units of Behavior","Inheritance is the fundamental reuse mechanism in object-oriented programming languages; its most prominent variants are single inheritance, multiple inheritance, and mixin inheritance. In the first part of this paper, we identify and illustrate the conceptual and practical reusability problems that arise with these forms of inheritance. We then present a simple compositional model for structuring object-oriented programs, which we call traits. Traits are essentially groups of methods that serve as building blocks for classes and are primitive units of code reuse. In this model, classes are composed from a set of traits by specifying glue code that connects the traits together and accesses the necessary state. We demonstrate how traits overcome the problems arising with the different variants of inheritance, we discuss how traits can be implemented effectively, and we summarize our experience applying traits to refactor an existing class hierarchy."
54,"distance teaching comparing two online information literacy courses",3200,"Distance Teaching: Comparing Two Online Information Literacy Courses","This article explores the similarities and differences between two asynchronous online information literacy courses. Details of the courses and how the ACRL information literacy standards are incorporated will be outlined. In exploring distance learning and distance teaching, the article will discuss issues related to online information literacy learning experiences and suggest ways to address those issues and improve teaching and learning."
55,"searching the world wide web",3271,"Searching the World Wide Web","The coverage and recency of the major World Wide Web search engines was analyzed, yielding some surprising results. The coverage of any one engine is significantly limited: No single engine indexes more than about one-third of the &#034;indexable Web,&#034; the coverage of the six engines investigated varies by an order of magnitude, and combining the results of the six engines yields about 3.5 times as many documents on average as compared with the results from only one engine. Analysis of the overlap between pairs of engines gives an estimated lower bound on the size of the indexable Web of 320 million pages."
56,"organizing programs without classes",3298,"Organizing Programs Without Classes",". All organizational functions carried out by classes can be accomplished in a simple and natural way by object inheritance in classless languages, with no need for special mechanisms. A single model---dividing types into prototypes and traits---supports sharing of behavior and extending or replacing representations. A natural extension, dynamic object inheritance, can model behavioral modes. Object inheritance can also be used to provide structured name spaces for well-known objects. Classless ..."
57,"propagation of trust and distrust",3450,"Propagation of Trust and Distrust","A (directed) network of people connected by ratings or trust scores, and a model for propagating those trust scores, is a fundamental building block in many of today's most successful e-commerce and recommendation systems. We develop a framework of trust propagation schemes, each of which may be appropriate in certain circumstances, and evaluate the schemes on a large trust network consisting of 800K trust scores expressed among 130K people. We show that a small number of expressed trusts/distrust per individual allows us to predict trust between any two people in the system with high accuracy. Our work appears to be the first to incorporate distrust in a computational trust propagation setting."
58,"a conceptual framework and a toolkit for supporting the rapid prototyping of contextaware applications",3521,"A conceptual framework and a toolkit for supporting the rapid prototyping of context-aware applications","Computing devices and applications are now used beyond the desktop, in diverse environments, and this trend toward ubiquitous computing is accelerating. One challenge that remains in this emerging research field is the ability to enhance the behavior of any application by informing it of the context of its use. By context, we refer to any information that characterizes a situation related to the interaction between humans, applications, and the surrounding environment. Context-aware applications promise richer and easier interaction, but the current state of research in this field is still far removed from that vision. This is due to 3 main problems: (a) the notion of context is still ill defined, (b) there is a lack of conceptual models and methods to help drive the design of context-aware applications, and (c) no tools are available to jump-start the development of context-aware applications. In this anchor article, we address these 3 problems in turn. We first define context, identify categories of contextual information, and characterize context-aware application behavior. Though the full impact of context-aware computing requires understanding very subtle and high-level notions of context, we are focusing our efforts on the pieces of context that can be inferred automatically from sensors in a physical environment. We then present a conceptual framework that separates the acquisition and representation of context from the delivery and reaction to context by a context-aware application. We have built a toolkit, the Context Toolkit, that instantiates this conceptual framework and supports the rapid development of a rich space of context-aware applications. We illustrate the usefulness of the conceptual framework by describing a number of context-aware applications that have been prototyped using the Context Toolkit. We also demonstrate how such a framework can support the investigation of important research challenges in the area of context-aware computing."
59,"letizia an agent that assists web browsing",3532,"Letizia: An Agent That Assists Web Browsing","Letizia is a user interface agent that assists a user browsing the World Wide Web. As the user operates a conventional Web browser such as Netscape, the agent tracks user behavior and attempts to anticipate items of interest by doing concurrent, autonomous exploration of links from the user's current position. The agent automates a browsing strategy consisting of a best-first search augmented by heuristics inferring user interest from browsing behavior.  1 Introduction  Letizia Álvarez de..."
60,"scattergather a clusterbased approach to browsing large document collections",3560,"Scatter/Gather: A Cluster-based Approach to Browsing Large Document Collections","Document clustering has not been well received as an information retrieval tool. Objections to its use fall into two main categories: first, that clustering is too slow for large corpora (with running time often quadratic in the number of documents); and second, that clustering does not appreciably improve retrieval. We argue that these problems arise only when clustering is used in an attempt to improve conventional search techniques. However, looking at clustering as an information access tool in its own right obviates these objections, and provides a powerful new access paradigm. We present a document browsing technique that employs document clustering as its primary operation. We also present fast (linear time) clustering algorithms which support this interactive browsing paradigm."
61,"initialization of iterative refinement clustering algorithms",3562,"Initialization of Iterative Refinement Clustering Algorithms","Iterative refinement clustering algorithms (e.g. K-Means, EM) converge to one of numerous local minima. It is known that they are especially sensitive to initial conditions. We present a procedure for computing a refined starting condition from a given initial one that is based on an efficient technique for estimating the modes of a distribution. The refined initial starting condition leads to convergence to &amp;quot;better &amp;quot; local minima. The procedure is applicable to a wide class of clustering algorithms for both discrete and continuous data. We demonstrate the application of this method to the Expectation Maximization (EM) clustering algorithm and show that refined initial points indeed lead to improved solutions. Refinement run time is considerably lower than the time required to cluster the full database. The method is scalable and can be coupled with a scalable clustering algorithm to address the large-scale clustering in data mining."
62,"collection development for new librarians advice from the trenches",4104,"Collection development for new librarians: Advice from the trenches","There are many challenges facing new librarians in the academic environment, including collection development. This article analyzes the topic of collection development and how it relates to new professionals in the field of librarianship. The article contains a literature review of papers discussing the collection development curriculum in library and information science programs, expected skills required of collection development offices, and library training programs for new librarians. The article also provides practical advise by recent graduates and their collection development experiences. Topics of discussion include acclimation to a new environment, collection development policies and procedures, liaison work, resource selection, and time management."
63,"knowledge management the benefits and limitations of computer systems",4119,"Knowledge Management: The Benefits and Limitations of Computer Systems","Much organisational effort has been put into knowledge management initiatives in recent years, and information and communication technologies (ICTs) have been central to many of these initiatives. However, organisations have found that levering knowledge through ICTs is often hard to achieve. This paper addresses the question of why this is the case, and what we can learn of value to the future practice of knowledge management. The analysis in the paper is based on a human-centred view of knowledge, emphasising the deep tacit knowledge which underpins human thought and action, and the complex sense-reading and sense-giving processes which human beings carry out in communicating with each other and 'sharing' knowledge. The paper concludes that computer-based systems can be of benefit in knowledge-based activities, but only if we are careful in using such systems to support the development and communication of human meaning."
64,"mathematics is biologys next microscope only better biology is mathematics next physics only better",4280,"Mathematics Is Biology's Next Microscope, Only Better; Biology Is Mathematics' Next Physics, Only Better","Although mathematics has long been intertwined with the biological sciences, an explosive synergy between biology and mathematics seems poised to enrich and extend both fi elds greatly in the coming decades (Levin 1992; Murray 1993; Jungck 1997; Hastings et al. 2003; Palmer et al. 2003; Hastings and Palmer 2003). Biology will increasingly stimulate the creation of qualitatively new realms of mathematics. Why? In biology, ensemble properties emerge at each level of organization from the interactions of heterogeneous biological units at that level and at lower and higher levels of organization (larger and smaller physical scales, faster and slower temporal scales). New mathematics will be required to cope with these ensemble properties and with the heterogeneity of the biological units that compose ensembles at each level."
65,"building programmable jigsaw puzzles with rna",4297,"Building programmable jigsaw puzzles with RNA.","One challenge in supramolecular chemistry is the design of versatile, self-assembling building blocks to attain total control of arrangement of matter at a molecular level. We have achieved reliable prediction and design of the three-dimensional structure of artificial RNA building blocks to generate molecular jigsaw puzzle units called tectosquares. They can be programmed with control over their geometry, topology, directionality, and addressability to algorithmically self-assemble into a variety of complex nanoscopic fabrics with predefined periodic and aperiodic patterns and finite dimensions. This work emphasizes the modular and hierarchical characteristics of RNA by showing that small RNA structural motifs can code the precise topology of large molecular architectures. It demonstrates that fully addressable materials based on RNA can be synthesized and provides insights into self-assembly processes involving large populations of RNA molecules."
66,"consistent hashing and random trees distributed caching protocols for relieving hot spots on the world wide web",4303,"Consistent hashing and random trees: distributed caching protocols for relieving hot spots on the World Wide Web","We describe a family of caching protocols for distributed networks that can be used to decrease or eliminate the occurrence of hot spots in the network. Our protocols are particularly designed for use with very large networks such as the Internet, where delays caused by hot spots can be severe, and where it is not feasible for every server to have complete information about the current state of the entire network. The protocols are easy to implement using existing network protocols such as TCP/IP, and require very little overhead. The protocols work with local control, make efficient use of existing resources, and scale gracefully as the network grows. Our caching protocols are based on a special kind of hashing that we call consistent hashing. Roughly speaking, a consistent hash function is one which changes minimally as the range of the function changes. Through the development of good consistent hash functions, we are able to develop caching protocols which do not require users to ..."
67,"peertopeer architecture case study gnutella network",4304,"Peer-to-Peer Architecture Case Study: Gnutella Network","Despite recent excitement generated by the P2P paradigm and despite surprisingly fast deployment of some P2P applications, there are few quantitative evaluations of P2P system behavior. Due to its open architecture and achieved scale, Gnutella is an interesting P2P architecture case study. Gnutella, like most other P2P applications, builds at the application level a virtual network with its own routing mechanisms. The topology of this overlay network and the routing mechanisms used have a significant influence on application properties such as performance, reliability, and scalability. We built a 'crawler' to extract the topology of Gnutella's application level network, we analyze the topology graph and evaluate generated network traffic. We find that although Gnutella is not a pure power-law network, its current configuration has the benefits and drawbacks of a power-law structure. These findings lead us to propose changes to the Gnutella protocol and implementations that bring significant performance and scalability improvements"
68,"simple fast and practical nonblocking and blocking concurrent queue algorithms",4448,"Simple, fast, and practical non-blocking and blocking concurrent queue algorithms","Drawing ideas from previous authors, we present a new non-blocking concurrent queue algorithm and a new twolock queue algorithm in which one enqueue and one dequeue can proceed concurrently. Both algorithms are simple, fast, and practical; we were surprised not to find them in the literature. Experiments on a 12-node SGI Challenge multiprocessor indicate that the new non-blocking queue consistently outperforms the best known alternatives; it is the clear algorithm of choice for machines that..."
69,"wireless broadband drivers and their social implications",8851,"Wireless broadband drivers and their social implications","Wireless local area networks now offer high-speed Internet access at numerous locations in both public and private environments. Associated with this rapid growth, numerous social implications come to the fore, especially relating to practices, such as the free use and shar- ing of bandwidth. Using case-based comparative analysis, we examine three primary strate- gies involved in providing wireless broadband access. Based on this research, we discuss the future of Wi-Fi growth, emergent competing technologies, and the broad social implications of this phenomenon."
70,"the network paradigm in organizational research a review and typology",32678,"The Network Paradigm in Organizational Research: A Review and Typology","In this paper, we review and analyze the emerging network paradigm in organizational research. We begin with a conventional review of recent research organized around recognized research streams. Next, we analyze this research, developing a set of dimensions along which network studies vary, including direction of causality, levels of analysis, explanatory goals, and explanatory mechanisms. We use the latter two dimensions to construct a 2-by-2 table cross-classifying studies of network consequences into four canonical types: structural social capital, social access to resources, contagion, and environmental shaping. We note the rise in popularity of studies with a greater sense of agency than was traditional in network research. 10.1016/S0149-2063_03_00087-4"
71,"higher education management and policy volume issue complete edition",45311,"Higher Education Management and Policy: Volume 16 Issue 2 Complete Edition","The Journal of OECD&#39;s Programme on Institutional Management in Higher Education. This issue features article on fair access, assessment of personnel, a merger, private university policy initiatives, accessibility and equity, internationalisation, and enrollment management.  - A Faustian Bargain&#63; Institutional Responses to National and International Rankings -  - &#34;Standards Will Drop&#34; - and Other Fears about the Equality Agenda in Higher Education -"
72,"becoming an online teacher adapting to a changed environment for teaching and learning in higher education",52185,"Becoming an Online Teacher: Adapting to a Changed Environment for Teaching and Learning in Higher Education","Advancements in online technologies have facilitated a convergence of distance and campus-based learning and, thus, offer new opportunities for all students through better access to resources, increased interaction between staff and students and greater flexibility in place and time. However, the transition to online teaching and learning presents new challenges as the roles and expectations of both staff and students evolve. An online teacher must create a coherent learning experience for students with whom they may not meet face-to-face and, therefore, must develop new support strategies that maintain motivation and encourage interaction. Adapting student-centred approaches to the online environment has required the development of new skills and changes to teaching practices. This paper presents an analysis of the changed environment for teachers and learners in a post-graduate coursework programme based on constructivist principles that has moved from predominately on-campus delivery to online mode. The authors examine the impact of changes to teaching and learning over the past 5 years of the programme's development and reflect on the implications of these for becoming an online teacher. <b>Devenir un professeur en ligne : Adaptation à un environnement varié d'enseignement et de formation supérieure.</b> Les progrès de technologie en ligne ont facilité une convergence de formation par correspondance et de formation au campus et ceci offre de nouvelles possibilités pour tous les étudiants par un meilleur accès aux ressources, une interaction augmentée entre le corps enseignant et les étudiants et une plus grande flexibilité temporelle et cohérente. Cependant, la transition vers l'enseignement et la formation en ligne représente de nouveaux défis étant donné que les rôles et les expectatives du corps enseignant ainsi que des étudiants altèrent. Un professeur en ligne doit créer une expérience de formation cohérente pour des étudiants qu'il ne rencontre pas personnellement et par conséquent il doit développer une nouvelle stratégie de soutien qui maintient la motivation et encourage l'interaction. L'adaption des approches concentrées sur les étudiants à un environnement en ligne exige un développement de nouveaux talents et des changements de pratiques d'enseignement. Cet exposé présente une analyse de la convergence d'environnements de formation qui a tourné d'une assistance prédominante du côté du campus à un mode en ligne et de l'effet de cette convergence sur les professeurs et les étudiants d'un programme d'études basé sur des principes constructifs. Les auteurs examinent l'impact des changements d'enseignement et de formation pendant les cinq dernières années du développement de ce programme et considèrent ses répercussions sur l'éducation d'un professeur en ligne. <b>Ausbildung zur Online-Lehrkraft: Anpassung an ein verändertes Lern- und Lehrumfeld im Bereich der Hochschulausbildung.</b> Die Fortschritte in der Online-Technologie haben eine Annäherung zwischen Fernstudium und hochschulbasiertem Lernen ermöglicht, und dies eröffnet neue Möglichkeiten für alle Studenten durch einen besseren Zugang zu Ressourcen, einer verbesserten Interaktion zwischen Lehrerkollegium und Studenten und einer größeren räumlichen und zeitlichen Flexibilität. Der Übergang zum Online-Unterricht und Lernen stellt außerdem neue Herausforderungen dar, da sich die Rolle und die Erwartungen sowohl des Lehrerkollegiums als auch der Studenten verändern. Eine Online-Lehrkraft muss ein schlüssiges Lernerlebnis für Studenten schaffen, die sie gegebenenfalls nie persönlich kennen lernt. Aus diesem Grunde muss sie neue Betreuungsstrategien entwickeln, die die Motivation aufrechterhalten und die Interaktion fördern. Durch die Anpassung der studentischen Lernansätze an ein Online-Umfeld ist die Entwicklung neuer Fähigkeiten und eine Änderung der Lehrpraktiken erforderlich geworden. Dieser Bericht stellt eine Analyse des veränderten Lernumfelds für Lehrer und Studenten in einem auf konstruktivistischen Prinzipien basierten weiterführenden Studienprogramm vor, das von einer überwiegend hochschulbasierten Wissensvermittlung zu Online-Methoden übergeht. Die Autoren untersuchen den Einfluss der Lehr- und Lernveränderungen in den vergangenen fünf Jahren der Programmentwicklung und betrachten deren Auswirkungen auf die Ausbildung einer Online-Lehrkraft."
73,"an actornetwork critique of community in higher education implications for networked learning",70786,"An actor-network critique of community in higher education: implications for networked learning","This article provides an actor-network critique of ideas on community that are influential in higher education and draws implications for networked learning theory and practice. Networked learning is examined as an educational movement which contains alternative models of learning but which offers to create a sense of virtual community within the structures of mass higher education. Benedict Anderson’s work is drawn upon to understand the notion of community and to examine ‘the nation’ as  prototypical community. Aspects of actor-network theory are discussed and illustrated, from which a critique of the idea of community in higher education is developed."
74,"codata and comonads in haskell",70816,"Codata and Comonads in Haskell","Haskell, a wide spectrum, functional programming language, provides means to define and use an extremely rich variety of data including free, polymorphic datatypes, type classes, and data with additional computational structure abstracted by monads. Somewhat less attention has been given to supporting abstract data types, which we shall call codata types. Monomorphic, parameterless versions of codata types can be defined by modules, but Haskell's module system is not comparably powerful with the class system."
75,"random early detection gateways for congestion avoidance",71724,"Random early detection gateways for congestion avoidance","This paper presents Random Early Detection (RED) gateways for congestion avoidance in packet-switched networks. The gateway detects incipient congestion by computing the average queue size. The gateway could notify connections of congestion either by dropping packets arriving at the gateway or by setting a bit in packet headers. When the average queue size exceeds a preset threshold, the gateway drops or marks each arriving packet with a certain probability, where the exact probability is a function of the average queue size. RED gateways keep the average queue size low while allowing occasional bursts of packets in the queue. During congestion, the probability that the gateway notifies a particular connection to reduce its window is roughly proportional to that connection's share of the bandwidth through the gateway. RED gateways are designed to accompany a transport-layer congestion control protocol such as TCP. The RED gateway has no bias against bursty traffic and avoids the global synchronization of many connections decreasing their window at the same time. Simulations of a TCP/IP network are used to illustrate the performance of RED gateways."
76,"a measurement study of the bittorrent peertopeer filesharing system",71746,"A measurement study of the bittorrent peer-to-peer file-sharing system","P2P systems for sharing content have become very popular over the last few  years. However, despite the increasing attention of both the research community  and large numbers of users, the actual behavior of these systems over prolonged periods  of time is still poorly understood. This paper presents a detailed measurement  study over a period of eight months of BitTorrent/Suprnova, a P2P file-sharing  system that is quickly gaining in popularity. In particular, we show measurement  results of..."
77,"monadic parser combinators",71783,"Monadic Parser Combinators","In functional programming, a popular approach to building recursive descent parsers is to model parsers as functions, and to define higher-order functions (or combinators) that implement grammar constructions such as sequencing, choice, and repetition. Such parsers form an instance of a monad , an algebraic structure from mathematics that has proved useful for addressing a number of computational problems. The purpose of this article is to provide a step-by-step tutorial on the monadic approach to building functional parsers, and to explain some of the benefits that result from exploiting monads. No prior knowledge of parser combinators or of monads is assumed. Indeed, this article can also be viewed as a first introduction to the use of monads in programming.  2 Graham Hutton and Erik Meijer  Contents  1 Introduction 3 2 Combinator parsers 4 2.1 The type of parsers 4 2.2 Primitive parsers 4 2.3 Parser combinators 5 3 Parsers and monads 8 3.1 The parser monad 8 3.2 Monad comprehension ..."
78,"document clustering by concept factorization",73383,"Document clustering by concept factorization","In this paper, we propose a new data clustering method called concept factorization that models each concept as a linear combination of the data points, and each data point as a linear combination of the concepts. With this model, the data clustering task is accomplished by computing the two sets of linear coefficients, and this linear coefficients computation is carried out by finding the non-negative solution that minimizes the reconstruction error of the data points. The cluster label of each data point can be easily derived from the obtained linear coefficients. This method differs from the method of clustering based on non-negative matrix factorization (NMF) in that it can be applied to data containing negative values and the method can be implemented in the kernel space. Our experimental results show that the proposed data clustering method and its variations performs best among 11 algorithms and their variations that we have evaluated on both TDT2 and Reuters-21578 corpus. In addition to its good performance, the new method also has the merit in its easy and reliable derivation of the clustering results"
79,"guided docking approaches to structurebased design and screening",74514,"Guided Docking Approaches to Structure-Based Design and Screening","			With the number of protein-ligand complexes available in the Protein Data Bank constantly growing, structure-based approaches to drug design and screening have become increasingly important. Alongside this explosion of structural information, a number of molecular docking methods have been developed over the last years with the aim of maximally exploiting all available structural and chemical information that can be derived from proteins, from ligands, and from protein-ligand complexes. In this respect, the term &#039;guided docking&#039; is introduced to refer to docking approaches that incorporate some degree of chemical information to actively guide the orientation of the ligand into the binding site. To reflect the focus on the use of chemical information, a classification scheme for guided docking approaches is proposed. In general terms, guided docking approaches can be divided into indirect and direct approaches. Indirect approaches incorporate chemical information implicitly, having an effect on scoring but not on orienting the ligand during sampling. In contrast, direct approaches incorporate chemical information explicitly, thus actively guiding the orientation of the ligand during sampling. Direct approaches can be further divided into protein-based, mapping-based, and ligandbased approaches to reflect the source used to derive the features capturing the chemical information inside the protein cavity. Within each category, a representative list of docking approaches is discussed. In view of the limitations of current scoring functions, it was generally found that making optimal use of chemical information represents an efficient knowledge-based strategy for improving binding affinity estimations, ligand binding-mode predictions, and virtual screening enrichments obtained from protein-ligand docking."
80,"neuroethics the practical and the philosophical",76440,"Neuroethics: the practical and the philosophical"," In comparison with the ethical issues surrounding molecular genetics, there has been little public awareness of the ethical implications of neuroscience. Yet recent progress in cognitive neuroscience raises a host of ethical issues of at least comparable importance. Some are of a practical nature, concerning the applications of neurotechnology and their likely implications for individuals and society. Others are more philosophical, concerning the way we think about ourselves as persons, moral agents and spiritual beings. This article reviews key examples of each type of issue, including the relevant advances in science and technology and their accompanying social and philosophical problems."
81,"simultaneous determination of protein structure and dynamics",76468,"Simultaneous determination of protein structure and dynamics","We present a protocol for the experimental determination of ensembles of protein conformations that represent simultaneously the native structure and its associated dynamics. The procedure combines the strengths of nuclear magnetic resonance spectroscopy—for obtaining experimental information at the atomic level about the structural and dynamical features of proteins—with the ability of molecular dynamics simulations to explore a wide range of protein conformations. We illustrate the method for human ubiquitin in solution and find that there is considerable conformational heterogeneity throughout the protein structure. The interior atoms of the protein are tightly packed in each individual conformation that contributes to the ensemble but their overall behaviour can be described as having a significant degree of liquid-like character. The protocol is completely general and should lead to significant advances in our ability to understand and utilize the structures of native proteins."
82,"the location stack a layered model for location in ubiquitous computing",78006,"The location stack: A layered model for location in ubiquitous computing","Based on five design principles extracted from a survey of location systems, we present the Location Stack, a layered software engineering model for location in ubiquitous computing. Our model is similar in spirit to the seven-layer Open System Interconnect (OSI) model for computer networks. We map two existing ubiquitous computing systems to the model to illustrate the leverage the Location Stack provides. By encouraging system designers to think of their applications in this way, we hope to drive location-based computing toward a common vocabulary and standard infrastructure, permitting members of the ubiquitous computing community to easily evaluate and build on each other’s work."
83,"towards flexible teamwork",79158,"Towards Flexible Teamwork","Many AI researchers are today striving to build agent teams for complex, dynamic multi-agent domains, with intended applications in arenas such as education, training, entertainment, information integration, and collective robotics. Unfortunately, uncertainties in these complex, dynamic domains obstruct coherent teamwork. In particular, team members often encounter differing, incomplete, and possibly inconsistent views of their environment. Furthermore, team members can unexpectedly fail in..."
84,"integrating ethics and science in the international hapmap project",79713,"Integrating ethics and science in the International HapMap Project.","Genomics resources that use samples from identified populations raise scientific, social and ethical issues that are, in many ways, inextricably linked. Scientific decisions about which populations to sample to produce the HapMap, an international genetic variation resource, have raised questions about the relationships between the social identities used to recruit participants and the biological findings of studies that will use the HapMap. The sometimes problematic implications of those complex relationships have led to questions about how to conduct genetic variation research that uses identified populations in an ethical way, including how to involve members of a population in evaluating the risks and benefits posed for everyone who shares that identity. The ways in which these issues are linked is increasingly drawing the scientific and ethical spheres of genomics research closer together."
85,"evolutionary dynamics on graphs",80418,"Evolutionary dynamics on graphs","Evolutionary dynamics have been traditionally studied in the context of homogeneous or spatially extended populations1, 2, 3, 4. Here we generalize population structure by arranging individuals on a graph. Each vertex represents an individual. The weighted edges denote reproductive rates which govern how often individuals place offspring into adjacent vertices. The homogeneous population, described by the Moran process3, is the special case of a fully connected graph with evenly weighted edges. Spatial structures are described by graphs where vertices are connected with their nearest neighbours. We also explore evolution on random and scale-free networks5, 6, 7. We determine the fixation probability of mutants, and characterize those graphs for which fixation behaviour is identical to that of a homogeneous population7. Furthermore, some graphs act as suppressors and others as amplifiers of selection. It is even possible to find graphs that guarantee the fixation of any advantageous mutant. We also study frequency-dependent selection and show that the outcome of evolutionary games can depend entirely on the structure of the underlying graph. Evolutionary graph theory has many fascinating applications ranging from ecology to multi-cellular organization and economics."
86,"assortative mixing in networks",81500,"Assortative mixing in networks.","A network is said to show assortative mixing if the nodes in the network that have many connections tend to be connected to other nodes with many connections. We define a measure of assortative mixing for networks and use it to show that social networks are often assortatively mixed, but that technological and biological networks tend to be disassortative. We propose a model of an assortative network, which we study both analytically and numerically. Within the framework of this model we find that assortative networks tend to percolate more easily than their disassortative counterparts and that they are also more robust to vertex removal."
87,"arrayexpressa public repository for microarray gene expression data at the ebi",82207,"ArrayExpress--a public repository for microarray gene expression data at the EBI","ArrayExpress is a new public database of microarray gene expression data at the EBI, which is a generic gene expression database designed to hold data from all microarray platforms. ArrayExpress uses the annotation standard Minimum Information About a Microarray Experiment (MIAME) and the associated XML data exchange format Microarray Gene Expression Markup Language (MAGE-ML) and it is designed to store well annotated data in a structured way. The ArrayExpress infrastructure consists of the database itself, data submissions in MAGE-ML format or via an online submission tool MIAMExpress, online database query interface, and the Expression Profiler online analysis tool. ArrayExpress accepts three types of submission, arrays, experiments and protocols, each of these is assigned an accession number. Help on data submission and annotation is provided by the curation team. The database can be queried on parameters such as author, laboratory, organism, experiment or array types. With an increasing number of organisations adopting MAGE-ML standard, the volume of submissions to ArrayExpress is increasing rapidly. The database can be accessed at http://www.ebi.ac.uk/arrayexpress."
88,"online mendelian inheritance in man omim a knowledgebase of human genes and genetic disorders",82216,"Online Mendelian Inheritance in Man (OMIM), a knowledgebase of human genes and genetic disorders.","Online Mendelian Inheritance in Man (OMIM) is a comprehensive, authoritative and timely knowledgebase of human genes and genetic disorders compiled to support research and education in human genomics and the practice of clinical genetics. Started by Dr Victor A. McKusick as the definitive reference Mendelian Inheritance in Man, OMIM (www.ncbi.nlm.nih.gov/omim) is now distributed electronically by the National Center for Biotechnology Information (NCBI), where it is integrated with the Entrez suite of databases. Derived from the biomedical literature, OMIM is written and edited at Johns Hopkins University with input from scientists and physicians around the world. Each OMIM entry has a full-text summary of a genetically determined phenotype and/or gene and has numerous links to other genetic databases such as DNA and protein sequence, PubMed references, general and locus-specific mutation databases, approved gene nomenclature, and the highly detailed mapviewer, as well as patient support groups and many others. OMIM is an easy and straightforward portal to the burgeoning information in human genetics."
89,"emsd an integrated data resource for bioinformatics",82275,"E-MSD: an integrated data resource for bioinformatics","The Macromolecular Structure Database (MSD) group (http://www.ebi.ac.uk/msd/) continues to enhance the quality and consistency of macromolecular structure data in the worldwide Protein Data Bank (wwPDB) and to work towards the integration of various bioinformatics data resources. One of the major obstacles to the improved integration of structural databases such as MSD and sequence databases like UniProt is the absence of up to date and well-maintained mapping between corresponding entries. We have worked closely with the UniProt group at the EBI to clean up the taxonomy and sequence cross-reference information in the MSD and UniProt databases. This information is vital for the reliable integration of the sequence family databases such as Pfam and Interpro with the structure-oriented databases of SCOP and CATH. This information has been made available to the eFamily group (http://www.efamily.org.uk/) and now forms the basis of the regular interchange of information between the member databases (MSD, UniProt, Pfam, Interpro, SCOP and CATH). This exchange of annotation information has enriched the structural information in the MSD database with annotation from wider sequence-oriented resources. This work was carried out under the  Structure Integration with Function, Taxonomy and Sequences (SIFTS)' initiative (http://www.ebi.ac.uk/msd-srv/docs/sifts) in the MSD group. 10.1093/nar/gki058"
90,"rabbit a compiler for scheme",82938,"Rabbit: A Compiler for Scheme","We have developed a compiler for the lexically-scoped dialect of LISP known as SCHEME. The compiler knows relatively little about specific data manipulation primitives such as arithmetic operators, but concentrates on general issues of environment and control. Rather than having specialized knowledge about a large variety of control and environment constructs, the compiler handles only a small basis set which reflects the semantics of lambda-calculus. All of the traditional imperative constructs, such as sequencing, assignment, looping, GO TO, as well as many standard LISP constructs such as AND, OR and COND, are expressed as macros in terms of the applicative basis set. A small number of optimization techniques, coupled with the treatment of function calls as GO TO statements, serves to produce code as good as that produced by more traditional compilers."
91,"data mining approaches for intrusion detection",83492,"Data mining approaches for intrusion detection","In this paper we discuss our research in developing general and systematic methods for intrusion detection. The key ideas are to use data mining techniques to discover consistent and useful patterns of system features that describe program and user behavior, and use the set of relevant system features to compute (inductively learned) classifiers that can recognize anomalies and known intrusions. Using experiments on the sendmail system call data and the network tcpdump data, we demonstrate that ..."
92,"learning the parts of objects by nonnegative matrix factorization",83540,"Learning the parts of objects by non-negative matrix factorization.","Is perception of the whole based on perception of its parts? There is psychological1 and physiological2, 3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4, 5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign."
93,"humancomputer interaction psychology as a science of design",85519,"Human-Computer Interaction: Psychology as a Science of Design","Human-computer interaction (HCI) is the area of intersection between psychology and the social sciences, on the one hand, and computer science and technology, on the other. HCI researchers analyse and design-specific user-interface technologies (e.g. three-dimensional pointing devices, interactive video). They study and improve the processes of technology development (e.g. usability evaluation, design rationale). They develop and evaluate new applications of technology (e.g. computer conferencing, software design environments). Through the past two decades, HCI has progressively integrated its scientific concerns with the engineering goal of improving the usability of computer systems and applications, thus establishing a body of technical knowledge and methodology. HCI continues to provide a challenging test domain for applying and developing psychology and social science in the context of technology development and use."
94,"glide a new approach for rapid accurate docking and scoring method and assessment of docking accuracy",86440,"Glide:  A New Approach for Rapid, Accurate Docking and Scoring. 1. Method and Assessment of Docking Accuracy","PMID: 15027865 Unlike other methods for docking ligands to the rigid 3D structure of a known protein receptor, Glide approximates a complete systematic search of the conformational, orientational, and positional space of the docked ligand. In this search, an initial rough positioning and scoring phase that dramatically narrows the search space is followed by torsionally flexible energy optimization on an OPLS-AA nonbonded potential grid for a few hundred surviving candidate poses. The very best candidates are further refined via a Monte Carlo sampling of pose conformation; in some cases, this is crucial to obtaining an accurate docked pose. Selection of the best docked pose uses a model energy function that combines empirical and force-field-based terms. Docking accuracy is assessed by redocking ligands from 282 cocrystallized PDB complexes starting from conformationally optimized ligand geometries that bear no memory of the correctly docked pose. Errors in geometry for the top-ranked pose are less than 1 Å in nearly half of the cases and are greater than 2 Å in only about one-third of them. Comparisons to published data on rms deviations show that Glide is nearly twice as accurate as GOLD and more than twice as accurate as FlexX for ligands having up to 20 rotatable bonds. Glide is also found to be more accurate than the recently described Surflex method."
95,"early aspects a model for aspectoriented requirements engineering",86776,"Early Aspects: A Model for Aspect-Oriented Requirements Engineering","Effective RE must reconcile the need to achieve separation of concerns with the need to satisfy broadly scoped requirements and constraints. Techniques such as use cases and viewpoints help achieve separation of stakeholders' concerns but ensuring their consistency with global requirements and constraints is largely unsupported. We build on recent work that has emerged from the aspect-oriented programming (AOP) community to propose a general model for aspect oriented requirements engineering (AORE). The model supports separation of crosscutting functional and non-functional properties at the requirements level. We argue that early separation of such crosscutting properties supports effective determination of their mapping and influence on artefacts at later development stages. A realisation of the model based on a case study of a toll collection system is presented."
96,"a neuroeconomics approach to inferring utility functions in sensorimotor control",87024,"A neuroeconomics approach to inferring utility functions in sensorimotor control.","Making choices is a fundamental aspect of human life. For over a century experimental economists have characterized the decisions people make based on the concept of a utility function. This function increases with increasing desirability of the outcome, and people are assumed to make decisions so as to maximize utility. When utility depends on several variables, indifference curves arise that represent outcomes with identical utility that are therefore equally desirable. Whereas in economics utility is studied in terms of goods and services, the sensorimotor system may also have utility functions defining the desirability of various outcomes. Here, we investigate the indifference curves when subjects experience forces of varying magnitude and duration. Using a two-alternative forced-choice paradigm, in which subjects chose between different magnitude–duration profiles, we inferred the indifference curves and the utility function. Such a utility function defines, for example, whether subjects prefer to lift a 4-kg weight for 30 s or a 1-kg weight for a minute. The measured utility function depends nonlinearly on the force magnitude and duration and was remarkably conserved across subjects. This suggests that the utility function, a central concept in economics, may be applicable to the study of sensorimotor control."
97,"position specific variation in the rate of evolution in transcription factor binding sites",87035,"Position specific variation in the rate of evolution in transcription factor binding sites","BACKGROUND: The binding sites of sequence specific transcription factors are an important and relatively well-understood class of functional non-coding DNAs. Although a wide variety of experimental and computational methods have been developed to characterize transcription factor binding sites, they remain difficult to identify. Comparison of non-coding DNA from related species has shown considerable promise in identifying these functional non-coding sequences, even though relatively little is known about their evolution. RESULTS: Here we analyse the genome sequences of the budding yeasts Saccharomyces cerevisiae, S. bayanus, S. paradoxus and S. mikatae to study the evolution of transcription factor binding sites. As expected, we find that both experimentally characterized and computationally predicted binding sites evolve slower than surrounding sequence, consistent with the hypothesis that they are under purifying selection. We also observe position-specific variation in the rate of evolution within binding sites. We find that the position-specific rate of evolution is positively correlated with degeneracy among binding sites within S. cerevisiae. We test theoretical predictions for the rate of evolution at positions where the base frequencies deviate from background due to purifying selection and find reasonable agreement with the observed rates of evolution. Finally, we show how the evolutionary characteristics of real binding motifs can be used to distinguish them from artefacts of computational motif finding algorithms. CONCLUSION: As has been observed for protein sequences, the rate of evolution in transcription factor binding sites varies with position, suggesting that some regions are under stronger functional constraint than others. This variation likely reflects the varying importance of different positions in the formation of the protein-DNA complex. The characterization of the pattern of evolution in known binding sites will likely contribute to the effective use of comparative sequence data in the identification of transcription factor binding sites and is an important step toward understanding the evolution of functional non-coding DNA."
98,"dynamics of the hippocampal ensemble code for space",90414,"Dynamics of the hippocampal ensemble code for space","Ensemble recordings of 73 to 148 rat hippocampal neurons were used to predict accurately the animals' movement through their environment, which confirms that the hippocampus transmits an ensemble code for location. In a novel space, the ensemble code was initially less robust but improved rapidly with exploration. During this period, the activity of many inhibitory cells was suppressed, which suggests that new spatial information creates conditions in the hippocampal circuitry that are conducive to the synaptic modification presumed to be involved in learning. Development of a new population code for a novel environment did not substantially alter the code for a familiar one, which suggests that the interference between the two spatial representations was very small. The parallel recording methods outlined here make possible the study of the dynamics of neuronal interactions during unique behavioral events."
99,"reactivation of hippocampal ensemble memories during sleep",90415,"Reactivation of hippocampal ensemble memories during sleep.","Simultaneous recordings were made from large ensembles of hippocampal ""place cells"" in three rats during spatial behavioral tasks and in slow-wave sleep preceding and following these behaviors. Cells that fired together when the animal occupied particular locations in the environment exhibited an increased tendency to fire together during subsequent sleep, in comparison to sleep episodes preceding the behavioral tasks. Cells that were inactive during behavior, or that were active but had non-overlapping spatial firing, did not show this increase. This effect, which declined gradually during each post-behavior sleep session, may result from synaptic modification during waking experience. Information acquired during active behavior is thus re-expressed in hippocampal circuits during sleep, as postulated by some theories of memory consolidation."
100,"temporal structure in neuronal activity during working memory in macaque parietal cortex",90416,"Temporal structure in neuronal activity during working memory in macaque parietal cortex.","A number of cortical structures are reported to have elevated single unit firing rates sustained throughout the memory period of a working memory task. How the nervous system forms and maintains these memories is unknown but reverberating neuronal network activity is thought to be important. We studied the temporal structure of single unit (SU) activity and simultaneously recorded local field potential (LFP) activity from area LIP in the inferior parietal lobe of two awake macaques during a memory-saccade task. Using multitaper techniques for spectral analysis, which play an important role in obtaining the present results, we find elevations in spectral power in a 50--90 Hz (gamma) frequency band during the memory period in both SU and LFP activity. The activity is tuned to the direction of the saccade providing evidence for temporal structure that codes for movement plans during working memory. We also find SU and LFP activity are coherent during the memory period in the 50--90 Hz gamma band and no consistent relation is present during simple fixation. Finally, we find organized LFP activity in a 15--25 Hz frequency band that may be related to movement execution and preparatory aspects of the task. Neuronal activity could be used to control a neural prosthesis but SU activity can be hard to isolate with cortical implants. As the LFP is easier to acquire than SU activity, our finding of rich temporal structure in LFP activity related to movement planning and execution may accelerate the development of this medical application."
101,"inference and computation with population codes",90466,"Inference and computation with population codes.","In the vertebrate nervous system, sensory stimuli are typically encoded through the concerted activity of large populations of neurons. Classically, these patterns of activity have been treated as encoding the value of the stimulus (e.g., the orientation of a contour), and computation has been formalized in terms of function approximation. More recently, there have been several suggestions that neural computation is akin to a Bayesian inference process, with population activity patterns representing uncertainty about stimuli in the form of probability distributions (e.g., the probability density function over the orientation of a contour). This paper reviews both approaches, with a particular emphasis on the latter, which we see as a very promising framework for future modeling and experimental work."
102,"bbc news",92730," BBC News "," Africa   BBC Wap use flourishing in Africa BBC Wap use flourishing in Africa  Mobile use in Africa is leapfrogging PC technology Africa, in particular Nigeria, is dominating international mobile phone access to the BBC's website. According to July's statistics, 61% of the BBC's international Wap users came from Nigeria and 19% from South Africa.  ""Wap is the one platform where African countries continue to appear in the top five in our statistics,"" said BBC developer Gareth Owen.  Africa is the world's largest-growing mobile phone market with unreliable landlines encouraging the growth.  Wap technology - which stands for wireless application protocol - allows people to access basic information on the internet, like news summaries, through their mobile phone handset.   I'm in Uganda and the only access I have 2 the outside world is this pinhole 2 info Ugandan texter to the BBC  Do you need a computer? Ringing in changes in Nigeria  According to the BBC's statistics, page views for Wap usage are growing at 100% year on year.  UK users account for 65% of Wap traffic; and international usage for 35%. Mobile phone providers in many African countries have only recently begun rolling out Wap-enabled handsets.  And the large take up of BBC news via mobiles in Nigeria contrasts starkly with the relatively small number of users accessing the internet via pcs - hampered by slow and unreliable landlines.  The BBC's Technology correspondent Mark Ward says that in many places on the continent PC ownership is low but PC literacy surprisingly high.  Internet cafes tend to be very popular, as much a meeting place as well as a place where people access their email, he says.    The BBC receives regular messages of thanks from people in Africa, who say the only access they have to news is via their mobiles.  ""I'm in Uganda and the only access I have 2 the outside world is this pinhole 2 info cause I don't have access to TV. Thanx,"" said one texter from Uganda. The country accounted for 7% of BBC Wap usage in July.  Other top countries helping account for the 58m Wap page views in July were Jamaica, Singapore and Israel.  In the UK, the BBC has about a 20% share of the market with a reach of 1.2m users monthly."
103,"the parttime parliament",92789,"The part-time parliament","Digital Equipment Corporation Recent archaeological discoveries on the island of Paxos reveal that the parliament functioned despite the peripatetic propensity of its part-time legislators. The legislators maintained consistent copies of the parliamentary record, despite their frequent forays from the chamber and the forgetfulness of their messengers. The Paxon parliament’s protocol provides a new way of implementing the state-machine approach to the design of distributed systems."
104,"how to build a highly available system using consensus",92797,"How to Build a Highly Available System Using Consensus","Abstract. Lamport showed that a replicated deterministic state machine is a general way to implement a highly available system, given a consensus algorithm that the replicas can use to agree on each input. His Paxos algorithm is the most fault-tolerant way to get consensus without real-time guarantees. Because general consensus is expensive, practical systems reserve it for emergencies and use leases (locks that time out) for most of the computing. This paper explains the general scheme for efficient highly available computing, gives a general method for understanding concurrent and fault-tolerant programs, and derives the Paxos algorithm as an example of the method. 1"
105,"indivisible labor and the business cycle",93116,"Indivisible labor and the business cycle","A growth model with shocks to technology is studied. Labor is indivisible, so all variability in hours worked is due to fluctuations in the number employed. We find that, unlike previous equilibrium models of the business cycle, this economy displays large fluctuations in hours worked and relatively small fluctuations in productivity. This finding is independent of individuals' willingness to substitute leisure across time. This and other findings are the result of studying and comparing summary statistics describing this economy, an economy with divisible labor, and post-war U.S. time series."
106,"the bargaining problem",93163,"The Bargaining Problem","A new treatment is presented of a classical economic problem, one which occurs in many forms, as bargaining, bilateral monopoly, etc. It may also be regarded as a nonzero-sum two-person game. In this treatment a few general assumptions are made concerning the behavior of a single individual and of a group of two individuals in certain economic environments. From these, the solution (in the sense of this paper) of the classical problem may be obtained. In the terms of game theory, values are found for the game."
107,"perfect equilibrium in a bargaining model",93166,"Perfect Equilibrium in a Bargaining Model","Two players have to reach an agreement on the partition of a pie of size 1. Each has to make in turn, a proposal as to how it should be divided. After one player has made an offer, the other must decide either to accept it, or to reject it and continue the bargaining. Several properties which the players' preferences possess are assumed. The Perfect Equilibrium Partitions (P.E.P.) are characterized in all the modesls satisfying these assumptions. Specially, it is proved that when every player bears a fixed bargaining cost for each period (c""1 and c""2), then: (i) if c""1 < c""2 the only P.E.P. gives all the pie to 1; (ii) if c""1 > c""2 the only P.E.P. gives to 1; only c""2. In the case where each player has a fixed discounting factor (@d""1 and @d""2) the only P.E.P. is (1 - @d""2)/(1 - @d""1@d""2)."
108,"highly conserved noncoding sequences are associated with vertebrate development",93513,"Highly Conserved Non-Coding Sequences Are Associated with Vertebrate Development","In addition to protein coding sequence, the human genome contains a significant amount of regulatory DNA, the identification of which is proving somewhat recalcitrant to both in silico and functional methods. An approach that has been used with some success is comparative sequence analysis, whereby equivalent genomic regions from different organisms are compared in order to identify both similarities and differences. In general, similarities in sequence between highly divergent organisms imply functional constraint. We have used a whole-genome comparison between humans and the pufferfish, Fugu rubripes, to identify nearly 1,400 highly conserved non-coding sequences. Given the evolutionary divergence between these species, it is likely that these sequences are found in, and furthermore are essential to, all vertebrates. Most, and possibly all, of these sequences are located in and around genes that act as developmental regulators. Some of these sequences are over 90&#37; identical across more than 500 bases, being more highly conserved than coding sequence between these two species. Despite this, we cannot find any similar sequences in invertebrate genomes. In order to begin to functionally test this set of sequences, we have used a rapid in vivo assay system using zebrafish embryos that allows tissue-specific enhancer activity to be identified. Functional data is presented for highly conserved non-coding sequences associated with four unrelated developmental regulators (SOX21, PAX6, HLXB9, and SHH), in order to demonstrate the suitability of this screen to a wide range of genes and expression patterns. Of 25 sequence elements tested around these four genes, 23 show significant enhancer activity in one or more tissues. We have identified a set of non-coding sequences that are highly conserved throughout vertebrates. They are found in clusters across the human genome, principally around genes that are implicated in the regulation of development, including many transcription factors. These highly conserved non-coding sequences are likely to form part of the genomic circuitry that uniquely defines vertebrate development."
109,"gender identity and language use in teenage blogs",94299,"Gender, Identity, and Language Use in Teenage Blogs","This study examines issues of online identity and language use among male and female teenagers who created and maintained weblogs, personal journals made publicly accessible on the World Wide Web. Online identity and language use were examined in terms of the disclosure of personal information, sexual identity, emotive features, and semantic themes. Male and female teenagers presented themselves similarly in their blogs, often revealing personal information such as their real names, ages, and locations. Males more so than females used emoticons, employed an active and resolute style of language, and were more likely to present themselves as gay. The results suggest that teenagers stay closer to reality in their online expressions of self than has previously been suggested, and that these explorations involve issues, such as learning about their sexuality, that commonly occur during the adolescent years."
110,"clustal w improving the sensitivity of progressive multiple sequence alignment through sequence weighting positionspecific gap penalties and weight matrix choice",94348,"CLUSTAL W: improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-specific gap penalties and weight matrix choice.","The sensitivity of the commonly used progressive multiple sequence alignment method has been greatly improved for the alignment of divergent protein sequences. Firstly, individual weights are assigned to each sequence in a partial alignment in order to down-weight near-duplicate sequences and up-weight the most divergent ones. Secondly, amino acid substitution matrices are varied at different alignment stages according to the divergence of the sequences to be aligned. Thirdly, residue-specific gap penalties and locally reduced gap penalties in hydrophilic regions encourage new gaps in potential loop regions rather than regular secondary structure. Fourthly, positions in early alignments where gaps have been opened receive locally reduced gap penalties to encourage the opening up of new gaps at these positions. These modifications are incorporated into a new program, CLUSTAL W which is freely available."
111,"the prosite database its status in",94461,"The PROSITE database, its status in 2002.","PROSITE [Bairoch and Bucher (1994) Nucleic Acids Res., 22, 3583-3589; Hofmann et al. (1999) Nucleic Acids Res., 27, 215-219] is a method of identifying the functions of uncharacterized proteins translated from genomic or cDNA sequences. The PROSITE database (http://www.expasy.org/prosite/) consists of biologically significant patterns and profiles designed in such a way that with appropriate computational tools it can rapidly and reliably help to determine to which known family of proteins (if any) a new sequence belongs, or which known domain(s) it contains."
112,"microbial gene identification using interpolated markov models",94463,"Microbial gene identification using interpolated Markov models","This paper describes a new system, {GLIMMER}, for finding genes in microbial genomes. {I}n a series of tests on {H}aemophilus influenzae , {H}elicobacter pylori and other complete microbial genomes, this system has proven to be very accurate at locating virtually all the genes in these sequences, outperforming previous methods. {A} conservative estimate based on experiments on {H}.pylori and {H}. influenzae is that the system finds >97% of all genes. {GLIMMER} uses interpolated {M}arkov models ({IMM}s) as a framework for capturing dependencies between nearby nucleotides in a {DNA} sequence. {A}n {IMM}-based method makes predictions based on a variable context; i.e., a variable-length oligomer in a {DNA} sequence. {T}he context used by {GLIMMER} changes depending on the local composition of the sequence. {A}s a result, {GLIMMER} is more flexible and more powerful than fixed-order {M}arkov methods, which have previously been the primary content-based technique for finding genes in microbial {DNA}."
113,"on the minimal synchronism needed for distributed consensus",94493,"On the minimal synchronism needed for distributed consensus","Reaching agreement is a primitive of distributed computing. Whereas this poses no problem in an ideal, failure-free environment, it imposes certain constraints on the capabilities of an actual system: A system is viable only if it permits the existence of consensus protocols tolerant to some number of failures. Fischer et al. have shown that in a completely asynchronous model, even one failure cannot be tolerated. In this paper their work is extended: Several critical system parameters, including various synchrony conditions, are identified and how varying these affects the number of faults that can be tolerated is examined. The proofs expose general heuristic principles that explain why consensus is possible in certain models but not possible in others."
114,"the weakest failure detector for solving consensus",94495,"The weakest failure detector for solving consensus","Abstract. We determine what information about failures is necessary and sufficient to solve Consensus in asynchronous distributed systems subject to crash failures. In Chandra and Toueg [1996], it is shown that eventually weak , a failure detector that provides surprisingly little information about which processes have crashed, is sufficient to solve Consensus in asynchronous systems with a majority of correct processes. In this paper, we prove that to solve Consensus, any failure detector has to provide at least as much information as eventual weak . Thus, eventual weak is indeed the weakest failure detector for solving Consensus in asynchronous systems with a majority of correct processes."
115,"social dilemmas the anatomy of cooperation",95902,"Social Dilemmas: The Anatomy of Cooperation","The study of social dilemmas is the study of the tension between individual and collective rationality. In a social dilemma, individually reasonable behavior leads to a situation in which everyone is worse off. The first part of this review is a discussion of categories of social dilemmas and how they are modeled. The key two-person social dilemmas (PrisonerÃ­s Dilemma, Assurance, Chicken) and multiple-person social dilemmas (public goods dilemmas and commons dilemmas) are examined. The second part is an extended treatment of possible solutions for social dilemmas. These solutions are organized into three broad categories based on whether the solutions assume egoistic actors and whether the structure of the situation can be changed: Motivational solutions assume actors are not completely egoistic and so give some weight to the outcomes of their partners. Strategic solutions assume egoistic actors, and neither of these categories of solutions involve changing the fundamental structure of the situation. Solutions that do involve changing the rules of the game are considered in the section on structural solutions. I conclude the review with a discussion of current research and directions for future work."
116,"npcomplete problems and physical reality",95939,"NP-complete Problems and Physical Reality","Can NP-complete problems be solved efficiently in the physical universe? I survey proposals including soap bubbles, protein folding, quantum computing, quantum advice, quantum adiabatic algorithms, quantum-mechanical nonlinearities, hidden variables, relativistic time dilation, analog computing, Malament-Hogarth spacetimes, quantum gravity, closed timelike curves, and “anthropic computing. ” The section on soap bubbles even includes some “experimental ” results. While I do not believe that any of the proposals will let us solve NP-complete problems efficiently, I argue that by studying them, we can learn something not only about computation but also about physics. 1"
117,"local graph alignment and motif search in biological networks",96165,"Local graph alignment and motif search in biological networks.","Interaction networks are of central importance in postgenomic molecular biology, with increasing amounts of data becoming available by high-throughput methods. Examples are gene regulatory networks or protein interaction maps. The main challenge in the analysis of these data is to read off biological functions from the topology of the network. Topological motifs, i.e., patterns occurring repeatedly at different positions in the network, have recently been identified as basic modules of molecular information processing. In this article, we discuss motifs derived from families of mutually similar but not necessarily identical patterns. We establish a statistical model for the occurrence of such motifs, from which we derive a scoring function for their statistical significance. Based on this scoring function, we develop a search algorithm for topological motifs called graph alignment, a procedure with some analogies to sequence alignment. The algorithm is applied to the gene regulation network of Escherichia coli."
118,"a translation approach to portable ontology specifications",96396,"A Translation Approach to Portable Ontology Specifications","To support the sharing and reuse of formally represented knowledge among AI systems, it is useful to define the common vocabulary in which shared knowledge is represented. A specification of a representational vocabulary for a shared domain of discourse––definitions of classes, relations, functions, and other objects––is called an ontology. This paper describes a mechanism for defining ontologies that are portable over representation systems. Definitions written in a standard format for predicate calculus are translated by a system called Ontolingua into specialized representations, including frame-based systems as well as relational languages. This allows researchers to share and reuse ontologies, while retaining the computational benefits of specialized implementations. We discuss how the translation approach to portability addresses several technical problems. One problem is how to accommodate the stylistic and organizational differences among representations while preserving declarative content. Another is how to translate from a very expressive language into restricted languages, remaining system-independent while preserving the computational efficiency of implemented systems. We describe how these problems are addressed by basing Ontolingua itself on an ontology of domain-independent, representational idioms."
119,"ourgrid an approach to easily assemble grids with equitable resource sharing",96402,"OurGrid: An approach to easily assemble grids with equitable resource sharing","Available grid technologies like the Globus Toolkit make possible for one to run a parallel application on resources distributed across several administrative domains. Most grid computing users, however, don’t have access to more than a handful of resources onto which they can use this technologies. This happens mainly because gaining access to resources still depends on personal negotiations between the user and each resource owner of resources. To address this problem, we are developing the OurGrid resources sharing system, a peer-to-peer network of sites that share resources equitably in order to form a grid to which they all have access. The resources are shared accordingly to a network of favors model, in which each peer prioritizes those who have credit in their past history of bilateral interactions. The emergent behavior in the system is that peers that contribute more to the community are prioritized when they request resources. We expect, with OurGrid, to solve the access gaining problem for users of bag-of-tasks applications (those parallel applications whose tasks are independent)."
120,"decoding by linear programming",97088,"Decoding by Linear Programming","This paper considers a natural error correcting problem with real valued input/output. We wish to recover an input vector f∈R n  from corrupted measurements y=Af+e. Here, A is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. Is it possible to recover f exactly from the data y? We prove that under suitable conditions on the coding matrix A, the input f is the unique solution to the ℓ 1 -minimization problem (  x   ℓ1 :=Σ i  x i  ) min(g∈R n )   y - Ag   ℓ1  provided that the support of the vector of errors is not too large,   e   ℓ0 := {i:e i  ≠ 0} ≤ρ·m for some ρ>0. In short, f can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program). In addition, numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted. This work is related to the problem of finding sparse solutions to vastly underdetermined systems of linear equations. There are also significant connections with the problem of recovering signals from highly incomplete measurements. In fact, the results introduced in this paper improve on our earlier work. Finally, underlying the success of ℓ 1  is a crucial property we call the uniform uncertainty principle that we shall describe in detail."
121,"culture and the self implications for cognition emotion and motivation",99601,"Culture and the Self: Implications for Cognition, Emotion, and Motivation","People in different cultures have strikingly different construals of the self, of others, and of the interdependence of the 2.  These construals can influence, and in many cases determine, the very nature of individual experience, including cognition, emotion, and motivation.  Many Asian cultures have distinct conceptions of individuality that insist on the fundamental relatedness of individuals to each other.  The emphasis is on attending to others, fitting in, and harmonious interdependence with them.  American culture neither assumes nor values such an overt connectedness among individuals.  In contrast, individuals seek to maintain their independence from others by attending to the self and by discovering and expressing their unique inner attributes.  As proposed herein, these construals are even more powerful than previously imagined.  Theories of the self from both psychology and anthropology are integrated to define in detail the difference between a construal of the self as independent and a construal of the self as interdependent.  Each of these divergent construals should have a set of specific consequences for cognition, emotion, and motivation; these consequences are proposed and relevant empirical literature is reviewed.  Focusing on differences in self-construals enables apparently inconsistent empirical findings to be reconciled, and raises questions about what have been thought to be culture-free aspects of cognition, emotion, and motivation."
122,"testing intrusion detection systems a critique of the and darpa intrusion detection system evaluations as performed by lincoln laboratory",99849,"Testing Intrusion detection systems: a critique of the 1998 and 1999 DARPA intrusion detection system evaluations as performed by Lincoln Laboratory","In 1998 and again in 1999, the Lincoln Laboratory of MIT conducted a comparative evaluation of intrusion detection systems (IDSs) developed under DARPA funding. While this evaluation represents a significant and monumental undertaking, there are a number of issues associated with its design and execution that remain unsettled. Some methodologies used in the evaluation are questionable and may have biased its results. One problem is that the evaluators have published relatively little concerning some of the more critical aspects of their work, such as validation of their test data. The appropriateness of the evaluation techniques used needs further investigation. The purpose of this article is to attempt to identify the shortcomings of the Lincoln Lab effort in the hope that future  efforts of this kind will be placed on a sounder footing. Some of the problems that the article points out might well be resolved if the evaluators were to publish a detailed description of their procedures and the rationale that led to their adoption, but other problems would clearly remain./par>"
123,"a point process framework for relating neural spiking activity to spiking history neural ensemble and extrinsic covariate effects",99888,"A Point Process Framework for Relating Neural Spiking Activity to Spiking History, Neural Ensemble, and Extrinsic Covariate Effects","Multiple factors simultaneously affect the spiking activity of individual neurons. Determining the effects and relative importance of these factors is a challenging problem in neurophysiology. We propose a statistical framework based on the point process likelihood function to relate a neuron's spiking probability to three typical covariates: the neuron's own spiking history, concurrent ensemble activity, and extrinsic covariates such as stimuli or behavior. The framework uses parametric models of the conditional intensity function to define a neuron's spiking probability in terms of the covariates. The discrete time likelihood function for point processes is used to carry out model fitting and model analysis. We show that, by modeling the logarithm of the conditional intensity function as a linear combination of functions of the covariates, the discrete time point process likelihood function is readily analyzed in the generalized linear model (GLM) framework. We illustrate our approach for both GLM and non-GLM likelihood functions using simulated data and multivariate single-unit activity data simultaneously recorded from the motor cortex of a monkey performing a visuomotor pursuit-tracking task. The point process framework provides a flexible, computationally efficient approach for maximum likelihood estimation, goodness-of-fit assessment, residual analysis, model selection, and neural decoding. The framework thus allows for the formulation and analysis of point process models of neural spiking activity that readily capture the simultaneous effects of multiple covariates and enables the assessment of their relative importance. 10.1152/jn.00697.2004"
124,"scalefree brain functional networks",99930,"Scale-free brain functional networks","Functional magnetic resonance imaging (fMRI) is used to extract <em> functional networks</em> connecting correlated human brain sites. Analysis of the resulting networks in different tasks shows that: (a) the distribution of functional connections, and the probability of finding a link vs. distance are both scale-free, (b) the characteristic path length is small and comparable with those of equivalent random networks, and (c) the clustering coefficient is orders of magnitude larger than those of equivalent random networks. All these properties, typical of scale-free small world networks, reflect important functional information about brain states."
125,"the semantic web",99931,"The Semantic Web","The entertainment system was belting out the Beatles' ""We Can Work It Out"" when the phone rang. When Pete answered, his phone turned the sound down by sending a message to all the other local devices that had a volume control. His sister, Lucy, was on the line from the doctor's office: ""Mom needs to see a specialist and then has to have a series of physical therapy sessions. Biweekly or something. I'm going to have my agent set up the appointments."" Pete immediately agreed to share the chauffeuring."
126,"judgment under uncertainty heuristics and biases",100015,"Judgment under Uncertainty: Heuristics and Biases","The thirty-five chapters in this book describe various judgmental heuristics and the biases they produce, not only in laboratory experiments but in important social, medical, and political situations as well. Individual chapters discuss the representativeness and availability heuristics, problems in judging covariation and control, overconfidence, multistage inference, social perception, medical diagnosis, risk perception, and methods for correcting and improving judgments under uncertainty. About half of the chapters are edited versions of classic articles; the remaining chapters are newly written for this book. Most review multiple studies or entire subareas of research and application rather than describing single experimental studies. This book will be useful to a wide range of students and researchers, as well as to decision makers seeking to gain insight into their judgments and to improve them."
127,"ownership types for object encapsulation",100050,"Ownership types for object encapsulation","Ownership types provide a statically enforceable way of specifying object encapsulation and enable local reasoning about program correctness in object-oriented languages. However, a type system that enforces strict object encapsulation is too constraining: it does not allow efficient implementation of important constructs like iterators. This paper argues that the right way to solve the problem is to allow objects of classes defined in the same module to have privileged access to each other's representations; we show how to do this for inner classes. This approach allows programmers to express constructs like iterators and yet supports local reasoning about the correctness of the classes, because a class and its inner classes together can be reasoned about as a module. The paper also sketches how we use our variant of ownership types to enable efficient software upgrades in persistent object stores."
128,"how social structure improves distributed reputation systems three hypotheses",100145,"How Social Structure Improves Distributed Reputation Systems - Three Hypotheses","Reputation systems provide an incentive for cooperation in artificial societies by keeping track of the behavior of autonomous entities. The self-organization of P2P systems demands for the distribution of the reputation system to the autonomous entities themselves. They may cooperate by issuing recommendations of other entities’ trustworthiness. The recipient of a recommendation has to assess its truthfulness and consistency before taking it into account. The current assessment methods are based on plausibility considerations that have several inherent limitations. In our previous work, we have suggested the application of non-repudiable tokens that overcome most of the limitations. However, there remain limitations that are not overcome or only partly  overcome. Therefore, in this paper, we propose social structure as a complementary means of overcoming the remaining limitations of plausibility considerations. For this purpose, we examine the properties of social structure and discuss how distributed reputation systems can make use of them. This leads us to the formulation of three hypotheses of how social structure overcomes the limitations of plausibility considerations. The hypotheses are tested by the means of simulation. The simulation results corroborate two hypotheses and indicate the validity of the third hypothesis."
129,"inferring cellular networks using probabilistic graphical models",100166,"Inferring Cellular Networks Using Probabilistic Graphical Models","High-throughput genome-wide molecular assays, which probe cellular networks from different perspectives, have become central to molecular biology. Probabilistic graphical models are useful for extracting meaningful biological insights from the resulting data sets. These models provide a concise representation of complex cellular networks by composing simpler submodels. Procedures based on well-understood principles for inferring such models from data facilitate a model-based methodology for analysis and discovery. This methodology and its capabilities are illustrated by several recent applications to gene expression data. 10.1126/science.1094068"
130,"peertopeer communication across network address translators",100184,"Peer-to-Peer Communication Across Network Address Translators","J’fais des trous, des petits trous... toujours des petits trous- S. Gainsbourg Network Address Translation (NAT) causes well-known difficulties for peer-to-peer (P2P) communication, since the peers involved may not be reachable at any globally valid IP address. Several NAT traversal techniques are known, but their documentation is slim, and data about their robustness or relative merits is slimmer. This paper documents and analyzes one of the simplest but most robust and practical NAT traversal techniques, commonly known as “hole punching. ” Hole punching is moderately well-understood for UDP communication, but we show how it can be reliably used to set up peer-to-peer TCP streams as well. After gathering data on the reliability of this technique on a wide variety of deployed NATs, we find that about 82 % of the NATs tested support hole punching for UDP, and about 64 % support hole punching for TCP streams. As NAT vendors become increasingly conscious of the needs of important P2P applications such as Voice over IP and online gaming protocols, support for hole punching is likely to increase in the future. 1"
131,"unraveling the web services web an introduction to soap wsdl and uddi",100185,"Unraveling the Web services web: an introduction to SOAP, WSDL, and UDDI","Today, Web services are emerging to provide a systematic and extensible framework for application-to-application interaction, built on top of existing Web protocols and based on open XML standards. At this point, Web services technology is still emerging, and researchers are still developing important pieces. Developers can take advantage of the available specifications and tooling now incorporate more modules as the technology matures."
132,"the psychology of personal information management",100219,"The psychology of personal information management","A requirement of 'The Office of the Future' is that it provides us with an effective way of storing and retrieving information. But existing IT products go nowhere near supporting the variety of activities which can be observed in paper-based offices, and it is not surprising that concepts of the 'paperless office' are as far off as they were when the idea was first mooted. This paper illustrates how many of the issues involved in the automation of information management are essentially psychological in nature. These principally devolve upon the processes of recall, recognition and categorisation. Examples of existing information management techniques show how there is a trend to automate with a view to simulating office practices, or to develop according to the availability of technological solutions. Both of these are inefficient with respect to the user's psychological needs. A framework for developing user-oriented information management systems is discussed and relevant research issues presented."
133,"neural coding of basic reward terms of animal learning theory game theory microeconomics and behavioural ecology",100359,"Neural coding of basic reward terms of animal learning theory, game theory, microeconomics and behavioural ecology.","Neurons in a small number of brain structures detect rewards and reward-predicting stimuli and are active during the expectation of predictable food and liquid rewards. These neurons code the reward information according to basic terms of various behavioural theories that seek to explain reward-directed learning, approach behaviour and decision-making. The involved brain structures include groups of dopamine neurons, the striatum including the nucleus accumbens, the orbitofrontal cortex and the amygdala. The reward information is fed to brain structures involved in decision-making and organisation of behaviour, such as the dorsolateral prefrontal cortex and possibly the parietal cortex. The neural coding of basic reward terms derived from formal theories puts the neurophysiological investigation of reward mechanisms on firm conceptual grounds and provides neural correlates for the function of rewards in learning, approach behaviour and decision-making."
134,"ariadne a secure ondemand routing protocol for ad hoc networks",100476,"Ariadne: a secure on-demand routing protocol for ad hoc networks","An ad hoc network is a group of wireless mobile computers (or nodes), in which individual nodes cooperate by forwarding packets for each other to allow nodes to communicate beyond direct wireless transmission range. Prior research in ad hoc networking has generally studied the routing problem in a non-adversarial setting, assuming a trusted environment. In this paper, we present attacks against routing in ad hoc networks, and we present the design and performance evaluation of a new secure on-demand ad hoc network routing protocol, called Ariadne. Ariadne prevents attackers or compromised nodes from tampering with uncompromised routes consisting of uncompromised nodes, and also prevents many types of Denial-of-Service attacks. In addition, Ariadne is efficient, using only highly efficient symmetric cryptographic primitives."
135,"knowledge management in action integrating knowledge across communities",101942,"Knowledge management in action: integrating knowledge across communities","This paper offers a brief overview and critique of dominant approaches to knowledge management (KM) and its links with innovation. It then draws upon a case study example to offer a closer analysis of the link between KM and the development of communities of practice during processes of innovation. The paper argues, first, that in many cases innovation is an interactive process requiring knowledge and expertise from different functions and layers across the organization. In such cases critical problems concern the integration of knowledge across disparate communities, rather than the sharing of knowledge within communities. Second that if knowledge integration across communities is to develop, a more action-oriented perspective on KM and the development of KM tools is needed."
136,"functional cartography of complex metabolic networks",101983,"Functional cartography of complex metabolic networks","High-throughput techniques are leading to an explosive growth in the size of biological databases and creating the opportunity to revolutionize our understanding of life and disease. Interpretation of these data remains, however, a major scientific challenge. Here, we propose a methodology that enables us to extract and display information contained in complex networks1, 2, 3. Specifically, we demonstrate that we can find functional modules4, 5 in complex networks, and classify nodes into universal roles according to their pattern of intra- and inter-module connections. The method thus yields a ‘cartographic representation’ of complex networks. Metabolic networks6, 7, 8 are among the most challenging biological networks and, arguably, the ones with most potential for immediate applicability9. We use our method to analyse the metabolic networks of twelve organisms from three different superkingdoms. We find that, typically, 80% of the nodes are only connected to other nodes within their respective modules, and that nodes with different roles are affected by different evolutionary constraints and pressures. Remarkably, we find that metabolites that participate in only a few reactions but that connect different modules are more conserved than hubs whose links are mostly within a single module."
137,"the theory of planned behavior",103166,"The Theory of Planned Behavior","Research dealing with various aspects of the theory of planned behavior ( Ajzen, 1985 and Ajzen, 1987 ) is reviewed, and some unresolved issues are discussed. In broad terms, the theory is found to be well supported by empirical evidence. Intentions to perform behaviors of different kinds can be predicted with high accuracy from attitudes toward the behavior, subjective norms, and perceived behavioral control; and these intentions, together with perceptions of behavioral control, account for considerable variance in actual behavior. Attitudes, subjective norms, and perceived behavioral control are shown to be related to appropriate sets of salient behavioral, normative, and control beliefs about the behavior, but the exact nature of these relations is still uncertain. Expectancy-value formulations are found to be only partly successful in dealing with these relations. Optimal rescaling of expectancy and value measures is offered as a means of dealing with measurement limitations. Finally, inclusion of past behavior in the prediction equation is shown to provide a means of testing the theory's sufficiency, another issue that remains unresolved. The limited available evidence concerning this question shows that the theory is predicting behavior quite well in comparison to the ceiling imposed by behavioral reliability."
138,"framing processes and social movements an overview and assessment",103180,"Framing Processes and Social Movements: An Overview and Assessment","The recent proliferation of scholarship on collective action frames and framing processes in relation to social movements indicates that framing processes have come to be regarded, alongside resource mobilization and political opportunity processes, as a central dynamic in understanding the character and course of social movements. This review examines the analytic utility of the framing literature for understanding social movement dynamics. We first review how collective action frames have been conceptualized, including their characteristic and variable features. We then examine the literature related to framing dynamics and processes. Next we review the literature regarding various contextual factors that constrain and facilitate framing processes. We conclude with an elaboration of the consequences of framing processes for other movement processes and outcomes. We seek throughout to provide clarification of the linkages between framing concepts/processes and other conceptual and theoretical formulations relevant to social movements, such as schemas and ideology. Copyright © 2000 by Annual Reviews. All rights reserved."
139,"visualizing data",103243,"Visualizing data","Enormous quantities of data go unused or underused today, simply because people can't visualize the quantities and relationships in it. Using a downloadable programming environment developed by the author, Visualizing Data demonstrates methods for representing data accurately on the Web and elsewhere, complete with user interaction, animation, and more. How do the 3.1 billion A, C, G and T letters of the human genome compare to those of a chimp or a mouse? What do the paths that millions of visitors take through a web site look like? With Visualizing Data, you learn how to answer complex questions like these with thoroughly interactive displays. We're not talking about cookie-cutter charts and graphs. This book teaches you how to design entire interfaces around large, complex data sets with the help of a powerful new design and prototyping tool called ""Processing"". Used by many researchers and companies to convey specific data in a clear and understandable manner, the Processing beta is available free. With this tool and Visualizing Data as a guide, you'll learn basic visualization principles, how to choose the right kind of display for your purposes, and how to provide interactive features that will bring users to your site over and over. This book teaches you:        The seven stages of visualizing data -- acquire, parse, filter, mine, represent, refine, and interact   How all data problems begin with a question and end with a narrative construct that provides a clear answer without extraneous details   Several example projects with the code to make them work   Positive and negative points of each representation discussed. The focus is on customization so that each one best suits what you want to convey about your data set  The book does not provide ready-made ""visualizations"" that can be plugged into any data set. Instead, with chapters divided by types of data rather than types of display, you'll learn how each visualization conveys the unique properties of the data it represents -- why the data was collected, what's interesting about it, and what stories it can tell. Visualizing Data teaches you how to answer questions, not simply display information."
140,"multilevel statistical models",103291,"Multilevel Statistical Models","{This new edition of this classic incorporates the most recent thinking on methodology and software, as well as the latest literature on multilevel statistical models. Topics covered by multilevel models have increased in recent years, and the methods are widely applied in the social sciences<br>as well as in areas such as epidemiology, geography, education, surveys, and medicine. This third edition includes chapters on meta analysis, factor analysis and structural equation models, and has expanded sections on MCMC methods.}"
141,"the theory of incentives the principalagent model",103567,"The Theory of Incentives: The Principal-agent Model","{Economics has much to do with incentives--not least, incentives to work hard, to produce quality products, to study, to invest, and to save. Although Adam Smith amply confirmed this more than two hundred years ago in his analysis of sharecropping contracts, only in recent decades has a theory begun to emerge to place the topic at the heart of economic thinking. In this book, Jean-Jacques Laffont and David Martimort present the most thorough yet accessible introduction to incentives theory to date. Central to this theory is a simple question as pivotal to modern-day management as it is to economics research: What makes people act in a particular way in an economic or business situation? In seeking an answer, the authors provide the methodological tools to design institutions that can ensure good incentives for economic agents.<p>This book focuses on the principal-agent model, the ""simple"" situation where a principal, or company, delegates a task to a single agent through a contract--the essence of management and contract theory. How does the owner or manager of a firm align the objectives of its various members to maximize profits? Following a brief historical overview showing how the problem of incentives has come to the fore in the past two centuries, the authors devote the bulk of their work to exploring principal-agent models and various extensions thereof in light of three types of information problems: adverse selection, moral hazard, and non-verifiability. Offering an unprecedented look at a subject vital to industrial organization, labor economics, and behavioral economics, this book is set to become the definitive resource for students, researchers, and others who might find themselves pondering what contracts, and the incentives they embody, are really all about.}"
142,"public choice iii",103573,"Public Choice III","{This book represents a considerable revision and expansion of Public Choice II (1989). As in the previous editions, all of the major topics of public choice are covered.  These include: why the state exists, voting rules, federalism, the theory of clubs, two-party and multiparty electoral systems, rent seeking, bureaucracy, interest groups, dictatorship, the size of government, voter participation, and political business cycles.  Normative issues in public choice are also examined.  The book is suitable for upper level courses in economics dealing with politics, and political science courses emphasizing rational actor models.} {This book represents a considerable revision and expansion of Public Choice II (1989). As in the previous editions, all of the major topics of public choice are covered.  These include: why the state exists, voting rules, federalism, the theory of clubs, two-party and multiparty electoral systems, rent seeking, bureaucracy, interest groups, dictatorship, the size of government, voter participation, and political business cycles.  Normative issues in public choice are also examined.  The book is suitable for upper level courses in economics dealing with politics, and political science courses emphasizing rational actor models.}"
143,"information extraction",104275,"Information extraction","The automatic extraction of information from unstructured sources has opened up new avenues for querying, organizing, and analyzing data by drawing upon the clean semantics of structured databases and the abundance of unstructured data. The field of information extraction has its genesis in the natural language processing community where the primary impetus came from competitions centered around the recognition of named entities like people names and organization from news articles. As society became more data oriented with easy online access to both structured and unstructured data, new applications of structure extraction came around. Now, there is interest in converting our personal desktops to structured databases, the knowledge in scientific publications to structured records, and harnessing the Internet for structured fact finding queries. Consequently, there are many different communities of researchers bringing in techniques from machine learning, databases, information retrieval, and computational linguistics for various aspects of the information extraction problem.  This review is a survey of information extraction research of over two decades from these diverse communities. We create a taxonomy of the field along various dimensions derived from the nature of the extraction task, the techniques used for extraction, the variety of input resources exploited, and the type of output produced. We elaborate on rule-based and statistical methods for entity and relationship extraction. In each case we highlight the different kinds of models for capturing the diversity of clues driving the recognition process and the algorithms for training and efficiently deploying the models. We survey techniques for optimizing the various steps in an information extraction pipeline, adapting to dynamic data, integrating with existing entities and handling uncertainty in the extraction process."
144,"callbyname callbyvalue and the lambdacalculus",104324,"Call-by-name, call-by-value, and the $\lambda$-calculus","This paper examines the old question of the relationship between ISWIM and the λ-calculus, using the distinction between call-by-value and call-by-name. It is held that the relationship should be mediated by a standardisation theorem. Since this leads to difficulties, a new λ-calculus is introduced whose standardisation theorem gives a good correspondence with ISWIM as given by the SECD machine, but without the  letrec  feature. Next a call-by-name variant of ISWIM is introduced which is in an analogous correspondence withthe usual λ-calculus. The relation between call-by-value and call-by-name is then studied by giving simulations of each language by the other and interpretations of each calculus in the other. These are obtained as another application of the continuation technique. Some emphasis is placed throughout on the notion of operational equality (or contextual equality). If terms can be proved equal in a calculus they are operationally equal in the corresponding language. Unfortunately, operational equality is not preserved by either of the simulations."
145,"geographic routing without location information",104354,"Geographic routing without location information","For many years, scalable routing for wireless communication systems was a compelling but elusive goal. Recently, several routing algorithms that exploit geographic information (e.g. {GPSR)} have been proposed to achieve this goal. These algorithms refer to nodes by their location, not address, and use those coordinates to route greedily, when possible, towards the destination. However, there are many situations where location information is not available at the nodes, and so geographic methods cannot be used. In this paper we define a scalable coordinate-based routing algorithm that does not rely on location information, and thus can be used in a wide variety of ad hoc and sensornet environments."
146,"combining the language model and inference network approaches to retrieval",104769,"Combining the language model and inference network approaches to retrieval","The inference network retrieval model, as implemented in the InQuery search engine, allows for richly structured queries. However, it incorporates a form of ad hoc tf.idf estimates for word probabilities. Language modeling offers more formal estimation techniques. In this paper we combine the language modeling and inference network approaches into a single framework. The resulting model allows structured queries to be evaluated using language modeling estimates. We explore the issues involved, such as combining beliefs and smoothing of proximity nodes. Experimental results are presented comparing the query likelihood model, the InQuery system, and our new model. The results reaffirm that high quality structured queries outperform unstructured queries and show that our system consistently achieves higher average precision than InQuery."
147,"an instructional model for webbased elearning education with a blended learning process approach",104811,"An instructional model for web-based e-learning education with a blended learning process approach","Web-based e-learning education research and development now focuses on the inclusion of new technological features and the exploration of software standards. However, far less effort is going into finding solutions to psychopedagogical problems in this new educational category. This paper proposes a psychopedagogical instructional model based on content structure, the latest research into information processing psychology and social contructivism, and defines a blended approach to the learning process. Technologically speaking, the instructional model is supported by learning objects, a concept inherited from the object-oriented paradigm."
148,"multiagent systems a survey from a machine learning perspective",104965,"Multiagent Systems: A Survey from a Machine Learning Perspective","Distributed Artificial Intelligence (DAI) has existed as a subfield of AI for less than two decades. DAI is concerned with systems that consist of multiple independent entities that interact in a domain. Traditionally, DAI has been divided into two sub-disciplines: Distributed Problem Solving (DPS) focuses on the information management aspects of systems with several components working together towards a common goal; Multiagent Systems (MAS) deals with behavior management in collections of several independent entities, or agents. This survey of MAS is intended to serve as an introduction to the field and as an organizational framework. A series of general multiagent scenarios are presented. For each scenario, the issues that arise are described along with a sampling of the techniques that exist to deal with them. The presented techniques are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. When options exist, the techniques presented are biased towards machine learning approaches. Additional opportunities for applying machine learning to MAS are highlighted and robotic soccer is presented as an appropriate test bed for MAS. This survey does not focus exclusively on robotic systems. However, we believe that much of the prior research in non-robotic MAS is relevant to robotic MAS, and we explicitly discuss several robotic MAS, including all of those presented in this issue."
149,"recoverable onedimensional encoding of protein threedimensional structures",105195,"Recoverable one-dimensional encoding of protein three-dimensional structures.","Protein one-dimensional (1D) structures such as secondary structure and contact number provide intuitive pictures to understand how the native three-dimensional (3D) structure of a protein is encoded in the amino acid sequence. However, it has not been clear whether a given set of 1D structures contains sufficient information for recovering the underlying 3D structure. Here we show that the 3D structure of a protein can be recovered from a set of three types of 1D structures, namely, secondary structure, contact number and residue-wise contact order which is introduced here for the first time. Using simulated annealing molecular dynamics simulations, the structures satisfying the given native 1D structural restraints were sought for 16 proteins of various structural classes and of sizes ranging from 56 to 146 residues. By selecting the structures best satisfying the restraints, all the proteins showed a coordinate RMS deviation of less than 4A from the native structure, and for most of them, the deviation was even less than 2A. The present result opens a new possibility to protein structure prediction and our understanding of the sequence-structure relationship."
150,"splitstream highbandwidth multicast in cooperative environments",105205,"Splitstream: High-bandwidth multicast in cooperative environments","In tree-based multicast systems, a relatively small number of interior nodes carry the load of forwarding multicast messages. This works well when the interior nodes are highly-available, dedicated infrastructure routers but it poses a problem for application-level multicast in peer-to-peer systems. SplitStream addresses this problem by striping the content across a forest of interior-node-disjoint multicast trees that distributes the forwarding load among all participating peers. For example, it is possible to construct efficient SplitStream forests in which each peer contributes only as much forwarding bandwidth as it receives. Furthermore, with appropriate content encodings, SplitStream is highly robust to failures because a node failure causes the loss of a single stripe on average. We present the design and implementation of SplitStream and show experimental results obtained on an Internet testbed and via large-scale network simulation. The results show that SplitStream distributes the forwarding load among all peers and can accommodate peers with different bandwidth capacities while imposing low overhead for forest construction and maintenance."
151,"cognitive psychology a students handbook",105569,"Cognitive Psychology: A Student's Handbook","{This is a thorough revision and updating of the extremely successful third edition. As in previous editions, the following three perspectives are considered in depth: experimental cognitive psychology; cognitive science, with its focus on cognitive modelling; and cognitive neuropsychology with its focus on cognition following brain damage. In addition, and new to this edition, is detailed discussion of the cognitive neuroscience perspective, which uses advanced brain-scanning techniques to clarify the functioning of the human brain. There is detailed coverage of the dynamic impact of these four perspectives on the main areas of cognitive psychology, including perception, attention, memory, knowledge representation, categorisation, language, problem-solving, reasoning, and judgement.<br> The aim is to provide comprehensive coverage that is up-to-date, authoritative, and accessible. All existing chapters have been extensively revised and re-organised. Some of the topics receiving much greater coverage in this edition are: brain structures in perception, visual attention, implicit learning, brain structures in memory, prospective memory, exemplar theories of categorisation, language comprehension, connectionist models in perception, neuroscience studies of thinking, judgement, and decision making.<br> Cognitive Psychology: A Students Handbook will be essential reading for undergraduate students of psychology. It will also be of interest to students taking related courses in computer science, education, linguistics, physiology, and medicine.}"
152,"email writing as a crosscultural learning experience",105594,"E-mail writing as a cross-cultural learning experience","This study looks into the cultural dimension involved in the e-mail correspondence between university EFL students in Taiwan and pre-service bilingual/ESL teachers in the USA. E-mail entries and end-of-project reports were analyzed to yield insights into the cross-cultural communication process. The data analysis focused on the types of cultural information transmitted and effects of cultural assumptions and values on communication effectiveness. The findings revealed perceived fundamental characteristics of both Chinese and American cultures by the two groups of participants. It was also found that curiosity toward the other culture was a motivating factor for on-going correspondence, but cultural presumptions were sometimes a hindrance for communication; positive interpretations of cultural differences and empathy were key factors contributing to the removal of communication obstacles. Although there is no substitute for actual experiences of immersing into the target culture, cross-cultural e-mail correspondence sensitized the participants to cultural differences and served as a learning experience for better cross-cultural understanding."
153,"independent component analysis",105835,"Independent Component Analysis","{A comprehensive introduction to ICA for students and practitioners<br>   Independent Component Analysis (ICA) is one of the most exciting new topics in fields such as neural networks, advanced statistics, and signal processing. This is the first book to provide a comprehensive introduction to this new technique complete with the fundamental mathematical background needed to understand and utilize it. It offers a general overview of the basics of ICA, important solutions and algorithms, and in-depth coverage of new applications in image processing, telecommunications, audio signal processing, and more.<br>   Independent Component Analysis is divided into four sections that cover:<br>   * General mathematical concepts utilized in the book<br>   * The basic ICA model and its solution<br>   * Various extensions of the basic ICA model<br>   * Real-world applications for ICA models<br>   Authors Hyvarinen, Karhunen, and Oja are well known for their contributions to the development of ICA and here cover all the relevant theory, new algorithms, and applications in various fields. Researchers, students, and practitioners from a variety of disciplines will find this accessible volume both helpful and informative.}"
154,"foundations of statistical natural language processing",105906,"Foundations of Statistical Natural Language Processing","{""Statistical natural-language processing is, in my estimation, one of the most fast-moving and exciting areas of computer science these days. Anyone who wants to learn this field would be well advised to get this book. For that matter, the same goes for anyone who is already in the field. I know that it is going to be one of the most well-thumbed books on my bookshelf."" -- Eugene Charniak, Department of Computer Science, Brown University  <P>Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.  <P>More on this book}"
155,"a new kind of science",106131,"A New Kind of Science","{Physics and computer science genius Stephen Wolfram, whose Mathematica computer language launched a multimillion-dollar company, now sets his sights on a more daunting goal: understanding the universe. Wolfram lets the world see his work in <I>A New Kind of Science</I>, a gorgeous, 1,280-page tome more than a decade in the making. With patience, insight, and self-confidence to spare, Wolfram outlines a fundamental new way of modeling complex systems.<p>  On the frontier of complexity science since he was a boy, Wolfram is a  champion of cellular automata--256 ""programs"" governed by simple  nonmathematical rules. He points out that even the most complex  equations fail to accurately model biological systems, but the simplest  cellular automata can produce results straight out of nature--tree  branches, stream eddies, and leopard spots, for instance. The graphics  in <I>A New Kind of Science</I> show striking resemblance to the  patterns we see in nature every day.<p>  Wolfram wrote the book in a distinct style meant to make it easy to read,   even for nontechies; a basic familiarity with logic is helpful but  not essential. Readers will find themselves swept away by the elegant  simplicity of Wolfram's ideas and the accidental artistry of the  cellular automaton models. Whether or not Wolfram's revolution  ultimately gives us the keys to the universe, his new science is  absolutely awe-inspiring. <I>--Therese Littleton</I>} {This long-awaited work from one of the world's most respected scientists presents a series of dramatic discoveries never before made public. Starting from a collection of simple computer experiments---illustrated in the book by striking computer graphics---Wolfram shows how their unexpected results force a whole new way of looking at the operation of our universe.  <P>Wolfram uses his approach to tackle a remarkable array of fundamental problems in science: from the origin of the Second Law of thermodynamics, to the development of complexity in biology, the computational limitations of mathematics, the possibility of a truly fundamental theory of physics, and the interplay between free will and determinism.  <P>Written with exceptional clarity, and illustrated by more than a thousand original pictures, this seminal book allows scientists and non-scientists alike to participate in what promises to be a major intellectual revolution.}"
156,"electric fields of the brain the neurophysics of eeg",106318,"Electric Fields of the Brain: The Neurophysics of EEG","Electroencephalography (EEG) is practiced by neurologists, cognitive neuroscientists, and others interested in functional brain imaging. Whether for clinical or experimental purposes, all studies share a common purpose-to relate scalp potentials to the underlying neurophysiology. Electrical potentials on the scalp exhibit spatial and temporal patterns that depend on the nature and location of the sources and the way that currents and fields spread through tissue. Because these dynamic patterns are correlated with behavior and cognition, EEG provides a ""window on the mind,"" correlating physiology and psychology. This classic and widely acclaimed text, originally published in 1981, filled the large gap between EEG and the physical sciences. It has now been brought completely up to date and will again serve as an invaluable resource for understanding the principles of electric fields in living tissue and for using hard science to study human consciousness and cognition. No comparable volume exists for it is no easy task to explain the problems of EEG in clear language, with mathematics presented mainly in appendices. Among the many topics covered by the Second Edition are micro and meso (intermediate scale) synaptic sources, electrode placement, choice of reference, volume conduction, power and coherence measures, projection of scalp potentials to dura surface, dynamic signatures of conscious experience, neural networks immersed in global fields of synaptic action, and physiological bases for brain source dynamics. The Second Edition is an invaluable resource for neurologists, neuroscientists (especially cognitive neuroscientists), biomedical engineers, and their students and trainees. It will also appeal to physicists, mathematicians, computer scientists, psychiatrists, and industrial engineers interested in EEG."
157,"the success of open source",106560,"The Success of Open Source","{<p> Much of the innovative programming that powers the Internet, creates operating systems, and produces software is the result of ""open source"" code, that is, code that is freely distributed--as opposed to being kept secret--by those who write it. Leaving source code open has generated some of the most sophisticated developments in computer technology, including, most notably, Linux and Apache, which pose a significant challenge to Microsoft in the marketplace. As Steven Weber discusses, open source's success in a highly competitive industry has subverted many assumptions about how businesses are run, and how intellectual products are created and protected. </p><p> Traditionally, intellectual property law has allowed companies to control knowledge and has guarded the rights of the innovator, at the expense of industry-wide cooperation. In turn, engineers of new software code are richly rewarded; but, as Weber shows, in spite of the conventional wisdom that innovation is driven by the promise of individual and corporate wealth, ensuring the free distribution of code among computer programmers can empower a more effective process for building intellectual products. In the case of Open Source, independent programmers--sometimes hundreds or thousands of them--make unpaid contributions to software that develops organically, through trial and error. </p><p> Weber argues that the success of open source is not a freakish exception to economic principles. The open source community is guided by standards, rules, decisionmaking procedures, and sanctioning mechanisms. Weber explains the political and economic dynamics of this mysterious but important market development.  </p>}"
158,"the network society a crosscultural perspective",106572,"The Network Society: A Cross-Cultural Perspective","{Manuel Castells - one of the world&#146;s pre-eminent social scientists - has drawn together a stellar group of contributors to explore the patterns and dynamics of the network society in its cultural and institutional diversity. The book analyzes the technological, cultural and institutional transformation of societies around the world in terms of the critical role of electronic communication networks in business, everyday life, public services, social interaction and politics. The contributors demonstrate that the network society is the new form of social organization in the Information age, replacing the Industrial society. The book analyzes processes of technological transformation in interaction with social culture in different cultural and institutional contexts: the United States of America, the United Kingdom, Finland, Russia, China, India, Canada, and Catalonia. The topics examined include business productivity, global financial markets, cultural identity, the uses of the Internet in education and health, the anti-globalization movement, political processes, media and identity, and public policies to guide technological development. Taken together these studies show that the network society adopts very different forms, depending on the cultural and institutional environments in which it evolves. The Network Society, now available in paperback, is an outstanding and original volume of direct interest in academia - particularly in the fields of social sciences, communication studies, and business schools - as well as for policymakers engaged in technological policy and economic development. Business and management experts will also discover much of value to them within this book.}"
159,"a comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis",106629,"A comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis.","MOTIVATION: Cancer diagnosis is one of the most important emerging clinical applications of gene expression microarray technology. We are seeking to develop a computer system for powerful and reliable cancer diagnostic model creation based on microarray data. To keep a realistic perspective on clinical applications we focus on multicategory diagnosis. To equip the system with the optimum combination of classifier, gene selection and cross-validation methods, we performed a systematic and comprehensive evaluation of several major algorithms for multicategory classification, several gene selection methods, multiple ensemble classifier methods and two cross-validation designs using 11 datasets spanning 74 diagnostic categories and 41 cancer types and 12 normal tissue types. RESULTS: Multicategory support vector machines (MC-SVMs) are the most effective classifiers in performing accurate cancer diagnosis from gene expression data. The MC-SVM techniques by Crammer and Singer, Weston and Watkins and one-versus-rest were found to be the best methods in this domain. MC-SVMs outperform other popular machine learning algorithms, such as k-nearest neighbors, backpropagation and probabilistic neural networks, often to a remarkable degree. Gene selection techniques can significantly improve the classification performance of both MC-SVMs and other non-SVM learning algorithms. Ensemble classifiers do not generally improve performance of the best non-ensemble models. These results guided the construction of a software system GEMS (Gene Expression Model Selector) that automates high-quality model construction and enforces sound optimization and performance estimation procedures. This is the first such system to be informed by a rigorous comparative analysis of the available algorithms and datasets. AVAILABILITY: The software system GEMS is available for download from http://www.gems-system.org for non-commercial use. CONTACT: alexander.statnikov@vanderbilt.edu."
160,"ondemand multipath routing for mobile ad hoc networks",106735,"On-demand multipath routing for mobile ad hoc networks","Mobile ad hoc networks are characterized by multi-hop wireless links, absence of any cellular infrastructure, and frequent host mobility. Design of efficient routing protocols in such networks is a challenging issue. A class of routing protocols called on-demand protocols has recently attracted attention because of their low routing overhead. The on-demand protocols depend on query floods to discover routes whenever a new route is needed. Such floods take up a substantial portion of network bandwidth. We focus on a particular on-demand protocol, called dynamic source routing, and show how intelligent use of multipath techniques can reduce the frequency of query floods. We develop an analytic modeling framework to determine the relative frequency of query floods for various techniques. Results show that while multipath routing is significantly better than single path routing, the performance advantage is small beyond a few paths and for long path lengths. It also shows that providing all intermediate nodes in the primary (shortest) route with alternative paths has a significantly better performance than providing only the source with alternate paths"
161,"highly dynamic destinationsequenced distancevector routing dsdv for mobile computers",106736,"Highly dynamic Destination-Sequenced Distance-Vector routing (DSDV) for mobile computers","An  ad-hoc  network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks. Our modifications address some of the previous  objections  to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks."
162,"semantic matching of web services capabilities",107519,"Semantic Matching of Web Services Capabilities","The Web is moving from being a collection of pages toward a collection of services that interoperate through the Internet. The first step toward this interoperation is the location of other services that can help toward the solution of a problem. In this paper we claim that lo- cation of web services should be based on the semantic match between a declarative description of the service being sought, and a description of the service being offered. Furthermore, we claim that this match is outside the representation capabilities of registries such as UDDI and languages such as WSDL. We propose a solution based on DAML-S, a DAML-based language for service description, and we show how service capabilities are presented in the Profile section of a DAML-S description and how a semantic match between advertisements and requests is performed."
163,"a performance comparison of multihop wireless ad hoc network routing protocols",108491,"A performance comparison of multi-hop wireless ad hoc network routing protocols","An ad hoc network is a collection of wireless mobile nodes dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration. Due to the limited transmission range of wireless network interfaces, multiple network ""hops"" may be needed for one node to exchange data with another across the network. In recent years, a variety of new routing protocols targeted specifically at this environment have been developed, but little performance information on each protocol and no realistic performance comparison between them is available. This paper presents the results of a detailed packet-level simulation comparing four multi-hop wireless ad hoc network routing protocols that cover a range of design choices: DSDV, TORA, DSR, and AODV. We have extended the ns-2 network simulator to accurately model the MAC and physical-layer behavior of the IEEE 802.11 wireless LAN standard, including a realistic wireless transmission channel model, and present the results of simulations of networks of 50 mobile nodes."
164,"comparing images using the hausdorff distance",109109,"Comparing images using the Hausdorff distance","The Hausdorff distance measures the extent to which each point of a model set lies near some point of an image set and vice versa. Thus, this distance can be used to determine the degree of resemblance between two objects that are superimposed on one another. Efficient algorithms for computing the Hausdorff distance between all possible relative positions of a binary image and a model are presented. The focus is primarily on the case in which the model is only allowed to translate with respect to the image. The techniques are extended to rigid motion. The Hausdorff distance computation differs from many other shape comparison methods in that no correspondence between the model and the image is derived. The method is quite tolerant of small position errors such as those that occur with edge detectors and other feature extraction methods. It is shown that the method extends naturally to the problem of comparing a portion of a model against an image."
165,"automatic subspace clustering of high dimensional data for data mining applications",109748,"Automatic subspace clustering of high dimensional data for data mining applications","Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate clusters in large high dimensional datasets."
166,"informationtheoretic coclustering",109791,"Information-theoretic co-clustering","Two-dimensional contingency or co-occurrence tables arise frequently in important applications such as text, web-log and market-basket data analysis. A basic problem in contingency table analysis is  co-clustering: simultaneous clustering  of the rows and columns. A novel theoretical formulation views the contingency table as an empirical joint probability distribution of two discrete random variables and poses the co-clustering problem as an optimization problem in  information theory ---the optimal co-clustering maximizes the mutual information between the clustered random variables subject to constraints on the number of row and column clusters. We present an innovative co-clustering algorithm that monotonically increases the preserved mutual information by intertwining both the row and column clusterings at all stages. Using the practical example of simultaneous word-document clustering, we demonstrate that our algorithm works well in practice, especially in the presence of sparsity and high-dimensionality."
167,"database techniques for the worldwide web a survey",109806,"Database Techniques for the World-Wide Web: {A} Survey","The primary goal of this survey is to classify the different tasks to which database concepts have been applied, and to emphasize the technical innovations that are required to do so. We focus on three classes of tasks related to information management on the WWW: (1) Modeling and querying the web, (2) Information extraction and integration, and (3) Web site construction and restructuring. The survey showed that powerful abstractions developed in the database community may prove to be the key..."
168,"genomewide analysis of dna copynumber changes using cdna microarrays",109996,"Genome-wide analysis of {DNA} copy-number changes using c{DNA} microarrays","Gene amplifications and deletions frequently contribute to tumorigenesis. Characterization of these DNA copy-number changes is important for both the basic understanding of cancer and its diagnosis. Comparative genomic hybridization (CGH) was developed to survey DNA copy-number variations across a whole genome1. With CGH, differentially labelled test and reference genomic DNAs are co-hybridized to normal metaphase chromosomes, and fluorescence ratios along the length of chromosomes provide a cytogenetic representation of DNA copy-number variation. CGH, however, has a limited (~20 Mb) mapping resolution, and higher-resolution techniques, such as fluorescence in situ hybridization (FISH), are prohibitively labour-intensive on a genomic scale. Array-based CGH, in which fluorescence ratios at arrayed DNA elements provide a locus-by-locus measure of DNA copy-number variation, represents another means of achieving increased mapping resolution2, 3, 4. Published array CGH methods have relied on large genomic clone (for example BAC) array targets and have covered only a small fraction of the human genome. cDNAs representing over 30,000 radiation-hybrid (RH)−mapped human genes5, 6 provide an alternative and readily available genomic resource for mapping DNA copy-number changes. Although cDNA microarrays have been used extensively to characterize variation in human gene expression7, 8, 9, human genomic DNA is a far more complex mixture than the mRNA representation of human cells. Therefore, analysis of DNA copy-number variation using cDNA microarrays would require a sensitivity of detection an order of magnitude greater than has been routinely reported7. We describe here a cDNA microarray-based CGH method, and its application to DNA copy-number variation analysis in breast cancer cell lines and tumours. Using this assay, we were able to identify gene amplifications and deletions genome-wide and with high resolution, and compare alterations in DNA copy number and gene expression."
169,"interpreting patterns of gene expression with selforganizing maps methods and application to hematopoietic differentiation",110014,"Interpreting patterns of gene expression with self-organizing maps: Methods and application to hematopoietic differentiation","Array technologies have made it straightforward to monitor simultaneously the expression pattern of thousands of genes. The challenge now is to interpret such massive data sets. The first step is to extract the fundamental patterns of gene expression inherent in the data. This paper describes the application of self-organizing maps, a type of mathematical cluster analysis that is particularly well suited for recognizing and classifying features in complex, multidimensional data. The method has been implemented in a publicly available computer package, , that performs the analytical calculations and provides easy data visualization. To illustrate the value of such analysis, the approach is applied to hematopoietic differentiation in four well studied models (HL-60, U937, Jurkat, and NB4 cells). Expression patterns of some 6,000 human genes were assayed, and an online database was created. was used to organize the genes into biologically relevant clusters that suggest novel hypotheses about hematopoietic differentiation‚{Ä}{\\^\\i}for example, highlighting certain genes and pathways involved in ‚{Ä}{ú}differentiation therapy‚{Ä}{ù} used in the treatment of acute promyelocytic leukemia."
170,"annual report",110425,"Annual Report 2000-2001","The report summarizes the activities of IOC carried out in 1997 and is presented under the following major headings: 1) Implementation of IOC Governing Body Resolutions -- Resolutions adopted by the Nineteenth Session of the Assembly, Resolutions adopted by the Twenty-ninth Session of the Executive Council, Overview of IOC programme structure; 2) Programme activities -- Ocean sciences, Ocean services, Global Ocean Observing System, Capacity building in marine sciences, services and observations: TEMA, Regional activities; and, 3) Cooperation and development -- Cooperation with other organizations of the United Nations system and other bodies, Follow-up to UNCED and UNCLOS, The 1998 International Year of the Ocean, Development of IOC within UNESCO, and List of publications."
171,"real life information retrieval a study of user queries on the web",111657,"Real life information retrieval: a study of user queries on the Web","We analyzed transaction logs of a set of 51,473 queries posed by 18,113 users of  Excite,  a major Internet search service. We provide data on: (i)  queries  --- the number of search terms, and the use of logic and modifiers, (ii)  sessions  --- changes in queries during a session, number of pages viewed, and use of relevance feedback, and (iii)  terms  --- their rank/frequency distribution and the most highly used search terms. Common mistakes are also observed. Implications are discussed."
172,"visualizing knowledge domains",111761,"Visualizing {K}nowledge {D}omains","This chapter reviews visualization techniques that can not only be utilized to map the ever- growing domain structure of scientific disciplines but that also support information retrieval and classification. In contrast to the comprehensive surveys done in a traditional way by Howard White and Katherine McCain (1997; 1998), the current survey not only reviews emerging techniques in interactive data analysis and information visualization, but also visualizes bibliographical structures of the field as an integral part of our methodology. The chapter starts with a review of the history of knowledge domain visualizations. We then introduce a general process flow for the visualization of knowledge domains and explain commonly used techniques. In the interest of visualizing the domain this article reviews, we introduce a bibliographic data set of considerable size, which includes articles from the citation analysis, bibliometrics, semantics, and visualization literatures. Using a tutorial style, we then apply various algorithms to demonstrate the visualization effects produced by different approaches and compare the different visualization results. At the same time, the domain visualizations reveal the relationships within and between the four fields that together form the topic of this chapter, domain visualization. We conclude with a discussion of promising new avenues of research and a general discussion."
173,"efficient implementation of the smalltalk system",112856,"Efficient implementation of the smalltalk-80 system","The Smalltalk-80* programming language includes dynamic storage allocation, full upward funargs, and universally polymorphic procedures; the Smalltalk-80 programming system features interactive execution with incremental compilation, and implementation portability. These features of modern programming systems are among the most difficult to implement efficiently, even individually. A new implementation of the Smalltalk-80 system, hosted on a small microprocessor-based computer, achieves high performance while retaining complete (object code) compatibility with existing implementations. This paper discusses the most significant optimization techniques developed over the course of the project, many of which are applicable to other languages. The key idea is to represent certain runtime state (both code and data) in more than one form, and to convert between forms when needed."
174,"into the heart of darkness largescale clustering of human noncoding dna",112873,"Into the heart of darkness: large-scale clustering of human non-coding DNA","Motivation: It is currently believed that the human genome contains about twice as much non-coding functional regions as it does protein-coding genes, yet our understanding of these regions is very limited.  Results: We examine the intersection between syntenically conserved sequences in the human, mouse and rat genomes, and sequence similarities within the human genome itself, in search of families of non-protein-coding elements. For this purpose we develop a graph theoretic clustering algorithm, akin to the highly successful methods used in elucidating protein sequence family relationships.  The algorithm is applied to a highly filtered set of about 700 000 human-rodent evolutionarily conserved regions, not resembling any known coding sequence, which encompasses 3.7% of the human genome. From these, we obtain roughly 12 000 non-singleton clusters, dense in significant sequence similarities. Further analysis of genomic location, evidence of transcription and RNA secondary structure reveals many clusters to be significantly homogeneous in one or more characteristics. This subset of the highly conserved non-protein-coding elements in the human genome thus contains rich family-like structures, which merit in-depth analysis.  Availability: Supplementary material to this work is available at http://www.soe.ucsc.edu/~jill/dark.html 10.1093/bioinformatics/bth946"
175,"identification and characterization of multispecies conserved sequences",112877,"Identification and Characterization of Multi-Species Conserved Sequences","10.1101/gr.1602203 Comparative sequence analysis has become an essential component of studies aiming to elucidate genome function. The increasing availability of genomic sequences from multiple vertebrates is creating the need for computational methods that can detect highly conserved regions in a robust fashion. Towards that end, we are developing approaches for identifying sequences that are conserved across multiple species; we call these “Multi-species Conserved Sequences” (or MCSs). Here we report two strategies for MCS identification, demonstrating their ability to detect virtually all known actively conserved sequences (specifically, coding sequences) but very little neutrally evolving sequence (specifically, ancestral repeats). Importantly, we find that a substantial fraction of the bases within MCSs (∼70%) resides within non-coding regions; thus, the majority of sequences conserved across multiple vertebrate species has no known function. Initial characterization of these MCSs has revealed sequences that correspond to clusters of transcription factor-binding sites, non-coding RNA transcripts, and other candidate functional elements. Finally, the ability to detect MCSs represents a valuable metric for assessing the relative contribution of a species' sequence to identifying genomic regions of interest, and our results indicate that the currently available genome sequences are insufficient for the comprehensive identification of MCSs in the human genome."
176,"the definition of standard ml revised",113339,"The Definition of Standard ML - Revised","Standard ML is a general-purpose programming language designed for large projects. This book provides a formal definition of Standard ML for the benefit of all concerned with the language, including users and implementers. Because computer programs are increasingly required to withstand rigorous analysis, it is all the more important that the language in which they are written be defined with full rigor.<br /> <br /> One purpose of a language definition is to establish a theory of meanings upon which the understanding of particular programs may rest. To properly define a programming language, it is necessary to use some form of notation other than a programming language. Given a concern for rigor, mathematical notation is an obvious choice. The authors have defined their semantic objects in mathematical notation that is completely independent of Standard ML.<br /> <br /> In defining a language one must also define the rules of evaluation precisely--that is, define what meaning results from evaluating any phrase of the language. The definition thus constitutes a formal specification for an implementation. The authors have developed enough of their theory to give sense to their rules of evaluation.<br /> <br /> <I>The Definition of Standard ML</I> is the essential point of reference for Standard ML. Since its publication in 1990, the implementation technology of the language has advanced enormously and the number of users has grown. The revised edition includes a number of new features, omits little-used features, and corrects mistakes of definition."
177,"a survey of kernels for structured data",113926,"A survey of kernels for structured data.","Kernel methods in general and support vector machines in particular have been successful in various learning tasks on data represented in a single table. Much 'real-world' data, however, is structured - it has no natural representation in a single table. Usually, to apply kernel methods to 'real-world' data, extensive pre-processing is performed to embed the data into areal vector space and thus in a single table. This survey describes several approaches of defining positive definite kernels on structured instances directly."
178,"exploiting virtual synchrony in distributed systems",113948,"Exploiting virtual synchrony in distributed systems","We describe applications of a  virtually synchronous  environment for distributed programming, which underlies a collection of distributed programming tools in the  ISIS 2  system. A virtually synchronous environment allows processes to be structured into  process groups , and makes events like broadcasts to the group as an entity, group membership changes, and even migration of an activity from one place to another appear to occur instantaneously &mdash; in other words, synchronously. A major advantage to this approach is that many aspects of a distributed application can be treated independently without compromising correctness. Moreover, user code that is designed as if the system were synchronous can often be executed concurrently. We argue that this approach to building distributed and fault-tolerant software is more straightforward, more flexible, and more likely to yield correct solutions than alternative approaches."
179,"independent component analysis a new concept",114027,"Independent component analysis - a new concept?","The independent component analysis (ICA) of a random vector consists of searching for a linear transformation that minimizes the statistical dependence between its components. In order to define suitable search criteria, the expansion of mutual information is utilized as a function of cumulants of increasing orders. An efficient algorithm is proposed, which allows the computation of the ICA of a data matrix within a polynomial time. The concept of ICA may actually be seen as an extension of the principal component analysis (PCA), which can only impose independence up to the second order and, consequently, defines directions that are orthogonal. Potential applications of ICA include data analysis and compression, Bayesian detection, localization of sources, and blind identification and deconvolution."
180,"independent component analysis algorithms and applications",114040,"Independent Component Analysis: Algorithms and Applications","A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject."
181,"mindstorms children computers and powerful ideas",114177,"Mindstorms: Children, Computers and Powerful Ideas","The Gears of My Childhood   Before I was two years old I had developed an intense involvement with automobiles. The names of car parts made up a very substantial portion of my vocabulary: I was particularly proud of knowing about the parts of the transmission system, the gearbox, and most especially the differential. It was, of course, many years later before I understood how gears work; but once I did, playing with gears became a favorite pastime. I loved rotating circular objects against one another in gearlike motions and, naturally, my first ""erector set"" project was a crude gear system.   I became adept at turning wheels in my head and at making chains of cause and effect: ""This one turns this way so that must turn that way so . . . "" I found particular pleasure in such systems as the differential gear, which does not follow a simple linear chain of causality since the motion in the transmission shaft can be distributed in many different ways to the two wheels depending on what resistance they encounter. I remember quite vividly my excitement at discovering that a system could be lawful and completely comprehensible without being rigidly deterministic.   I believe that working with differentials did more for my mathematical development than anything I was taught in elementary school. Gears, serving as models, carried many otherwise abstract ideas into my head. I clearly remember two examples from school math. I saw multiplication tables as gears, and my first brush with equations in two variables (e.g., 3 x  + 4 y  = 10) immediately evoked the differential. By the time I had made a mental gear model of the relation between  x  and  y , figuring how many teeth each gear needed, the equation had become a comfortable friend.   Many years later when I read Piaget this incident served me as a model for his notion of assimilation, except I was immediately struck by the fact that his discussion does not do full justice to his own idea. He talks almost entirely about cognitive aspects of assimilation. But there is also an affective component. Assimilating equations to gears certainly is a powerful way to bring old knowledge to bear on a new object. But it does more as well. I am sure that such assimilations helped to endow mathematics, for me, with a positive affective tone that can be traced back to my infantile experiences with cars. I believe Piaget really agrees. As I came to know him personally I understood that his neglect of the affective comes more from a modest sense that little is known about it than from an arrogant sense of its irrelevance. But let me return to my childhood.   One day I was surprised to discover that some adults---even most adults---did not understand or even care about the magic of the gears. I no longer think much about gears, but I have never turned away from the questions that started with that discovery: How could what was so simple for me be incomprehensible to other people? My proud father suggested ""being clever"" as an explanation. But I was painfully aware that some people who could not understand the differential could easily do things I found much more difficult. Slowly I began to formulate what I still consider the fundamental fact about learning: Anything is easy if you can assimilate it to your collection of models. If you can't, anything can be painfully difficult. Here too I was developing a way of thinking that would be resonant with Piaget's.  The understanding of learning must be genetic . It must refer to the genesis of knowledge. What an individual can learn, and how he learns it, depends on what models he has available. This raises, recursively, the question of how he learned these models. Thus the ""laws of learning"" must be about how intellectual structures grow out of one another and about how, in the process, they acquire both logical and emotional form.   This book is an exercise in an applied genetic epistemology expanded beyond Piaget's cognitive emphasis to include a concern with the affective. It develops a new perspective for education research focused on creating the conditions under which intellectual models will take root. For the last two decades this is what I have been trying to do. And in doing so I find myself frequently reminded of several aspects of my encounter with the differential gear. First, I remember that no one told me to learn about differential gears. Second, I remember that there was  feeling, love , as well as understanding in my relationship with gears. Third, I remember that my first encounter with them was in my second year. If any ""scientific"" educational psychologist had tried to ""measure"" the effects of this encounter, he would probably have failed. It had profound consequences but, I conjecture, only very many years later. A ""pre- and post-"" test at age two would have missed them.   Piaget's work gave me a new framework for looking at the gears of my childhood. The gear can be used to illustrate many powerful ""advanced"" mathematical ideas, such as groups or relative motion. But it does more than this. As well as connecting with the formal knowledge of mathematics, it also connects with the ""body knowledge,"" the sensorimotor schemata of a child. You can be the gear, you can understand how it turns by projecting yourself into its place and turning with it. It is this double relationship---both abstract and sensory---that gives the gear the power to carry powerful mathematics into the mind. In a terminology I shall develop in later chapters, the gear acts here as a  transitional object .   A modern-day Montessori might propose, if convinced by my story, to create a gear set for children. Thus every child might have the experience I had. But to hope for this would be to miss the essence of the story.  I fell in love with the gears . This is something that cannot be reduced to purely ""cognitive"" terms. Something very personal happened, and one cannot assume that it would be repeated for other children in exactly the same form.   My thesis could be summarized as: What the gears cannot do the computer might. The computer is the Proteus of machines. Its essence is its universality, its power to simulate. Because it can take on a thousand forms and can serve a thousand functions, it can appeal to a thousand tastes. This book is the result of my own attempts over the past decade to turn computers into instruments flexible enough so that many children can each create for themselves something like what the gears were for me."
182,"genetic algorithms in search optimization and machine learning",114190,"Genetic Algorithms in Search, Optimization, and Machine Learning","{David Goldberg's <i>Genetic Algorithms in Search, Optimization and Machine Learning</i> is by far the bestselling introduction to genetic algorithms. Goldberg is one of the preeminent researchers in the field--he has published over 100 research articles on genetic algorithms and is a student of John Holland, the father of genetic algorithms--and his deep understanding of the material shines through. The book contains a complete listing of a simple genetic algorithm in Pascal, which C programmers can easily understand. The book covers all of the important topics in the field, including crossover, mutation, classifier systems, and fitness scaling, giving a novice with a computer science background enough information to implement a genetic algorithm and describe genetic algorithms to a friend.}"
183,"adaptation in natural and artificial systems",114194,"Adaptation in Natural and Artificial Systems","Genetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic theory to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. Adaptation in Natural and Artificial Systems is the book that initiated this field of study, presenting the theoretical foundations and exploring applications.  In its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. In this now classic work, Holland presents a mathematical model that allows for the nonlinearity of such complex interactions. He demonstrates the model's universality by applying it to economics, physiological psychology, game theory, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics.  Initially applying his concepts to simply defined artificial systems with limited numbers of parameters, Holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. Along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements."
184,"the tragedy of the commons",114199,"The Tragedy of the Commons","Hardin, professor of biology, University of California, Santa Barbara presented this as a presidential address to the Pacific Division of the American Association for the Advancement of Science at Utah State University, Logan, 25 June 1968. Garrett Hardin described the pathogenic effects of conscience when asking someone to desist exploiting the commons in the name of conscience. One is faced with a Bateson’s “double bind” of being accused of being an irresponsible citizen for exploiting the commons or a simpleton who stands aside while everyone else exploits the commons."
185,"quantum dots for live cells in vivo imaging and diagnostics",114704,"Quantum Dots for Live Cells, in Vivo Imaging, and Diagnostics","Research on fluorescent semiconductor nanocrystals (also known as quantum dots or qdots) has evolved over the past two decades from electronic materials science to biological applications. We review current approaches to the synthesis, solubilization, and functionalization of qdots and their applications to cell and animal biology. Recent examples of their experimental use include the observation of diffusion of individual glycine receptors in living neurons and the identification of lymph nodes in live animals by near-infrared emission during surgery. The new generations of qdots have far-reaching potential for the study of intracellular processes at the single-molecule level, high-resolution cellular imaging, long-term in vivo observation of cell trafficking, tumor targeting, and diagnostics."
186,"molecular motors",114769,"Molecular motors","{The latest knowledge on molecular motors is vital for the understanding of a wide range of biological and medical topics: cell motility, organelle movement, virus transport, developmental asymmetry, myopathies, and sensory defects are all related to the function or malfunction of these minute molecular machines. Since there is a vast amount of information on motor mechanisms and potential biomedical and nanobiotechnological applications, this handbook fulfills the need for a collection of current research results on the functionality, regulation, and interactions of cytoskeletal, DNA, and rotary motors. Here, leading experts present a concise insight, ranging from atomic structure, biochemistry, and biophysics to cell biology, developmental biology and pathology. Basic principles and applications make this book a valuable reference tool for researchers, professionals, and clinicians alike - all set to become a ""classic"" in the years to come.}"
187,"process modeling",114954,"Process Modeling","Traditionally, the modeling of information systems has focused on analyzing data flows and transformations. This modeling accounted only for the organization's data and that portion of its processes that interacted with data. Newer uses of in- formation technology extend computer use beyond transaction processing into commu- nication and coordination. Successfully integrating these systems into the enterprise often requires modeling even the manual organizational processes into which these systems intervene. The following are three such applications: ..."
188,"inversion of control containers and the dependency injection pattern",114960,"Inversion of Control Containers and the Dependency Injection pattern","In the Java community there's been a rush of lightweight containers that help to assemble components from different projects into a cohesive application. Underlying these containers is a common pattern to how they perform the wiring, a concept they refer under the very generic name of ""Inversion of Control"". In this article I dig into how this pattern works, under the more specific name of ""Dependency Injection"", and contrast it with the Service Locator alternative. The choice between them is less important than the principle of separating configuration from use."
189,"htn planning for web service composition using shop",115003,"HTN Planning for Web Service Composition Using SHOP2","Automated composition of Web Services can be achieved by using AI planning techniques. Hierarchical Task Network (HTN) planning is especially well-suited for this task. In this paper, we describe how HTN planning system SHOP2 can be used with OWL-S Web Service descriptions. We provide a sound and complete algorithm to translate OWL-S service descriptions to a SHOP2 domain. We prove the correctness of the algorithm by showing the correspondence to the situation calculus semantics of OWL-S. We implemented a system that plans over sets of OWL-S descriptions using SHOP2 and then executes the resulting plans over the Web. The system is also capable of executing information-providing Web Services during the planning process. We discuss the challenges and difficulties of using planning in the information-rich and human-oriented context of Web Services."
190,"web services",115155,"Web Services","{Like many other incipient technologies, Web services are still surrounded by a tremendous level of noise. This noise results from the always dangerous combination of wishful thinking on the part of research and industry and of a lack of clear understanding of how Web services came to be. On the one hand, multiple contradictory interpretations are created by the many attempts to realign existing technology and strategies with Web services. On the other hand, the emphasis on what could be done with Web services in the future often makes us lose track of what can be really done with Web services today and in the short term. These factors make it extremely difficult to get a coherent picture of what Web services are, what they contribute, and where they will be applied. Alonso and his co-authors deliberately take a step back. Based on their academic and industrial experience with middleware and enterprise application integration systems, they describe the fundamental concepts behind the notion of Web services and present them as the natural evolution of conventional middleware, necessary to meet the challenges of the Web and of B2B application integration.  Rather than providing a reference guide or a ""how to write your first Web service"" kind of book, they discuss the main objectives of Web services, the challenges that must be faced to achieve them, and the opportunities that this novel technology provides. Established, as well as recently proposed, standards and techniques (e.g., WSDL, UDDI, SOAP, WS-Coordination, WS-Transactions, and BPEL), are then examined in the context of this discussion in order to emphasize their scope, benefits, and shortcomings. Thus, the book is ideally suited both for professionals considering the development of application integration solutions and for research and students interesting in understanding and contributing to the evolution of enterprise application technologies.}"
191,"structural mechanisms for domain movements in proteins",115168,"Structural mechanisms for domain movements in proteins","We survey all the known instances of domain movements in proteins for which there is crystallographic evidence for the movement. We explain these domain movements in terms of the repertoire of low-energy conformation changes that are known to occur in proteins. We first describe the basic elements of this repertoire, hinge and shear motions, and then show how the elements of the repertoire can be combined to produce domain movements. We emphasize that the elements used in particular proteins are determined mainly by the structure of the interfaces between the domains."
192,"freedom evolves",115283,"Freedom Evolves","Daniel Dennett's latest book _Freedom Evolves_ continues the themes that havebecome his trademark in previous titles such as _Consciousness Explained_ and_Darwin's Dangerous Idea_. His task is to give a thorough account of how we--and our minds--evolved and to calm fears that such an account presents athreat to the concept of free will.In one of the most arresting and important chapters in the book, Dennett laysbare several common misconceptions about determinism and introduces a toymodel which demonstrates how simple, mindlessly deterministic automata appearto make rational 'choices' to avoid harm in their limited environment. Dennettclaims that misunderstanding of determinism is still prevalent amongscientists and philosophers who subsequently misrepresent his views as theycontinue to resist a materialistic treatment of mind. Their fear is that if weshould ever be revealed to be 'mere machines' this will bring with it a deathsentence to consciousness and free-will. Such fears resist Dennett's argumentas wrong and an insult to our sense of human dignity. After carefullyaddressing those fears, Dennett goes on to show how we humans can be both acreation of and a creator of culture; arguing that we are of course a speciesof animal but the emergence of human culture is a major innovation inevolutionary history providing our species with new tools to use, new topicsto think about and new perspectives to think from.What makes Dennett such an unforgettably stimulating philosopher is not justthe breadth of his inter-disciplinary knowledge or his boldness andoriginality, it is that--knowing how difficult it is to get people to acceptcounter-intuitive ideas--he helps the reader visualise hismaterialistic/naturalistic world-view. There is undoubtedly still work to doto reconcile the philosophical implications of Darwinian materialism and whatmakes Dennett genuinely important is that he is set on trying to bring ourprecious values, including the notion of freedom, into line with Darwin andnew found scientific discoveries.He is encouraging us to drop the self-image we inherited from Christianity andthe Western philosophical tradition with all its argument about a specialextra added ingredient called consciousness that is unique to humans. Sure wehave consciousness, but there's no magic in it, says Dennett. What we need,what Dennett is offering us, is a new improved self-image. Just because thereisn't a self to be found sitting inside our brains looking out into the worldand making decisions doesn't mean the self is an illusion.There are other, better ways to think about the self, he stresses. He alsoargues that even though we are made of tiny mindless little robots that areoblivious to our hopes and needs, there's no shame in that and no reason foralarm. What we are made of and what we can hope and strive for are differentthings. _Freedom Evolves_ is the culmination of three decades worth ofresearch. --_Larry Brown_"
193,"localization from mere connectivity",115371,"Localization From Mere Connectivity","It is often useful to know the geographic positions of nodes in a communications network, but adding GPS receivers or other sophisticated sensors to every node can be expensive. We present an algorithm that uses connectivity information---  who is within communications range of whom---to derive the locations of the nodes in the network. The method can take advantage of additional information, such as estimated distances between neighbors or known positions for certain anchor nodes, if it is..."
194,"multiinterval discretization of continuousvalued attributes for classification learning",115458,"Multi-interval discretization of continuous-valued attributes for classification learning","Since most real-world applications of classification learning involve continuous-valued attributes, properly addressing the discretization process is an important problem. This paper addresses the use of the entropy minimization heuristic for discretizing the range of a continuous-valued attribute into multiple intervals. We briefly present theoretical evidence for the appropriateness of this heuristic for use in the binary discretization algorithm used in ID3, C4, CART, and other learning algorithms. The results serve to justify extending the algorithm to derive multiple intervals. We formally derive a criterion based on the minimum description length principle for deciding the partitioning of intervals. We demonstrate via empirical evaluation on several real-world data sets that better decision trees are obtained using the new multi-interval algorithm."
195,"a new approach to linear filtering and prediction problems",115575,"A New Approach to Linear Filtering and Prediction Problems","The classical filtering and prediction problem is re-examined using the Bode- Shannon representation of random processes and the “state transition” method of analysis of dynamic systems.  New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite- memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error.  From the solution of this equation the coefficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix."
196,"induction of decision trees",115578,"Induction of Decision Trees","The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions."
197,"graphical models exponential families and variational inference",115613,"Graphical models, exponential families, and variational inference","The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances — including the key problems of computing marginals and modes of probability distributions — are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide variety of algorithms — among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations — can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models."
198,"regression shrinkage and selection via the lasso",115640,"Regression shrinkage and selection via the lasso","We propose a new method for estimation in linear models. The &#034;lasso&#034; minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly zero and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described. Keywords: regression, subset selection, shrinkage, quadratic programming. 1 Introduction Consider the usual regression situation: we h..."
199,"a view of the em algorithm that justifies incremental sparse and other variants",115652,"A view of the {EM} algorithm that justifies incremental, sparse, and other variants",". The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible. 1. Introduction The Expectation-Maximization (EM) algorithm finds maximum likelihood parameter estimates in problems where some variables were unobserved. Special cases of the algorithm date back several dec..."
200,"classification and regression trees",115676,"Classification and Regression Trees","{The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.}"
201,"controlling the false discovery rate a practical and powerful approach to multiple testing",115695,"Controlling the false discovery rate: a practical and powerful approach to multiple testing","The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses - the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples."
202,"a comparison of algorithms for maximum entropy parameter estimation",115711,"A comparison of algorithms for maximum entropy parameter estimation","A comparison of algorithms for maximum entropy parameter estimation Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices."
203,"bagging predictors",115751,"Bagging Predictors","Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy. 1. Introduction A learning set of L consists of data f(y n ; x n ), n = 1; : : : ; Ng where the y's are either class labels or a numerical response. We have a procedure for using this learning set to form a predictor '(x; L) --- if the input is x we ..."
204,"a comparison of event models for naive bayes text classification",115777,"A Comparison of Event Models for Naive {B}ayes Text Classification","Recent approaches to text classification have used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes—providing on average a 27 % reduction in error over the multi-variate Bernoulli model at any vocabulary size."
205,"markov chain sampling methods for dirichlet process mixture models",115791,"Markov chain sampling methods for {D}irichlet process mixture models","This article reviews Markov chain methods for sampling from the posterior distribution of a Dirichlet process mixture model and presents two new classes of methods. One new approach is to make Metropolis-Hastings updates of the indicators specifying which mixture component is associated with each observation, perhaps supplemented with a partial form of Gibbs sampling. The other new approach extends Gibbs sampling for these indicators by using a set of auxiliary parameters. These methods are simple to implement and are more efficient than previous ways of handling general Dirichlet process mixture models with non-conjugate priors."
206,"variational extensions to em and multinomial pca",115830,"Variational extensions to {EM} and multinomial {PCA}","Several authors in recent years have proposed discrete analogues to principle component analysis intended to handle discrete or positive only data, for instance suited to analyzing sets of documents. Methods include non-negative matrix factorization, probabilistic latent semantic analysis, and latent Dirichlet allocation. This paperbegins with a review of the basic theory of the variational extension to the expectation-maximization algorithm, and then presents discrete component finding algorithms in that light. Experiments are conducted on both bigram word data and document bag-of-word to expose some of the subtleties of this new class of algorithms."
207,"toward alternative metrics of journal impact a comparison of download and citation data",115881,"Toward alternative metrics of journal impact: A comparison of download and citation data","We generated networks of journal relationships from citation and download data, and determined journal impact rankings from these networks using a set of social network centrality metrics. The resulting journal impact rankings were compared to the {ISI} {IF.} Results indicate that, although social network metrics and {ISI} {IF} rankings deviate moderately for citation-based journal networks, they differ considerably for journal networks derived from download data. We believe the results represent a unique aspect of general journal impact that is not captured by the {ISI} {IF.} These results furthermore raise questions regarding the validity of the {ISI} {IF} as the sole assessment of journal impact, and suggest the possibility of devising impact metrics based on usage information in general."
208,"between aesthetics and utility designing ambient information visualizations",116378,"Between aesthetics and utility: designing ambient information visualizations","Unlike traditional information visualization, ambient information visualizations reside in the environment of the user rather than on the screen of a desktop computer. Currently, most dynamic information that is displayed in public places consists of text and numbers. We argue that information visualization can be employed to make such dynamic data more useful and appealing. However, visualizations intended for non-desktop spaces will have to both provide valuable information and present an attractive addition to the environment - they must strike a balance between aesthetical appeal and usefulness. To explore this, we designed a real-time visualization of bus departure times and deployed it in a public space, with about 300 potential users. To make the presentation more visually appealing, we took inspiration from a modern abstract artist. The visualization was designed in two passes. First, we did a preliminary version that was presented to and discussed with prospective users. Based on their input, we did a final design. We discuss the lessons learned in designing this and previous ambient information visualizations, including how visual art can be used as a design constraint, and how the choice of information and the placement of the display affect the visualization."
209,"estimation and inference via bayesian simulation an introduction to markov chain monte carlo",116388,"Estimation and Inference via Bayesian Simulation: An Introduction to Markov Chain Monte Carlo","Bayesian statistics have made great strides in recent years, developing a class of methods for estimation and inference via stochastic simulation known as Markov Chain Monte Carlo (MCMC) methods. MCMC constitutes a revolution in statistical practice with effects beginning to be felt in the social sciences: models long consigned to the ""too hard"" basket are now within reach of quantitative researchers. I review the statistical pedigree of MCMC and the underlying statistical concepts. I demonstrate some of the strengths and weaknesses of MCMC and offer practical suggestions for using MCMC in social-science settings. Simple, illustrative examples include a probit model of voter turnout and a linear regression for time-series data with autoregressive disturbances. I conclude with a more challenging application, a multinomial probit model, to showcase the power of MCMC methods."
210,"a model of saliencybased visual attention for rapid scene analysis",117527,"A Model of Saliency-Based Visual Attention for Rapid Scene Analysis","A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail. Index terms: Visual ..."
211,"characteristics of consumer terminology for health information retrieval",117555,"Characteristics of consumer terminology for health information retrieval.","OBJECTIVES: As millions of consumers perform health information retrieval online, the mismatch between their terminology and the terminologies of the information sources could become a major barrier to successful retrievals. To address this problem, we studied the characteristics of consumer terminology for health information retrieval. METHODS: Our study focused on consumer queries that were used on a consumer health service Web site and a consumer health information Web site. We analyzed data from the site-usage logs and conducted interviews with patients. RESULTS: Our findings show that consumers' information retrieval performance is very poor. There are significant mismatches at all levels (lexical, semantic and mental models) between the consumer terminology and both the information source terminology and standard medical vocabularies. CONCLUSIONS: Comprehensive terminology support on all levels is needed for consumer health information retrieval."
212,"reapproaching nearness online communication and its place in praxis",118608,"Re–approaching nearness: online communication and its place in Praxis","An interesting transposition has happened. It used to be that the farther things were, the more difficult it was to know them. Today, thanks to communication technologies, we often develop relationships with what is far at the expense of what is immediately around us. This paper explores the increased irrelevancy that the near acquires through our use of online technologies. But by proposing a model of praxis that incorporates our actions online as well as offline, this paper also argues that online technologies can play an important part in bringing the epistemologically far near to us, and making the physically near relevant again."
213,"genome evolution in yeasts",118690,"Genome evolution in yeasts.","Identifying the mechanisms of eukaryotic genome evolution by comparative genomics is often complicated by the multiplicity of events that have taken place throughout the history of individual lineages, leaving only distorted and superimposed traces in the genome of each living organism. The hemiascomycete yeasts, with their compact genomes, similar lifestyle and distinct sexual and physiological properties, provide a unique opportunity to explore such mechanisms. We present here the complete, assembled genome sequences of four yeast species, selected to represent a broad evolutionary range within a single eukaryotic phylum, that after analysis proved to be molecularly as diverse as the entire phylum of chordates. A total of approximately 24,200 novel genes were identified, the translation products of which were classified together with Saccharomyces cerevisiae proteins into about 4,700 families, forming the basis for interspecific comparisons. Analysis of chromosome maps and genome redundancies reveal that the different yeast lineages have evolved through a marked interplay between several distinct molecular mechanisms, including tandem gene repeat formation, segmental duplication, a massive genome duplication and extensive gene loss."
214,"a memoryefficient dynamic programming algorithm for optimal alignment of a sequence to an rna secondary structure",118691,"A memory-efficient dynamic programming algorithm for optimal alignment of a sequence to an RNA secondary structure.","Abstract Background Covariance models (CMs) are probabilistic models of RNA secondary structure, analogous to profile hidden Markov models of linear sequence. The dynamic programming algorithm for aligning a CM to an RNA sequence of length N is O(N3) in memory. This is only practical for small RNAs. Results I describe a divide and conquer variant of the alignment algorithm that is analogous to memory-efficient Myers/Miller dynamic programming algorithms for linear sequence alignment. The new algorithm has an O(N2 log N) memory complexity, at the expense of a small constant factor in time. Conclusions Optimal ribosomal RNA structural alignments that previously required up to 150 GB of memory now require less than 270 MB."
215,"noncoding rna genes",118692,"Noncoding RNA genes.","Some genes produce RNAs that are functional instead of encoding proteins. Noncoding RNA genes are surprisingly numerous. Recently, active research areas include small nucleolar RNAs, antisense riboregulator RNAs, and RNAs involved in X-dosage compensation. Genome sequences and new algorithms have begun to make systematic computational screens for noncoding RNA genes possible."
216,"rna sequence analysis using covariance models",118693,"RNA sequence analysis using covariance models.","We describe a general approach to several RNA sequence analysis problems using probabilistic models that flexibly describe the secondary structure and primary sequence consensus of an RNA sequence family. We call these models covariance models'. A covariance model of tRNA sequences is an extremely sensitive and discriminative tool for searching for additional tRNAs and tRNA-related sequences in sequence databases. A model can be built automatically from an existing sequence alignment. We also describe an algorithm for learning a model and hence a consensus secondary structure from initially unaligned example sequences and no prior structural information. Models trained on unaligned tRNA examples correctly predict tRNA scondary structure and produce high-quality multiple alignments. The approach may be applied to any family of small RNA sequences. 10.1093/nar/22.11.2079"
217,"design of multistable rna molecules",118699,"Design of multistable RNA molecules.","We show that the problem of designing RNA sequences that can fold into multiple stable secondary structures can be transformed into a combinatorial optimization problem that can be solved by means of simple heuristics. Hence it is feasible to design RNA switches with prescribed structural alternatives. We discuss the theoretical background and present an efficient tool that allows the design of various types of switches. We argue that both the general properties of the sequence structure map of RNA secondary structures and the ease with which our design tool finds bistable RNAs strongly indicates that RNA switches are easily accessible in evolution. Thus conformational switches are yet another function for which RNA can be employed."
218,"prediction of locally stable rna secondary structures for genomewide surveys",118725,"Prediction of locally stable RNA secondary structures for genome-wide surveys.","MOTIVATION: Recently novel classes of functional RNAs, most prominently the miRNAs have been discovered, strongly suggesting that further types of functional RNAs are still hidden in the recently completed genomic DNA sequences. Only few techniques are known, however, to survey genomes for such RNA genes. When sufficiently similar sequences are not available for comparative approaches the only known remedy is to search directly for structural features. RESULTS: We present here efficient algorithms for computing locally stable RNA structures at genome-wide scales. Both the minimum energy structure and the complete matrix of base pairing probabilities can be computed in theta(N x L2) time and theta(N + L2) memory in terms of the length N of the genome and the size L of the largest secondary structure motifs of interest. In practice, the 100 Mb of the complete genome of Caenorhabditis elegans can be folded within about half a day on a modern PC with a search depth of L = 100. This is sufficient example for a survey for miRNAs. AVAILABILITY: The software described in this contribution will be available for download at http://www.tbi.univie.ac.at/~ivo/RNA/ as part of the Vienna RNA Package."
219,"largescale transcriptional activity in chromosomes and",118737,"Large-Scale Transcriptional Activity in Chromosomes 21 and 22","The sequences of the human chromosomes 21 and 22 indicate that there are approximately 770 well-characterized and predicted genes. In this study, empirically derived maps identifying active areas of RNA transcription on these chromosomes have been constructed with the use of cytosolic polyadenylated RNA obtained from 11 human cell lines. Oligonucleotide arrays containing probes spaced on average every 35 base pairs along these chromosomes were used. When compared with the sequence annotations available for these chromosomes, it is noted that as much as an order of magnitude more of the genomic sequence is transcribed than accounted for by the predicted and characterized exons."
220,"sequencing and comparison of yeast species to identify genes and regulatory elements",118738,"Sequencing and comparison of yeast species to identify genes and regulatory elements.","{Identifying the functional elements encoded in a genome is one of the principal challenges in modern biology. Comparative genomics should offer a powerful, general approach. Here, we present a comparative analysis of the yeast Saccharomyces cerevisiae based on high-quality draft sequences of three related species (S. paradoxus, S. mikatae and S. bayanus). We first aligned the genomes and characterized their evolution, defining the regions and mechanisms of change. We then developed methods for direct identification of genes and regulatory motifs. The gene analysis yielded a major revision to the yeast gene catalogue, affecting approximately 15\% of all genes and reducing the total count by about 500 genes. The motif analysis automatically identified 72 genome-wide elements, including most known regulatory motifs and numerous new motifs. We inferred a putative function for most of these motifs, and provided insights into their combinatorial interactions. The results have implications for genome analysis of diverse organisms, including the human.}"
221,"proof and evolutionary analysis of ancient genome duplication in the yeast saccharomyces cerevisiae",118739,"Proof and evolutionary analysis of ancient genome duplication in the yeast Saccharomyces cerevisiae","Whole-genome duplication followed by massive gene loss and specialization has long been postulated as a powerful mechanism of evolutionary innovation. Recently, it has become possible to test this notion by searching complete genome sequence for signs of ancient duplication. Here, we show that the yeast Saccharomyces cerevisiae arose from ancient whole-genome duplication, by sequencing and analysing Kluyveromyces waltii, a related yeast species that diverged before the duplication. The two genomes are related by a 1:2 mapping, with each region of K. waltii corresponding to two regions of S. cerevisiae, as expected for whole-genome duplication. This resolves the long-standing controversy on the ancestry of the yeast genome, and makes it possible to study the fate of duplicated genes directly. Strikingly, 95% of cases of accelerated evolution involve only one member of a gene pair, providing strong support for a specific model of evolution, and allowing us to distinguish ancestral and derived functions."
222,"a computational screen for methylation guide snornas in yeast",118757,"A computational screen for methylation guide snoRNAs in yeast.","Small nucleolar RNAs (snoRNAs) are required for ribose 2'-O-methylation of eukaryotic ribosomal RNA. Many of the genes for this snoRNA family have remained unidentified in Saccharomyces cerevisiae, despite the availability of a complete genome sequence. Probabilistic modeling methods akin to those used in speech recognition and computational linguistics were used to computationally screen the yeast genome and identify 22 methylation guide snoRNAs, snR50 to snR71. Gene disruptions and other experimental characterization confirmed their methylation guide function. In total, 51 of the 55 ribose methylated sites in yeast ribosomal RNA were assigned to 41 different guide snoRNAs."
223,"rna pseudoknot prediction in energybased models",118759,"RNA Pseudoknot Prediction in Energy-Based Models","RNA molecules are sequences of nucleotides that serve as more than mere intermediaries between DNA and proteins, e.g., as catalytic molecules. Computational prediction of RNA secondary structure is among the few structure prediction problems that can be solved satisfactorily in polynomial time. Most work has been done to predict structures that do not contain pseudoknots. Allowing pseudoknots introduces modeling and computational problems. In this paper we consider the problem of predicting RNA secondary structures with pseudoknots based on free energy minimization. We first give a brief comparison of energy-based methods for predicting RNA secondary structures with pseudoknots. We then prove that the general problem of predicting RNA secondary structures containing pseudoknots is NP complete for a large class of reasonable models of pseudoknots."
224,"using an rna secondary structure partition function to determine confidence in base pairs predicted by free energy minimization",118767,"Using an {RNA} secondary structure partition function to determine confidence in base pairs predicted by free energy minimization","A partition function calculation for RNA secondary structure is presented that uses a current set of nearest neighbor parameters for conformational free energy at 37 degrees C, including coaxial stacking. For a diverse database of RNA sequences, base pairs in the predicted minimum free energy structure that are predicted by the partition function to have high base pairing probability have a significantly higher positive predictive value for known base pairs. For example, the average positive predictive value, 65.8%, is increased to 91.0% when only base pairs with probability of 0.99 or above are considered. The quality of base pair predictions can also be increased by the addition of experimentally determined constraints, including enzymatic cleavage, flavin mono-nucleotide cleavage, and chemical modification. Predicted secondary structures can be color annotated to demonstrate pairs with high probability that are therefore well determined as compared to base pairs with lower probability of pairing."
225,"fast algorithm for predicting the secondary structure of singlestranded rna",118786,"Fast algorithm for predicting the secondary structure of single-stranded RNA.","A computer method is presented for finding the most stable secondary structures in long single-stranded RNAs. It is 1-2 orders of magnitude faster than existing codes. The time required for its application increases as N3 for a chain N nucleotides long. As many as 1000 nucleotides can be searched in a single run. The approach is systematic and builds an optimal structure in a straightforward inductive procedure based on an exact mathematical algorithm. Two simple half-matrices are constructed and the best folded form is read directly from the second matrix by a simple back-tracking procedure. The program utilizes published values for base-pairing energies to compute one structure with the lowest free energy."
226,"secondary structure alone is generally not statistically significant for the detection of noncoding rnas",118801,"Secondary structure alone is generally not statistically significant for the detection of noncoding RNAs.","MOTIVATION: Several results in the literature suggest that biologically interesting RNAs have secondary structures that are more stable than expected by chance. Based on these observations, we developed a scanning algorithm for detecting noncoding RNA genes in genome sequences, using a fully probabilistic version of the Zuker minimum-energy folding algorithm. RESULTS: Preliminary results were encouraging, but certain anomalies led us to do a carefully controlled investigation of this class of methods. Ultimately, our results argue that for the probabilistic model there is indeed a statistical effect, but it comes mostly from local base-composition bias and not from RNA secondary structure. For the thermodynamic implementation (which evaluates statistical significance by doing Monte Carlo shuffling in fixed-length sequence windows, thus eliminating the base-composition effect) the signals for noncoding RNAs are still usually indistinguishable from noise, especially when certain statistical artifacts resulting from local base-composition inhomogeneity are taken into account. We conclude that although a distinct, stable secondary structure is undoubtedly important in most noncoding RNAs, the stability of most noncoding RNA secondary structures is not sufficiently different from the predicted stability of a random sequence to be useful as a general genefinding approach."
227,"noncoding rna gene detection using comparative sequence analysis",118802,"Noncoding RNA gene detection using comparative sequence analysis.","BACKGROUND: Noncoding RNA genes produce transcripts that exert their function without ever producing proteins. Noncoding RNA gene sequences do not have strong statistical signals, unlike protein coding genes. A reliable general purpose computational genefinder for noncoding RNA genes has been elusive. RESULTS: We describe a comparative sequence analysis algorithm for detecting novel structural RNA genes. The key idea is to test the pattern of substitutions observed in a pairwise alignment of two homologous sequences. A conserved coding region tends to show a pattern of synonymous substitutions, whereas a conserved structural RNA tends to show a pattern of compensatory mutations consistent with some base-paired secondary structure. We formalize this intuition using three probabilistic ""pair-grammars"": a pair stochastic context free grammar modeling alignments constrained by structural RNA evolution, a pair hidden Markov model modeling alignments constrained by coding sequence evolution, and a pair hidden Markov model modeling a null hypothesis of position-independent evolution. Given an input pairwise sequence alignment (e.g. from a BLASTN comparison of two related genomes) we classify the alignment into the coding, RNA, or null class according to the posterior probability of each class. CONCLUSIONS: We have implemented this approach as a program, QRNA, which we consider to be a prototype structural noncoding RNA genefinder. Tests suggest that this approach detects noncoding RNA genes with a fair degree of reliability."
228,"computational identification of noncoding rnas in e coli by comparative genomics",118803,"Computational identification of noncoding RNAs in E. coli by comparative genomics.","{aHoward} Hughes Medical Institute and Department of Genetics, Washington University School of Medicine, Saint Louis, {MO} 63110, {USA} Some genes produce noncoding transcripts that function directly as structural, regulatory, or even catalytic {RNAs} [1] and [2]. Unlike protein-coding genes, which can be detected as open reading frames with distinctive statistical biases, noncoding {RNA} {(ncRNA)} gene sequences have no obvious inherent statistical biases [3]. Thus, genome sequence analyses reveal novel protein-coding genes, but any novel {ncRNA} genes remain invisible. Here, we describe a computational comparative genomic screen for {ncRNA} genes. The key idea is to distinguish conserved {RNA} secondary structures from a background of other conserved sequences using probabilistic models of expected mutational patterns in pairwise sequence alignments. We report the first whole-genome screen for {ncRNA} genes done with this method, in which we applied it to the “intergenic” spacers of Escherichia coli using comparative sequence data from four related bacteria. Starting from {\\textgreater}23,000 conserved interspecies pairwise alignments, the screen predicted 275 candidate structural {RNA} loci. A sample of 49 candidate loci was assayed experimentally. At least 11 loci expressed small, apparently noncoding {RNA} transcripts of unknown function. Our computational approach may be used to discover structural {ncRNA} genes in any genome for which appropriate comparative genome sequence data are available. The key idea of our approach for distinguishing a small number of structural {RNAs} from a large background of other conserved sequences is to exploit a distinctive pattern of mutation, as opposed to simple conservation, as sketched in Figure 1. A conserved structural {RNA} tends to show a pattern of compensatory mutations consistent with some base-paired secondary structure. A conserved coding region tends to show a pattern of synonymous codon substitutions [9] and [10]. Other types of conserved regions may be approximated by a “null hypothesis” that mutations occur position independently, with no special pattern. Thus, we should be able to use position-specific mutational models to separate conserved regions into three types: probable structural {RNAs,} probable coding regions, and probable “other” sequences. Badger and Olsen [9] described a two-model protein gene-finding approach that distinguishes coding alignments from conserved noncoding alignments. We extended this idea by including a third model of {RNA} structure evolution, by allowing the input alignments to be gapped, and by using full Bayesian probabilistic models. Our three probabilistic models {(RNA,} {COD,} and {IND)} are stochastic “pair grammars”: a pair stochastic context-free grammar {(SCFG)} as the {RNA} model, and pair hidden Markov models for both {COD} and {IND} [11]. The secondary structure model in the {pair-SCFG} is a full probabilistic analog of the Zuker {MFOLD} algorithm [12], including terms for base stacking, loop lengths, etc. that have been trained on a database of {tRNA} and {rRNA} secondary structures [3]. The evolutionary distance component of all three models is derived directly (for {COD)} or indirectly (for {RNA} and {IND)} from a single choice of amino acid substitution matrix {(BLOSUM62)} [13] and [14]. It is essential that all three models are at the same evolutionary distance. Otherwise, models might distinguish alignments solely based on their level of conservation rather than on the pattern of mutation. Given an input pairwise sequence alignment (for instance, from a {BLASTN} comparison of two related genomes), we score the alignment with each of the three models. The scoring algorithms are all dynamic programming algorithms; the rate-limiting one is the {O(L2)} time, {O(L3)} memory {pair-SCFG} alignment algorithm used to score the {RNA} model, which must be done as an {SCFG} Inside algorithm [11], summing over all possible {RNA} secondary structures. The scores are log likelihoods that are then used to calculate a final log odds score for the {RNA} model compared to the other two models. {Non-RNA} alignments get negative scores; increasingly positive scores indicate increasingly strong comparative evidence that the alignment contains a conserved structural {RNA.} A satisfactory mathematical description of the approach is beyond the scope of this paper and will be published elsewhere {(E.R.} and {S.R.E.,} submitted). We have implemented the complete approach in a program, {QRNA} [15]. The goal of this brief communication is to describe the first whole-genome screen we have done with {QRNA.} We used {QRNA} to screen the complete genome of Escherichia coli K12 {MG1655} (version M52) [16]. We chose E. coli because there is a complete genome sequence, it is a simple model genetic system in which we could readily test our predictions experimentally, and there is extensive comparative sequence coverage from at least 15 related species [17]. We chose four related enterobacterial genomes for comparison: Salmonella typhi [18], S. paratyphi A [19], S. enteriditis [20], and Klebisella pneumoniae [19]. We started with all annotated intergenic sequences with a length of ≥50 nt in E. coli according to the University of Wisconsin's annotation of 115 {ncRNA} genes and 4290 coding {ORFs} [21]. This gave 2367 intergenic sequences, totaling about 500 kb. The average sequence length was 211 nt; the longest was 1729 nt. Four known {ncRNA} genes {(csrB,} {oxyS,} {micF,} and {rprA)} [2] and [22] were unannotated and were left in the dataset as positive controls. Each “intergenic” region was used as a query to search the four comparative genome databases using {BLASTN} [23] and [24] (version {2.0MP-WashU/12} Feb 01, using default parameters and scoring matrix). All alignments with an E value of {\\textless}0.01, a length of ≥50 nt, and an overall identity of ≥65% were collected, giving a database of 23,674 pairwise {BLASTN} alignments (12,037 from S. typhi, 5,239 from S. paratyphi, 4,260 from S. enteriditis, and 2,138 from K. pneumoniae). Each pairwise alignment was analyzed with {QRNA,} in “local Viterbi” mode (where it finds a locally optimal {RNA} structure, allowing the {BLASTN} alignment to extend into other conserved sequences flanking the {RNA),} scanning the pairwise alignment in overlapping 200-nt windows and moving 50 nt at a time. All windows classified as {“RNA”} by the program with a log odds score of ≥5 bits were kept, and overlapping windows were merged. {(Annotated} {ncRNAs} in E. coli score between 5.6 and 41.1 bits. {Non-RNA} alignments rarely score above 0.) This resulted in 556 candidate {RNA} loci. All four positive controls were classified as {RNA} (5.6 for {micF,} 5.7 for {oxyS,} 10.6 for {csrB,} and 13.6 for {rprA).} The {COD} class also detected 160 candidates for conserved small {ORFs} that were not examined further. The complete screen took about 20 hr on a single {SGI} Origin200 {R10K} processor. A variety of tests of {QRNA's} performance suggest that, under the conditions above, it has a sensitivity of about 80% on known structural {RNAs} {(E.R.} and {S.R.E.,} submitted). As a test of specificity, we shuffled each of the 23,674 input alignments by aligned columns (preserving % identity, while scrambling the sequences and any position-specific mutational pattern in the alignment) and applied the same procedure, which produced 73 false-positive {“RNA”} loci with scores over 5 bits. Therefore, about 85% of our 556 candidate loci should be “true positives”, in the sense that they are not just the result of expected statistical noise. Because {QRNA} screens for conserved {RNA} secondary structure, we expected it to detect various nongenic sequences with conserved {RNA} structure, including rho-independent terminators, {rRNA} spacers, transcriptional attenuators in ribosomal protein and amino acid biosynthetic operons [25], other cis-regulatory {RNA} structures [26], and even certain repetitive elements [27] and [28]. We removed 281 loci that plausibly fell into one of these nongenic classes, leaving a total of 275 candidate loci (see Table S1 in the Supplementary material available with this article online for a list). It must also be noted that not all {ncRNA} genes conserve an intramolecular secondary structure; for example, {QRNA} does not detect {C/D} box small nucleolar {RNAs} [29] in yeast or Pyrococcus alignments. We expected these 275 loci to be a mix of {ncRNA} genes, cis-regulatory {RNA} structures, and false positives. A criterion that tends to distinguish an {ncRNA} gene from either cis-regulatory structures or false-positive signals is the expression of a distinct transcript independent of adjacent coding genes. We assayed 49 candidate loci for expression by Northern blotting (see Table S2 in the Supplementary material for a list). For 11 loci, we observed discrete small {RNA} transcripts {\\textless}400 nt {(Figure} 2). Six others showed larger products that were interpreted as coding {mRNA} transcripts. Two showed multiple discrete bands that we could not interpret. The remaining 30 loci were not expressed under these growth conditions. We cannot interpret a negative result because several known {ncRNAs} are expressed only under specific conditions (for example, {OxyS} {RNA} is expressed under oxidative stress conditions but not in normal lab growth [30]). The 11 loci that express small {RNAs} are listed in Table 1. Expression is suggestive but not entirely sufficient to define a candidate as a new {ncRNA} gene. For example, the cis-acting transcriptional attenuator of the his operon [31] is detected by {QRNA,} is flanked by a strong consensus promoter and an obvious rho-independent terminator, and has no significant coding potential (the {hisL} leader peptide is only 22 aa long); Northern analysis detects the his leader {RNA} as a distinct 170-nt transcript (data not shown). Candidates t44, tpk1, and tpk2 are directly upstream of coding genes in the same orientation and may be attenuators. We also cannot exclude the possibility that expressed candidates are protein-coding genes with small {ORFs.} For example, candidate k4 is classified as {RNA} in a Klebsiella alignment but is classified as coding in Salmonella alignments; the sequence appears to contain both a 72-aa conserved {ORF} with a reasonable translational initiation consensus and a conserved structural {RNA} motif, overlapping each other. The semantic concept of a “gene” is slippery to begin with, especially when the gene is noncoding. Therefore, although we conclude from our Northern assays that a significant number of our 275 candidate loci do indeed correspond to independent {ncRNA} genes, each individual candidate will require detailed study. We have deliberately not assigned any new E. coli gene names to our loci at this point, pending more complete experimental characterization that is ongoing in our lab. While this paper was in preparation, two groups reported exciting results of different screens for small {ncRNAs} in the intergenic regions of the E. coli genome sequence [32] and [33]. Wassarman et al. used sequence conservation coupled with microarray expression analysis and found 17 new {ncRNAs} [32]. Argaman et al. used sequence conservation coupled with promoter and rho-independent terminator prediction and found 14 new {ncRNAs} [33]. Both groups report extensive experimental characterization of the new loci. The overlap of these experimentally confirmed {ncRNA} genes with our results gives us additional confidence in {QRNA's} sensitivity. Of the 14 {RNAs} reported by Argaman et al., 10 are in our list of 275 candidate loci; of the 4 that we do not detect {(sraD,} {sraH,} {sraI,} {sraL),} 3 have scores only slightly below our 5-bit cutoff, and only {sraI} was completely missed. Of the 17 {RNAs} reported by Wassarman et al., 14 were detected by {QRNA;} of the 3 that we missed {(ryeA,} {ryhA,} and {ryjA),} 2 were just below our cutoff, and 1 {(ryeA)} was detected in the initial list of 556 {QRNA} candidates, but we mistakenly discarded it, thinking it was just a rho-independent terminator. On the other hand, only 4 out of 11 of our confirmed candidates were detected and confirmed by one of the other screens {(Table} 1), which suggests that {QRNA's} sensitivity is higher than either the Argaman et al. or the Wassarman et al. screens and that neither of these screens saturated the E. coli genome for novel {ncRNAs.} These data, though experimentally preliminary, nonetheless validate {QRNA} as a powerful and general means for identifying candidate structural {ncRNA} loci. Because we use no organism-specific information (such as promoter or terminator consensus sequences), {QRNA} will be applicable in any organism for which appropriate comparative genomic data are available. We have already anecdotally observed that signal/noise is sufficient to screen the human genome using low-pass mouse shotgun sequence coverage; for example, a {QRNA} screen of the 196-kb draft sequence of a human {BAC} {(GenBank} accession {AL357874)} spanning the cartilage hair hypoplasia {(CHH)} locus [34], using unassembled {1.7X} mouse shotgun coverage {(Mouse} Sequencing Consortium, unpublished), predicts two {ncRNA} loci (data not shown), one of which corresponds to the 265-nt {RNase} {MRP} {ncRNA} locus that has recently been implicated as the gene responsible for {CHH} [34]. Given the recent surge in comparative genome sequencing, we will be able to screen for structural {ncRNAs} in all the major systems, including human (by comparison to mouse), Caenorhabditis elegans (via C. briggsae), Drosophila melanogaster (via D. pseudoobscura), Saccharomyces cerevisiae (via multiple other yeast genomes), and Arabidopsis thaliana (via Brassica). Supplementary material including Table S1, which lists all 275 candidate {RNA} loci ranked by {QRNA} score in bits, and Table S2, which lists only the 49 loci that were tested for expression by Northern blot, is available at http://images.cellpress.com/supmat/supmain.htm. This work was supported by the Howard Hughes Medical Institute {(HHMI),} the National Institutes of Health National Human Genome Research Institute, a Sloan Foundation postdoctoral fellowship to {E.R.,} and an {HHMI} graduate fellowship to {R.J.K.}"
229,"no evidence that mrnas have lower folding free energies than random sequences with the same dinucleotide distribution",118842,"No evidence that mRNAs have lower folding free energies than random sequences with the same dinucleotide distribution.","This work investigates whether mRNA has a lower estimated folding free energy than random sequences. The free energy estimates are calculated by the mfold program for prediction of RNA secondary structures. For a set of 46 mRNAs it is shown that the predicted free energy is not significantly different from random sequences with the same dinucleotide distribution. For random sequences with the same mononucleotide distribution it has previously been shown that the native mRNA sequences have a lower predicted free energy, which indicates a more stable structure than random sequences. However, dinucleotide content is important when assessing the significance of predicted free energy as the physical stability of RNA secondary structure is known to depend on dinucleotide base stacking energies. Even known RNA secondary structures, like tRNAs, can be shown to have predicted free energies indistinguishable from randomized sequences. This suggests that the predicted free energy is not always a good determinant for RNA folding."
230,"distributed rational decision making",119509,"Distributed rational decision making","Introduction  Automated negotiation systems with self-interested agents are becoming increasingly important. One reason for this is the technology push of a growing standardized communication infrastructure---Internet, WWW, NII, EDI, KQML, FIPA, Concordia, Voyager, Odyssey, Telescript, Java, etc---over which separately designed agents belonging to different organizations can interact in an open environment in realtime and safely carry out transactions. The second reason is strong application pull for computer support for negotiation at the operative decision making level. For example, we are witnessing the advent of small transaction electronic commerce on the Internet for purchasing goods, information, and communication bandwidth [29]. There is also an industrial trend toward virtual enterprises: dynamic alliances of small, agile enterprises which together can take advantage of economies of scale when available (e.g., respond to mor"
231,"a framework for understanding and classifying ontology applications",120137,"A {F}ramework for {U}nderstanding and {C}lassifying {O}ntology {A}pplications","For 1 ontologies to be cost-effectively deployed, we require a clear understanding of the various ways that ontologies are being used today. To achieve this end, we present a framework for understanding and classifying ontology applications. We identify four main categories of ontology applications: 1) neutral authoring, 2) ontology as specification, 3) common access to information, and 4) ontology-based search. In each category, we identify specific ontology application scenarios. For each, we indicate their intended purpose, the role of the ontology, the supporting technologies, who the principal actors are and what they do. We illuminate the similarities and differences between scenarios. We draw on work from other communities, such as software developers and standards organizations. We use a relatively broad definition of ‘ontology’, to show that much of the work being done by those communities may be viewed as practical applications of ontologies. The common thread is the need for sharing the meaning of terms in a given domain, which is a central role of ontologies. An additional aim of this paper is to draw attention to common goals and supporting technologies of these relatively distinct communities to facilitate closer cooperation and faster progress 2. 1"
232,"the equilibrium partition function and base pair binding probabilities for rna secondary structure",120141,"The equilibrium partition function and base pair binding probabilities for RNA secondary structure","A novel application of dynamic programming to the folding problem for RNA enables one to calculate the full equilibrium partition function for secondary structure and the probabilities of various substructures. In particular, both the partition function and the probabilities of all base pairs are computed by a recursive scheme of polynomial order N3 in the sequence length N. The temperature dependence of the partition function gives information about melting behavior for the secondary structure. The pair binding probabilities, the computation of which depends on the partition function, are visually summarized in a ""box matrix"" display and this provides a useful tool for examining the full ensemble of probable alternative equilibrium structures. The calculation of this ensemble representation allows a proper application and assessment of the predictive power of the secondary structure method, and yields important information on alternatives and intermediates in addition to local information about base pair opening and slippage. The results are illustrated for representative tRNA, 5S RNA, and self-replicating and self-splicing RNA molecules, and allow a direct comparison with enzymatic structure probes. The effect of changes in the thermodynamic parameters on the equilibrium ensemble provides a further sensitivity check to the predictions."
233,"features of similarity",121025,"Features of Similarity","Questions the metric and dimensional assumptions that underlie the geometric representation of similarity on both theoretical and empirical grounds. A new set-theoretical approach to similarity is developed in which objects are represented as collections of features and similarity is described as a feature-matching process. Specifically, a set of qualitative assumptions is shown to imply the contrast model, which expresses the similarity between objects as a linear combination of the measures of their common and distinctive features. Several predictions of the contrast model are tested in studies of similarity with both semantic and perceptual stimuli. The model is used to uncover, analyze, and explain a variety of empirical phenomena such as the role of common and distinctive features, the relations between judgments of similarity and difference, the presence of asymmetric similarities, and the effects of context on judgments of similarity. The contrast model generalizes standard representations of similarity data in terms of clusters and trees. It is also used to analyze the relations of prototypicality and family resemblance. (39 ref) {(PsycINFO} Database Record (c) 2009 {APA,} all rights reserved)"
234,"design and evaluation of a widearea event notification service",121079,"Design and evaluation of a wide-area event notification service","The components of a loosely coupled system are typically designed to operate by generating and responding to asynchronous events. An  event notification service  is an application-independent infrastructure that supports the construction of event-based systems, whereby generators of events publish event notifications to the infrastructure and consumers of events subscribe with the infrastructure to receive relevant notifications. The two primary services that should be provided to components by the infrastructure are notification selection (i. e., determining which notifications match which subscriptions) and notification delivery (i.e., routing matching notifications from publishers to subscribers). Numerous event notification services have been developed for local-area networks, generally based on a centralized server to select and deliver event  notifications. Therefore, they suffer from an inherent inability to scale to wide-area networks, such as the Internet, where the number and physical distribution of the service's clients can quickly overwhelm a centralized solution. The critical challenge in the setting of a wide-area network is to maximize the expressiveness in the selection mechanism without sacrificing scalability in the delivery mechanism. This paper presents SIENA, an event notification service that we have designed and implemented to exhibit both expressiveness and scalability. We describe the service's interface to applications, the algorithms used by networks of servers to select and deliver event notifications, and the strategies used to optimize performance. We also present results of simulation studies that  examine the scalability and performance of the service."
235,"on powerlaw relationships of the internet topology",121098,"On Power-Law Relationships of the Internet Topology","Despite the apparent randomness of the Internet, we discover some surprisingly simple power-laws of the Internet topology. These power-laws hold for three snapshots of the Internet, between November 1997 and December 1998, despite a 45% growth of its size during that period. We show that our power-laws fit the real data very well resulting in correlation coefficients of 96% or higher.Our observations provide a novel perspective of the structure of the Internet. The power-laws describe concisely skewed distributions of graph properties such as the node outdegree. In addition, these power-laws can be used to estimate important parameters such as the average neighborhood size, and facilitate the design and the performance analysis of protocols. Furthermore, we can use them to generate and select realistic topologies for simulation purposes."
236,"peertopeer computing",121161,"Peer-to-Peer Computing","The term ""peer-to-peer refers to a class of systems and applications that employ distributed resources to perform a critical function in a decentralized manner. With the pervasive deployment of computers, P2P is increasingly receiving attention in research, product development, and investment circles. This interest ranges from enthusiasm, through hype, to disbelief in its potential. Some of the benefits of a P2P approach include: improving scalability by avoiding dependency on centralized points; eliminating the need for costly infrastructure by enabling direct communication among clients; and enabling resource aggregation. This survey reviews the field of P2P systems and applications by summarizing the key concepts and giving an overview of the most important systems. Design and implementation issues of P2P systems are analyzed in general, and then revisited for each of the case studies described in Section 6. This survey will help people understand the potential benefits of P2P in the research community and industry. For people unfamiliar with the field it provides a general overview, as well as detailed case studies. It is also intended for users, developers, and information technologies maintaining systems, in particular comparison of P2P solutions with alternative architectures and models."
237,"development and application of a metric on semantic nets",121192,"Development and application of a metric on semantic nets","Motivated by the properties of spreading activation and conceptual distance, the authors propose a metric, called distance, on the power set of nodes in a semantic net. Distance is the average minimum path length over all pairwise combinations of nodes between two subsets of nodes. Distance can be successfully used to assess the conceptual distance between sets of concepts when used on a semantic net of hierarchical relations. When other kinds of relationships, like `cause', are used, distance must be amended but then can again be effective. The judgements of distance significantly correlate with the distance judgements that people make and help to determine whether one semantic net is better or worse than another. The authors focus on the mathematical characteristics of distance that presents novel cases and interpretations. Experiments in which distance is applied to pairs of concepts and to sets of concepts in a hierarchical knowledge base show the power of hierarchical relations in representing information about the conceptual distance between concepts"
238,"a scalable content addressable network",121196,"A Scalable Content Addressable Network","Hash tables - which map ""keys"" onto ""values"" - are an essential building block in modern software systems. We believe a similar functionality would be equally valuable to large distributed systems. In this paper, we introduce the concept of a Content-Addressable Network (CAN) as a distributed infrastructure that provides hash table-like functionality on Internet-like scales. The CAN is scalable, fault-tolerant and completely self-organizing, and we demonstrate its scalability, robustness and low-latency properties through simulation."
239,"flocks herds and schools a distributed behavioral model",121199,"Flocks, Herds, and Schools: A Distributed Behavioral Model","The aggregate motion of a flock of birds, a herd of land animals, or a school of fish is a beautiful and familiar part of the natural world. But this type of complex motion is rarely seen in computer animation. This paper explores an approach based on simulation as an alternative to scripting the paths of each bird individually. The simulated flock is an elaboration of a particle system, with the simulated birds being the particles. The aggregate motion of the simulated flock is created by a distributed behavioral model much like that at work in a natural flock; the birds choose their own course. Each simulated bird is implemented as an independent actor that navigates according to its local perception of the dynamic environment, the laws of simulated physics that rule its motion, and a set of behaviors programmed into it by the &#034;animator.&#034; The aggregate motion of the simulated flock is the result of the dense interaction of the relatively simple behaviors of the individual simulated b..."
240,"an analysis of internet content delivery systems",121209,"An Analysis of Internet Content Delivery Systems","In the span of only a few years, the Internet has experienced an astronomical increase in the use of specialized content delivery systems, such as content delivery networks and peer-to-peer file sharing systems. Therefore, an understanding of content delivery on the Internet now requires a detailed understanding of how these systems are used in practice. This paper examines content delivery from the point of view of four content delivery systems: HTTP web traffic, the Akamai content delivery network, and Kazaa and Gnutella peer-to-peer file sharing traffic. We collected a trace of all incoming and outgoing network traffic at the University of Washington, a large university with over 60,000 students, faculty, and staff. From this trace, we isolated and characterized traffic belonging to each of these four delivery classes. Our results (1) quantify the rapidly increasing importance of new content delivery systems, particularly peerto-peer networks, (2) characterize the behavior of these systems from the perspectives of clients, objects, and servers, and (3) derive implications for caching in these systems. 1"
241,"visualization of search results a comparative evaluation of text d and d interfaces",121542,"Visualization of {S}earch {R}esults: {A} {C}omparative {E}valuation of {T}ext, 2{D}, and 3{D} {I}nterfaces","Although there have been many prototypes of visualization in support of information retrieval, there has been little systematic evaluation that distinguishes the benefits of the visualization per se from that of various accompanying features. The current study focuses on such an evaluation of NIRVE, a tool that supports visualization of search results. Insofar as possible, functionally equivalent 3D, 2D, and text versions of NIRVE were implemented. Nine novices and six professional users.."
242,"a rational design process how and why to fake it",121580,"A rational design process: how and why to fake it","Software Engineers have been searching for the ideal software development process: a process in which programs are derived from specifications in the same way that lemmas and theorems are derived from axioms in published proofs. After explaining why we can never achieve it, this paper describes such a process. The process is described in terms of a sequence of documents that should be produced on the way to producing the software. We show that such documents can serve several purposes. They provide a basis for preliminary design review, serve as reference material during the coding, and guide the maintenance programmer in his work. We discuss how these documents can be constructed using the same principles that should guide the software design. The resulting documentation is worth much more than the ""afterthought"" documentation that is usually produced. If we take the care to keep all of the documents up-to-date, we can create the appearance of a fully rational design process."
243,"principles of categorization",121608,"Principles of {C}ategorization","The chapter is divided into five parts. The first part presents the two general principles that are proposed to underlie categorization systems. The second part shows the way in which these principles appear to result in a basic and primary level of categorization in the levels of abstraction in a taxonomy. It is essentially a summary of the research already reported on basic level objects (Rosch et al., 1976). Thus the second section may be omitted by the reader already sufficiently familiar with that material. The third part relates the principles of categorization to the formation of prototypes in those categories that are at the same level of abstraction in a taxonomy. In particular, this section attempts to clarify the operational concept of prototypicality and to separate that concept from claims concerning the role of prototypes in cognitive processing, representation, and learning for which there is little evidence. The fourth part presents two issues that are problematical for the abstract principles of categorization stated in Part I: (1) the relation of context to basic level objects and prototypes; and (2) assumptions about the nature of the attributes of real-world objects that underlie the claim that there is structure in the world. The fifth part is a report of initial attempts to base an analysis of the attributes, functions, and contexts of objects on a consideration of objects as props in culturally defined events."
244,"ontologies for enterprise knowledge management",121779,"Ontologies for {E}nterprise {K}nowledge {M}anagement","Ontologies are a key technology for enabling semantics-driven knowledge processing, and it is widely accepted that the next generation of knowledge management system will rely on conceptual models in the form of ontologies. Unfortunately, the development of real-world enterprise-wide ontology-based knowledge management systems is still in an early stage. The authors present an integrated enterprise knowledge management architecture developed within the Ontologging project dealing with several challenges related to applying ontologies in real-world environments. They focus on two important ontology management problems--namely, supporting multiple ontologies and managing ontology evolution."
245,"probability theory the logic of science",121870,"Probability Theory: The Logic of Science","{Going beyond the conventional mathematics of probability theory, this study views the subject in a wider context. It discusses new results, along with applications of probability theory to a variety of problems. The book contains many exercises and is suitable for use as a textbook on graduate-level courses involving data analysis. Aimed at readers already familiar with applied mathematics at an advanced undergraduate level or higher, it is of interest to scientists concerned with inference from incomplete information.}"
246,"cvs ii parallelizing software development",122299,"{CVS} {II}: Parallelizing Software Development","The program described in this paper fills a need in the UNIX community for a freely available tool to manage software revision and release control in a multi-developer,multi-directory,multi-group environment. This tool also addresses the increasing need for tracking third-party vendor source distributions while trying to maintain local modifications to earlier releases. 1. Background  In large software development projects, it is usually necessary for more than one software developer to be..."
247,"computability and logic",123095,"Computability and Logic","Computability and Logic has become a classic because of its accessibility to students without a mathematical background and because it covers not simply the staple topics of an intermediate logic course, such as Godel's incompleteness theorems, but also a large number of optional topics, from Turing's theory of computability to Ramsey's theorem. Including a selection of exercises, adjusted for this edition, at the end of each chapter, it offers a new and simpler treatment of the representability of recursive functions, a traditional stumbling block for students on the way to the Godel incompleteness theorems."
248,"dynamic logic",123644,"Dynamic Logic","Dynamic Logic (DL) is a formal system for reasoning about programs. Traditionally, this has meant formalizing correctness specifications and proving rigorously that those specifications are met by a particular program. Other activities fall into this category as well: determining the equivalence of programs, comparing the expressive power of various programming constructs, synthesizing programs from specifications, etc. Formal systems too numerous to mention have been proposed for these purposes, each with its own peculiarities. DL can be described as a blend of three complementary classical ingredients: first-order predicate logic, modal logic, and the algebra of regular events. These components merge to form a system of remarkable unity that is theoretically rich as well as practical. The name Dynamic Logic emphasizes the principal feature distinguishing it from classical predicate logic. In the latter, truth is static: the truth value of a formula ' is determined by a valuation of its free variables over some structure. The valuation and the truth value of ' it induces are regarded as immutable; there is no formalism relating them to any other valuations or truth values. In Dynamic Logic, there are explicit syntactic constructs called programs whose main role is to change the values of variables, thereby changing the truth values of formulas. For example, the program x: = x + 1 over the natural numbers changes the truth value of the formula x is even. Such changes occur on a metalogical level in classical predicate logic. For example, in Tarski's definition of truth of a formula, if u: fx; y; : : : g! N is a valuation of variables over the natural numbers N, then the formula 9x x 2 = y is defined to be true under the valuation u iff there exists an a 2 N such that the formula x"
249,"introduction to metamathematics",123866,"Introduction to Metamathematics","Stephen Cole Kleene was one of the greatest logicians of the twentieth century and this book is the influential textbook he wrote to teach the subject to the next generation. It was first published in 1952, some twenty years after the publication of Gödel's paper on the incompleteness of arithmetic, which marked, if not the beginning of modern logic, at least a turning point after which “nothing was ever the same.” Kleene was an important figure in logic, and lived a long full life of scholarship and teaching. The 1930s was a time of creativity and ferment in the subject, when the notion of “computable” moved from the realm of philosophical speculation to the realm of science. This was accomplished by the work of Kurt Göde1, Alan Turing, and Alonzo Church, who gave three apparently different precise definitions of “computable”. When they all turned out to be equivalent, there was a collective realization that this was indeed the “right notion”. Kleene played a key role in this process. One could say that he was “there at the beginning” of modern logic. He showed the equivalence of lambda calculus with Turing machines and with Gödel's recursion equations, and developed the modern machinery of partial recursive functions. This textbook played an invaluable part in educating the logicians of the present. It played an important role in their own logical education."
250,"naming and necessity",123906,"Naming and Necessity","{<p>  If there is such a thing as essential reading in metaphysics or in philosophy of language, this is it.  </p><p>  Ever since the publication of its original version, <i>Naming and Necessity</i> has had great and increasing influence. It redirected philosophical attention to neglected questions of natural and metaphysical necessity and to the connections between these and theories of reference, in particular of naming, and of identity. From a critique of the dominant tendency to assimilate names to descriptions and more generally to treat their reference as a function of their Fregean sense, surprisingly deep and widespread consequences may be drawn. The largely discredited distinction between accidental and essential properties, both of individual things (including people) and of kinds of things, is revived. So is a consequent view of science as what seeks out the essences of natural kinds. Traditional objections to such views are dealt with by sharpening distinctions between epistemic and metaphysical necessity; in particular by the startling admission of necessary a posteriori truths. From these, in particular from identity statements using rigid designators whether of things or of kinds, further remarkable consequences are drawn for the natures of things, of people, and of kinds; strong objections follow, for example to identity versions of materialism as a theory of the mind.  </p><p>  This seminal work, to which today's thriving essentialist metaphysics largely owes its impetus, is here published with a substantial new Preface by the author.  </p>}"
251,"adaptation in natural and artificial systems an introductory analysis with applications to biology control and artificial intelligence",125979,"Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence","{John Holland's <i>Adaptation in Natural and Artificial Systems</i> is one of the classics in the field of complex adaptive systems. Holland is known as the father of genetic algorithms and classifier systems and in this tome he describes the theory behind these algorithms. Drawing on ideas from the fields of biology and economics, he shows how computer programs can evolve. The book contains mathematical proofs that are accessible only to those with strong backgrounds in engineering or science.} {Genetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic theory to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. <I>Adaptation in Natural and Artificial Systems</I> is the book that initiated this field of study, presenting the theoretical foundations and exploring applications.<br /> <br /> In its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. In this now classic work, Holland presents a mathematical model that allows for the nonlinearity of such complex interactions. He demonstrates the model's universality by applying it to economics, physiological psychology, game theory, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics.<br /> <br /> Initially applying his concepts to simply defined artificial systems with limited numbers of parameters, Holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. Along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements.<br /> <br /> John H. Holland is Professor of Psychology and Professor of Electrical Engineering and Computer Science at the University of Michigan. He is also Maxwell Professor at the Santa Fe Institute and is Director of the University of Michigan/Santa Fe Institute Advanced Research Program.}"
252,"visual explanations images and quantities evidence and narrative",126681,"Visual Explanations: Images and Quantities, Evidence and Narrative","With {{\\textless}I{\\textgreater}Visual} {Explanations{\\textless}/I{\\textgreater},} Edward R. Tufte adds a third volume to his indispensable series on information display. The ﬁrst, {{\\textless}I{\\textgreater}The} Visual Display of Quantitative {Information,{\\textless}/I{\\textgreater}} which focuses on charts and graphs that display numerical information, virtually deﬁned the ﬁeld. The second, {{\\textless}I{\\textgreater}Envisioning} {Information,{\\textless}/I{\\textgreater}} explores similar territory but with an emphasis on maps and cartography. {{\\textless}I{\\textgreater}Visual} {Explanations{\\textless}/I{\\textgreater}} centers on dynamic data―information that changes over time. {(Tufte} has described the three books as being about, respectively, ""pictures of numbers, pictures of nouns, and pictures of verbs."") {{\\textless}P{\\textgreater}} Like its predecessors, {{\\textless}I{\\textgreater}Visual} {Explanations{\\textless}/I{\\textgreater}} is both intellectually stimulating and beautiful to behold. Tufte, a self-publisher, takes extraordinary pains with design and production. The book ranges through a variety of topics, including the explosion of the space shuttle {{\\textless}I{\\textgreater}Challenger{\\textless}/I{\\textgreater}} (which could have been prevented, Tufte argues, by better information display on the part of the rocket's engineers), magic tricks, a cholera epidemic in 19th-century London, and the principle of using ""the smallest eﬀective diﬀerence"" to display distinctions in data. Throughout, Tufte presents ideas with crystalline clarity and illustrates them in exquisitely rendered samples."
253,"djury a simple approach to improve protein structure predictions",127079,"3D-Jury: a simple approach to improve protein structure predictions.","Motivation: Consensus structure prediction methods (meta-predictors) have higher accuracy than individual structure prediction algorithms (their components). The goal for the development of the {3D-Jury} system is to create a simple but powerful procedure for generating meta-predictions using variable sets of models obtained from diverse sources. The resulting protocol should help to improve the quality of structural annotations of novel proteins. Results: The {3D-Jury} system generates meta-predictions from sets of models created using variable methods. It is not necessary to know prior characteristics of the methods. The system is able to utilize immediately new components (additional prediction providers). The accuracy of the system is comparable with other well-tuned prediction servers. The algorithm resembles methods of selecting models generated using ab initio folding simulations. It is simple and offers a portable solution to improve the accuracy of other protein structure prediction protocols. Availability: The {3D-Jury} system is available via the Structure Prediction Meta Server {(http://BioInfo.PL/Meta/)} to the academic community. Contact: leszek@bioinfo.pl Supplementary information: {3D-Jury} is coupled to the continuous online server evaluation program, {LiveBench} {(http://BioInfo.PL/LiveBench/)}"
254,"nonlinear responses in fmri the balloon model volterra kernels and other hemodynamics",128099,"Nonlinear responses in fMRI: the Balloon model, Volterra kernels, and other hemodynamics.","There is a growing appreciation of the importance of nonlinearities in evoked responses in fMRI, particularly with the advent of event-related fMRI. These nonlinearities are commonly expressed as interactions among stimuli that can lead to the suppression and increased latency of responses to a stimulus that are incurred by a preceding stimulus. We have presented previously a model-free characterization of these effects using generic techniques from nonlinear system identification, namely a Volterra series formulation. At the same time Buxton et al. (1998) described a plausible and compelling dynamical model of hemodynamic signal transduction in fMRI. Subsequent work by Mandeville et al. (1999) provided important theoretical and empirical constraints on the form of the dynamic relationship between blood flow and volume that underpins the evolution of the fMRI signal. In this paper we combine these system identification and model-based approaches and ask whether the Balloon model is sufficient to account for the nonlinear behaviors observed in real time series. We conclude that it can, and furthermore the model parameters that ensue are biologically plausible. This conclusion is based on the observation that the Balloon model can produce Volterra kernels that emulate empirical kernels. To enable this evaluation we had to embed the Balloon model in a hemodynamic input-state-output model that included the dynamics of perfusion changes that are contingent on underlying synaptic activation. This paper presents (i) the full hemodynamic model (ii), how its associated Volterra kernels can be derived, and (iii) addresses the model's validity in relation to empirical nonlinear characterizations of evoked responses in fMRI and other neurophysiological constraints."
255,"development of genetic circuitry exhibiting toggle switch or oscillatory behavior in escherichia coli",128240,"{Development of genetic circuitry exhibiting toggle switch or oscillatory behavior in Escherichia coli}","Analysis of the system design principles of signaling systems requires model systems where all components and regulatory interactions are known. Components of the Lac and Ntr systems were used to construct genetic circuits that display toggle switch or oscillatory behavior. Both devices contain an “activator module” consisting of a modified  glnA  promoter with  lac  operators, driving the expression of the activator, NRI. Since NRI activates the  glnA  promoter, this creates an autoactivated circuit repressible by LacI. The oscillator contains a “repressor module” consisting of the NRI-activated  glnK  promoter driving LacI expression. This circuitry produced synchronous damped oscillations in turbidostat cultures, with periods much longer than the cell cycle. For the toggle switch, LacI was provided constitutively; the level of active repressor was controlled by using a  lacY  mutant and varying the concentration of IPTG. This circuitry provided nearly discontinuous expression of activator."
256,"gene networks how to put the function in genomics",128303,"Gene networks: how to put the function in genomics.","An increasingly popular model of regulation is to represent networks of genes as if they directly affect each other. Although such gene networks are phenomenological because they do not explicitly represent the proteins and metabolites that mediate cell interactions, they are a logical way of describing phenomena observed with transcription profiling, such as those that occur with popular microarray technology. The ability to create gene networks from experimental data and use them to reason about their dynamics and design principles will increase our understanding of cellular function. We propose that gene networks are also a good way to describe function unequivocally, and that they could be used for genome functional annotation. Here, we review some of the concepts and methods associated with gene networks, with emphasis on their construction based on experimental data. A review of gene networks in the context of functional genomics. What are gene networks? How can they be uncovered? What can we learn about them?"
257,"creating the gene ontology resource design and implementation",128334,"{Creating the gene ontology resource: design and implementation}","The exponential growth in the volume of accessible biological information has generated a confusion of voices surrounding the annotation of molecular information about genes and their products. The Gene Ontology {(GO)} project seeks to provide a set of structured vocabularies for specific biological domains that can be used to describe gene products in any organism. This work includes building three extensive ontologies to describe molecular function, biological process, and cellular component, and providing a community database resource that supports the use of these ontologies. The {GO} Consortium was initiated by scientists associated with three model organism databases: {SGD,} the Saccharomyces Genome database; {FlyBase,} the Drosophila genome database; and {MGD/GXD,} the Mouse Genome Informatics databases. Additional model organism database groups are joining the project. Each of these model organism information systems is annotating genes and gene products using {GO} vocabulary terms and incorporating these annotations into their respective model organism databases. Each database contributes its annotation files to a shared {GO} data resource accessible to the public at http://www.geneontology.org/. The {GO} site can be used by the community both to recover the {GO} vocabularies and to access the annotated gene product data sets from the model organism databases. The {GO} Consortium supports the development of the {GO} database resource and provides tools enabling curators and researchers to query and manipulate the vocabularies. We believe that the shared development of this molecular annotation resource will contribute to the unification of biological information."
258,"genetic network inference from coexpression clustering to reverse engineering",128343,"Genetic network inference: from co-expression clustering to reverse engineering.","motivation: Advances in molecular biological, analytical and computational technologies are enabling us to systematically investigate the complex molecular processes underlying biological systems. In particular, using high-throughput gene expression assays, we are able to measure the output of the gene regulatory network. We aim here to review datamining and modeling approaches for conceptualizing and unraveling the functional relationships implicit in these datasets. Clustering of co-expression profiles allows us to infer shared regulatory inputs and functional pathways. We discuss various aspects of clustering, ranging from distance measures to clustering algorithms and multiple-cluster memberships. More advanced analysis aims to infer causal connections between genes directly, i.e. who is regulating whom and how. We discuss several approaches to the problem of reverse engineering of genetic networks, from discrete Boolean networks, to continuous linear and non-linear models. We conclude that the combination of predictive modeling with systematic experimental verification will be required to gain a deeper insight into living organisms, therapeutic targeting and bioengineering. Contact: patrik@cs.unm.edu; sliang@mail.arc.nasa.gov; rsomogyi@incyte.com 10.1093/bioinformatics/16.8.707"
259,"a genomic regulatory network for development",128350,"A genomic regulatory network for development.","Development of the body plan is controlled by large networks of regulatory genes. {A} gene regulatory network that controls the specification of endoderm and mesoderm in the sea urchin embryo is summarized here. {T}he network was derived from large-scale perturbation analyses, in combination with computational methodologies, genomic data, cis-regulatory analysis, and molecular embryology. {T}he network contains over 40 genes at present, and each node can be directly verified at the {DNA} sequence level by cis-regulatory analysis. {I}ts architecture reveals specific and general aspects of development, such as how given cells generate their ordained fates in the embryo and why the process moves inexorably forward in developmental time."
260,"exploring the metabolic and genetic control of gene expression on a genomic scale",128354,"{Exploring the metabolic and genetic control of gene expression on a genomic scale}","DNA microarrays containing virtually every gene of Saccharomyces cerevisiae were used to carry out a comprehensive investigation of the temporal program of gene expression accompanying the metabolic shift from fermentation to respiration. The expression profiles observed for genes with known metabolic functions pointed to features of the metabolic reprogramming that occur during the diauxic shift, and the expression patterns of many previously uncharacterized genes provided clues to their possible functions. The same DNA microarrays were also used to identify genes whose expression was affected by deletion of the transcriptional co-repressor TUP1 or overexpression of the transcriptional activator YAP1. These results demonstrate the feasibility and utility of this approach to genomewide exploration of gene expression patterns."
261,"protein function in the postgenomic era",128391,"{Protein function in the post-genomic era}","Faced with the avalanche of genomic sequences and data on messenger {RNA} expression, biological scientists are confronting a frightening prospect: piles of information but only flakes of knowledge. {H}ow can the thousands of sequences being determined and deposited, and the thousands of expression profiles being generated by the new array methods, be synthesized into useful knowledge? {W}hat form will this knowledge take? {T}hese are questions being addressed by scientists in the field known as 'functional genomics'."
262,"defining transcriptional networks through integrative modeling of mrna expression and transcription factor binding data",128425,"Defining transcriptional networks through integrative modeling of mRNA expression and transcription factor binding data.","BACKGROUND: Functional genomics studies are yielding information about regulatory processes in the cell at an unprecedented scale. In the yeast S. cerevisiae, DNA microarrays have not only been used to measure the mRNA abundance for all genes under a variety of conditions but also to determine the occupancy of all promoter regions by a large number of transcription factors. The challenge is to extract useful information about the global regulatory network from these data. RESULTS: We present MA-Networker, an algorithm that combines microarray data for mRNA expression and transcription factor occupancy to define the regulatory network of the cell. Multivariate regression analysis is used to infer the activity of each transcription factor, and the correlation across different conditions between this activity and the mRNA expression of a gene is interpreted as regulatory coupling strength. Applying our method to S. cerevisiae, we find that, on average, 58\% of the genes whose promoter region is bound by a transcription factor are true regulatory targets. These results are validated by an analysis of enrichment for functional annotation, response for transcription factor deletion, and over-representation of cis-regulatory motifs. We are able to assign directionality to transcription factors that control divergently transcribed genes sharing the same promoter region. Finally, we identify an intrinsic limitation of transcription factor deletion experiments related to the combinatorial nature of transcriptional control, to which our approach provides an alternative. CONCLUSION: Our reliable classification of ChIP positives into functional and non-functional TF targets based on their expression pattern across a wide range of conditions provides a starting point for identifying the unknown sequence features in non-coding DNA that directly or indirectly determine the context dependence of transcription factor action. Complete analysis results are available for browsing or download at http://bussemaker.bio.columbia.edu/papers/MA-Networker/."
263,"genomic expression programs in the response of yeast cells to environmental changes",128429,"{Genomic expression programs in the response of yeast cells to environmental changes}","We explored genomic expression patterns in the yeast Saccharomyces cerevisiae responding to diverse environmental transitions. DNA microarrays were used to measure changes in transcript levels over time for almost every yeast gene, as cells responded to temperature shocks, hydrogen peroxide, the superoxide-generating drug menadione, the sulfhydryl-oxidizing agent diamide, the disulfide-reducing agent dithiothreitol, hyper- and hypo-osmotic shock, amino acid starvation, nitrogen source depletion, and progression into stationary phase. A large set of genes (approximately 900) showed a similar drastic response to almost all of these environmental changes. Additional features of the genomic responses were specialized for specific conditions. Promoter analysis and subsequent characterization of the responses of mutant strains implicated the transcription factors Yap1p, as well as Msn2p and Msn4p, in mediating specific features of the transcriptional response, while the identification of novel sequence elements provided clues to novel regulators. Physiological themes in the genomic responses to specific environmental stresses provided insights into the effects of those stresses on the cell."
264,"correlation between transcriptome and interactome mapping data from saccharomyces cerevisiae",128436,"Correlation between transcriptome and interactome mapping data from Saccharomyces cerevisiae.","Genomic and proteomic approaches can provide hypotheses concerning function for the large number of genes predicted from genome sequences. Because of the artificial nature of the assays, however, the information from these high-throughput approaches should be considered with caution. Although it is possible that more meaningful hypotheses could be formulated by integrating the data from various functional genomic and proteomic projects, it has yet to be seen to what extent the data can be correlated and how such integration can be achieved. We developed a 'transcriptome-interactome correlation mapping' strategy to compare the interactions between proteins encoded by genes that belong to common expression-profiling clusters with those between proteins encoded by genes that belong to different clusters. Using this strategy with currently available data sets for Saccharomyces cerevisiae, we provide the first global evidence that genes with similar expression profiles are more likely to encode interacting proteins. We show how this correlation between transcriptome and interactome data can be used to improve the quality of hypotheses based on the information from both approaches. The strategy described here may help to integrate other functional genomic and proteomic data, both in yeast and in higher organisms."
265,"global analysis of protein expression in yeast",128478,"Global analysis of protein expression in yeast.","The availability of complete genomic sequences and technologies that allow comprehensive analysis of global expression profiles of messenger RNA have greatly expanded our ability to monitor the internal state of a cell. Yet biological systems ultimately need to be explained in terms of the activity, regulation and modification of proteins--and the ubiquitous occurrence of post-transcriptional regulation makes mRNA an imperfect proxy for such information. To facilitate global protein analyses, we have created a Saccharomyces cerevisiae fusion library where each open reading frame is tagged with a high-affinity epitope and expressed from its natural chromosomal location. Through immunodetection of the common tag, we obtain a census of proteins expressed during log-phase growth and measurements of their absolute levels. We find that about 80\% of the proteome is expressed during normal growth conditions, and, using additional sequence information, we systematically identify misannotated genes. The abundance of proteins ranges from fewer than 50 to more than 10(6) molecules per cell. Many of these molecules, including essential proteins and most transcription factors, are present at levels that are not readily detectable by other proteomic techniques nor predictable by mRNA levels or codon bias measurements."
266,"functional profiling of the saccharomyces cerevisiae genome",128479,"{Functional profiling of the Saccharomyces cerevisiae genome}","Determining the effect of gene deletion is a fundamental approach to understanding gene function. Conventional genetic screens exhibit biases, and genes contributing to a phenotype are often missed. We systematically constructed a nearly complete collection of gene-deletion mutants (96% of annotated open reading frames, or ORFs) of the yeast Saccharomyces cerevisiae. DNA sequences dubbed 'molecular bar codes' uniquely identify each strain, enabling their growth to be analysed in parallel and the fitness contribution of each gene to be quantitatively assessed by hybridization to high-density oligonucleotide arrays. We show that previously known and new genes are necessary for optimal growth under six well-studied conditions: high salt, sorbitol, galactose, pH 8, minimal medium and nystatin treatment. Less than 7% of genes that exhibit a significant increase in messenger RNA expression are also required for optimal growth in four of the tested conditions. Our results validate the yeast gene-deletion collection as a valuable resource for functional genomics."
267,"life with genes",128483,"{Life with 6000 genes}","The genome of the yeast Saccharomyces cerevisiae has been completely sequenced through a worldwide collaboration. The sequence of 12,068 kilobases defines 5885 potential protein-encoding genes, approximately 140 genes specifying ribosomal RNA, 40 genes for small nuclear RNA molecules, and 275 transfer RNA genes. In addition, the complete sequence provides information about the higher order organization of yeast's 16 chromosomes and allows some insight into their evolutionary history. The genome shows a considerable amount of apparent genetic redundancy, and one of the major problems to be tackled during the next stage of the yeast genome project is to elucidate the biological functions of all of these genes."
268,"a new approach to decoding life systems biology",128554,"{A new approach to decoding life: systems biology}","Systems biology studies biological systems by systematically perturbing them (biologically, genetically, or chemically); monitoring the gene, protein, and informational pathway responses; integrating these data; and ultimately, formulating mathematical models that describe the structure of the system and its response to individual perturbations. The emergence of systems biology is described, as are several examples of specific systems approaches."
269,"building with a scaffold emerging strategies for high to lowlevel cellular modeling",128558,"{Building with a scaffold: emerging strategies for high- to low-level cellular modeling}","Computational cellular models are becoming crucial for the analysis of complex biological systems. An important new paradigm for cellular modeling involves building a comprehensive scaffold of molecular interactions and then mining this scaffold to reveal a hierarchy of signaling, regulatory and metabolic pathways. We review the important trends that make this approach feasible and describe how they are spurring the development of models at multiple levels of abstraction. Pathway maps can be extracted from the scaffold using &#x2018;high-level&#x2019; computational models, which identify the key components, interactions and influences required for more detailed &#x2018;low-level&#x2019; models. Large-scale experimental measurements validate high-level models, whereas targeted experimental manipulations and measurements test low-level models."
270,"prediction and measurement of an autoregulatory genetic module",128561,"{Prediction and measurement of an autoregulatory genetic module}","10.1073/pnas.1332628100 The deduction of phenotypic cellular responses from the structure and  behavior of complex gene regulatory networks is one of the defining challenges  of systems biology. This goal will require a quantitative understanding of the  modular components that constitute such networks. We pursued an integrated  approach, combining theory and experiment, to analyze and describe the  dynamics of an isolated genetic module, an in vivo autoregulatory  gene network. As predicted by the model, temperature-induced protein  destabilization led to the existence of two expression states, thus  elucidating the trademark bistability of the positive feedback-network  architecture. After sweeping the temperature, observed population  distributions and coefficients of variation were in quantitative agreement  with those predicted by a stochastic version of the model. Because model  fluctuations originated from small molecule-number effects, the experimental  validation underscores the importance of internal noise in gene expression.  This work demonstrates that isolated gene networks, coupled with proper  quantitative descriptions, can elucidate key properties of functional genetic  modules. Such an approach could lead to the modular dissection of naturally  occurring gene regulatory networks, the deduction of cellular processes such  as differentiation, and the development of engineered cellular control."
271,"relating wholegenome expression data with proteinprotein interactions",128570,"{Relating whole-genome expression data with protein-protein interactions}","We investigate the relationship of protein-protein interactions with mRNA expression levels, by integrating a variety of data sources for yeast. We focus on known protein complexes that have clearly defined interactions between their subunits. We find that subunits of the same protein complex show significant coexpression, both in terms of similarities of absolute mRNA levels and expression profiles, e.g., we can often see subunits of a complex having correlated patterns of expression over a time course. We classify the yeast protein complexes as either permanent or transient, with permanent ones being maintained through most cellular conditions. We find that, generally, permanent complexes, such as the ribosome and proteasome, have a particularly strong relationship with expression, while transient ones do not. However, we note that several transient complexes, such as the RNA polymerase II holoenzyme and the replication complex, can be subdivided into smaller permanent ones, which do have a strong relationship to gene expression. We also investigated the interactions in aggregated, genome-wide data sets, such as the comprehensive yeast two-hybrid experiments, and found them to have only a weak relationship with gene expression, similar to that of transient complexes. (Further details on genecensus.org/expression/interactions and bioinfo.mbb.yale.edu/expression/interactions.)"
272,"threedimensional cluster analysis identifies interfaces and functional residue clusters in proteins",128640,"{Three-dimensional cluster analysis identifies interfaces and functional residue clusters in proteins}","Three-dimensional cluster analysis offers a method for the prediction of functional residue clusters in proteins. This method requires a representative structure and a multiple sequence alignment as input data. Individual residues are represented in terms of regional alignments that reflect both their structural environment and their evolutionary variation, as defined by the alignment of homologous sequences. From the overall (global) and the residue-specific (regional) alignments, we calculate the global and regional similarity matrices, containing scores for all pairwise sequence comparisons in the respective alignments. Comparing the matrices yields two scores for each residue. The regional conservation score (C(R)(x)) defines the conservation of each residue x and its neighbors in 3D space relative to the protein as a whole. The similarity deviation score (S(x)) detects residue clusters with sequence similarities that deviate from the similarities suggested by the full-length sequences. We evaluated 3D cluster analysis on a set of 35 families of proteins with available cocrystal structures, showing small ligand interfaces, nucleic acid interfaces and two types of protein-protein interfaces (transient and stable). We present two examples in detail: fructose-1,6-bisphosphate aldolase and the mitogen-activated protein kinase ERK2. We found that the regional conservation score (C(R)(x)) identifies functional residue clusters better than a scoring scheme that does not take 3D information into account. C(R)(x) is particularly useful for the prediction of poorly conserved, transient protein-protein interfaces. Many of the proteins studied contained residue clusters with elevated similarity deviation scores. These residue clusters correlate with specificity-conferring regions: 3D cluster analysis therefore represents an easily applied method for the prediction of functionally relevant spatial clusters of residues in proteins."
273,"network component analysis reconstruction of regulatory signals in biological systems",128668,"Network component analysis: reconstruction of regulatory signals in biological systems.","High-dimensional data sets generated by high-throughput technologies, such as DNA microarray, are often the outputs of complex networked systems driven by hidden regulatory signals. Traditional statistical methods for computing low-dimensional or hidden representations of these data sets, such as principal component analysis and independent component analysis, ignore the underlying network structures and provide decompositions based purely on a priori statistical constraints on the computed component signals. The resulting decomposition thus provides a phenomenological model for the observed data and does not necessarily contain physically or biologically meaningful signals. Here, we develop a method, called network component analysis, for uncovering hidden regulatory signals from outputs of networked systems, when only a partial knowledge of the underlying network topology is available. The a priori network structure information is first tested for compliance with a set of identifiability criteria. For networks that satisfy the criteria, the signals from the regulatory nodes and their strengths of influence on each output node can be faithfully reconstructed. This method is first validated experimentally by using the absorbance spectra of a network of various hemoglobin species. The method is then applied to microarray data generated from yeast Saccharamyces cerevisiae and the activities of various transcription factors during cell cycle are reconstructed by using recently discovered connectivity information for the underlying transcriptional regulatory networks."
274,"structure and function of the feedforward loop network motif",128690,"{Structure and function of the feed-forward loop network motif}","Engineered systems are often built of recurring circuit modules that carry out key functions. {T}ranscription networks that regulate the responses of living cells were recently found to obey similar principles: they contain several biochemical wiring patterns, termed network motifs, which recur throughout the network. {O}ne of these motifs is the feed-forward loop ({FFL}). {T}he {FFL}, a three-gene pattern, is composed of two input transcription factors, one of which regulates the other, both jointly regulating a target gene. {T}he {FFL} has eight possible structural types, because each of the three interactions in the {FFL} can be activating or repressing. {H}ere, we theoretically analyze the functions of these eight structural types. {W}e find that four of the {FFL} types, termed incoherent {FFL}s, act as sign-sensitive accelerators: they speed up the response time of the target gene expression following stimulus steps in one direction (e.g., off to on) but not in the other direction (on to off). {T}he other four types, coherent {FFL}s, act as sign-sensitive delays. {W}e find that some {FFL} types appear in transcription network databases much more frequently than others. {I}n some cases, the rare {FFL} types have reduced functionality (responding to only one of their two input stimuli), which may partially explain why they are selected against. {A}dditional features, such as pulse generation and cooperativity, are discussed. {T}his study defines the function of one of the most significant recurring circuit elements in transcription networks."
275,"evolving protein interaction networks through gene duplication",128746,"{Evolving protein interaction networks through gene duplication}","The topology of the proteome map revealed by recent large-scale hybridization methods has shown that the distribution of protein-protein interactions is highly heterogeneous, with many proteins having few edges while a few of them are heavily connected. This particular topology is shared by other cellular networks, such as metabolic pathways, and it has been suggested to be responsible for the high mutational homeostasis displayed by the genome of some organisms. In this paper we explore a recent model of proteome evolution that has been shown to reproduce many of the features displayed by its real counterparts. The model is based on gene duplication plus re-wiring of the newly created genes. The statistical features displayed by the proteome of well-known organisms are reproduced and suggest that the overall topology of the protein maps naturally emerges from the two leading mechanisms considered by the model."
276,"gene networks inference using dynamic bayesian networks",128750,"Gene networks inference using dynamic Bayesian networks.","This article deals with the identification of gene regulatory networks from experimental data using a statistical machine learning approach. A stochastic model of gene interactions capable of handling missing variables is proposed. It can be described as a dynamic Bayesian network particularly well suited to tackle the stochastic nature of gene regulation and gene expression measurement. Parameters of the model are learned through a penalized likelihood maximization implemented through an extended version of EM algorithm. Our approach is tested against experimental data relative to the S.O.S. DNA Repair network of the Escherichia coli bacterium. It appears to be able to extract the main regulations between the genes involved in this network. An added missing variable is found to model the main protein of the network. Good prediction abilities on unlearned data are observed. These first results are very promising: they show the power of the learning algorithm and the ability of the model to capture gene interactions. Keywords: gene regulatory networks, structure extraction, expression profiles, dynamic Bayesian networks, Kalman filter, penalized likelihood, EM algorithm. Contact: perrin@poleia.lip6.fr"
277,"wholegenome discovery of transcription factor binding sites by networklevel conservation",128757,"Whole-genome discovery of transcription factor binding sites by network-level conservation.","Comprehensive identification of DNA cis-regulatory elements is crucial for a predictive understanding of transcriptional network dynamics. Strong evidence suggests that these DNA sequence motifs are highly conserved between related species, reflecting strong selection on the network of regulatory interactions that underlie common cellular behavior. Here, we exploit a systems-level aspect of this conservation-the network-level topology of these interactions-to map transcription factor (TF) binding sites on a genomic scale. Using network-level conservation as a constraint, our algorithm finds 71% of known TF binding sites in the yeast Saccharomyces cerevisiae, using only 12% of the sequence of a phylogenetic neighbor. Most of the novel predicted motifs show strong features of known TF binding sites, such as functional category and/or expression profile coherence of their corresponding genes. Network-level conservation should provide a powerful constraint for the systematic mapping of TF binding sites in the larger genomes of higher eukaryotes."
278,"computational analysis of microarray data",128766,"Computational analysis of microarray data.","Microarray experiments are providing unprecedented quantities of genome-wide data on gene-expression patterns. Although this technique has been enthusiastically developed and applied in many biological contexts, the management and analysis of the millions of data points that result from these experiments has received less attention. Sophisticated computational tools are available, but the methods that are used to analyse the data can have a profound influence on the interpretation of the results. A basic understanding of these computational tools is therefore required for optimal experimental design and meaningful data analysis."
279,"c elegans orfeome version experimental verification of the genome annotation and resource for proteomescale protein expression",128770,"C. elegans ORFeome version 1.1: experimental verification of the genome annotation and resource for proteome-scale protein expression.","To verify the genome annotation and to create a resource to functionally characterize the proteome, we attempted to Gateway-clone all predicted protein-encoding open reading frames (ORFs), or the 'ORFeome,' of Caenorhabditis elegans. We successfully cloned approximately 12,000 ORFs (ORFeome 1.1), of which roughly 4,000 correspond to genes that are untouched by any cDNA or expressed-sequence tag (EST). More than 50\% of predicted genes needed corrections in their intron-exon structures. Notably, approximately 11,000 C. elegans proteins can now be expressed under many conditions and characterized using various high-throughput strategies, including large-scale interactome mapping. We suggest that similar ORFeome projects will be valuable for other organisms, including humans."
280,"genomewide location and function of dna binding proteins",128773,"{Genome-wide location and function of DNA binding proteins}","Understanding how DNA binding proteins control global gene expression and chromosomal maintenance requires knowledge of the chromosomal locations at which these proteins function in vivo. We developed a microarray method that reveals the genome-wide location of DNA-bound proteins and used this method to monitor binding of gene-specific transcription activators in yeast. A combination of location and expression profiles was used to identify genes whose expression is directly controlled by Gal4 and Ste12 as cells respond to changes in carbon source and mating pheromone, respectively. The results identify pathways that are coordinately regulated by each of the two activators and reveal previously unknown functions for Gal4 and Ste12. Genome-wide location analysis will facilitate investigation of gene regulatory networks, gene function, and genome maintenance."
281,"regulondb version transcriptional regulation operon organization and growth conditions in escherichia coli k",128791,"RegulonDB (version 4.0): transcriptional regulation, operon organization and growth conditions in Escherichia coli K-12.","Regulon{DB} is the primary database of the major international maintained curation of original literature with experimental knowledge about the elements and interactions of the network of transcriptional regulation in {E}scherichia coli {K}-12. {T}his includes mechanistic information about operon organization and their decomposition into transcription units ({TU}s), promoters and their sigma type, binding sites of specific transcriptional regulators ({TR}s), their organization into 'regulatory phrases', active and inactive conformations of {TR}s, as well as terminators and ribosome binding sites. {T}he database is complemented with clearly marked computational predictions of {TU}s, promoters and binding sites of {TR}s. {T}he current version has been expanded to include information beyond specific mechanisms aimed at gathering different growth conditions and the associated induced and/or repressed genes. {R}egulon{DB} is now linked with {S}wiss-{P}rot, with microarray databases, and with a suite of programs to analyze and visualize microarray experiments. {W}e provide a summary of the biological knowledge contained in {R}egulon{DB} and describe the major changes in the design of the database. {R}egulon{DB} can be accessed on the web at the {URL}: http://www.cifn.unam.mx/{C}omputational_{B}iology/regulondb/."
282,"parallel human genome analysis microarraybased expression monitoring of genes",128799,"Parallel human genome analysis: microarray-based expression monitoring of 1000 genes.","Microarrays containing 1046 human cDNAs of unknown sequence were printed on glass with high-speed robotics. These 1.0-cm2 DNA ""chips"" were used to quantitatively monitor differential expression of the cognate human genes using a highly sensitive two-color hybridization assay. Array elements that displayed differential expression patterns under given experimental conditions were characterized by sequencing. The identification of known and novel heat shock and phorbol ester-regulated genes in human T cells demonstrates the sensitivity of the assay. Parallel gene analysis with microarrays provides a rapid and efficient method for large-scale human gene discovery."
283,"genomewide discovery of transcriptional modules from dna sequence and gene expression",128811,"{Genome-wide discovery of transcriptional modules from DNA sequence and gene expression}","In this paper, we describe an approach for understanding transcriptional regulation from both gene expression and promoter sequence data. We aim to identify transcriptional modules--sets of genes that are co-regulated in a set of experiments, through a common motif profile. Using the EM algorithm, our approach refines both the module assignment and the motif profile so as to best explain the expression data as a function of transcriptional motifs. It also dynamically adds and deletes motifs, as required to provide a genome-wide explanation of the expression data. We evaluate the method on two Saccharomyces cerevisiae gene expression data sets, showing that our approach is better than a standard one at recovering known motifs and at generating biologically coherent modules. We also combine our results with binding localization data to obtain regulatory relationships with known transcription factors, and show that many of the inferred relationships have support in the literature. Contact: eran@cs.stanford.edu Keywords: probabilistic models, gene expression, transcriptional regulation."
284,"detailed map of a cisregulatory input function",128818,"{Detailed map of a cis-regulatory input function}","Most genes are regulated by multiple transcription factors that bind specific sites in {DNA} regulatory regions. These cis-regulatory regions perform a computation: the rate of transcription is a function of the active concentrations of each of the input transcription factors. Here, we used accurate gene expression measurements from living cell cultures, bearing {GFP} reporters, to map in detail the input function of the classic {lacZYA} operon of Escherichia coli, as a function of about a hundred combinations of its two inducers, {cAMP} and isopropyl {{beta}-D-thiogalactoside} {(IPTG).} We found an unexpectedly intricate function with four plateau levels and four thresholds. This result compares well with a mathematical model of the binding of the regulatory proteins {cAMP} receptor protein {(CRP)} and {LacI} to the lac regulatory region. The model is also used to demonstrate that with few mutations, the same region could encode much purer {AND-like} or even {OR-like} functions. This possibility means that the wild-type region is selected to perform an elaborate computation in setting the transcription rate. The present approach can be generally used to map the input functions of other genes."
285,"cytoscape a software environment for integrated models of biomolecular interaction networks",128819,"Cytoscape: a software environment for integrated models of biomolecular interaction networks.","Cytoscape is an open source software project for integrating biomolecular interaction networks with high-throughput expression data and other molecular states into a unified conceptual framework. Although applicable to any system of molecular components and interactions, Cytoscape is most powerful when used in conjunction with large databases of protein-protein, protein-DNA, and genetic interactions that are increasingly available for humans and model organisms. Cytoscape's software Core provides basic functionality to layout and query the network; to visually integrate the network with expression profiles, phenotypes, and other molecular states; and to link the network to databases of functional annotations. The Core is extensible through a straightforward plug-in architecture, allowing rapid development of additional computational analyses and features. Several case studies of Cytoscape plug-ins are surveyed, including a search for interaction pathways correlating with changes in gene expression, a study of protein complexes involved in cellular recovery to DNA damage, inference of a combined physical/functional interaction network for Halobacterium, and an interface to detailed stochastic/kinetic gene regulatory models."
286,"reverse engineering gene networks integrating genetic perturbations with dynamical modeling",128876,"Reverse engineering gene networks: integrating genetic perturbations with dynamical modeling.","While the fundamental building blocks of biology are being tabulated by the various genome projects, microarray technology is setting the stage for the task of deducing the connectivity of large-scale gene networks. {W}e show how the perturbation of carefully chosen genes in a microarray experiment can be used in conjunction with a reverse engineering algorithm to reveal the architecture of an underlying gene regulatory network. {O}ur iterative scheme identifies the network topology by analyzing the steady-state changes in gene expression resulting from the systematic perturbation of a particular node in the network. {W}e highlight the validity of our reverse engineering approach through the successful deduction of the topology of a linear in numero gene network and a recently reported model for the segmentation polarity network in {D}rosophila melanogaster. {O}ur method may prove useful in identifying and validating specific drug targets and in deconvolving the effects of chemical compounds."
287,"the packing density in proteins standard radii and volumes",128893,"{The packing density in proteins: standard radii and volumes}","The sizes of atomic groups are a fundamental aspect of protein structure. They are usually expressed in terms of standard sets of radii for atomic groups and of volumes for both these groups and whole residues. Atomic groups, which subsume a heavy-atom and its covalently attached hydrogen atoms into one moiety, are used because the positions of hydrogen atoms in protein structures are generally not known. We have calculated new values for the radii of atomic groups and for the volumes of atomic groups. These values should prove useful in the analysis of protein packing, protein recognition and ligand design. Our radii for atomic groups were derived from intermolecular distance calculations on a large number (approximately 30,000) of crystal structures of small organic compounds that contain the same atomic groups to those found in proteins. Our radii show significant differences to previously reported values. We also use this new radii set to determine the packing efficiency in different regions of the protein interior. This analysis shows that, if the surface water molecules are included in the calculations, the overall packing efficiency throughout the protein interior is high and fairly uniform. However, if the water structure is removed, the packing efficiency in peripheral regions of the protein interior is underestimated, by approximately 3.5 \%."
288,"detecting putative orthologs",128931,"{Detecting putative orthologs}","Summary: We developed an algorithm that improves upon the common procedure of taking reciprocal best blast hits (rbh) in the identification of orthologs. The method--reciprocal smallest distance algorithm (rsd)--relies on global sequence alignment and maximum likelihood estimation of evolutionary distances to detect orthologs between two genomes. rsd finds many putative orthologs missed by rbh because it is less likely than rbh to be misled by the presence of a close paralog.  Availability: A Python program and ReadMe file are freely available from: http://charles.stanford.edu/~dennis/research.html 10.1093/bioinformatics/btg213"
289,"global analysis of protein activities using proteome chips",128968,"{Global analysis of protein activities using proteome chips}","To facilitate studies of the yeast proteome, we cloned 5800 open reading frames and overexpressed and purified their corresponding proteins. The proteins were printed onto slides at high spatial density to form a yeast proteome microarray and screened for their ability to interact with proteins and phospholipids. We identified many new calmodulin- and phospholipid-interacting proteins; a common potential binding motif was identified for many of the calmodulin-binding proteins. Thus, microarrays of an entire eukaryotic proteome can be prepared and screened for diverse biochemical activities. The microarrays can also be used to screen protein-drug interactions and to detect posttranslational modifications."
290,"natural gradient works efficiently in learning",129007,"Natural Gradient Works Efficiently in Learning","When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed."
291,"improving support vector machine classifiers by modifying kernel functions",129008,"Improving support vector machine classifiers by modifying kernel functions","We propose a method of modifying a kernel function to improve the performance of a support  vector machine classifier. This is based on the Riemannian geometrical structure induced by the  kernel function. The idea is to enlarge the spatial resolution around the separating boundary  surface by a conformal mapping such that the separability between classes is increased. Examples  are given specifically for modifying Gaussian Radial Basis Function kernels. Simulation results for  both artificial and real data show remarkable improvement of generalization errors, supporting our  idea.  1 Introduction  Support Vector Machine (SVM) is a new promising pattern classification technique proposed recently by Vapnik and co-workers (Boser et al., 1992, Cortes and Vapnik, 1995, and Vapnik, 1995). Unlike traditional methods which minimize the empirical training error, SVM aims at minimizing an upper bound of the generalization error through maximizing the margin between the separating hyperplane and..."
292,"detecting protein function and proteinprotein interactions from genome sequences",129019,"Detecting Protein Function and Protein-Protein Interactions from Genome Sequences","A computational method is proposed for inferring protein interactions from genome sequences on the basis of the observation that some pairs of interacting proteins have homologs in another organism fused into a single protein chain. {S}earching sequences from many genomes revealed 6809 such putative protein-protein interactions in {E}scherichia coli and 45,502 in yeast. {M}any members of these pairs were confirmed as functionally related; computational filtering further enriches for interactions. {S}ome proteins have links to several other proteins; these coupled links appear to represent functional interactions such as complexes or pathways. {E}xperimentally confirmed interacting pairs are documented in a {D}atabase of {I}nteracting {P}roteins."
293,"the spectrum kernel a string kernel for svm protein classification",129026,"The spectrum kernel: a string kernel for SVM protein classification","We introduce a new sequence-similarity kernel, the spectrum kernel, for use with support vector machines (SVMs) in a discriminative approach to the protein classification problem. Our kernel is conceptually simple and efficient to compute and, in experiments on the SCOP database, performs well in comparison with state-of-the-art methods for homology detection. Moreover, our method produces an SVM classifier that allows linear time classification of test sequences. Our experiments provide evidence that string-based kernels, in conjunction with SVMs, could offer a viable and computationally efficient alternative to other methods of protein classification and homology detection."
294,"support vector machine classification and validation of cancer tissue samples using microarray expression data",129029,"Support Vector Machine Classification and Validation of Cancer Tissue Samples Using Microarray Expression Data","Motivation: DNA microarray experiments generating thousands of gene expression measurements, are being used to gather information from tissue and cell samples regarding gene expression differences that will be useful in diagnosing disease. We have developed a new method to analyse this kind of data using support vector machines (SVMs). This analysis consists of both classification of the tissue samples, and an exploration of the data for mis-labeled or questionable tissue results.  Results: We demonstrate the method in detail on samples consisting of ovarian cancer tissues, normal ovarian tissues, and other normal tissues. The dataset consists of expression experiment   results for 97802 cDNAs for each tissue. As a result of computational analysis, a tissue sample is discovered and confirmed to be wrongly labeled. Upon correction of this mistake and the removal of an outlier, perfect classification of tissues is achieved, but not with high confidence. We identify and analyse a subset of genes from the ovarian dataset whose expression is highly   differentiated between the types of tissues. To show robustness of the SVM method, two previously published datasets from other types of tissues or cells are analysed. The results are comparable to those previously obtained. We show that other machine learning methods also perform comparably to the SVM on many of those datasets.  Availability: The SVM software is available at http://www.cs.columbia.edu/~bgrundy/svm.  Contact: booch@cse.ucsc.edu 10.1093/bioinformatics/16.10.906"
295,"support vector clustering",129038,"Support Vector Clustering","We present a novel clustering method using the approach of support vector machines. Data points are mapped by means of a Gaussian kernel to a high dimensional feature space, where we search for the minimal enclosing sphere. This sphere, when mapped back to data space, can separate into several components, each enclosing a separate cluster of points. We present a simple algorithm for identifying these clusters. The width of the Gaussian kernel controls the scale at which the data is probed while the soft margin constant helps coping with outliers and overlapping clusters. The structure of a dataset is explored by varying the two parameters, maintaining a minimal number of support vectors to assure smooth cluster boundaries. We demonstrate the performance of our algorithm on several datasets."
296,"text classification using string kernels",129039,"Text Classification using String Kernels","We propose a novel approach for categorizing text documents based on the use of a special kernel. The kernel is an inner product in the feature space generated by all subsequences of length &lt;em&gt;k&lt;/em&gt;. A subsequence is any ordered sequence of &lt;em&gt;k&lt;/em&gt; characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of &lt;em&gt;k&lt;/em&gt;, since the dimension of the feature space grows exponentially with &lt;em&gt;k&lt;/em&gt;. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. Experimental comparisons of the performance of the kernel compared with a standard word feature space kernel (Joachims, 1998) show positive results on modestly sized datasets. The case of contiguous subsequences is also considered for comparison with the subsequences kernel with different decay factors. For larger documents and datasets the paper introduces an approximation technique that is shown to deliver good approximations efficiently for large datasets."
297,"deterministic scalefree networks",129085,"Deterministic scale-free networks","Scale-free networks are abundant in nature and society, describing such diverse systems as the world wide web, the web of human sexual contacts, or the chemical network of a cell. All models used to generate a scale-free topology are stochastic, that is they create networks in which the nodes appear to be randomly connected to each other. Here we propose a simple model that generates scale-free networks in a deterministic fashion. We solve exactly the model, showing that the tail of the degree distribution follows a power law."
298,"spline models for observational data",129100,"Spline Models for Observational Data"," For the last two decades the author has been associated with pioneer work in the area of nonparametric regression and spline smoothing. In the monograph under review this vast amount of information is made available in a consolidated form along with a brief survey of the work of other researchers. The general smoothing problem is presented using reproducing kernel (r.k.) Hilbert spaces. The solution of variational problems arising in univariate and multivariate smoothing, partial spline models, additive and interaction spline models is characterized in terms of a relevant r.k. The r.k. as a covariance kernel provides the link between spline estimates and Bayes estimates. The convergence rates of various estimates are obtained in terms of the rate of decay of the eigenvalues of the r.k. and the Fourier-Bessel coefficients of the function being estimated with respect to the eigenfunction. From a practical point of view the choice of a good basis and smoothing parameter is crucial. For the latter problem data-dependent techniques such as cross-validation and general cross-validation are discussed in detail through illustrative examples. Numerical methods for computing the estimates and the available packages are also indicated. The solution of integral equations of the first kind and partial spline models in nonlinear regression are briefly considered. <P> The reviewer considers the monograph a valuable contribution and recommends it strongly to everyone with some interest in this important area of statistics."
299,"principal component analysis",129118,"Principal component analysis","Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. It is extremely versatile with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years."
300,"molecular cloning a laboratory manual",129240,"Molecular Cloning: A Laboratory Manual","The first two editions of this manual have been mainstays of molecular biology for nearly twenty years, with an unrivalled reputation for reliability, accuracy, and clarity. <P>In this new edition, authors Joe Sambrook and David Russell have completely updated the book, revising every protocol and adding a mass of new material, to broaden its scope and maintain its unbeatable value for studies in genetics, molecular cell biology, developmental biology, microbiology, neuroscience, and immunology. <P>Handsomely redesigned and presented in new bindings of proven durability, this three-volume work is essential for everyone using today&#146;s biomolecular techniques. <P>The opening chapters describe essential techniques, some well-established, some new, that are used every day in the best laboratories for isolating, analyzing and cloning DNA molecules, both large and small. <P>These are followed by chapters on cDNA cloning and exon trapping, amplification of DNA, generation and use of nucleic acid probes, mutagenesis, and DNA sequencing. <P>The concluding chapters deal with methods to screen expression libraries, express cloned genes in both prokaryotes and eukaryotic cells, analyze transcripts and proteins, and detect protein-protein interactions. <P>The Appendix is a compendium of reagents, vectors, media, technical suppliers, kits, electronic resources and other essential information. <P>As in earlier editions, this is the only manual that explains how to achieve success in cloning and provides a wealth of information about why techniques work, how they were first developed, and how they have evolved."
301,"exploration of essential gene functions via titratable promoter alleles",129291,"Exploration of essential gene functions via titratable promoter alleles","Nearly 20% of yeast genes are required for viability, hindering genetic analysis with knockouts. We created promoter-shutoff strains for over two-thirds of all essential yeast genes and subjected them to morphological analysis, size profiling, drug sensitivity screening, and microarray expression profiling. We then used this compendium of data to ask which phenotypic features characterized different functional classes and used these to infer potential functions for uncharacterized genes. We identified genes involved in ribosome biogenesis (HAS1, URB1, and URB2), protein secretion (SEC39), mitochondrial import (MIM1), and tRNA charging (GSN1). In addition, apparent negative feedback transcriptional regulation of both ribosome biogenesis and the proteasome was observed. We furthermore show that these strains are compatible with automated genetic analysis. This study underscores the importance of analyzing mutant phenotypes and provides a resource to complement the yeast knockout collection."
302,"complex acts of knowing paradox and descriptive selfawareness",129335,"Complex Acts of Knowing: Paradox and Descriptive Self-awareness","We are reaching the end of the second generation of knowledge management, with its focus on tacit-explicit knowledge conversion. Triggered by the SECI model of Nonaka, it replaced a first generation focus on timely information provision for decision support and in support of BPR initiatives. Like BPR it has substantially failed to deliver on its promised benefits. The third generation requires the clear separation of context, narrative and content management and challenges the orthodoxy of scientific management. Complex adaptive systems theory is used to create a sense-making model that utilises self-organising capabilities of the informal communities and identifies a natural flow model of knowledge creation, disruption and utilisation. However, the argument from nature of many complexity thinkers is rejected given the human capability to create order and predictability through collective and individual acts of freewill. disillusionment was creeping in, organisations Knowledge is seen paradoxically, as both a thing and a flow requiring diverse management approaches."
303,"managing gigabytes compressing and indexing documents and images",129356,"Managing Gigabytes: Compressing and Indexing Documents and Images","{Of all the tasks programmers are asked to perform, storing, compressing, and retrieving information are some of the most challenging--and critical to many applications. <I>Managing Gigabytes: Compressing and Indexing Documents and Images</I> is a treasure trove of theory, practical illustration, and general discussion in this fascinating technical subject.<p> Ian Witten, Alistair Moffat, and Timothy Bell have updated their original work with this even more impressive second edition. This version adds recent techniques such as block-sorting, new indexing techniques, new lossless compression strategies, and many other elements to the mix. In short, this work is a comprehensive summary of text and image compression, indexing, and querying techniques. The history of relevant algorithm development is woven well with a practical discussion of challenges, pitfalls, and specific solutions.<p> This title is a textbook-style exposition on the topic, with its information organized very clearly into topics such as compression, indexing, and so forth. In addition to diagrams and example text transformations, the authors use ""pseudo-code"" to present algorithms in a language-independent manner wherever possible. They also supplement the reading with <I>mg</I>--their own implementation of the techniques. The mg C language source code is freely available on the Web. <p> Alone, this book is an impressive collection of information. Nevertheless, the authors list numerous titles for further reading in selected topics. Whether you're in the midst of application development and need solutions fast or are merely curious about how top-notch information management is done, this hardcover is an excellent investment. <I>--Stephen W. Plain</I><p> <B>Topics covered</B>: Text compression models, including Huffman, LZW, and their variants; trends in information management; index creation and compression; image compression; performance issues; and overall system implementation.} {<p>In this fully updated second edition of the highly acclaimed <b>Managing Gigabytes</b>, authors Witten, Moffat, and Bell continue to provide unparalleled coverage of state-of-the-art techniques for compressing and indexing data. Whatever your field, if you work with large quantities of information, this book is essential reading--an authoritative theoretical resource and a practical guide to meeting the toughest storage and access challenges. It covers the latest developments in compression and indexing and their application on the Web and in digital libraries. It also details dozens of powerful techniques supported by mg, the authors' own system for compressing, storing, and retrieving text, images, and textual images. mg's source code is freely available on the Web.<br><br>* Up-to-date coverage of new text compression algorithms such as block sorting, approximate arithmetic coding, and fat Huffman coding<br>* New sections on content-based index compression and distributed querying, with 2 new data structures for fast indexing<br>* New coverage of image coding, including descriptions of de facto standards in use on the Web (GIF and PNG), information on CALIC, the new proposed JPEG Lossless standard, and JBIG2<br>* New information on the Internet and WWW, digital libraries, web search engines, and agent-based retrieval<br>* Accompanied by a public domain system called MG which is a fully worked-out operational example of the advanced techniques developed and explained in the book<br>* New appendix on an existing digital library system that uses the MG software}"
304,"conduction of heat in solids",129936,"Conduction of Heat in Solids","Based on earlier "" Introduction to Mathematical Theory of Conduction of Heat in Solids,"" volume gives general theory together with its application to linear flow of heat and to flow of heat in rod, rectangular parallelepiped, circular cylinder, sphere and cone; Laplace transformation method of dealing with problems in conduction of heat is used instead of older method of contour integrals. Eng Soc Lib, NY. ($8.00)"
305,"computer simulation of liquids",130030,"Computer Simulation of Liquids","{Computer simulation is an essential tool in studying the chemistry and physics of liquids.  Simulations allow us to develop models and to test them against experimental data. They can be used to evaluate approximate theories of liquids, and to provide detailed information on the structure and dynamics of model liquids at the molecular level. This book is an introduction and practical guide to the molecular dynamics and Monte Carlo methods.    The first four chapters describe these methods in detail, and provide the essential background in intermolecular forces and statistical mechanics.  Chapters 5 and 6 emphasize the practical aspects of writing efficient programs and analysing the simulation results.  The remaining chapters cover advanced techniques, non-equilibrium methods, Brownian dynamics, quantum simulations, and some important applications.  FORTRAN code is presented in the text.}"
306,"on the detection of semantic concepts at trecvid",130273,"On the detection of semantic concepts at TRECVID","Semantic multimedia management is necessary for the effective and widespread utilization of multimedia repositories and realizing the potential that lies untapped in the rich multimodal information content. This challenge has driven researchers to devise new algorithms and systems that enable automatic or semi-automatic tagging of large scale multimedia content with rich semantics. An emerging research area is the detection of a predetermined set of semantic concepts that can act as semantic filters and aid in search, and manipulation. The NIST {TRECVid} benchmark has responded by creating a task that has evaluated the performance of concept detection. Within the scope of this benchmark task, this paper studies trends in the emerging concept detection systems, architectures and algorithms. It also analyzes strategies that have yielded reasonable success, and challenges and gaps that lie ahead."
307,"graphbased algorithms for boolean function manipulation",130315,"Graph-Based Algorithms for Boolean Function Manipulation","In this paper we present a new data structure for representing Boolean functions and an associated set of manipulation algorithms. Functions are represented by directed, acyclic graphs in a manner similar to the representations introduced by Lee [1] and Akers [2], but with further restrictions on the ordering of decision variables in the graph. Although a function requires, in the worst case, a graph of size exponential in the number of arguments, many of the functions encountered in typical applications have a more reasonable representation. Our algorithms have time complexity proportional to the sizes of the graphs being operated on, and hence are quite efficient as long as the graphs do not grow too large. We present experimental results from applying these algorithms to problems in logic design verification that demonstrate the practicality of our approach."
308,"engineered riboregulators enable posttranscriptional control of gene expression",130783,"Engineered riboregulators enable post-transcriptional control of gene expression.","Recent studies have demonstrated the important enzymatic, structural and regulatory roles of RNA in the cell. Here we present a post-transcriptional regulation system in Escherichia coli that uses RNA to both silence and activate gene expression. We inserted a complementary cis sequence directly upstream of the ribosome binding site in a target gene. Upon transcription, this cis-repressive sequence causes a stem-loop structure to form at the 5'-untranslated region of the mRNA. The stem-loop structure interferes with ribosome binding, silencing gene expression. A small noncoding RNA that is expressed in trans targets the cis-repressed RNA with high specificity, causing an alteration in the stem-loop structure that activates expression. Such engineered riboregulators may lend insight into mechanistic actions of endogenous RNA-based processes and could serve as scalable components of biological networks, able to function with any promoter or gene to directly control gene expression."
309,"understanding faulttolerant distributed systems",131324,"Understanding fault-tolerant distributed systems","We propose a small number of basic concepts that can be used to explain the architecture of fault-tolerant distributed systems and we discuss a list of architectural issues that we find useful to consider when designing or examining such systems. For each issue we present known solutions and design alternatives, we discuss their relative merits and we give examples of systems which adopt one approach or the other. The aim is to introduce some order in the complex discipline of designing and..."
310,"mathematical models in biology",131662,"Mathematical Models in Biology","Mathematical Models in Biology is an introductory book for readers interested in biological applications of mathematics and modeling in biology. A favorite in the mathematical biology community since its first publication in 1988, the book shows how relatively simple mathematics can be applied to a variety of models to draw interesting conclusions. Connections are made between diverse biological examples linked by common mathematical themes. A variety of discrete and continuous ordinary and partial differential equation models are explored. Although great advances have taken place in many of the topics covered, the simple lessons contained in Mathematical Models in Biology are still important and informative. <P>Shortly after the first publication of Mathematical Models in Biology, the genomics revolution turned Mathematical Biology into a prominent area of interdisciplinary research. In this new millennium, biologists have discovered that mathematics is not only useful, but indispensable! As a result, there has been much resurgent interest in, and a huge expansion of, the fields collectively called mathematical biology. This book serves as a basic introduction to concepts in deterministic biological modeling. <P>Mathematical Models in Biology does not assume too much background knowledgeessentially some calculus and high-school algebra. It was originally written with third- and fourth-year undergraduate mathematical-biology majors in mind; however, it was picked up by beginning graduate students as well as a number of researchers in math (and some in biology) who wanted to learn about this field."
311,"can programming be liberated from the von neumann style a functional style and its algebra of programs",132329,"Can programming be liberated from the von {N}eumann style? {A} functional style and its algebra of programs","Conventional programming languages are growing ever more enormous, but not stronger. Inherent defects at the most basic level cause them to be both fat and weak: their primitive word-at-a-time style of programming inherited from their common ancestor&mdash;the von Neumann computer, their close coupling of semantics to state transitions, their division of programming into a world of expressions and a world of statements, their inability to effectively use powerful combining forms for building new programs from existing ones, and their lack of useful mathematical properties for reasoning about programs.   An alternative functional style of programming is founded on the use of combining forms for creating programs. Functional programs deal with structured data, are often nonrepetitive and nonrecursive, are hierarchically constructed, do not name their arguments, and do not require the complex machinery of procedure declarations to become generally applicable. Combining forms can use high level programs to build still higher level ones in a style not possible in conventional languages.   Associated with the functional style of programming is an algebra of programs whose variables range over programs and whose operations are combining forms. This algebra can be used to transform programs and to solve equations whose &ldquo;unknowns&rdquo; are programs in much the same way one transforms equations in high school algebra. These transformations are given by algebraic laws and are carried out in the same language in which programs are written. Combining forms are chosen not only for their programming power but also for the power of their associated algebraic laws. General theorems of the algebra give the detailed behavior and termination conditions for large classes of programs.   A new class of computing systems uses the functional programming style both in its programming language and in its state transition rules. Unlike von Neumann languages, these systems have semantics loosely coupled to states&mdash;only one state transition occurs per major computation."
312,"functional analysis",132420,"Functional Analysis","{This classic text is written for graduate courses in functional analysis. This text is used in modern investigations in analysis and applied mathematics. This new edition includes up-to-date presentations of topics as well as more examples and exercises. New topics include Kakutani's fixed point theorem, Lamonosov's invariant subspace theorem, and an ergodic theorem. <p> This text is part of the Walter Rudin Student Series in Advanced Mathematics.}"
313,"connectionist models and their properties",132451,"Connectionist Models and Their Properties","Much of the progress in the fields constituting cognitive science has been based upon the use of explicit information processing models, almost exclusively patterned after conventional serial computers. An extension of these ideas to massively parallel, connectionist models appears to offer a number of advantages. After a preliminary discussion, this paper introduces a general connectionist model and considers how it might be used in cognitive science. Among the issues addressed are: stability and noise-sensitivity, distributed decision-making, time and sequence problems, and the representation of complex concepts."
314,"a logical calculus of the ideas immanent in nervous activity",132637,"A logical calculus of the ideas immanent in nervous activity","Abstract&nbsp;&nbsp;Because of the �? all-or-noneâ? character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed."
315,"explicit substitutions",133546,"Explicit Substitutions","The oe-calculus is a refinement of the -calculus where substitutions are manipulated explicitly. The oe-calculus provides a setting for studying the theory of substitutions, with pleasant mathematical properties. It is also a useful bridge between the classical -calculus  and concrete implementations.   Digital Equipment Corporation, Systems Research Center.  y  Ecole Normale Sup'erieure; part of this work was completed while at Digital Equipment Corporation, Systems Research Center.  z  INRIA..."
316,"link prediction in relational data",134227,"Link prediction in relational data","Many real-world domains are relational in nature, consisting of a set of objects  related to each other in complex ways. This paper focuses on predicting the  existence and the type of links between entities in such domains. We apply the  relational Markov network framework of Taskar et al. to define a joint probabilistic  model over the entire link graph --- entity attributes and links. The application  of the RMN algorithm to this task requires the definition of probabilistic patterns  over subgraph structures. We apply this method to two new relational datasets,  one involving university webpages, and the other a social network. We show that  the collective classification approach of RMNs, and the introduction of subgraph  patterns over link labels, provide significant improvements in accuracy over flat  classification, which attempts to predict each link in isolation."
317,"learning in graphical models",134378,"Learning in Graphical Models","Statistical applications in fields such as bioinformatics, information retrieval, speech processing, image processing and communications often involve large-scale models in which thousands or millions of random variables are linked in complex ways. Graphical models provide a general methodology for approaching these problems, and indeed many of the models developed by researchers in these applied fields are instances of the general graphical model formalism. We review some of the basic ideas underlying graphical models, including the algorithmic ideas that allow graphical models to be deployed in large-scale data analysis problems. We also present examples of graphical models in bioinformatics, error-control coding and language processing. Key words and phrases: Probabilistic graphical models, junction tree algorithm, sum-product algorithm, Markov chain Monte Carlo, variational inference, bioinformatics, error-control coding."
318,"expectation propagation for approximate bayesian inference",134382,"Expectation Propagation for approximate {B}ayesian inference","One of the major obstacles to using Bayesian methods for pattern recognition has been its computational expense. This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible. This method and ``Expectation Propagation and '' unifies and generalizes two previous techniques: assumed-density filtering and an extension of the Kalman filter and and loopy belief propagation and an extension of belief propagation in Bayesian networks. The unification shows how both of these algorithms can be viewed as approximating the true posterior distribution with a simpler distribution and which is close in the sense of KL-divergence. Expectation Propagation exploits the best of both algorithms: the generality of assumed-density filtering and the accuracy of loopy belief propagation. Loopy belief propagation and because it propagates exact belief states and is useful for limited types of belief networks and such as purely discrete networks. Expectation Propagation approximates the belief states with expectations and such as means and variances and giving it much wider scope. Expectation Propagation also extends belief propagation in the opposite direction---propagating richer belief states which incorporate correlations between variables. This framework is demonstrated in a variety of statistical models using synthetic and real-world data. On Gaussian mixture problems and Expectation Propagation is found and for the same amount of computation and to be convincingly better than rival approximation techniques: Monte Carlo and Laplace's method and and variational Bayes. For pattern recognition and Expectation Propagation provides an algorithm for training Bayes Point Machine classifiers that is faster and more accurate than any previously known. The resulting classifiers outperform Support Vector Machines on several standard datasets and in addition to having a comparable training time. Expectation Propagation can also be used to choose an appropriate feature set for classification and via Bayesian model selection."
319,"a phylogenomic approach to bacterial phylogeny evidence of a core of genes sharing a common history",134409,"A phylogenomic approach to bacterial phylogeny: evidence of a core of genes sharing a common history.","It has been claimed that complete genome sequences would clarify phylogenetic relationships between organisms, but up to now, no satisfying approach has been proposed to use efficiently these data. For instance, if the coding of presence or absence of genes in complete genomes gives interesting results, it does not take into account the phylogenetic information contained in sequences and ignores hidden paralogies by using a BLAST reciprocal best hit definition of orthology. In addition, concatenation of sequences of different genes as well as building of consensus trees only consider the few genes that are shared among all organisms. Here we present an attempt to use a supertree method to build the phylogenetic tree of 45 organisms, with special focus on bacterial phylogeny. This led us to perform a phylogenetic study of congruence of tree topologies, which allows the identification of a core of genes supporting similar species phylogeny. We then used this core of genes to infer a tree. This phylogeny presents several differences with the rRNA phylogeny, notably for the position of hyperthermophilic bacteria."
320,"webdava an administratorfree approach to web filesharing",134506,"WebDAVA: An administrator-free approach to Web file-sharing","Collaboration over the Internet depends on the ability of the members of a group to exchange data in a secure yet unobtrusive manner. WebDAVA is a system that allows the users to define their own access-control policies to network resources that they control, enabling secure data sharing within the enterprise. Our design allows users to selectively give fine-grain access to their resources without involving their system administrators. We accomplish this by using authorization credentials that define the users' privileges. Our prototype implements a file-sharing service, where users maintain sensitive-information folders and can allow others to access parts of these. Clients interact with the server over HTTP via a Java applet that transparently handles credential management. This mechanism allows users to share information with users not a priori known to the system, enabling administrator-free management."
321,"managing in an age of modularity",135035,"Managing in an Age of Modularity","Modularity is a familiar principle in the computer industry. Different companies can independently design and produce components, suck as disk drives or operating software, and those modules will fit together into a complex and smoothly functioning product because the module makers obey a given set of design rules. Modularity in manufacturing is already common in many companies. But now a number of them are beginning to extend the approach into the design of their products and services. Modularity in design should tremendously boost the rate of innovation in many industries as it did in the computer industry. As businesses as diverse as auto manufacturing and financial services move toward modular designs, the authors say, competitive dynamics will change enormously. No longer will assemblers control the final product: suppliers of key modules will gain leverage and even take on responsibility for design rules. Companies will compete either by specifying the dominant design rules (as Microsoft does) or by producing excellent modules (as disk drive maker Quantum does). Leaders in a modular industry will control less, so they will have to watch the competitive environment closely for opportunities to link up with other module makers. They will also need to know more: engineering details that seemed trivial at the corporate level may now play a large part in strategic decisions. Leaders will also become knowledge managers internally because they will need to coordinate the efforts of development groups in order to keep them focused on the modular strategies the company is pursuing."
322,"engineering design a systematic approach",135987,"Engineering design : a systematic approach.","{<P>Effective engineering design must be carefully planned and systematically executed. In particular, engineering design methods must integrate the many different aspects of designing and the priorities of the end-user.</P> <P>This proven and internationally recognized text teaches the methods and ideas of engineering design as a condition of successful product development. It breaks down the design process into phases and then into distinct steps, each with its own working methods. Having established itself, in earlier editions, as a key text, <EM>Engineering Design</EM> (3<SUP>rd</SUP> edition) is enhanced with more input from practising engineers, providing more examples of product development; it also tightens the scientific bases of its design ideas with new solution fields in composite components, building methods, mechatronics and adaptronics and pays attention to the economic aspects of design and development including quality assurance. The third edition also integrates electronic design process technology into its methods.</P> <P><EM>Engineering Design</EM> (3<SUP>rd</SUP> edition) is translated and edited by Ken Wallace, Chairman of the Engineering Design Centre at the University of Cambridge and Luciënne Blessing, Professor of Engineering Design at the Technical University of Berlin. It is translated from the sixth German edition.</P> <P>Topics covered include:</P> <UL> <LI>psychology of design; </LI> <LI>product planning and development; </LI> <LI>the design process including design for recycling; </LI> <LI>conceptual design; </LI> <LI>embodiment design; </LI> <LI>size ranges and modular products; </LI> <LI>design for quality and minimum cost.<U> </U></LI></UL> <P>Written to provide students and tutors of engineering design with all the fundamental information they require in a crucial subject, <EM>Engineering Design</EM> (3<SUP>rd</SUP> edition) will also be of immense value as a reference to anyone working in the area.</P>}"
323,"the artificial life roots of artificial intelligence",136219,"The Artificial Life Roots of Artificial Intelligence","Behavior-oriented AI is a scientific discipline that studies how behavior  of agents emerges and becomes intelligent and adaptive. Success of the field  is defined in terms of success in building physical agents that are capable  of maximising their own self-preservation in interaction with a dynamically  changing environment. The paper addresses this artificial life route towards  artificial intelligence and reviews some of the results obtained so far.  1  Official reference: Steels, L. (1994) ..."
324,"distributed operating systems",136259,"Distributed Operating Systems","Distributed operating systems have many aspects in common with centralized ones, but they also differ in certain ways. This paper is intended as an introduction to distributed operating systems, and especially to current university research about them. After a discussion of what constitutes a distributed operating system and how it is distinguished from a computer network, various key design issues are discussed. Then several examples of current research projects are examined in some detail, namely, the Cambridge Distributed Computing System, Amoeba, V, and Eden."
325,"robust and optimal control",136459,"Robust and Optimal Control","{This book provides a comprehensive, step-by-step treatment of the state-space <I>H</I></B></U>&#133;à control theory, reflecting recent theoretical developments this area, in particular, and in the area of robust and <I>H</I></B></U>&#133;  à control theory in general. It offers as self-contained a  presentation as possible and, for reference sake, includes many background  results on linear systems, the theory and application of Riccati equations and model reduction.}"
326,"tools for loading medline into a local relational database",136480,"Tools for loading MEDLINE into a local relational database","BACKGROUND: Researchers who use MEDLINE for text mining, information extraction, or natural language processing may benefit from having a copy of MEDLINE that they can manage locally. The National Library of Medicine (NLM) distributes MEDLINE in eXtensible Markup Language (XML)-formatted text files, but it is difficult to query MEDLINE in that format. We have developed software tools to parse the MEDLINE data files and load their contents into a relational database. Although the task is conceptually straightforward, the size and scope of MEDLINE make the task nontrivial. Given the increasing importance of text analysis in biology and medicine, we believe a local installation of MEDLINE will provide helpful computing infrastructure for researchers. RESULTS: We developed three software packages that parse and load MEDLINE, and ran each package to install separate instances of the MEDLINE database. For each installation, we collected data on loading time and disk-space utilization to provide examples of the process in different settings. Settings differed in terms of commercial database-management system (IBM DB2 or Oracle 9i), processor (Intel or Sun), programming language of installation software (Java or Perl), and methods employed in different versions of the software. The loading times for the three installations were 76 hours, 196 hours, and 132 hours, and disk-space utilization was 46.3 GB, 37.7 GB, and 31.6 GB, respectively. Loading times varied due to a variety of differences among the systems. Loading time also depended on whether data were written to intermediate files or not, and on whether input files were processed in sequence or in parallel. Disk-space utilization depended on the number of MEDLINE files processed, amount of indexing, and whether abstracts were stored as character large objects or truncated. CONCLUSIONS: Relational database (RDBMS) technology supports indexing and querying of very large datasets, and can accommodate a locally stored version of MEDLINE. RDBMS systems support a wide range of queries and facilitate certain tasks that are not directly supported by the application programming interface to PubMed. Because there is variation in hardware, software, and network infrastructures across sites, we cannot predict the exact time required for a user to load MEDLINE, but our results suggest that performance of the software is reasonable. Our database schemas and conversion software are publicly available at http://biotext.berkeley.edu."
327,"the polyadic picalculus a tutorial",136504,"The polyadic pi-calculus: a tutorial","The pi-calculus is a model of concurrent computation based upon the notion of naming. It is first presented in its simplest and original form, with the help of several illustrative applications. Then it is generalized from monadic to polyadic form. Semantics is done in terms of both a reduction system and a version of labelled transitions called commitment; the known algebraic axiomatization of strong bisimilarity is given in the new setting, and so also is a characterization in modal logic. Some theorems about the replication operator are proved. Justification for the polyadic form is provided by the concept of sort, sorting and sort discipline which it supports. Several illustrations of different sortings are given. One example is the presentation of data structures as processes which respect a particular sorting; another is the sorting for a known translation of the lambda-calculus in to pi-calculus. For this translation, the equational validity of beta-conversion is proved with the help of replication theorems. The paper ends with an extension of the pi-calculus to w-order processes, and a brief account of the demonstration by Davide Sangiorgi that higher-order processes may be faithfully encoded at first-order. This extends and strengthens the original result of this kind given by Bent Thomsen for second-order processes."
328,"mining the biomedical literature in the genomic era an overview",136656,"Mining the biomedical literature in the genomic era: an overview.","The past decade has seen a tremendous growth in the amount of experimental and computational biomedical data, specifically in the areas of genomics and proteomics. This growth is accompanied by an accelerated increase in the number of biomedical publications discussing the findings. In the last few years, there has been a lot of interest within the scientific community in literature-mining tools to help sort through this abundance of literature and find the nuggets of information most relevant and useful for specific analysis tasks. This paper provides a road map to the various literature-mining methods, both in general and within bioinformatics. It surveys the disciplines involved in unstructured-text analysis, categorizes current work in biomedical literature mining with respect to these disciplines, and provides examples of text analysis methods applied towards meeting some of the current challenges in bioinformatics."
329,"recognizing names in biomedical texts a machine learning approach",137270,"Recognizing names in biomedical texts: a machine learning approach.","MOTIVATION: With an overwhelming amount of textual information in molecular biology and biomedicine, there is a need for effective and efficient literature mining and knowledge discovery that can help biologists to gather and make use of the knowledge encoded in text documents. In order to make organized and structured information available, automatically recognizing biomedical entity names becomes critical and is important for information retrieval, information extraction and automated knowledge acquisition. RESULTS: In this paper, we present a named entity recognition system in the biomedical domain, called PowerBioNE. In order to deal with the special phenomena of naming conventions in the biomedical domain, we propose various evidential features: (1) word formation pattern; (2) morphological pattern, such as prefix and suffix; (3) part-of-speech; (4) head noun trigger; (5) special verb trigger and (6) name alias feature. All the features are integrated effectively and efficiently through a hidden Markov model (HMM) and a HMM-based named entity recognizer. In addition, a k-Nearest Neighbor (k-NN) algorithm is proposed to resolve the data sparseness problem in our system. Finally, we present a pattern-based post-processing to automatically extract rules from the training data to deal with the cascaded entity name phenomenon. From our best knowledge, PowerBioNE is the first system which deals with the cascaded entity name phenomenon. Evaluation shows that our system achieves the F-measure of 66.6 and 62.2 on the 23 classes of GENIA V3.0 and V1.1, respectively. In particular, our system achieves the F-measure of 75.8 on the ""protein"" class of GENIA V3.0. For comparison, our system outperforms the best published result by 7.8 on GENIA V1.1, without help of any dictionaries. It also shows that our HMM and the k-NN algorithm outperform other models, such as back-off HMM, linear interpolated HMM, support vector machines, C4.5, C4.5 rules and RIPPER, by effectively capturing the local context dependency and resolving the data sparseness problem. Moreover, evaluation on GENIA V3.0 shows that the post-processing for the cascaded entity name phenomenon improves the F-measure by 3.9. Finally, error analysis shows that about half of the errors are caused by the strict annotation scheme and the annotation inconsistency in the GENIA corpus. This suggests that our system achieves an acceptable F-measure of 83.6 on the 23 classes of GENIA V3.0 and in particular 86.2 on the ""protein"" class, without help of any dictionaries. We think that a F-measure of 90 on the 23 classes of GENIA V3.0 and in particular 92 on the ""protein"" class, can be achieved through refining of the annotation scheme in the GENIA corpus, such as flexible annotation scheme and annotation consistency, and inclusion of a reasonable biomedical dictionary. AVAILABILITY: A demo system is available at http://textmining.i2r.a-star.edu.sg/NLS/demo.htm. Technology license is available upon the bilateral agreement."
330,"typed memory management via static capabilities",139333,"Typed memory management via static capabilities","Region-based memory management is an alternative to standard tracing garbage collection that makes operation such as memory deallocation explicit but verifiably safe. In this article, we present a new compiler intermediate language, called the Capability Language (CL), that supports region-based memory management and enjoys a provably safe type systems. Unlike previous region-based type system, region lifetimes need not be lexically scoped, and yet the language may be checked for safety without complex analyses. Therefore, our type system may be deployed in settings such as extensible operating systems where both the performance and safety of untrusted code is important. The central novelty of the language is the use of static capabilities to specify the permissibility of various  operations, such as memory access and deallocation. In order to ensure capabilities are relinquished properly, the type system tracks aliasing information using a form of bounded quantification. Moreover, unlike previous work on region-based type systems, the proof of soundness of our type system is relatively simple, employing only standard syntactic techniques. In order to show how our language may be used in practice, we show how to translate a variant of Tofte and Talpin's high-level type-and-effects system for region-based memory management into our language. When combined with known region inference algorithms, this translation provides a way to compile source-level languages to CL."
331,"development of unified statistical potentials describing proteinprotein interactions",139500,"Development of unified statistical potentials describing protein-protein interactions.","A residue-based and a heavy atom-based statistical pair potential are developed for use in assessing the strength of protein-protein interactions. To ensure the quality of the potentials, a nonredundant, high-quality dimer database is constructed. The protein complexes in this dataset are checked by a literature search to confirm that they form multimers, and the pairwise amino acid preference to interact across a protein-protein interface is analyzed and pair potentials constructed. The performance of the residue-based potentials is evaluated by using four jackknife tests and by assessing the potentials' ability to select true protein-protein interfaces from false ones. Compared to potentials developed for monomeric protein structure prediction, the interdomain potential performs much better at distinguishing protein-protein interactions. The potential developed from homodimer interfaces is almost the same as that developed from heterodimer interfaces with a correlation coefficient of 0.92. The residue-based potential is well suited for genomic scale protein interaction prediction and analysis, such as in a recently developed threading-based algorithm, MULTIPROSPECTOR. However, the more time-consuming atom-based potential performs better in identifying near-native structures from docking generated decoys."
332,"multimeric threadingbased prediction of proteinprotein interactions on a genomic scale application to the saccharomyces cerevisiae proteome",139501,"Multimeric threading-based prediction of protein-protein interactions on a genomic scale: application to the Saccharomyces cerevisiae proteome.","MULTIPROSPECTOR, a multimeric threading algorithm for the prediction of protein-protein interactions, is applied to the genome of Saccharomyces cerevisiae. Each possible pairwise interaction among more than 6000 encoded proteins is evaluated against a dimer database of 768 complex structures by using a confidence estimate of the fold assignment and the magnitude of the statistical interfacial potentials. In total, 7321 interactions between pairs of different proteins are predicted, based on 304 complex structures. Quality estimation based on the coincidence of subcellular localizations and biological functions of the predicted interactors shows that our approach ranks third when compared with all other large-scale methods. Unlike other in silico methods, MULTIPROSPECTOR is able to identify the residues that participate directly in the interaction. Three hundred seventy-four of our predictions can be found by at least one of the other studies, which is compatible with the overlap between two different other methods. From the analysis of the mRNA abundance data, our method does not bias towards proteins with high abundance. Finally, several relevant predictions involved in various functions are presented. In summary, we provide a novel approach to predict protein-protein interactions on a genomic scale that is a useful complement to experimental methods."
333,"hot regions in proteinprotein interactions the organization and contribution of structurally conserved hot spot residues",139503,"Hot regions in protein--protein interactions: the organization and contribution of structurally conserved hot spot residues.","Structurally conserved residues at protein-protein interfaces correlate with the experimental alanine-scanning hot spots. Here, we investigate the organization of these conserved, computational hot spots and their contribution to the stability of protein associations. We find that computational hot spots are not homogeneously distributed along the protein interfaces; rather they are clustered within locally tightly packed regions. Within the dense clusters, they form a network of interactions and consequently their contributions to the stability of the complex are cooperative; however the contributions of independent clusters are additive. This suggests that the binding free energy is not a simple summation of the single hot spot residue contributions. As expected, around the hot spot residues we observe moderately conserved residues, further highlighting the crucial role of the conserved interactions in the local densely packed environment. The conserved occurrence of these organizations suggests that they are advantageous for protein-protein associations. Interestingly, the total number of hydrogen bonds and salt bridges contributed by hot spots is as expected. Thus, H-bond forming residues may use a ""hot spot for water exclusion"" mechanism. Since conserved residues are located within highly packed regions, water molecules are easily removed upon binding, strengthening electrostatic contributions of charge-charge interactions. Hence, the picture that emerges is that protein-protein associations are optimized locally, with the clustered, networked, highly packed structurally conserved residues contributing dominantly and cooperatively to the stability of the complex. When addressing the crucial question of ""what are the preferred ways of proteins to associate"", these findings point toward a critical involvement of hot regions in protein-protein interactions."
334,"identifying biological themes within lists of genes with ease",139978,"Identifying biological themes within lists of genes with EASE","EASE is a customizable software application for rapid biological interpretation of gene lists that result from the analysis of microarray, proteomics, SAGE and other high-throughput genomic data. The biological themes returned by EASE recapitulate manually determined themes in previously published gene lists and are robust to varying methods of normalization, intensity calculation and statistical selection of genes. EASE is a powerful tool for rapidly converting the results of functional genomics studies from 'genes' to 'themes'."
335,"probabilistic reasoning in intelligent systems networks of plausible inference",140049,"Probabilistic Reasoning in Intelligent Systems: Networks of plausible inference","<p><I>Probabilistic Reasoning in Intelligent Systems</I> is a complete and accessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic.<br><p>The author distinguishes syntactic and semantic approaches to uncertainty--and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition--in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information.</p><br><p><I>Probabilistic Reasoning in Intelligent Systems</I> will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.</p>"
336,"the meaning of things domestic symbols and the self",140455,"The Meaning of Things: Domestic Symbols and the Self","The meaning of things is a study of the significance of material possessions in contemporary urban life, and of the ways people carve meaning out of their domestic environment. Drawing on a survey of eighty families in Chicago who were interviewed on the subject of their feelings about common household objects, Mihaly Csikszentmihalyi and Eugene Rochberg-Halton provide a unique perspective on materialism, American culture, and the self. They begin by reviewing what social scientists and philosophers have said about the transactions between people and things. In the model of 'personhood' that the authors develop, goal-directed action and the cultivation of meaning through signs assume central importance. They then relate theoretical issues to the results of their survey. An important finding is the distinction between objects valued for action and those valued for contemplation. The authors compare families who have warm emotional attachments to their homes with those in which a common set of positive meanings is lacking, and interpret the different patterns of involvement. They then trace the cultivation of meaning in case studies of four families. Finally, the authors address what they describe as the current crisis of environmental and material exploitation, and suggest that human capacities for the creation and redirection of meaning offer the only hope for survival. A wide range of scholars - urban and family sociologists, clinical, developmental and environmental psychologists, cultural anthropologists and philosophers, and many general readers - will find this book stimulating and compelling. Cambridge, UK"
337,"a logic of authentication",140474,"A Logic of Authentication","Questions of belief are essential in analyzing protocols for the authentication of principals in distributed computing systems. In this paper we motivate, set out, and exemplify a logic specifically designed for this analysis; we show how various protocols differ subtly with respect to the required initial assumptions of the participants and their final beliefs. Our formalism has enabled us to isolate and express these differences with a precision that was not previously possible. It has drawn attention to features of protocols of which we and their authors were previously unaware, and allowed us to suggest improvements to the protocols. The reasoning about some protocols has been mechanically verified. This paper starts with an informal account of the problem, goes on to explain the formalism to be used, and gives examples of its application to protocols from the literature, both with shared-key cryptography and with public-key cryptography. Some of the examples are chosen because of their practical importance, while others serve to illustrate subtle points of the logic and to explain how we use it. We discuss extensions of the logic motivated by actual practice---for example, in order to account for the use of hash functions in signatures. The final sections contain a formal semantics of the logic and some conclusions. SRC Research Report 39 was originally published on February 28, 1989, and revised on February 22, 1990. This is the main body of the revised version. An appendix to the revised version is available separately. c flDigital Equipment Corporation 1989, 1990 This work may not be copied or reproduced in whole or in part for any commercial"
338,"software engineering a practitioners approach",140507,"Software Engineering: A Practitioner's Approach","For over 20 years, _Software Engineering: A Practitioner's Approach_ has been the best selling guide to software engineering for students and industry professionals alike.  The sixth edition continues to lead the way in software engineering. A new Part 4 on Web Engineering presents a complete engineering approach for the analysis, design, and testing of Web Applications, increasingly important for today's students. Additionally, the UML coverage has been enhanced and signficantly increased in this new edition.  The pedagogy has also been improved in the new edition to include sidebars. They provide information on relevant softare tools, specific work flow for specific kinds of projects, and additional information on various topics. Additionally, Pressman provides a running case study called ""Safe Home"" throughout the book, which provides the application of software engineering to an industry project.  New additions to the book also include chapters on the Agile Process Models, Requirements Engineering, and Design Engineering. The book has been completely updated and contains hundreds of new references to software tools that address all important topics in the book.  The ancillary material for the book includes an expansion of the case study, which illustrates it with UML diagrams. The On-Line Learning Center includes resources for both instructors and students such as checklists, 700 categorized web references, Powerpoints, a test bank, and a software engineering library-containing over 500 software engineering papers. TAKEAWY HERE IS THE FOLLOWING: 1. AGILE PROCESS METHODS ARE COVERED EARLY IN CH. 4 2. NEW PART ON WEB APPLICATIONS --5 CHAPTERS"
339,"the invisible computer why good products can fail the personal computer is so complex and information appliances are the solution",140523,"The Invisible Computer: Why Good Products Can Fail, the Personal Computer Is So Complex, and Information Appliances Are the Solution","{While Donald Norman acknowledges in <I>The Invisible Computer </I> that the personal computer allows for ""flexibility and power,"" he also makes its limitations perfectly clear. Currently, computer users must navigate a sea of guidebooks, frequently asked questions (FAQs), and wizards to perform a task such as searching the Web or creating a spreadsheet. ""The personal computer is perhaps the most frustrating technology ever,"" he writes. ""It should be quiet, invisible, unobtrusive."" His vision is that of the ""information appliance"", digital tools created to answer our specific needs, yet interconnected to allow communication between devices.<p> His solution? ""Design the tool to fit so well that the tool becomes a part of the task."" He proposes using the PC as the infrastructure for devices hidden in walls, in car dashboards, and held in the palm of the hand. A word of caution: some of Norman's zealotry leads to a certain creepiness (global positioning body implants) and goofiness (electric-power-generating plants in shoes). His message, though, is reasonably situated in the concept that the tools should bend to fit us and our goals: we sit down to write, not to word process; to balance bank accounts, not to fill in cells on a spreadsheet. In evenly measuring out the future of humanity's technological needs--and the limitations of the PC's current incarnation--Norman presents a formidable argument for a renaissance of the information appliance. --<I>Jennifer Buckendorff </I>}"
340,"the social life of information",140584,"The Social Life of Information","How many times has your {PC} crashed today? While Gordon Moore's now famous law projecting the doubling of computer power every 18 months has more than borne itself out, it's too bad that a similar trajectory projecting the reliability and usefulness of all that power didn't come to pass, as well. Advances in information technology are most often measured in the cool numbers of megahertz, throughput, and bandwidth{\\textendash}but, for many us, the experience of these advances may be better measured in hours of frustration. The gap between the hype of the Information Age and its reality is often wide and deep, and it's into this gap that John Seely Brown and Paul Duguid plunge. Not that these guys are Luddites{\\textendash}far from it. Brown, the chief scientist at Xerox and the director of its Palo Alto Research Center {(PARC),} and Duguid, a historian and social theorist who also works with {PARC,} measure how information technology interacts and meshes with the social fabric. They write, {'Technology} design often takes aim at the surface of life. There it undoubtedly scores lots of worthwhile hits. But such successes can make designers blind to the difficulty of more serious challenges{\\textendash}primarily the resourcefulness that helps embed certain ways of doing things deep in our lives.' The authors cast their gaze on the many trends and ideas proffered by infoenthusiasts over the years, such as software agents, 'still a long way from the predicted insertion into the woof and warp of ordinary life'; the electronic cottage that Alvin Toffler wrote about 20 years ago and has yet to be fully realized; and the rise of knowledge management and the challenges it faces trying to manage how people actually work and learn in the workplace. Their aim is not to pass judgment but to help remedy the tunnel vision that prevents technologists from seeing larger the social context that their ideas must ultimately inhabit. The Social Life of Information is a thoughtful and challenging read that belongs on the bookshelf of anyone trying to invent or make sense of the new world of information. From the chief scientist of Xerox Corporation and a research specialist in cultural studies at {UC-Berkeley} comes a treatise that casts a critical eye at all the hype surrounding the boom of the information age. The authors' central complaint is that narrowly focusing on new ways to provide information will not create the cyber-revolution so many technology designers have visualized. The problem (or joy) is that information acquires meaning only through social context. Brown and Duguid add a humanist spin to this idea by arguing, for example, that 'trust' is a deep social relation among people and cannot be reduced to logic, and that a satisfying 'conversation' cannot be held in an Internet chat room because too much social context is stripped away and cannot be replaced by just adding more information, such as pictures and biographies of the participants. From this standpoint, Brown and Duguid contemplate the future of digital agents, the home office, the paperless society, the virtual firm and the online university. Though they offer many insightful opinions, they have not produced an easy read. As they point out, theirs is 'more a book of questions than answers' and they often reject 'linear thinking.' Like most futurists, they are fond of long neologisms, but they are given to particularly unpronounceable ones like 'infoprefixification' (the tendency to put 'info' in front of words). The result is an intellectual gem in which the authors have polished some facets and, annoyingly, left others uncut. {(Mar.)}"
341,"literate programming",140600,"Literate Programming","Literate programming is a programming methodology that combines a programming language with a documentation language, making programs more robust, more portable, and more easily maintained than programs written only in a high-level language. Computer programmers already know both kinds of languages; they need only learn a few conventions about alternating between languages to create programs that are works of literature. A literate programmer is an essayist who writes programs for humans to understand, instead of primarily writing instructions for machines to follow. When programs are written in the recommended style they can be transformed into documents by a document compiler and into efficient code by an algebraic compiler. This anthology of essays from the inventor of literate programming includes Knuth's early papers on related topics such as structured programming, as well as the Computer Journal article that launched literate programming itself."
342,"scripting higher level programming for the st century",140611,"Scripting: Higher Level Programming for the 21st Century","A fundamental change is occurring in the way people write computer programs, away from system programming languages such as C or C++ to scripting languages such as Perl or Tcl. Although many people are participating in the change, few realize that the change is occurring and even fewer know why it is happening. This article explains why scripting languages will handle many of the programming tasks in the next century better than system programming languages. System programming languages were designed for building data structures and algorithms from scratch, starting from the most primitive computer elements. Scripting languages are designed for gluing. They assume the existence of a set of powerful components and are intended primarily for connecting components"
343,"evolution of networks from biological nets to the internet and www",140673,"Evolution of Networks: From Biological Nets to the {I}nternet and {WWW}","Only recently did mankind realize that it resides on a world of networks. The Internet and the World Wide Web are changing our life. Our physical existence is based on various biological networks. We have recently learned that the term """"network"""" turns out to be a central notion in our time, and the onsequent explosion of interest in networks is a social and cultural phenomenon. The principles of the complex organization and evolution of networks, natural and artificial are the topic of this book, which is written by physicists and is addressed to all involved researchers and students. The aim of the text is to understand networks and the basic principles of their structural organization and evolution. The ideas are presneted in a clear and pedegogical way, with minimal mathematics, so even students without a deep knowledge of mathematics and statistical physics will be able to rely on this as a reference. Special attention is given to real networks, both natural and artificial. Collected empirical data and numerous real applications of existing theories are discussed in detail, as well as the topical problems of communication networks."
344,"program slicing",140701,"Program Slicing","Program slicing is a method used by experienced computer programmers for abstracting from programs. Starting from a subset of a program's behavior, slicing reduces that program to a minimal form which still produces that behavior. The reduced program, called a &ldquo;slice&rdquo;, is an independent program guaranteed to faithfully represent the original program within the domain of the specified subset of behavior.   Finding a slice is in general unsolvable. A dataflow algorithm is presented for approximating slices when the behavior subset is specified as the values of a set of variables at a statement. Experimental evidence is presented that these slices are used by programmers during debugging. Experience with two automatic slicing tools is summarized. New measures of program complexity are suggested based on the organization of a program's slices."
345,"software systems as complex networks structure function and evolvability of software collaboration graphs",140708,"Software Systems as Complex Networks: Structure, Function, and Evolvability of Software Collaboration Graphs","Software systems emerge from mere keystrokes to form intricate functional networks connecting many collaborating modules, objects, classes, methods, and subroutines. Building on recent advances in the study of complex networks, I have examined software collaboration graphs contained within several open-source software systems, and have found them to reveal scale-free, small-world networks similar to those identified in other technological, sociological, and biological systems. I present several measures of these network topologies, and discuss their relationship to software engineering practices. I also present a simple model of software system evolution based on refactoring processes which captures some of the salient features of the observed systems. Some implications of object-oriented design for questions about network robustness, evolvability, degeneracy, and organization are discussed in the wake of these findings."
346,"growth dynamics of the worldwide web",140717,"Growth Dynamics of the {W}orld-{W}ide {W}eb","The exponential growth of the World-Wide Web has transformed it into an ecology of knowledge in which highly diverse information is linked in an extremely complex and arbitrary manner. But even so, as we show here, there is order hidden in the web. We find that web pages are distributed among sites according to a universal power law: many sites have only a few pages, whereas very few sites have hundreds of thousands of pages. This universal distribution can be explained by using a simple stochastic dynamical growth model."
347,"the fractal geometry of nature",140728,"The Fractal Geometry of Nature","{Imagine an equilateral triangle. Now, imagine smaller equilateral triangles perched in the center of each side of the original triangle--you have a Star of David. Now, place still smaller equilateral triangles in the center of each of the star's 12 sides. Repeat this process infinitely and you have a Koch snowflake, a mind-bending geometric figure with an infinitely large perimeter, yet with a finite area. This is an example of the kind of mathematical puzzles that this book addresses.<p> <I>The Fractal Geometry of Nature</I> is a mathematics text. But buried in the deltas and lambdas and integrals, even a layperson can pick out and appreciate Mandelbrot's point: that somewhere in mathematics, there is an explanation for nature. It is not a coincidence that fractal math is so good at generating images of cliffs and shorelines and capillary beds.}"
348,"exploiting software how to break code",140843,"Exploiting Software: How to Break Code","{Computing hardware would have no value without software; software tells hardware what to do. Software therefore must have special authority within computing systems. All computer security problems stem from that fact, and <I>Exploiting Software: How to Break Code</I> shows you how to design your software so it's as resistant as possible to attack. Sure, everything's phrased in offensive terms (as instructions for the attacker, that is), but this book has at least as much value in showing designers what sorts of attacks their software will face (the book could serve as a checklist for part of a pre-release testing regimen). Plus, the clever reverse-engineering strategies that Greg Hoglund and Gary McGraw teach will be useful in many legitimate software projects. Consider this a recipe book for mayhem, or a compendium of lessons learned by others. It depends on your situation.<p>  PHP programmers will take issue with the authors' blanket assessment of their language (""PHP is a study in bad security""), much of which seems based on older versions of the language that had some risky default behaviors--but those programmers will also double-check their servers' register_globals settings. Users of insufficiently patched Microsoft and Oracle products will worry about the detailed attack instructions this book contains. Responsible programmers and administrators will appreciate what amounts to documentation of attackers' rootkits for various operating systems, and will raise their eyebrows at the techniques for writing malicious code to unused EEPROM chips in target systems. <I>--David Wall</I><p>  <B>Topics covered</B>: How to make software fail, either by doing something it wasn't designed to do, or by denying its use to its rightful users. Techniques--including reverse engineering, buffer overflow, and particularly provision of unexpected input--are covered along with the tools needed to carry them out. A section on hardware viruses is detailed and frightening.}"
349,"introduction to probability",140845,"Introduction to Probability","{An intuitive, yet precise introduction to probability theory, stochastic processes, and probabilistic models used in science, engineering, economics, and related fields. This is the currently used textbook for ""Probabilistic Systems Analysis,"" an introductory probability course at the Massachusetts Institute of Technology, attended by a large number of undergraduate and graduate students.    The book covers the fundamentals of probability theory (probabilistic models, discrete and continuous random variables, multiple random variables, and limit theorems), which are typically part of a first course on the subject. It also contains, a number of more advanced topics, from which an instructor can choose to match the goals of a particular course. These topics include transforms, sums of random variables, least squares estimation, the bivariate normal distribution, and a fairly detailed introduction to Bernoulli, Poisson, and Markov processes.    The book strikes a balance between simplicity in exposition and sophistication in analytical reasoning. Some of the more mathematically rigorous analysis has been just intuitively explained in the text, but is developed in detail (at the level of advanced calculus) in the numerous solved theoretical problems.     The book has been widely adopted for classroom use in introductory probability courses within the USA and abroad.}"
350,"an introduction to database systems",140848,"An Introduction to Database Systems","From the Publisher: For over 25 years, C. J. Date's An Introduction to Database Systems has been the authoritative resource for readers interested in gaining insight into and understanding of the principles of database systems. This revision continues to provide a solid grounding in the foundations of database technology and to provide some ideas as to how the field is likely to develop in the future.. ""Readers of this book will gain a strong working knowledge of the overall structure, concepts, and objectives of database systems and will become familiar with the theoretical principles underlying the construction of such systems."
351,"learning belief networks in the presence of missing values and hidden variables",140994,"Learning Belief Networks in the Presence of Missing Values and Hidden Variables","In recent years there has been a flurry of works on learning probabilistic belief networks. Current state of the art methods have been shown to be successful for two learning scenarios: learning both network structure and parameters from complete data, and learning parameters for a fixed network from incomplete data---that is, in the presence of missing values ---or hidden variables. However, no method has yet been demonstrated to effectively learn network structure from incomplete data. In this paper, we propose a new method for learning network structure from incomplete data. This method is based on an extension of the Expectation-Maximization (EM) algorithm for model selection problems that performs search for the best structure inside the EM procedure. We prove the convergence of this algorithm, and adapt it for learning belief networks. We then describe how to learn networks in two scenarios: when the data contains missing values, and in the presence of hidden variables. We provide experimental results that show the effectiveness of our procedure in both scenarios."
352,"resolution of the early placental mammal radiation using bayesian phylogenetics",141652,"Resolution of the Early Placental Mammal Radiation Using Bayesian Phylogenetics","Molecular phylogenetic studies have resolved placental mammals into four major groups, but have not established the full hierarchy of interordinal relationships, including the position of the root. The latter is critical for understanding the early biogeographic history of placentals. We investigated placental phylogeny using Bayesian and maximum-likelihood methods and a 16.4-kilobase molecular data set. Interordinal relationships are almost entirely resolved. The basal split is between Afrotheria and other placentals, at about 103 million years, and may be accounted for by the separation of South America and Africa in the Cretaceous. Crown-group Eutheria may have their most recent common ancestry in the Southern Hemisphere (Gondwana)."
353,"globus a metacomputing infrastructure toolkit",141729,"Globus: A Metacomputing Infrastructure Toolkit","Emerging high-performance applications require the ability to exploit diverse, geographically distributed resources. These applications use high-speed networks to integrate supercomputers, large databases, archival storage devices, advanced visualization devices, and/or scientific instruments to form networked virtual supercomputers or metacomputers. While the physical infrastructure to build such systems is becoming widespread, the heterogeneous and dynamic nature of the metacomputing environment poses new challenges for developers of system software, parallel tools, and applications. In this article, we introduce Globus, a system that we are developing to address these challenges. The Globus system is intended to achieve a vertically integrated treatment of application, middleware, and network. A low-level toolkit provides basic mechanisms such as communication, authentication, network information, and data access. These mechanisms are used to construct various higher-level metacomputing services, such as parallel programming tools and schedulers. Our long-term goal is to build an Adaptive Wide Area Resource Environment (AWARE), an integrated set of higher-level services that enable applications to adapt to heterogeneous and dynamically changing metacomputing environments. Preliminary versions of Globus components were deployed successfully as part of the I-WAY networking experiment. 1"
354,"algorithms for estimating relative importance in networks",142464,"Algorithms for estimating relative importance in networks","Large and complex graphs representing relationships among sets of entities are an increasingly common focus of interest in data analysis---examples include social networks, Web graphs, telecommunication networks, and biological networks. In interactive analysis of such data a natural query is ""which entities are most important in the network relative to a particular individual or set of individuals?"" We investigate the problem of answering such queries in this paper, focusing in particular on defining and computing the importance of nodes in a graph relative to one or more root nodes. We define a general framework and a number of different algorithms, building on ideas from social networks, graph theory, Markov models, and Web graph analysis. We experimentally evaluate the different properties of these algorithms on toy graphs and demonstrate how our approach can be used to study relative importance in real-world networks including a network of interactions among September 11th terrorists, a network of collaborative research in biotechnology among companies and universities, and a network of co-authorship relationships among computer science researchers."
355,"blink the power of thinking without thinking",142681,"Blink : The Power of Thinking Without Thinking","{<I>Blink</I> is about the first two seconds of looking--the decisive glance that knows in an instant. Gladwell, the best-selling author of <I>The Tipping Point</I>, campaigns for snap judgments and mind reading with a gift for translating research into splendid storytelling. Building his case with scenes from a marriage, heart attack triage, speed dating, choking on the golf course, selling cars, and military maneuvers, he persuades readers to think small and focus on the meaning of ""thin slices"" of behavior. The key is to rely on our ""adaptive unconscious""--a 24/7 mental valet--that provides us with instant and sophisticated information to warn of danger, read a stranger, or react to a new idea. <p>  Gladwell includes caveats about leaping to conclusions: marketers can manipulate our first impressions, high arousal moments make us ""mind blind,"" focusing on the wrong cue leaves us vulnerable to ""the Warren Harding Effect"" (i.e., voting for a handsome but hapless president). In a provocative chapter that exposes the ""dark side of blink,"" he illuminates the failure of rapid cognition in the tragic stakeout and murder of Amadou Diallo in the Bronx. He underlines studies about autism, facial reading and cardio uptick to urge training that enhances high-stakes decision-making.  In this brilliant, cage-rattling book, one can only wish for a thicker slice of Gladwell's ideas about what Blink Camp might look like.  <I>--Barbara Mackoff</I>} {How do we make decisions--good and bad--and why are some people so much better at it than others? Thats the question Malcolm Gladwell asks and answers in the follow-up to his huge bestseller, The Tipping Point. Utilizing case studies as diverse as speed dating, pop music, and the shooting of Amadou Diallo, Gladwell reveals that what we think of as decisions made in the blink of an eye are much more complicated than assumed. Drawing on cutting-edge neuroscience and psychology, he shows how the difference between good decision-making and bad has nothing to do with how much information we can process quickly, but on the few particular details on which we focus. Leaping boldly from example to example, displaying all of the brilliance that made The Tipping Point a classic, Gladwell reveals how we can become better decision makers--in our homes, our offices, and in everyday life. The result is a book that is surprising and transforming. Never again will you think about thinking the same way.}"
356,"unsupervised models for named entity classification",142705,"Unsupervised models for named entity classification","This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier. However, we show that the use of unlabeled  data can reduce the requirements for supervision to just 7 simple seed rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling ..."
357,"genomewide strategies for detecting multiple loci that influence complex diseases",143446,"Genome-wide strategies for detecting multiple loci that influence complex diseases","After nearly 10 years of intense academic and commercial research effort, large genome-wide association studies for common complex diseases are now imminent. Although these conditions involve a complex relationship between genotype and phenotype, including interactions between unlinked loci1, the prevailing strategies for analysis of such studies focus on the locus-by-locus paradigm. Here we consider analytical methods that explicitly look for statistical interactions between loci. We show first that they are computationally feasible, even for studies of hundreds of thousands of loci, and second that even with a conservative correction for multiple testing, they can be more powerful than traditional analyses under a range of models for interlocus interactions. We also show that plausible variations across populations in allele frequencies among interacting loci can markedly affect the power to detect their marginal effects, which may account in part for the well-known difficulties in replicating association results. These results suggest that searching for interactions among genetic loci can be fruitfully incorporated into analysis strategies for genome-wide association studies."
358,"revealing modularity and organization in the yeast molecular network by integrated analysis of highly heterogeneous genomewide data",143501,"Revealing modularity and organization in the yeast molecular network by integrated analysis of highly heterogeneous genomewide data","AB - The dissection of complex biological systems is a challenging task, made difficult by the size of the underlying molecular network and the heterogeneous nature of the control mechanisms involved. Novel high-throughput techniques are generating massive data sets on various aspects of such systems. Here, we perform analysis of a highly diverse collection of genomewide data sets, including gene expression, protein interactions, growth phenotype data, and transcription factor binding, to reveal the modular organization of the yeast system. By integrating experimental data of heterogeneous sources and types, we are able to perform analysis on a much broader scope than previous studies. At the core of our methodology is the ability to identify modules, namely, groups of genes with statistically significant correlated behavior across diverse data sources. Numerous biological processes are revealed through these modules, which also obey global hierarchical organization. We use the identified modules to study the yeast transcriptional network and predict the function of >800 uncharacterized genes. Our analysis framework, SAMBA (Statistical-Algorithmic Method for Bicluster Analysis), enables the processing of current and future sources of biological information and is readily extendable to experimental techniques and higher organisms"
359,"an expanded genomescale model of escherichia coli k ijr gsmgpr",143515,"An expanded genome-scale model of Escherichia coli K-12 (iJR904 GSM/GPR)","BACKGROUND: Diverse datasets, including genomic, transcriptomic, proteomic and metabolomic data, are becoming readily available for specific organisms. There is currently a need to integrate these datasets within an in silico modeling framework. Constraint-based models of Escherichia coli K-12 MG1655 have been developed and used to study the bacterium's metabolism and phenotypic behavior. The most comprehensive E. coli model to date (E. coli iJE660a GSM) accounts for 660 genes and includes 627 unique biochemical reactions. RESULTS: An expanded genome-scale metabolic model of E. coli (iJR904 GSM/GPR) has been reconstructed which includes 904 genes and 931 unique biochemical reactions. The reactions in the expanded model are both elementally and charge balanced. Network gap analysis led to putative assignments for 55 open reading frames (ORFs). Gene to protein to reaction associations (GPR) are now directly included in the model. Comparisons between predictions made by iJR904 and iJE660a models show that they are generally similar but differ under certain circumstances. Analysis of genome-scale proton balancing shows how the flux of protons into and out of the medium is important for maximizing cellular growth. CONCLUSIONS: E. coli iJR904 has improved capabilities over iJE660a. iJR904 is a more complete and chemically accurate description of E. coli metabolism than iJE660a. Perhaps most importantly, iJR904 can be used for analyzing and integrating the diverse datasets. iJR904 will help to outline the genotype-phenotype relationship for E. coli K-12, as it can account for genomic, transcriptomic, proteomic and fluxomic data simultaneously."
360,"metaphoric structuring understanding time through spatial metaphors",144073,"Metaphoric Structuring: Understanding time through spatial metaphors","{The present paper evaluates the claim that abstract conceptual domains are structured through metaphorical mappings from domains grounded directly in experience. In particular, the paper asks whether the abstract domain of time gets its relational structure from the more concrete domain of space. Relational similarities between space and time are outlined along with several explanations of how these similarities may have arisen. Three experiments designed to distinguish between these explanations are described. The results indicate that (1) the domains of space and time do share conceptual structure, (2) spatial relational information is just as useful for thinking about time as temporal information, and (3) with frequent use, mappings between space and time come to be stored in the domain of time and so thinking about time does not necessarily require access to spatial schemas. These findings provide some of the first empirical evidence for Metaphoric Structuring. It appears that abstract domains such as time are indeed shaped by metaphorical mappings from more concrete and experiential domains such as space.}"
361,"speaking from intention to articulation",144184,"Speaking : From intention to articulation.","(from the jacket) In ""Speaking,"" Willem ""Pim"" Levelt, Director of the Max-Planck-Institut fur Psycholinguistik, accomplishes the formidable task of covering the entire process of speech production, from constraints on conversational appropriateness to articulation and self-monitoring of speech. ""Speaking"" is unique in its balanced coverage of all major aspects of the production of speech, in the completeness of its treatment of the entire speech process, and in its strategy of exemplifying rather than formalizing theoretical issues. /// Levelt provides a theoretically coherent picture of the speaker as information processor. He proposes a modular organization of relatively autonomous processors for message generation, grammatical encoding, phonological encoding, and articulation. For each processor, he distinguishes carefully between its operations and its input and output representation. The chapters are arranged so that the reader acquires more knowledge of semantics, syntax, lexical structure, and phonology as it becomes necessary in order to understand processing. /// Throughout the book, Levelt acknowledges the purposes served by the speaker's information processing--particularly in the sections on discourse processing, requesting, referring, monitoring, and self-repair. (PsycINFO Database Record (c) 2005 APA, all rights reserved)."
362,"the connectivity structure giant strong component and centrality of metabolic networks",144289,"The connectivity structure, giant strong component and centrality of metabolic networks","Motivation:Structural and functional analysis of genome-based  large-scale metabolic networks is important for understanding the design principles and regulation of the metabolism at a system level. The metabolic network  is conventionally considered to be highly integrated and very complex. A  rational reduction of the metabolic network to its core structure and a deeper  understanding of its functional modules are important.Results: In this work, we show that the metabolites in a metabolic  network are far from fully connected. A connectivity structure consisting of  four major subsets of metabolites and reactions, i.e. a fully connected  sub-network, a substrate subset, a product subset and an isolated subset is  found to exist in metabolic networks of 65 fully sequenced organisms. The  largest fully connected part of a metabolic network, called âthe giant strong component (GSC)â, represents the most complicated part and the core  of the network and has the feature of scale-free networks. The average path  length of the whole network is primarily determined by that of the GSC. For most  of the organisms, GSC normally contains less than one-third of the nodes of  the network. This connectivity structure is very similar to theâ bow-tieâ  structure of World Wide Web. Our results indicate that the bow-tie structure  may be common for large-scale directed networks. More importantly, the  uncovered structure feature makes a structural and functional analysis of large-scale metabolic network more amenable. As shown in this work,  comparing the closeness centrality of the nodes in the GSC can identify the most  central metabolites of a metabolic network. To quantitatively characterize  the overall connection structure of the GSC we introduced the term âoverall  closeness centralization index (OCCI)â. OCCI correlates well with the average path length of the GSC and is a useful parameter for a system-level  comparison of metabolic networks of different organisms.Contact: aze@gbf.deSupplementary Information: http://genome.gbf.de/bioinformatics/"
363,"bayes empirical bayes inference of amino acid sites under positive selection",147383,"Bayes Empirical Bayes Inference of Amino Acid Sites Under Positive Selection","Codon-based substitution models have been widely used to identify amino acid sites under positive selection in comparative analysis of protein-coding DNA sequences. The nonsynonymous-synonymous substitution rate ratio (dN/dS, denoted {omega}) is used as a measure of selective pressure at the protein level, with {omega} > 1 indicating positive selection. Statistical distributions are used to model the variation in {omega} among sites, allowing a subset of sites to have {omega} > 1 while the rest of the sequence may be under purifying selection with {omega} < 1. An empirical Bayes (EB) approach is then used to calculate posterior probabilities that a site comes from the site class with {omega} > 1. Current implementations, however, use the naive EB (NEB) approach and fail to account for sampling errors in maximum likelihood estimates of model parameters, such as the proportions and {omega} ratios for the site classes. In small data sets lacking information, this approach may lead to unreliable posterior probability calculations. In this paper, we develop a Bayes empirical Bayes (BEB) approach to the problem, which assigns a prior to the model parameters and integrates over their uncertainties. We compare the new and old methods on real and simulated data sets. The results suggest that in small data sets the new BEB method does not generate false positives as did the old NEB approach, while in large data sets it retains the good power of the NEB approach for inferring positively selected sites. 10.1093/molbev/msi097"
364,"the social cost of cheap pseudonyms",148884,"The social cost of cheap pseudonyms","We conside r the problems of societal norms for cooperation and reputation when it is possible to obtain cheap pseudonyms , something that is becoming quite common in a wide variety of interactions on the Internet . This intro- duces opportunities to misbehave without paying reputational consequences. A large degree of cooperation can still emerge, through a conventio n in which newcomer s ``pay their dues '' by accepting poor treatment from players who have established positive reputations. One might hope for an open society where newcomers are treate d well, but there is an inherent social cost in making the spread of reputations optional. We prove that no equilibrium can sustai n signi??cantly more cooperation than the dues-paying equilibrium in a repeated random matching game with a large number of players in which players have ??nite lives and the ability to change their identities, and there is a small but nonvanishing probability of mistakes. Although one could remove the inef??ciency of mistreating newcomers by disallowing anonymity, this is not practica l or desirable in a wide variety of transactions. We discus s the use of entry fees, which permits newcomer s to be trusted but excludes some players with low payoffs, thus introducing a different inef??ciency. We also discuss the use of free but unreplaceabl e pseudonyms , and describe a mech- anism that implements them using standar d encryption techniques, which could be practically implemented in electronic transactions."
365,"ant colony system a cooperative learning approach to the traveling salesman problem",149346,"Ant Colony System: A Cooperative Learning Approach to the Traveling Salesman Problem","This paper introduces the ant colony system (ACS), a distributed algorithm that is applied to the traveling salesman problem (TSP). In the ACS, a set of cooperating agents called ants cooperate to find good solutions to TSPs. Ants cooperate using an indirect form of communication mediated by a pheromone they deposit on the edges of the TSP graph while building solutions. We study the ACS by running experiments to understand its operation. The results show that the ACS outperforms other nature-inspired algorithms such as simulated annealing and evolutionary computation, and we conclude comparing ACS-3-opt, a version of the ACS augmented with a local search procedure, to some of the best performing algorithms for symmetric and asymmetric TSPs"
366,"principled design of the modern web architecture",149347,"Principled design of the modern Web architecture","The World Wide Web has succeeded in large part because its software architecture has been designed to meet the needs of an Internet-scale distributed hypermedia application. The modern Web architecture emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. In this article we introduce the Representational State Transfer {(REST)} architectural style, developed as an abstract model of the Web architecture and used to guide our redesign and definition of the Hypertext Transfer Protocol and Uniform Resource Identifiers. We describe the software engineering principles guiding {REST} and the interaction constraints chosen to retain those principles, contrasting them to the constraints of other architectural styles. We then compare the abstract model to the currently deployed Web architecture in order to elicit mismatches between the existing protocols and the applications they are intended to support."
367,"an optimal algorithm for approximate nearest neighbor searching fixed dimensions",150208,"An optimal algorithm for approximate nearest neighbor searching fixed dimensions","Consider a set of  S  of  n  data points  in real  d -dimensional space, R d , where distances are measured using any Minkowski metric. In nearest neighbor searching, we preprocess  S  into a data structure, so that given any query point  q   &isin;  R d , is the closest point of S to  q  can be reported quickly. Given any positive real &egr;, data point  p  is a (1 +&egr;)- approximate nearest neighbor  of  q  if its distance from  q  is within a factor of (1 + &egr;) of the distance to the true nearest neighbor. We show that it is possible to preprocess a    set of  n  points in     R d  in  O(dn  log  n ) time and  O(dn)  space, so that given a query point   q     &isin;  R d , and &egr; &gt; 0, a (1 + &egr;)-approximate nearest neighbor of  q  can be computed in  O ( c d , &egr;  log  n ) time, where  c d,&egr; &le; d     1 + 6d/ e ; d  is a factor depending only on dimension and &egr;. In general, we show that given an integer  k  &ge; 1, (1 + &egr;)-approximations  to the   k  nearest neighbors of  q  can  be computed in additional  O(kd  log  n ) time."
368,"ten lectures on wavelets",152287,"Ten Lectures on Wavelets","This book is both a tutorial on wavelets and a review of the most advanced research in this domain. The topics covered include the continuous wavelet transform of J. Morlet and A. Grossmann, several important results on time-frequency localization (such as the Balian-Low theorem and the Landau-Pollack-Slepian theory), a complete study of the ""frame"" structure for a discrete set of wavelets, the multiresolution analysis and orthonormal bases of Y. Meyer, and finally the beautiful construction of compactly supported wavelets from filter banks (which is due to the author). <P> As mentioned in the introduction, this is a mathematics book that states and proves many theorems. In addition, it also gives many practical examples and describes several applications (in particular, in signal processing, image coding and numerical analysis)."
369,"feature selection evaluation application and small sample performance",152445,"Feature selection - evaluation, application, and small sample performance","A large number of algorithms have been proposed for feature subset selection. Our experimental results show that the sequential forward floating selection (SFFS) algorithm, proposed by Pudil et al., dominates the other algorithms tested. We study the problem of choosing an optimal feature set for land use classification based on SAR satellite images using four different texture models. Pooling features derived from different texture models, followed by a feature selection results in a substantial improvement in the classification accuracy. We also illustrate the dangers of using feature selection in small sample size situations."
370,"genome sequence assembly algorithms and issues",153816,"Genome Sequence Assembly: Algorithms and Issues","Ultimately, genome sequencing seeks to provide an organism's complete DNA sequence. Automation of DNA sequencing allowed scientists to decode entire genomes and gave birth to genomics, the analytic and comparative study of genomes. Although genomes can include billions of nucleotides, the chemical reactions researchers use to decode the DNA are accurate for only about 600 to 700 nucleotides at a time. The DNA reads that sequencing produces must then be assembled into a complete picture of the genome. Errors and certain DNA characteristics complicate assembly. Resolving these problems entails an additional and costly finishing phase that involves extensive human intervention. Assembly programs can dramatically reduce this cost by taking into account additional information obtained during finishing. The paper considers how algorithms that can assemble millions of DNA fragments into gene sequences underlie the current revolution in biotechnology, helping researchers build the growing database of complete genomes"
371,"a wholegenome assembly of drosophila",153821,"A Whole-Genome Assembly of Drosophila","We report on the quality of a whole-genome assembly of Drosophila melanogaster and the nature of the computer algorithms that accomplished it. Three independent external data sources essentially agree with and support the assembly's sequence and ordering of contigs across the euchromatic portion of the genome. In addition, there are isolated contigs that we believe represent nonrepetitive pockets within the heterochromatin of the centromeres. Comparison with a previously sequenced 2.9- megabase region indicates that sequencing accuracy within nonrepetitive segments is greater than 99. 99% without manual curation. As such, this initial reconstruction of the Drosophila sequence should be of substantial value to the scientific community."
372,"a comparative review of statistical methods for discovering differentially expressed genes in replicated microarray experiments",153873,"A comparative review of statistical methods for discovering differentially expressed genes in replicated microarray experiments","Motivation: A common task in analyzing microarray data is to   determine which genes are differentially expressed across two kinds of tissue samples or samples obtained under two experimental   conditions. Recently several statistical methods have been proposed to accomplish this goal when there are replicated samples under each condition. However, it may not be clear how these methods compare with each other. Our main goal here is to compare three methods, the t-test, a regression modeling approach (Thomas et al. , Genome Res. , 11, 1227-1236, 2001) and a mixture model approach (Pan et al. , http://www.biostat.umn.edu/cgi-bin/rrs?print+2001,2001a,b) with particular attention to their different modeling assumptions.  Results: It is pointed out that all the three methods are based   on using the two-sample t-statistic or its minor variation,   but they differ in how to associate a statistical significance level to the corresponding statistic, leading to possibly large difference in the resulting significance levels and the numbers of genes detected. In particular, we give an explicit formula for the test statistic used in the regression approach. Using the leukemia data of Golub et al. (Science , 285,  531-537, 1999), we illustrate these points. We also briefly   compare the results with those of several other methods, including   the empirical Bayesian method of Efron et al. (J. Am. Stat. Assoc. , to appear, 2001) and the Significance Analysis of Microarray (SAM) method of Tusher et al. (Proc. Natl Acad. Sci. USA , 98, 5116-5121, 2001).  Contact: weip@biostat.umn.edu 10.1093/bioinformatics/18.4.546"
373,"applied regression analysis wiley series in probability and statistics",155094,"Applied Regression Analysis (Wiley Series in Probability and Statistics)","{An outstanding introduction to the fundamentals of regression analysis-updated and expanded The methods of regression analysis are the most widely used statistical tools for discovering the relationships among variables. This classic text, with its emphasis on clear, thorough presentation of concepts and applications, offers a complete, easily accessible introduction to the fundamentals of regression analysis. Assuming only a basic knowledge of elementary statistics, Applied Regression Analysis, Third Edition focuses on the fitting and checking of both linear and nonlinear regression models, using small and large data sets, with pocket calculators or computers. This Third Edition features separate chapters on multicollinearity, generalized linear models, mixture ingredients, geometry of regression, robust regression, and resampling procedures. Extensive support materials include sets of carefully designed exercises with full or partial solutions and a series of true/false questions with answers. All data sets used in both the text and the exercises can be found on the companion disk at the back of the book. For analysts, researchers, and students in university, industrial, and government courses on regression, this text is an excellent introduction to the subject and an efficient means of learning how to use a valuable analytical tool. It will also prove an invaluable reference resource for applied scientists and statisticians.}"
374,"variational inference for dirichlet process mixtures",155605,"Variational inference for {D}irichlet process mixtures","Abstract. Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP mixtures has enabled the application of nonparametric Bayesian methods to a variety of practical data analysis problems. However, MCMC sampling can be prohibitively slow, and it is important to explore alternatives. One class of alternatives is provided by variational methods, a class of deterministic algorithms that convert inference problems into optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far, variational methods have mainly been explored in the parametric setting, in particular within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001; Blei et al. 2003). In this paper, we present a variational inference algorithm for DP mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms for DP mixtures of Gaussians and present an application to a large-scale image analysis problem."
375,"mixtures of dirichlet processes with applications to bayesian nonparametric problems",155617,"Mixtures of {D}irichlet processes with applications to {B}ayesian nonparametric problems","A random process called the Dirichlet process whose sample functions are almost surely probability measures has been proposed by Ferguson as an approach to analyzing nonparametric problems from a Bayesian viewpoint. An important result obtained by Ferguson in this approach is that if observations are made on a random variable whose distribution is a random sample function of a Dirichlet process, then the conditional distribution of the random measure can be easily calculated, and is again a Dirichlet process. This paper extends Ferguson's result to cases where the random measure is a mixing distribution for a parameter which determines the distribution from which observations are made. The conditional distribution of the random measure, given the observations, is no longer that of a simple Dirichlet process, but can be described as being a mixture of Dirichlet processes. This paper gives a formal definition for these mixtures and develops several theorems about their properties, the most important of which is a closure property for such mixtures. Formulas for computing the conditional distribution are derived and applications to problems in bio-assay, discrimination, regression, and mixing distributions are given."
376,"a tutorial on principal component analysis",155618,"A Tutorial on Principal Component Analysis","This tutorial is designed to give the reader an understanding of Principal Components Analysis (PCA). PCA is a useful statistical technique that has found application in fields such as face recognition and image compression, and is a common technique for finding patterns in data of high dimension. Before getting to a description of PCA, this tutorial first introduces mathematical concepts that will be used in PCA. It covers standard deviation, covariance, eigenvec- tors and eigenvalues. This background knowledge is meant to make the PCA section very straightforward, but can be skipped if the concepts are already familiar. There are examples all the way through this tutorial that are meant to illustrate the concepts being discussed. If further information is required, the mathematics textbook “Elementary Linear Algebra 5e” by Howard Anton, Publisher John Wiley & Sons Inc, ISBN 0-471-85223-6 is a good source of information regarding the mathematical back- ground"
377,"variational algorithms for approximate bayesian inference",155625,"Variational algorithms for approximate {B}ayesian inference","The Bayesian framework for machine learning allows for the incorporation of prior knowledge in a coherent way, avoids overfitting problems, and provides a principled basis for selecting between alternative models. Unfortunately the computations required are usually intractable. This thesis presents a unified variational Bayesian (VB) framework which approximates these computations in models with latent variables using a lower bound on the marginal likelihood. Chapter 1 presents background material on Bayesian inference, graphical models, and propaga- tion algorithms. Chapter 2 forms the theoretical core of the thesis, generalising the expectation- maximisation (EM) algorithm for learning maximum likelihood parameters to the VB EM al- gorithm which integrates over model parameters. The algorithm is then specialised to the large family of conjugate-exponential (CE) graphical models, and several theorems are presented to pave the road for automated VB derivation procedures in both directed and undirected graphs (Bayesian and Markov networks, respectively). Chapters 3-5 derive and apply the VB EM algorithm to three commonly-used and important models: mixtures of factor analysers, linear dynamical systems, and hidden Markov models. It is shown how model selection tasks such as determining the dimensionality, cardinality, or number of variables are possible using VB approximations. Also explored are methods for combining sampling procedures with variational approximations, to estimate the tightness of VB bounds and to obtain more effective sampling algorithms. Chapter 6 applies VB learning to a long-standing problem of scoring discrete-variable directed acyclic graphs, and compares the performance to annealed importance sampling amongst other methods. Throughout, the VB approximation is compared to other methods including sampling, Cheeseman-Stutz, and asymptotic approximations such as BIC. The thesis concludes with a discussion of evolving directions for model selection including infinite models and alternative approximations to the marginal likelihood."
378,"modeling annotated data",155630,"Modeling annotated data","We consider the problem of modeling annotated data-data with multiple types where the instance of one type (such as a caption) serves as a description of the other type (such as an image). We describe three hierarchical probabilistic mixture models which aim to describe such data, culminating in correspondence latent Dirichlet allocation, a latent variable model that is effective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type. We conduct experiments on the Corel database of images and captions, assessing performance in terms of held-out likelihood, automatic annotation, and text-based image retrieval. Sponsors: ACM/SIGIR"
379,"latent dirichlet allocation",155631,"Latent {D}irichlet Allocation","We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of  discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each  item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in  turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of  text modeling, the topic probabilities provide an explicit representation of a document. We present  efficient approximate inference techniques based on variational methods and an EM algorithm for  empirical Bayes parameter estimation. We report results in document modeling, text classification,  and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI  model."
380,"hierarchical topic models and the nested chinese restaurant process",155633,"Hierarchical Topic Models and the Nested {C}hinese Restaurant Process","We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts."
381,"a bayesian analysis of some nonparametric problems",155650,"A {B}ayesian analysis of some nonparametric problems","The Bayesian approach to statistical problems, though fruitful in many ways, has been rather unsuccessful in treating nonparametric problems. This is due primarily to the difficulty in finding workable prior distributions on the parameter space, which in nonparametric ploblems is taken to be a set of probability distributions on a given sample space. There are two desirable properties of a prior distribution for nonparametric problems. (I) The support of the prior distribution should be large--with respect to some suitable topology on the space of probability distributions on the sample space. (II) Posterior distributions given a sample of observations from the true probability distribution should be manageable analytically. These properties are antagonistic in the sense that one may be obtained at the expense of the other. This paper presents a class of prior distributions, called Dirichlet process priors, broad in the sense of (I), for which (II) is realized, and for which treatment of many nonparametric statistical problems may be carried out, yielding results that are comparable to the classical theory. In Section 2, we review the properties of the Dirichlet distribution needed for the description of the Dirichlet process given in Section 3. Briefly, this process may be described as follows. Let $\mathscr{X}$ be a space and $\mathscr{A}$ a $\sigma$-field of subsets, and let $\alpha$ be a finite non-null measure on $(\mathscr{X}, \mathscr{A})$. Then a stochastic process $P$ indexed by elements $A$ of $\mathscr{A}$, is said to be a Dirichlet process on $(\mathscr{X}, \mathscr{A})$ with parameter $\alpha$ if for any measurable partition $(A_1, \cdots, A_k)$ of $\mathscr{X}$, the random vector $(P(A_1), \cdots, P(A_k))$ has a Dirichlet distribution with parameter $(\alpha(A_1), \cdots, \alpha(A_k)). P$ may be considered a random probability measure on $(\mathscr{X}, \mathscr{A})$, The main theorem states that if $P$ is a Dirichlet process on $(\mathscr{X}, \mathscr{A})$ with parameter $\alpha$, and if $X_1, \cdots, X_n$ is a sample from $P$, then the posterior distribution of $P$ given $X_1, \cdots, X_n$ is also a Dirichlet process on $(\mathscr{X}, \mathscr{A})$ with a parameter $\alpha + \sum^n_1 \delta_{x_i}$, where $\delta_x$ denotes the measure giving mass one to the point $x$. In Section 4, an alternative definition of the Dirichlet process is given. This definition exhibits a version of the Dirichlet process that gives probability one to the set of discrete probability measures on $(\mathscr{X}, \mathscr{A})$. This is in contrast to Dubins and Freedman [2], whose methods for choosing a distribution function on the interval [0, 1] lead with probability one to singular continuous distributions. Methods of choosing a distribution function on [0, 1] that with probability one is absolutely continuous have been described by Kraft [7]. The general method of choosing a distribution function on [0, 1], described in Section 2 of Kraft and van Eeden [10], can of course be used to define the Dirichlet process on [0, 1]. Special mention must be made of the papers of Freedman and Fabius. Freedman [5] defines a notion of tailfree for a distribution on the set of all probability measures on a countable space $\mathscr{X}$. For a tailfree prior, posterior distribution given a sample from the true probability measure may be fairly easily computed. Fabius [3] extends the notion of tailfree to the case where $\mathscr{X}$ is the unit interval [0, 1], but it is clear his extension may be made to cover quite general $\mathscr{X}$. With such an extension, the Dirichlet process would be a special case of a tailfree distribution for which the posterior distribution has a particularly simple form. There are disadvantages to the fact that $P$ chosen by a Dirichlet process is discrete with probability one. These appear mainly because in sampling from a $P$ chosen by a Dirichlet process, we expect eventually to see one observation exactly equal to another. For example, consider the goodness-of-fit problem of testing the hypothesis $H_0$ that a distribution on the interval [0, 1] is uniform. If on the alternative hypothesis we place a Dirichlet process prior with parameter $\alpha$ itself a uniform measure on [0, 1], and if we are given a sample of size $n \geqq 2$, the only nontrivial nonrandomized Bayes rule is to reject $H_0$ if and only if two or more of the observations are exactly equal. This is really a test of the hypothesis that a distribution is continuous against the hypothesis that it is discrete. Thus, there is still a need for a prior that chooses a continuous distribution with probability one and yet satisfies properties (I) and (II). Some applications in which the possible doubling up of the values of the observations plays no essential role are presented in Section 5. These include the estimation of a distribution function, of a mean, of quantiles, of a variance and of a covariance. A two-sample problem is considered in which the Mann-Whitney statistic, equivalent to the rank-sum statistic, appears naturally. A decision theoretic upper tolerance limit for a quantile is also treated. Finally, a hypothesis testing problem concerning a quantile is shown to yield the sign test. In each of these problems, useful ways of combining prior information with the statistical observations appear. Other applications exist. In his Ph. D. dissertation [1], Charles Antoniak finds a need to consider mixtures of Dirichlet processes. He treats several problems, including the estimation of a mixing distribution, bio-assay, empirical Bayes problems, and discrimination problems."
382,"probabilistic models for unified collaborative and contentbased recommendation in sparsedata environments",155697,"Probabilistic Models for Unified Collaborative and Content-Based Recommendation in Sparse-Data Environments","Recommender systems leverage product and community information to target products to consumers. Researchers have developed collaborative recommenders, content-based recommenders, and a few hybrid systems. We propose a unified probabilistic framework for merging collaborative and content-based recommendations. We extend Hofmann&#039;s (1999) aspect model to incorporate three-way co-occurrence data among users, items, and item content. The relative influence of collaboration data versus content data is not imposed as an exogenous parameter, but rather emerges naturally from the given data sources. However, global probabilistic models coupled with standard EM learning algorithms tend to drastically overfit in the sparsedata situations typical of recommendation applications. We show that secondary content information can often be used to overcome sparsity. Experiments on data from the ResearchIndex library of Computer Science publications show that appropriate mixture models incorporating secondary data produce significantly better quality recommenders than k-nearest neighbors (k-NN). Global probabilistic models also allow more general inferences than local methods like k-NN."
383,"normalized cuts and image segmentation",155710,"Normalized cuts and image segmentation","We propose a novel approach for solving the perceptual grouping problem in vision.  Rather than focusing on local features and their consistencies in the image data,  our approach aims at extracting the global impression of an image. We treat image  segmentation as a graph partitioning problem and propose a novel global criterion, the  normalized cut, for segmenting the graph. The normalized cut criterion measures both  the total dissimilarity between the different groups as well as the total similarity within  the groups. We show that an efficient computational technique based on a generalized  eigenvalue problem can be used to optimize this criterion. We have applied this approach  to segmenting static images as well as motion sequences and found results very  encouraging.  Keywords: grouping, image segmentation, graph partition  1 Introduction  Nearly 75 years ago, Wertheimer[24] launched the Gestalt approach which laid out the importance of perceptual grouping and organization in v..."
384,"statistical models for text segmentation",155728,"Statistical models for text segmentation","This paper introduces a new statistical approach to automatically partitioning text into coherent segments. The approach is based on a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text. The models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may be domain-specific, that tend to be used near segment boundaries. Assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, Wall Street Journal news articles and television broadcast news story transcripts. Quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. This metric is used to make a quantitative assessment of the relative contributions of the different feature types, as well as a comparison with decision trees and previously proposed text segmentation algorithms."
385,"the omega test a fast and practical integer programming algorithm for dependence analysis",157135,"The Omega test: a fast and practical integer programming algorithm for dependence analysis","The Omega testi s ani nteger programmi ng algori thm that can determi ne whether a dependence exi sts between two array references, and i so, under what condi7: ns. Conventi nalwi[A m holds thati nteger programmiB techni:36 are far too expensi e to be used for dependence analysi6 except as a method of last resort for si:8 ti ns that cannot be deci:A by si[976 methods. We present evi[77B that suggests thiwi sdomi s wrong, and that the Omega testi s competi ti ve wi th approxi mate algori thms..."
386,"seriously considering play designing interactive learning environments based on the blending of microworlds simulations and games",158329,"Seriously Considering Play: Designing interactive learning environments based on the blending of microworlds, simulations, and games","Abstract Little attention has been given to the psychological and sociological value of play despite its many advantages to guiding the design of interactive multimedia learning environments for children and adults. This paper provides a brief overview of the history, research, and theory related to play. Research from education, psychology, and anthropology suggests that play is a powerful mediator for learning throughout a person's life. The time has come to couple the ever increasing processing capabilities of computers with the advantages of play. The design of hybrid interactive learning environments is suggested based on the constructivist concept of a microworld and supported with elements of both games and simulations."
387,"practical lessons from protein structure prediction",158597,"Practical lessons from protein structure prediction.","Despite recent efforts to develop automated protein structure determination protocols, structural genomics projects are slow in generating fold assignments for complete proteomes, and spatial structures remain unknown for many protein families. Alternative cheap and fast methods to assign folds using prediction algorithms continue to provide valuable structural information for many proteins. The development of high-quality prediction methods has been boosted in the last years by objective community-wide assessment experiments. This paper gives an overview of the currently available practical approaches to protein structure prediction capable of generating accurate fold assignment. Recent advances in assessment of the prediction quality are also discussed."
388,"the proposition bank an annotated corpus of semantic roles",159098,"The Proposition Bank: An Annotated Corpus of Semantic Roles","The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated. We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ""trace"" categories of the treebank."
389,"testing a roadmap",160019,"Testing: a roadmap","Testing is an important process that is performed to support quality assurance.  Testing activities support quality assurance by gathering information about the nature of the software being studied.  These activities consist of designing test cases, executing the software with those test cases, and examining the results produced by those executions. Studies indicate that more than ﬁfty percent of the cost of software development is devoted to testing, with the percentage for testing critical software being even higher.  As software becomes more pervasive and is used more often to perform critical tasks, it will be required to be of higher quality. Unless we can ﬁnd eﬃcient ways to perform eﬀective testing, the percentage of development costs devoted to testing will increase signiﬁcantly. This report brieﬂy assesses the state of the art in software testing, outlines some future directions in software testing, and gives some pointers to software testing resources."
390,"a model for types and levels of human interaction with automation",160143,"A model for types and levels of human interaction with automation","Technical developments in computer hardware and software now make it possible to introduce automation into virtually all aspects of human-machine systems. Given these technical capabilities, which system functions should be automated and to what extent? We outline a model for types and levels of automation that provides a framework and an objective basis for deciding which system functions should be automated and to what extent. Appropriate selection is important because automation does not merely supplant but changes human activity and can impose new coordination demands on the human operator. We propose that automation can be applied to four broad classes of functions: 1) information acquisition; 2) information analysis; 3) decision and action selection; and 4) action implementation. Within each of these types, automation can be applied across a continuum of levels from low to high, i.e., from fully manual to fully automatic. A particular system can involve automation of all four types at different levels. The human performance consequences of particular types and levels of automation constitute primary evaluative criteria for automation design using our model. Secondary evaluative criteria include automation reliability and the costs of decision/action consequences, among others. Examples of recommended types and levels of automation are provided to illustrate the application of the model to automation design"
391,"an accurate sensitive and scalable method to identify functional sites in protein structures",161150,"An Accurate, Sensitive, and Scalable Method to Identify Functional Sites in Protein Structures","Functional sites determine the activity and interactions of proteins and as such constitute the targets of most drugs. However, the exponential growth of sequence and structure data far exceeds the ability of experimental techniques to identify their locations and key amino acids. To fill this gap we developed a computational Evolutionary Trace method that ranks the evolutionary importance of amino acids in protein sequences. Studies show that the best-ranked residues form fewer and larger structural clusters than expected by chance and overlap with functional sites, but until now the significance of this overlap has remained qualitative. Here, we use 86 diverse protein structures, including 20 determined by the structural genomics initiative, to show that this overlap is a recurrent and statistically significant feature. An automated ET correctly identifies seven of ten functional sites by the least favorable statistical measure, and nine of ten by the most favorable one. These results quantitatively demonstrate that a large fraction of functional sites in the proteome may be accurately identified from sequence and structure. This should help focus structure-function studies, rational drug design, protein engineering, and functional annotation to the relevant regions of a protein."
392,"anchor residues in proteinprotein interactions",161170,"Anchor residues in protein-protein interactions","We show that the mechanism for molecular recognition requires one of the interacting proteins, usually the smaller of the two, to anchor a specific side chain in a structurally constrained binding groove of the other protein, providing a steric constraint that helps to stabilize a native-like bound intermediate. We identify the anchor residues in 39 protein-protein complexes and verify that, even in the absence of their interacting partners, the anchor side chains are found in conformations similar to those observed in the bound complex. These ready-made recognition motifs correspond to surface side chains that bury the largest solvent-accessible surface area after forming the complex (> or =100 A2). The existence of such anchors implies that binding pathways can avoid kinetically costly structural rearrangements at the core of the binding interface, allowing for a relatively smooth recognition process. Once anchors are docked, an induced fit process further contributes to forming the final high-affinity complex. This later stage involves flexible (solvent-exposed) side chains that latch to the encounter complex in the periphery of the binding pocket. Our results suggest that the evolutionary conservation of anchor side chains applies to the actual structure that these residues assume before the encounter complex and not just to their loci. Implications for protein docking are also discussed. [Journal Article; In English; United States]"
393,"vivaldi a decentralized network coordinate system",162250,"Vivaldi: a decentralized network coordinate system","Large-scale Internet applications can benefit from an ability to predict round-trip times to other hosts without having to contact them first. Explicit measurements are often unattractive because the cost of measurement can outweigh the benefits of exploiting proximity information. Vivaldi is a simple, light-weight algorithm that assigns synthetic coordinates to hosts such that the distance between the coordinates of two hosts accurately predicts the communication latency between the hosts. Vivaldi is fully distributed, requiring no fixed network infrastructure and no distinguished hosts. It is also efficient: a new host can compute good coordinates for itself after collecting latency information from only a few other hosts. Because it requires little com-munication, Vivaldi can piggy-back on the communication patterns of the application using it and scale to a large number of hosts. An evaluation of Vivaldi using a simulated network whose latencies are based on measurements among 1740 Internet hosts shows that a 2-dimensional Euclidean model with  height vectors  embeds these hosts with low error (the median relative error in round-trip time prediction is 11 percent)."
394,"designing a dht for low latency and high throughput",162268,"Designing a {DHT} for low latency and high throughput","Designing a wide-area distributed hash table (DHT) that provides high- throughput and low-latency network storage is a challenge. Existing systems have explored a range of solutions, including iterative routing, recursive routing, proximity routing and neighbor selection, erasure coding, replication, and server selection. <P>  This paper explores the design of these techniques and their interaction in a complete system, drawing on the measured performance of a new DHT implementation and results from a simulator with an accurate Internet latency model. New techniques that resulted from this exploration include use of latency predictions based on synthetic coordinates, efficient integration of lookup routing and data fetching, and a congestion control mechanism suitable for fetching data striped over large numbers of servers. <P>  Measurements with 425 server instances running on 150 PlanetLab and RON hosts show that the latency optimizations reduce the time required to locate and fetch data by a factor of two. The throughput optimizations result in a sustainable bulk read throughput related to the number of DHT hosts times the capacity of the slowest access link; with 150 selected PlanetLab hosts, the peak aggregate throughput over multiple clients is 12.8 megabytes per second."
395,"ivy a readwrite peertopeer file system",162281,"Ivy: A Read/Write Peer-to-peer File System","Ivy is a multi-user read/write peer-to-peer file system. Ivy has no centralized or dedicated components, and it provides useful integrity properties without requiring users to fully trust either the underlying peer-to-peer storage system or the other users of the file system. An Ivy file system consists solely of a set of logs, one log per participant. Ivy stores its logs in the {DHash} distributed hash table. Each participant finds data by consulting all logs, but performs modifications by appending only to its own log. This arrangement allows Ivy to maintain meta-data consistency without locking. Ivy users can choose which other logs to trust, an appropriate arrangement in a semi-open peer-to-peer system. Ivy presents applications with a conventional file system interface. When the underlying network is fully connected, Ivy provides {NFS-like} semantics, such as close-to-open consistency. Ivy detects conflicting modifications made during a partition, and provides relevant version information to application-specific conflict resolvers. Performance measurements on a wide-area network show that Ivy is two to three times slower than {NFS.}"
396,"pastry scalable distributed object location and routing for largescale peertopeer systems",162288,"Pastry: Scalable, distributed object location and routing for large-scale peer-to-peer systems","This paper presents the design and evaluation of Pastry, a scalable, distributed object location and routing scheme for wide-area peer-to-peer applications. Pastry provides application-level routing and object location in a potentially very large overlay network of nodes connected via the Internet. It can be used to support a wide range of peer-to-peer applications like global data storage, global data sharing, and naming. An insert operation in Pastry stores an object at a user-defined number of diverse nodes within the Pastry network. A lookup operation reliably retrieves a copy of the requested object if one exists. Moreover, a lookup is usually routed to the node nearest the client issuing the lookup (by some measure of proximity), among the nodes storing the requested object. Pastry is completely decentralized, scalable, and self-configuring; it automatically adapts to the arrival, departure and failure of nodes. Experimental results obtained with a prototype implementation on a simulated network of 100,000 nodes confirm Pastry's scalability, its ability to self-configure and adapt to node failures, and its good network locality properties."
397,"storage management and caching in past a largescale persistent peertopeer storage utility",162289,"Storage management and caching in {PAST}, a large-scale, persistent peer-to-peer storage utility","This paper presents and evaluates the storage management and caching in PAST, a large-scale peer-to-peer persistent storage utility. PAST is based on a self-organizing, Internet-based overlay network of storage nodes that cooperatively route file queries, store multiple replicas of files, and cache additional copies of popular files.In the PAST system, storage nodes and files are each assigned uniformly distributed identifiers, and replicas of a file are stored at nodes whose identifier matches most closely the file's identifier. This statistical assignment of files to storage nodes approximately balances the number of files stored on each node. However, non-uniform storage node capacities and file sizes require more explicit storage load balancing to permit graceful behavior under high global storage utilization; likewise, non-uniform popularity of files requires caching to minimize fetch distance and to balance the query load.We present and evaluate PAST, with an emphasis on its storage management and caching system. Extensive trace-driven experiments show that the system minimizes fetch distance, that it balances the query load for popular files, and that it displays graceful degradation of performance as the global storage utilization increases beyond 95%."
398,"social bookmarking tools i a general review",162869,"Social Bookmarking Tools (I): A General Review","spacer Introduction Because, to paraphrase a pop music lyric from a certain rock and roll band of yesterday, ""the Web is old, the Web is new, the Web is all, the Web is you"", it seems like we might have to face up to some of these stark realities [n1]. With the introduction of new social software applications such as blogs, wikis, newsfeeds, social networks, and bookmarking tools (the subject of this paper), the claim that Shelley Powers makes in a Burningbird blog entry [1] seems apposite: ""This is the user's web now, which means it's my web and I can make the rules."" Reinvention is revolution – it brings us always back to beginnings. We are here going to remind you of hyperlinks in all their glory, sell you on the idea of bookmarking hyperlinks, point you at other folks who are doing the same, and tell you why this is a good thing. Just as long as those hyperlinks (or let's call them plain old links) are managed, tagged, commented upon, and published onto the Web, they represent a user's own personal library placed on public record, which – when aggregated with other personal libraries – allows for rich, social networking opportunities. Why spill any ink (digital or not) in rewriting what someone else has already written about instead of just pointing at the original story and adding the merest of titles, descriptions and tags for future reference? More importantly, why not make these personal 'link playlists' available to oneself and to others from whatever browser or computer one happens to be using at the time? This paper reviews some current initiatives, as of early 2005, in providing public link management applications on the Web – utilities that are often referred to under the general moniker of 'social bookmarking tools'. There are a couple of things going on here: 1) server-side software aimed specifically at managing links with, crucially, a strong, social networking flavour, and 2) an unabashedly open and unstructured approach to tagging, or user classification, of those links. A number of such utilities are presented here, together with an emergent new class of tools that caters more to the academic communities and that stores not only user-supplied tags, but also structured citation metadata terms wherever it is possible to glean this information from service providers. This provision of rich, structured metadata means that the user is provided with an accurate third-party identification of a document, which could be used to retrieve that document, but is also free to search on user-supplied terms so that documents of interest (or rather, references to documents) can be made discoverable and aggregated with other similar descriptions either recorded by the user or by other users."
399,"collapse how societies choose to fail or succeed",163529,"Collapse: How Societies Choose to Fail or Succeed","{Jared Diamond's <I>Collapse: How Societies Choose to Fail or Succeed</I> is the glass-half-empty follow-up to his Pulitzer Prize-winning <I>Guns, Germs, and Steel</I>. While <I>Guns, Germs, and Steel</I> explained the geographic and environmental reasons why some human populations have flourished, <I>Collapse</I> uses the same factors to examine why ancient societies, including the Anasazi of the American Southwest and the Viking colonies of Greenland, as well as modern ones such as Rwanda, have fallen apart. Not every collapse has an environmental origin, but an eco-meltdown is often the main catalyst, he argues, particularly when combined with society's response to (or disregard for) the coming disaster. Still, right from the outset of <I>Collapse</I>, the author makes clear that this is not a mere environmentalist's diatribe. He begins by setting the book's main question in the small communities of present-day Montana as they face a decline in living standards and a depletion of natural resources. Once-vital mines now leak toxins into the soil, while prion diseases infect some deer and elk and older hydroelectric dams have become decrepit. On all these issues, and particularly with the hot-button topic of logging and wildfires, Diamond writes with equanimity.  <p>   Because he's addressing such significant issues within a vast span of time, Diamond can occasionally speak too briefly and assume too much, and at times his shorthand remarks may cause careful readers to raise an eyebrow. But in general, Diamond provides fine and well-reasoned historical examples, making the case that many times, economic and environmental concerns are one and the same. With <I>Collapse</I>, Diamond hopes to jog our collective memory to keep us from falling for false analogies or forgetting prior experiences, and thereby save us from potential devastations to come. While it might seem a stretch to use medieval Greenland and the Maya to convince a skeptic about the seriousness of global warming, it's exactly this type of cross-referencing that makes <I>Collapse</I> so compelling. <I>--Jennifer Buckendorff</I>} { In his million-copy bestseller <I>Guns, Germs, and Steel</I>, Jared Diamond examined how and  why Western civilizations developed the technologies and immunities that allowed them to  dominate much of the world. Now in this brilliant companion volume, Diamond probes the other  side of the equation: What caused some of the great civilizations of the past to collapse into ruin,  and what can we learn from their fates? <P> As in <I>Guns, Germs, and Steel</I>, Diamond weaves an all-encompassing global thesis  through a series of fascinating historical-cultural narratives. Moving from the Polynesian cultures  on Easter Island to the flourishing American civilizations of the Anasazi and the Maya and finally  to the doomed Viking colony on Greenland, Diamond traces the fundamental pattern of  catastrophe. Environmental damage, climate change, rapid population growth, and unwise  political choices were all factors in the demise of these societies, but other societies found  solutions and persisted. Similar problems face us today and have already brought disaster to  Rwanda and Haiti, even as China and Australia are trying to cope in innovative ways. Despite our  own society&#146;s apparently inexhaustible wealth and unrivaled political power, ominous warning  signs have begun to emerge even in ecologically robust areas like Montana.<P> Brilliant, illuminating, and immensely absorbing, <I>Collapse</I> is destined to take its place as  one of the essential books of our time, raising the urgent question: How can our world best avoid  committing ecological suicide?}"
400,"biological sequence analysis probabilistic models of proteins and nucleic acids",163532,"Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids","{Probablistic models are becoming increasingly important in analyzing the huge amount of data being produced by large-scale DNA-sequencing efforts such as the Human Genome Project.  For example, hidden Markov models are used for analyzing biological sequences, linguistic-grammar-based probabilistic models for identifying RNA secondary structure, and probabilistic evolutionary models for inferring phylogenies of sequences from different organisms. This book gives a unified, up-to-date and self-contained account, with a Bayesian slant, of such methods, and more generally to probabilistic methods of sequence analysis. Written by an interdisciplinary team of authors, it is accessible to molecular biologists, computer scientists, and mathematicians with no formal knowledge of the other fields, and at the same time presents the state of the art in this new and important field.}"
401,"comparative genomics of the eukaryotes",163543,"Comparative Genomics of the Eukaryotes","A comparative analysis of the genomes ofDrosophila melanogaster, Caenorhabditis elegans, and Saccharomyces cerevisiaeâand the proteins they are predicted to encodeâwas undertaken in the context of cellular, developmental, and evolutionary processes. The nonredundant protein sets of flies and worms are similar in size and are only twice that of yeast, but different gene families are expanded in each genome, and the multidomain proteins and signaling pathways of the fly and worm are far more complex than those of yeast. The fly has orthologs to 177 of the 289 human disease genes examined and provides the foundation for rapid analysis of some of the basic processes involved in human disease."
402,"knowledge representation issues in semantic graphs for relationship detection",163559,"Knowledge Representation Issues in Semantic Graphs for Relationship Detection","An important task for Homeland Security is the prediction of threat vulnerabilities, such as through the detection of relationships between seemingly disjoint entities. A structure used for this task is a ""semantic graph"", also known as a ""relational data graph"" or an ""attributed relational graph"". These graphs encode relationships as ""typed"" links between a pair of ""typed"" nodes. Indeed, semantic graphs are very similar to semantic networks used in AI. The node and link types are related through an ontology graph (also known as a schema). Furthermore, each node has a set of attributes associated with it (e.g., ""age"" may be an attribute of a node of type ""person""). Unfortunately, the selection of types and attributes for both nodes and links depends on human expertise and is somewhat subjective and even arbitrary. This subjectiveness introduces biases into any algorithm that operates on semantic graphs. Here, we raise some knowledge representation issues for semantic graphs and provide some possible solutions using recently developed ideas in the field of complex networks. In particular, we use the concept of transitivity to evaluate the relevance of individual links in the semantic graph for detecting relationships. We also propose new statistical measures for semantic graphs and illustrate these semantic measures on graphs constructed from movies and terrorism data."
403,"mpi a messagepassing interface standard",164761,"MPI: A Message-Passing Interface Standard","process naming to allow libraries to describe their communication in terms suitable to their own data structures and algorithms,  ffl The ability to &#034;adorn&#034; a set of communicating processes with additional user-defined attributes, such as extra collective operations. This mechanism should provide a means for the user or library writer effectively to extend a message-passing notation. In addition, a unified mechanism or object is needed for conveniently denoting communication context, the group of communicating processes, to house abstract process naming, and to store adornments.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  5.1. INTRODUCTION 133 5.1.2 MPI&#039;s Support for Libraries  The corresponding concepts that MPI provides, specifically to support robust libraries, are as follows:  ffl Contexts of communication,  ffl Groups of processes,  ffl Virtual topologies,  ffl Attribute caching,  ffl Commun..."
404,"random forests",165116,"Random Forests","Abstract. Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund &amp; R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ∗∗∗, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression."
405,"part of speech tagging using a network of linear separators",165344,"Part of speech tagging using a network of linear separators","We present an architecture and an on-line learning algorithm and apply it to the problem of part-of-speech tagging. The architecture presented, {\em SNOW}, is a network of linear separators in the feature space, utilizing the Winnow update algorithm. Multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good behavior when applied to very high dimensional problems, and especially when the target concepts depend on only a small subset of the features in the feature space. In this paper we describe an architecture that utilizes this mistake-driven algorithm for multi-class prediction -- selecting the part of speech of a word. The experimental analysis presented here provides more evidence to that these algorithms are suitable for natural language problems. The algorithm used is an on-line algorithm: every example is used by the algorithm only once, and is then discarded. This has significance in terms of efficiency, as well as quick adaptation to new contexts. We present an extensive experimental study of our algorithm under various conditions; in particular, it is shown that the algorithm performs comparably to the best known algorithms for {\sc pos}."
406,"learning to parse natural language with maximum entropy models",165362,"Learning to Parse Natural Language with Maximum Entropy Models",". This paper presents a machine learning system for parsing natural language that learns from manually parsed example sentences, and parses unseen data at state-of-the-art accuracies. Its machine learning technology, based on the maximum entropy framework, is highly reusable and not specific to the parsing problem, while the linguistic hints that it uses to learn can be specified concisely. It therefore requires a minimal amount of human effort and linguistic knowledge for its construction. In..."
407,"bayesian learning of probabilistic language models",165434,"Bayesian learning of probabilistic language models","The general topic of this thesis is the probabilistic modeling of language, in particular natural language. In probabilistic language modeling, one characterizes the strings of phonemes, words, etc. of a certain domain in terms of a probability distribution over all possible strings within the domain. Probabilistic language modeling has been applied to a wide range of problems in recent years, from the traditional uses in speech recognition to more recent applications in biological sequence..."
408,"word association norms mutual information and lexicography",165438,"Word association norms, mutual information, and lexicography","The term  word association  is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word  nurse  if it follows a highly associated word such as  doctor . ) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the  association ratio , estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
409,"exploring the helixcoil transition via allatom equilibrium ensemble simulations",166210,"Exploring the Helix-Coil Transition via All-Atom Equilibrium Ensemble Simulations","The ensemble folding of two 21-residue α-helical peptides has been studied using all-atom simulations under several variants of the AMBER potential in explicit solvent using a global distributed computing network. Our extensive sampling, orders of magnitude greater than the experimental folding time, results in complete convergence to ensemble equilibrium. This allows for a quantitative assessment of these potentials, including a new variant of the AMBER-99 force field, denoted AMBER-99, which shows improved agreement with experimental kinetic and thermodynamic measurements. From bulk analysis of the simulated AMBER-99 equilibrium, we find that the folding landscape is pseudo-two-state, with complexity arising from the broad, shallow character of the “native” and “unfolded” regions of the phase space. Each of these macrostates allows for configurational diffusion among a diverse ensemble of conformational microstates with greatly varying helical content and molecular size. Indeed, the observed structural dynamics are better represented as a conformational diffusion than as a simple exponential process, and equilibrium transition rates spanning several orders of magnitude are reported. After multiple nucleation steps, on average, helix formation proceeds via a kinetic “alignment” phase in which two or more short, low-entropy helical segments form a more ideal, single-helix structure."
410,"bioconductor open software development for computational biology and bioinformatics",166220,"Bioconductor: open software development for computational biology and bioinformatics","The Bioconductor project is an initiative for the collaborative creation of extensible software for computational biology and bioinformatics. We detail some of the design decisions, software paradigms and operational strategies that have allowed a small number of researchers to provide a wide variety of innovative, extensible, software solutions in a relatively short time. The use of an object oriented programming paradigm, the adoption and development of a software package system, designing by contract, distributed development and collaboration with other projects are elements of this project's success. Individually, each of these concepts are useful and important but when combined they have provided a strong basis for rapid development and deployment of innovative and flexible research software for scientific computation. A primary objective of this initiative is achievement of total remote reproducibility of novel algorithmic research results."
411,"multiplelaboratory comparison of microarray platforms",166875,"Multiple-laboratory comparison of microarray platforms.","Microarray technology is a powerful tool for measuring {RNA} expression for thousands of genes at once. Various studies have been published comparing competing platforms with mixed results: some find agreement, others do not. As the number of researchers starting to use microarrays and the number of cross-platform meta-analysis studies rapidly increases, appropriate platform assessments become more important. Here we present results from a comparison study that offers important improvements over those previously described in the literature. In particular, we noticed that none of the previously published papers consider differences between labs. For this study, a consortium of ten laboratories from the Washington, {DC-Baltimore,} {USA,} area was formed to compare data obtained from three widely used platforms using identical {RNA} samples. We used appropriate statistical analysis to demonstrate that there are relatively large differences in data obtained in labs using the same platform, but that the results from the best-performing labs agree rather well."
412,"experiments with a new boosting algorithm",167628,"Experiments with a New Boosting Algorithm","In an earlier paper, we introduced a new “boosting” algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that consistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a “pseudo-loss ” which is a method for forcing a learning algorithm of multi-label conceptsto concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman’s “bagging ” method when used to aggregate various classifiers (including decision trees and single attribute-value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem."
413,"a framework for the comparative analysis of organizations",168520,"A Framework for the Comparative Analysis of Organizations","Complex organizations are conceptualized in terms of their technologies, or the work done on raw materials. Two aspects of technology vary independently: the number of exceptions that must be handled, and the degree to which search is an analyzable or unanalyzable procedure. If there is a large number of exceptions and search is not logical and analytic, the technology is described as nonroutine. Few exceptions and analyzable search procedures describe a routine technology. Two other types result from other combinations--craft and engineering technologies. Task structures vary with the technology utilized, and are analyzed in terms of control and coordination and three levels of management. Social structure in turn is related to technology and task structure. Finally, the variations in three types of goals are weakly related to the preceding variables in this conceptualization. The perspective provides a basis for comparing organizations which avoids many problems found in other schemes utilizing structure, function or goals as the basis for comparison. Furthermore, it allows one to selectively utilize competing organizational theories once it is understood that their relevance is restricted to organizations with specific kinds of technologies. The scheme makes apparent some errors in present efforts to compare organizations."
414,"structural inertia and organizational change",168529,"Structural Inertia and Organizational Change","Theory and research on organization-environment relations from a population ecology perspective have been based on the assumption that inertial pressures on structure are strong. This paper attempts to clarify the meaning of structural inertia and to derive propositions about structural inertia from an explicit evolutionary model. The proposed theory treats high levels of structural inertia as a consequence of a selection process rather than as a precondition for selection. It also considers how the strength of inertial forces varies with age, size, and complexity."
415,"open sources voices from the open source revolution oreilly open source",168559,"Open Sources: Voices from the Open Source Revolution (O'Reilly Open Source)","{<I>Open Sources: Voices from the Open Source Revolution</I> is a fascinating look at the raging debate that is its namesake. Filled with writings from the central players--from Linux creator Linus Torvalds to Perl creator Larry Wall--the book convinces the reader of the overwhelming merits of freeing up the many iterations of software's source code.<p> The open-source movement has become a cause c&#233;l&#232;bre in light of the widespread adoption of Linux, Perl, and Apache as well as its corporate support from Netscape, IBM, and Oracle--and strongly felt opposition from Microsoft. <I>Open Sources</I> doesn't address <I>why</I> these Microsoft foes are throwing their weight behind the movement. Instead, it focuses on the history and philosophy of open-source software (previously referred to as <I>freeware</I>) as an argument for shaping the future of programming. <I>Open Sources</I> is much larger than just a fight with any one company. Instead, it is a revolutionary call to release software development from the vested interests that label new directions in software development as threatening.<p> This is not to say that opening the source code is an entirely egalitarian and communistic endeavor. These are programmers and startup owners; they want to be able to continue to program for a living. To that end, <I>Open Sources</I> contains strong business profiles from entrepreneurs such as Apache's--and now, O'Reilly & Associates'--Brian Behlendorf, who discusses how to give away software in order to lure customers in for specialized versions. In many ways, this is a hands-on guide, displaying an insider's view of the development process and providing specifics on testing details and altering licensing agreements. However, interspersed with tech talk is a reader-friendly guide for those interested in the future of software development. <I>--Jennifer Buckendorff</I>}"
416,"what is beautiful is usable",168970,"What is beautiful is usable","An experiment was conducted to test the relationships between users' perceptions of a computerized system's beauty and usability. The experiment used a computerized application as a surrogate for an Automated Teller Machine (ATM). Perceptions were elicited before and after the participants used the system. Pre-experimental measures indicate strong correlations between system's perceived aesthetics and perceived usability. Post-experimental measures indicated that the strong correlation remained intact. A multivariate analysis of covariance revealed that the degree of system's aesthetics affected the post-use perceptions of both aesthetics and usability, whereas the degree of actual usability had no such effect. The results resemble those found by social psychologists regarding the effect of physical attractiveness on the valuation of other personality attributes. The findings stress the importance of studying the aesthetic aspect of human–computer interaction (HCI) design and its relationships to other design dimensions."
417,"a new location technique for the active office",169545,"A New Location Technique for the Active Office","Configuration of the computing and communications systems found at home and in the workplace is a complex task that currently requires the attention of the user. Researchers have begun to examine computers that would autonomously change their functionality based on observations of who or what was around them. By determining their context, using input from sensor systems distributed throughout the environment, computing devices could personalize themselves to their current user, adapt their behaviour according to their location, or react to their surroundings. The authors present a novel sensor system, suitable for large-scale deployment in indoor environments, which allows the locations of people and equipment to be accurately determined. We also describe some of the context-aware applications that might make use of this fine-grained location information"
418,"the active badge location system",169549,"The Active Badge Location System","A novel system for the location of people in an office environment is described. Members of staff wear badges that transmit signals providing information about their location to a centralized location service, through a network of sensors. The paper also examines alternative location techniques, system design issues and applications, particularly relating to telephone call routing. Location systems raise concerns about the privacy of an individual and these issues are also addressed."
419,"genome evolution reveals biochemical networks and functional modules",169806,"Genome evolution reveals biochemical networks and functional modules.","The analysis of completely sequenced genomes uncovers an astonishing variability between species in terms of gene content and order. {D}uring genome history, the genes are frequently rearranged, duplicated, lost, or transferred horizontally between genomes. {T}hese events appear to be stochastic, yet they are under selective constraints resulting from the functional interactions between genes. {T}hese genomic constraints form the basis for a variety of techniques that employ systematic genome comparisons to predict functional associations among genes. {T}he most powerful techniques to date are based on conserved gene neighborhood, gene fusion events, and common phylogenetic distributions of gene families. {H}ere we show that these techniques, if integrated quantitatively and applied to a sufficiently large number of genomes, have reached a resolution which allows the characterization of function at a higher level than that of the individual gene: global modularity becomes detectable in a functional protein network. {I}n {E}scherichia coli, the predicted modules can be benchmarked by comparison to known metabolic pathways. {W}e found as many as 74% of the known metabolic enzymes clustering together in modules, with an average pathway specificity of at least 84%. {T}he modules extend beyond metabolism, and have led to hundreds of reliable functional predictions both at the protein and pathway level. {T}he results indicate that modularity in protein networks is intrinsically encoded in present-day genomes."
420,"a garbage can model of organizational choice",171407,"A Garbage Can Model of Organizational Choice","Organized anarchies are organizations characterized by problematic preferences, unclear technology, and fluid participation. Recent studies of universities, a familiar form of organized anarchy, suggest that such organizations can be viewed for some purposes as collections of choices looking for problems, issues and feelings looking for decision situations in which they might be aired, solutions looking for issues to which they might be an answer, and decision makers looking for work. These ideas are translated into an explicit computer simulation model of a garbage can decision process. The general implications of such a model are described in terms of five major measures on the process. Possible applications of the model to more narrow predictions are illustrated by an examination of the model's predictions with respect to the effect of adversity on university decision making."
421,"inside pagerank",171416,"Inside PageRank","Although the interest of a Web page is strictly related to its content and to the subjective readers' cultural background, a measure of the page authority can be provided that only depends on the topological structure of the Web. PageRank is a noticeable way to attach a score to Web pages on the basis of the Web connectivity. In this article, we look inside PageRank to disclose its fundamental properties concerning stability, complexity of computational scheme, and critical role of parameters involved in the computation. Moreover, we introduce a circuit analysis that allows us to understand the distribution of the page score, the way different Web communities interact each other, the role of dangling pages (pages with no outlinks), and the secrets for promotion of Web pages."
422,"feature selection for svms",171427,"Feature Selection for SVMs","We introduce a method of feature selection for Support Vector Machines. The method is based upon finding those features which minimize bounds on the leave-one-out error. This search can be efficiently performed via gradient descent. The resulting algorithms are shown to be superior to some standard feature selection algorithms on both toy data and real-life problems of face recognition, pedestrian detection and analyzing DNA microarray data. 1"
423,"social contagion and innovation cohesion versus structural equivalence",171716,"Social contagion and innovation: Cohesion versus structural equivalence","Two classes of network models are used to reanalyze a sociological classic often cited as evidence of social contagion in the diffusion of technological innovation: Medical Innovation. Debate between the cohesion and structural equivalence models poses the following question for study: Did the physicians resolve the uncertainty of adopting the new drug through conversations with colleagues (cohesion) or through their perception of the action proper for an occupant of their position in the social structure of colleagues (structural equivalence)? The alternative models are defined, compared, and tested. Four conclusions are drawn: (a) Contagion was not the dominant factor driving tetracyclene's diffusion. Where there is evidence of contagion, there is evidence of personal preferences at work."
424,"internet paradox a social technology that reduces social involvement and psychological wellbeing",171730,"Internet paradox: A social technology that reduces social involvement and psychological well-being?","The Internet could change the lives of average citizens as much as did the telephone in the early part of the 20th century and television in the 1950s and 1960s. Re- searchers and social critics are debating whether the Internet is improving or harming participation in com- munity life and social relationships. This research exam- ined the social and psychological impact of the lnternet on 169 people in 73 households during their first i to 2 years on-line. We used longitudinal data to examine the effects of the Internet on social involvement and psycho- logical well-being. In this sample, the Internet was used extensively for communication. Nonetheless, greater use of the Internet was associated with declines in partici- pants'communication with family members in the house- hold, declines in the size of their social circle, and in- creases in their depression and loneliness. These findings have implications for research, for public policy, and for the design of technology."
425,"combinatorial microrna target predictions",172870,"Combinatorial microRNA target predictions","MicroRNAs are small noncoding RNAs that recognize and bind to partially complementary sites in the 3′ untranslated regions of target genes in animals and, by unknown mechanisms, regulate protein production of the target transcript1, 2, 3. Different combinations of microRNAs are expressed in different cell types and may coordinately regulate cell-specific target genes. Here, we present PicTar, a computational method for identifying common targets of microRNAs. Statistical tests using genome-wide alignments of eight vertebrate genomes, PicTar's ability to specifically recover published microRNA targets, and experimental validation of seven predicted targets suggest that PicTar has an excellent success rate in predicting targets for single microRNAs and for combinations of microRNAs. We find that vertebrate microRNAs target, on average, roughly 200 transcripts each. Furthermore, our results suggest widespread coordinate control executed by microRNAs. In particular, we experimentally validate common regulation of Mtpn by miR-375, miR-124 and let-7b and thus provide evidence for coordinate microRNA control in mammals."
426,"a genomic view of alternative splicing",173485,"A genomic view of alternative splicing.","Recent genome-wide analyses of alternative splicing indicate that 40-60\% of human genes have alternative splice forms, suggesting that alternative splicing is one of the most significant components of the functional complexity of the human genome. Here we review these recent results from bioinformatics studies, assess their reliability and consider the impact of alternative splicing on biological functions. Although the 'big picture' of alternative splicing that is emerging from genomics is exciting, there are many challenges. High-throughput experimental verification of alternative splice forms, functional characterization, and regulation of alternative splicing are key directions for research. We recommend a community-based effort to discover and characterize alternative splice forms comprehensively throughout the human genome."
427,"art of computer programming volume sorting and searching nd edition",175025,"Art of Computer Programming, Volume 3: Sorting and Searching (2nd Edition)","The first revision of this third volume is the most comprehensive survey of classical computer techniques for sorting and searching. It extends the treatment of data structures in Volume 1 to consider both large and small databases and internal and external memories. The book contains a selection of carefully checked computer methods, with a quantitative analysis of their efficiency. Outstanding features of the second edition include a revised section on optimum sorting and new discussions of the theory of permutations and of universal hashing."
428,"cooccurrence based metaanalysis of scientific texts retrieving biological relationships between genes",175375,"Co-occurrence based meta-analysis of scientific texts: retrieving biological relationships between genes","Motivation: The advent of high-throughput experiments in molecular biology creates a need for methods to efficiently extract and use information for large numbers of genes. Recently, the associative concept space (ACS) has been developed for the representation of information extracted from biomedical literature. The ACS is a Euclidean space in which thesaurus concepts are positioned and the distances between concepts indicates their relatedness. The ACS uses co-occurrence of concepts as a source of information. In this paper we evaluate how well the system can retrieve functionally related genes and we compare its performance with a simple gene co-occurrence method.  Results: To assess the performance of the ACS we composed a test set of five groups of functionally related genes. With the ACS good scores were obtained for four of the five groups. When compared to the gene co-occurrence method, the ACS is capable of revealing more functional biological relations and can achieve results with less literature available per gene. Hierarchical clustering was performed on the ACS output, as a potential aid to users, and was found to provide useful clusters. Our results suggest that the algorithm can be of value for researchers studying large numbers of genes.  Availability: The ACS program is available upon request from the authors.  Contact: r.jelier@erasmusmc.nl 10.1093/bioinformatics/bti268"
429,"classical mechanics",175909,"Classical Mechanics","{The series of texts on Classical Theoretical Physics is based on the highly successful series of courses given by Walter Greiner at the Johann Wolfgang Goethe University in Frankfurt am Main, Germany. Intended for advanced undergraduates and beginning graduate students, the volumes in the series provide not only a complete survey of classical theoretical physics but also an enormous number of worked examples and problems to show students clearly how to apply the abstract principles to realistic problems.}"
430,"a direct approach to false discovery rates",179333,"A Direct Approach to False Discovery Rates","Multiple&#150;hypothesis testing involves guarding against much more complicated errors than single&#150;hypothesis testing. Whereas we typically control the type I error rate for a single&#150;hypothesis test, a compound error rate is controlled for multiple&#150;hypothesis tests. For example, controlling the false discovery rate FDR traditionally involves intricate sequential p&#150;value rejection methods based on the observed data. Whereas a sequential p&#150;value method fixes the error rate and estimates its corresponding rejection region, we propose the opposite approach&#150;we fix the rejection region and then estimate its corresponding error rate. This new approach offers increased applicability, accuracy and power. We apply the methodology to both the positive false discovery rate pFDR and FDR, and provide evidence for its benefits. It is shown that pFDR is probably the quantity of interest over FDR. Also discussed is the calculation of the q&#150;value, the pFDR analogue of the p&#150;value, which eliminates the need to set the error rate beforehand as is traditionally done. Some simple numerical examples are presented that show that this new approach can yield an increase of over eight times in power compared with the Benjamini&#150;Hochberg FDR method."
431,"prediction of plant microrna targets",179399,"Prediction of plant microRNA targets","We predict regulatory targets for 14 Arabidopsis microRNAs (miRNAs) by identifying mRNAs with near complementarity. Complementary sites within predicted targets are conserved in rice. Of the 49 predicted targets, 34 are members of transcription factor gene families involved in developmental patterning or cell differentiation. The near-perfect complementarity between plant miRNAs and their targets suggests that many plant miRNAs act similarly to small interfering RNAs and direct mRNA cleavage. The targeting of developmental transcription factors suggests that many plant miRNAs function during cellular differentiation to clear key regulatory transcripts from daughter cell lineages."
432,"data integration a theoretical perspective",179467,"Data integration: a theoretical perspective","Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. The problem of designing data integration systems is important in current real world applications, and is characterized by a number of issues that are interesting from a theoretical point of view. This document presents on overview of the material to be presented in a tutorial on data integration. The tutorial is focused on some of the theoretical issues that are relevant for data integration. Special attention will be devoted to the following aspects: modeling a data integration application, processing queries in data integration, dealing with inconsistent data sources, and reasoning on queries."
433,"fast multiresolution image querying",179598,"Fast multiresolution image querying","We present a method for searching in an image database using a query image that is similar to the intended target. The query image may be a hand-drawn sketch or a (potentially low-quality) scan of the image to be retrieved. Our searching algorithm makes use of multiresolution wavelet decompositions of the query and database images. The coefficients of these decompositions are distilled into small signatures for each image. We introduce an image querying metric that operates on these..."
434,"neural networks for pattern recognition",179603,"Neural Networks for Pattern Recognition","{This book provides a solid statistical foundation for neural networks from a pattern recognition  perspective. The focus is on the types of neural nets that are most widely used in practical applications,  such as the multi-layer perceptron and radial basis function networks. Rather than trying to cover many  different types of neural networks, Bishop thoroughly covers topics such as density estimation, error  functions, parameter optimization algorithms, data pre-processing, and Bayesian methods. All topics are  organized well and all mathematical foundations are explained before being applied to neural networks.  The text is suitable for a graduate or advanced undergraduate level course on neural networks or for  practitioners interested in applying neural networks to real-world problems. The reader is assumed to have  the level of math knowledge necessary for an undergraduate science degree.} {This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications.  Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.}"
435,"sketch an interface for sketching d scenes",180259,"SKETCH: an interface for sketching 3D scenes","Sketching communicates ideas rapidly through approximate visual images with low overhead (pencil and paper), no need for precision or specialized knowledge, and ease of low-level correction and revision. In contrast, most 3D computer modeling systems are good at generating arbitrary views of precise 3D models and support high-level editing and revision. The SKETCH application described in this paper attempts to combine the advantages of each in order to create an environment for  rapidly  conceptualizing and editing  approximate  3D scenes. To achieve this, SKETCH uses simple non-photorealistic rendering and a purely gestural interface based on simplified line drawings of primitives that allows all operations to be specified  within  the 3D world."
436,"the selection of prior distributions by formal rules",180994,"The selection of prior distributions by formal rules","Subjectivism has become the dominant philosophical foundation for Bayesian inference. Yet in practice, most Bayesian analyses are performed with so-called ""noninformative"" priors, that is, priors constructed by some formal rule. We review the plethora of techniques for constructing such priors and discuss some of the practical and philosophical issues that arise when they are used. We give special emphasis to Jeffreys's rules and discuss the evolution of his viewpoint about the interpretation of priors, away from unique representation of ignorance toward the notion that they should be chosen by convention. We conclude that the problems raised by the research on priors chosen by formal rules are serious and may not be dismissed lightly: When sample sizes are small (relative to the number of parameters being estimated), it is dangerous to put faith in any ""default"" solution; but when asymptotics take over, Jeffreys's rules and their variants remain reasonable choices. We also provide an annotated bibliography."
437,"automatic personalization based on web usage mining",181740,"Automatic personalization based on Web usage mining","this paper we describe an approach to usage-based Web personalization taking into account the full spectrum of Web mining techniques and activities. Our approach is described by the architecture shown in Figure 1, which heavily uses data mining techniques, thus making the personalization process both automatic and dynamic, and hence up-to-date. Specifically, we have developed techniques for preprocessing of Web usage logs and grouping URL references into sets called user transactions [CMS99]. A ..."
438,"web mining",181748,"Web Mining","The World Wide Web is an enormous information repository, but in order to efficiently access the information contained in it, sophisticated software is needed. This software is to a large extent based on data mining technology. In this section we start with a brief discussion of the nature of the Web and the specific problems encountered when trying to find information. We then give an overview of the many techniques that exist for web mining, illustrated with applications of these techniques...."
439,"constructivist emergent and sociocultural perspectives in the context of developmental research",186777,"Constructivist, Emergent, and Sociocultural Perspectives in the Context of Developmental Research","Our overall intent is to clarify relations between the psychological constructivist, sociocultural, and emergent perspectives. We provide a grounding for the comparisons in the first part of the article by outlining an interpretive framework that we developed in the course of a classroom-based research project. At this level of classroom processes, the framework involves an emergent approach in which psychological constructivist analyses of individual activity are coordinated with interactionist analyses of classroom interactions and discourse. In the second part of the article, we describe an elaboration of the framework that locates classroom processes in school and societal contexts. The perspective taken at this level is broadly sociocultural and focuses on the influence of indlividuals' participation in culturally organized practices. In the third part of the article, we use the discussion of the framework as a backdrop against which to compare and contrast the three theoretical perspectives. We discuss how the emergent approach augments the psychological constructivist perspective by making it possible to locate analyses of individual students' constructive activities in social context. In addition, we consider the purposes for which the emergent and sociocultural perspectives might be particularly appropriate and observe that they together offer characterizations of individual students' activities, the classroom community, and broader communities of practice."
440,"the genome sequence of the filamentous fungus neurospora crassa",187152,"The genome sequence of the filamentous fungus Neurospora crassa","{Neurospora crassa is a central organism in the history of twentieth-century genetics, biochemistry and molecular biology. Here, we report a high-quality draft sequence of the N. crassa genome. The approximately 40-megabase genome encodes about 10,000 protein-coding genes--more than twice as many as in the fission yeast Schizosaccharomyces pombe and only about 25\% fewer than in the fruitfly Drosophila melanogaster. Analysis of the gene set yields insights into unexpected aspects of Neurospora biology including the identification of genes potentially associated with red light photobiology, genes implicated in secondary metabolism, and important differences in Ca2+ signalling as compared with plants and animals. Neurospora possesses the widest array of genome defence mechanisms known for any eukaryotic organism, including a process unique to fungi called repeat-induced point mutation (RIP). Genome analysis suggests that RIP has had a profound impact on genome evolution, greatly slowing the creation of new genes through genomic duplication and resulting in a genome with an unusually low proportion of closely related genes.}"
441,"development and validation of a genetic algorithm for flexible docking",190986,"Development and validation of a genetic algorithm for flexible docking","Prediction of small molecule binding modes to macromolecules of known three-dimensional structure is a problem of paramount importance in rational drug design (the “docking” problem). We report the development and validation of the program GOLD (Genetic Optimisation for Ligand Docking). GOLD is an automated ligand docking program that uses a genetic algorithm to explore the full range of ligand conformational flexibility with partial flexibility of the protein, and satisfies the fundamental requirement that the ligand must displace loosely bound water on binding. Numerous enhancements and modifications have been applied to the original technique resulting in a substantial increase in the reliability and the applicability of the algorithm. The advanced algorithm has been tested on a dataset of 100 complexes extracted from the Brookhaven Protein DataBank. When used to dock the ligand back into the binding site, GOLD achieved a 71% success rate in identifying the experimental binding mode."
442,"social capital in the creation of human capital",191640,"Social Capital In The Creation of Human Capital","In this paper, the concept of social capital is introduced and illustrated, its forms are described, thr social structural conditions under which it arises are examined, and it is using in an analysis of dropouts from high school. Use of the concept of social capital is part of a general theoretical strategy discussed in the paper: taking rational action as a starting point, but rejecting the extreme individualistic premises that often accompany it. The conception of social capital as a resource for action is one way of introducing social structure into the rational action paradigm. Three forms of social capital are examined: obligations and expectations, information channels, and social norms. The role of closure in the social structure in facilitating the first and third of these forms of social capital is described. An analysis of the effect of the lack of social capital available to high school sophomores on dropping out of school before graduation is carried out. The effect of social capital within the family and in the community outside the family is examined."
443,"dynamic ideal point estimation via markov chain monte carlo for the us supreme court",192188,"Dynamic Ideal Point Estimation via Markov Chain Monte Carlo for the U.S. Supreme Court, 1953-1999","At the heart of attitudinal and strategic explanations of judicial behavior is the assumption that justices have policy preferences. In this paper we employ Markov chain Monte Carlo methods to fit a Bayesian measurement model of ideal points for all justices serving on the U.S. Supreme Court from 1953 through 1999. We are particularly interested in determining to what extent ideal points of justices change throughout their tenure on the Court. This is important because judicial politics scholars oftentimes invoke preference measures that are time invariant. To investigate preference change, we posit a dynamic item response model that allows ideal points to change systematically over time. Additionally, we introduce Bayesian methods for fitting multivariate dynamic linear models to political scientists. Our results suggest that many justices do not have temporally constant ideal points. Moreover, our ideal point estimates outperform existing measures and explain judicial behavior quite well across civil rights, civil liberties, economics, and federalism cases. 10.1093/pan/10.2.134"
444,"diversity in tropical rain forests and coral reefs",193082,"Diversity in Tropical Rain Forests and Coral Reefs","The commonly observed high diversity of trees in tropical rain forests and corals on tropical reefs is a nonequilibrium state which, if not disturbed further, will progress toward a low-diversity equilibrium community. This may not happen if gradual changes in climate favor different species. If equilibrium is reached, a lesser degree of diversity may be sustained by niche diversification or by a compensatory mortality that favors inferior competitors. However, tropical forests and reefs are subject to severe disturbances often enough that equilibrium may never be attained. 10.1126/science.199.4335.1302"
445,"a systematic comparison of protein structure classifications scop cath and fssp",197224,"A systematic comparison of protein structure classifications: SCOP, CATH and FSSP.","Background:  Several methods of structural classification have been developed to introduce some order to the large amount of data present in the Protein Data Bank. Such methods facilitate structural comparisons and provide a greater understanding of structure and function. The most widely used and comprehensive databases are SCOP, CATH and FSSP, which represent three unique methods of classifying protein structures: purely manual, a combination of manual and automated, and purely automated, respectively. In order to develop reliable template libraries and benchmarks for protein-fold recognition, a systematic comparison of these databases has been carried out to determine their overall agreement in classifying protein structures. Results:  Approximately two-thirds of the protein chains in each database are common to all three databases. Despite employing different methods, and basing their systems on different rules of protein structure and taxonomy, SCOP, CATH and FSSP agree on the majority of their classifications. Discrepancies and inconsistencies are accounted for by a small number of explanations. Other interesting features have been identified, and various differences between manual and automatic classification methods are presented. Conclusions:  Using these databases requires an understanding of the rules upon which they are based; each method offers certain advantages depending on the biological requirements and knowledge of the user. The degree of discrepancy between the systems also has an impact on reliability of prediction methods that employ these schemes as benchmarks. To generate accurate fold templates for threading, we extract information from a consensus database, encompassing agreements between SCOP, CATH and FSSP."
446,"a module map showing conditional activity of expression modules in cancer",197226,"A module map showing conditional activity of expression modules in cancer.","DNA microarrays are widely used to study changes in gene expression in tumors, but such studies are typically system-specific and do not address the commonalities and variations between different types of tumor. Here we present an integrated analysis of 1,975 published microarrays spanning 22 tumor types. We describe expression profiles in different tumors in terms of the behavior of modules, sets of genes that act in concert to carry out a specific function. Using a simple unified analysis, we extract modules and characterize gene-expression profiles in tumors as a combination of activated and deactivated modules. Activation of some modules is specific to particular types of tumor; for example, a growth-inhibitory module is specifically repressed in acute lymphoblastic leukemias and may underlie the deregulated proliferation in these cancers. Other modules are shared across a diverse set of clinical conditions, suggestive of common tumor progression mechanisms. For example, the bone osteoblastic module spans a variety of tumor types and includes both secreted growth factors and their receptors. Our findings suggest that there is a single mechanism for both primary tumor proliferation and metastasis to bone. Our analysis presents multiple research directions for diagnostic, prognostic and therapeutic studies. {$[$}ABSTRACT FROM AUTHOR{$]$}"
447,"rules of play game design fundamentals",197257,"Rules of Play : Game Design Fundamentals","As pop culture, games are as important as film or television--but game design has yet to develop a theoretical framework or critical vocabulary. In Rules of Play Katie Salen and Eric Zimmerman present a much-needed primer for this emerging field. They offer a unified model for looking at all kinds of games, from board games and sports to computer and video games. As active participants in game culture, the authors have written Rules of Play as a catalyst for innovation, filled with new concepts, strategies, and methodologies for creating and understanding games..  Building an aesthetics of interactive systems, Salen and Zimmerman define core concepts like ""play,"" ""design,"" and ""interactivity."" They look at games through a series of eighteen ""game design schemas,"" or conceptual frameworks, including games as systems of emergence and information, as contexts for social play, as a storytelling medium, and as sites of cultural resistance.  Written for game scholars, game developers, and interactive designers, Rules of Play is a textbook, reference book, and theoretical guide. It is the first comprehensive attempt to establish a solid theoretical framework for the emerging discipline of game design."
448,"from barbie to mortal kombat gender and computer games",197288,"From Barbie to Mortal Kombat: Gender and Computer Games","This book explores the complicated issue of gender in computer games­-particularly the development of video games for girls. One side is the concern that the average computer game, being attractive primarily to boys, furthers the technology access gap between the genders. Yet attempts to create computer games that girls want to play brings about another set of concerns: should games be gendered at all? And does having boys' games and girls' games merely reinforce the way gender differences are socialized in play?<p> Cassell and Jenkins have gathered the thoughts of several feminist and media scholars to explore the issues from multiple perspectives, but this is not a work confined to ivory-tower theorizing. Alongside the philosophical explorations are pragmatic investigations of the hard-nosed, real world of computer-game manufacture and sales. Particularly enlightening is a section featuring interviews with several leading creators of games for girls. And while all agree that it's good to be past the days when women in computer games were limited to scantily clad background figures or damsels in distress, the visions of an appropriate future are both diverse and well defended. There is no pretense here of easy answers, but there are many excellent questions. <I>--Elizabeth Lewis</I> The game console may help to prepare children for participation in the digital world, but at the same time it socializes boys into misogyny and excludes girls from all but the most objectified positions. The contributors to <i>From Barbie® to Mortal Kombat</i> explore how assumptions about gender, games, and technology shape the design, development, and marketing of games as industry seeks to build the girl market. They describe and analyze the games currently on the market and propose tactical approaches for avoiding the stereotypes that dominate most toy store aisles. The lively mix of perspectives and voices includes those of media and technology scholars, educators, psychologists, developers of today's leading games, industry insiders, and girl gamers."
449,"phylogenomic inference of protein molecular function advances and challenges",197344,"Phylogenomic inference of protein molecular function: advances and challenges.","MOTIVATION: Protein families evolve a multiplicity of functions through gene duplication, speciation and other processes. As a number of studies have shown, standard methods of protein function prediction produce systematic errors on these data. Phylogenomic analysis--combining phylogenetic tree construction, integration of experimental data and differentiation of orthologs and paralogs--has been proposed to address these errors and improve the accuracy of functional classification. The explicit integration of structure prediction and analysis in this framework, which we call structural phylogenomics, provides additional insights into protein superfamily evolution. RESULTS: Results of protein functional classification using phylogenomic analysis show fewer expected false positives overall than when pairwise methods of functional classification are employed. We present an overview of the motivations and fundamental principles of phylogenomic analysis, new methods developed for the key tasks, benchmark datasets for these tasks (when available) and suggest procedures to increase accuracy. We also discuss some of the methods used in the Celera Genomics high-throughput phylogenomic classification of the human genome. AVAILABILITY: Software tools from the Berkeley Phylogenomics Group are available at http://phylogenomics.berkeley.edu"
450,"development and testing of a general amber force field",197425,"Development and testing of a general amber force field","We describe here a general Amber force field (GAFF) for organic molecules. GAFF is designed to be compatible with existing Amber force fields for proteins and nucleic acids, and has parameters for most organic and pharmaceutical molecules that are composed of H, C, N, O, S, P, and halogens. It uses a simple functional form and a limited number of atom types, but incorporates both empirical and heuristic models to estimate force constants and partial atomic charges. The performance of GAFF in test cases is encouraging. In test I, 74 crystallographic structures were compared to GAFF minimized structures, with a root-mean-square displacement of 0.26 Å, which is comparable to that of the Tripos 5.2 force field (0.25 Å) and better than those of MMFF 94 and CHARMm (0.47 and 0.44 Å, respectively). In test II, gas phase minimizations were performed on 22 nucleic acid base pairs, and the minimized structures and intermolecular energies were compared to MP2/6-31G* results. The RMS of displacements and relative energies were 0.25 Å and 1.2 kcal/mol, respectively. These data are comparable to results from Parm99/RESP (0.16 Å and 1.18 kcal/mol, respectively), which were parameterized to these base pairs. Test III looked at the relative energies of 71 conformational pairs that were used in development of the Parm99 force field. The RMS error in relative energies (compared to experiment) is about 0.5 kcal/mol. GAFF can be applied to wide range of molecules in an automatic fashion, making it suitable for rational drug design and database searching. © 2004 Wiley Periodicals, Inc. J Comput Chem 25: 1157-1174, 2004"
451,"a wellbehaved electrostatic potential based method using charge restraints for deriving atomic charges the resp model",197446,"A Well-Behaved Electrostatic Potential Based Method Using Charge Restraints for Deriving Atomic charges: The RESP Model","We present a new approach to generating electrostatic potential (ESP) derived charges for molecules. The major strength of electrostatic potential derived charges is that they optimally reproduce the intermolecular interaction properties of molecules with a simple two-body additive potential, provided, of course, that a suitably accurate level of quantum mechanical calculation is used to derive the E S P around the molecule. Previously, the major weaknesses of these charges have been that they were not easily transferable between common functional groups in related molecules, they have often been conformationally dependent, and the large charges that frequently occur can be problematic for simulating intramolecular interactions. Introducing restraints in the form of a penalty function into the fitting process considerably reduces the above problems, with only a minor decrease in the quality of the fit to the quantum mechanical ESP. Several other refinements in addition to the restrained electrostatic potential (RESP) fit yield a general and algorithmic charge fitting procedure for generating atom-centered point charges. This approach can thus be recommended for general use in molecular mechanics, molecular dynamics, and free energy calculations for any organic or bioorganic system."
452,"creating an online dictionary of abbreviations from medline",197592,"Creating an Online Dictionary of Abbreviations from MEDLINE","OBJECTIVE: The growth of the biomedical literature presents special challenges for both human readers and automatic algorithms. One such challenge derives from the common and uncontrolled use of abbreviations in the literature. Each additional abbreviation increases the effective size of the vocabulary for a field. Therefore, to create an automatically generated and maintained lexicon of abbreviations, we have developed an algorithm to match abbreviations in text with their expansions. DESIGN: Our method uses a statistical learning algorithm, logistic regression, to score abbreviation expansions based on their resemblance to a training set of human-annotated abbreviations. We applied it to Medstract, a corpus of MEDLINE abstracts in which abbreviations and their expansions have been manually annotated. We then ran the algorithm on all abstracts in MEDLINE, creating a dictionary of biomedical abbreviations. To test the coverage of the database, we used an independently created list of abbreviations from the China Medical Tribune. MEASUREMENTS: We measured the recall and precision of the algorithm in identifying abbreviations from the Medstract corpus. We also measured the recall when searching for abbreviations from the China Medical Tribune against the database. RESULTS: On the Medstract corpus, our algorithm achieves up to 83\\ recall at 80% precision. Applying the algorithm to all of MEDLINE yielded a database of 781,632 high-scoring abbreviations. Of all the abbreviations in the list from the China Medical Tribune, 88% were in the database. CONCLUSION: We have developed an algorithm to identify abbreviations from text. We are making this available as a public abbreviation server at {\\textbackslash}url[http://abbreviation.stanford.edu/]."
453,"determining the density of states for classical statistical models a random walk algorithm to produce a flat histogram",200385,"Determining the density of states for classical statistical models: A random walk algorithm to produce a flat histogram","We describe an efficient Monte Carlo algorithm using a random walk in energy space to obtain a very accurate estimate of the density of states for classical statistical models. The density of states is modified at each step when the energy level is visited to produce a flat histogram. By carefully controlling the modification factor, we allow the density of states to converge to the true value very quickly, even for large systems. From the density of states at the end of the random walk, we can estimate thermodynamic quantities such as internal energy and specific heat capacity by calculating canonical averages at any temperature. Using this method, we not only can avoid repeating simulations at multiple temperatures, but we can also estimate the free energy and entropy, quantities that are not directly accessible by conventional Monte Carlo simulations. This algorithm is especially useful for complex systems with a rough landscape since all possible energy levels are visited with the same probability. As with the multicanonical Monte Carlo technique, our method overcomes the tunneling barrier between coexisting phases at first-order phase transitions. In this paper, we apply our algorithm to both first- and second-order phase transitions to demonstrate its efficiency and accuracy. We obtained direct simulational estimates for the density of states for two-dimensional ten-state Potts models on lattices up to  200×200  and Ising models on lattices up to  256×256.  Our simulational results are compared to both exact solutions and existing numerical data obtained using other methods. Applying this approach to a three-dimensional  ± J  spin-glass model, we estimate the internal energy and entropy at zero temperature; and, using a two-dimensional random walk in energy and order-parameter space, we obtain the (rough) canonical distribution and energy landscape in order-parameter space. Preliminary data suggest that the glass transition temperature is about  1.2  and that better estimates can be obtained with more extensive application of the method. This simulational method is not restricted to energy space and can be used to calculate the density of states for any parameter by a random walk in the corresponding space."
454,"growing up digital the rise of the net generation",200747,"Growing Up Digital: The Rise of the Net Generation","{Don Tapscott, author of <i>The Digital  Economy</i>, turns his attention to the way young people--surrounded by high-tech toys and tools from birth--will likely affect the future. In <i>Growing Up Digital: The Rise of the Net Generation</i>, Tapscott parlays some 300 interviews into predictions on how today's 2- to 22-year-olds might reshape society. His observations about this enormously influential population, which will total 88 million in North America alone by the year 2000, range from the kind of employees they may eventually be to how they could be reached by marketers.} {The bestselling book announcing the arrival of the Net Generation--those kids who are growing up digital--now in paperback. Heraled by Library Journal as one of the Best Business Books of 1997, Growing Up Digital tells how the N-Generation is learning to communicate, work, shop and play in profoundly new ways--and what implications this has for the world and business.  <P> Growing Up Digital offers an overview of the N-Generation, the generation of children who in the year 2000 will be between the ages of two and twenty-two. This group is a ""tsunami"" that will force changes in communications, retailing, branding, advertising, education, etc. Tapscott commends that the N-Generation are becoming so technologically proficient that they will ""lap"" their parents and leave them behind.  <P> The book also demonstrates the common characteristics of the N-Generation: <br> acceptance of diversity, because the Net doesn't distinguish between racial or gender identities, curiosity about exploring and discovering new worlds over the Internet and assertiveness and self-reliance, which result when these kids realize they know more about technology than the adults around them.} {Tapscott, who coined the term ""Net Generation,"" profiles this new group and tells how its use of digital technology is reshaping the way society and individuals interact. 15 illustrations. 256 pp. $75,000 marketing. 100,000 print. (Business)                                                 }"
455,"publics and counterpublics",200770,"Publics and Counterpublics","{Most of the people around us belong to our world not directly, as kin or comrades, but as strangers. How do we recognize them as members of our world? We are related to them as transient participants in common publics. Indeed, most of us would find it nearly impossible to imagine a social world without publics. In the eight essays in this book, Michael Warner addresses the question: What is a public?<br /> <br /> According to Warner, the idea of a public is one of the central fictions of modern life. Publics have powerful implications for how our social world takes shape, and much of modern life involves struggles over the nature of publics and their interrelations. The idea of a public contains ambiguities, even contradictions. As it is extended to new contexts, politics, and media, its meaning changes in ways that can be difficult to uncover.<br /> <br /> Combining historical analysis, theoretical reflection, and extensive case studies, Warner shows how the idea of a public can reframe our understanding of contemporary literary works and politics and of our social world in general. In particular, he applies the idea of a public to the junction of two intellectual traditions: public-sphere theory and queer theory.}"
456,"the evolution of reciprocal altruism",201547,"The evolution of reciprocal altruism","A model is presented to account for the natural selection of what is termed reciprocally altruistic behavior. The model shows how selection can operate against the cheater (non-reciprocator) in the system. Three instances of altruistic behavior are discussed, the evolution of which the model can explain: (1) behavior involved in cleaning symbioses; (2) warning cries in birds; and (3) human reciprocal altruism. Regarding human reciprocal altruism, it is shown that the details of the psychological system that regulates this altruism can be explained by the model. Specifically, friendship, dislike, moralistic aggression, gratitude, sympathy, trust, suspicion, trustworthiness, aspects of guilt, and some forms of dishonesty and hypocrisy can be explained as important adaptations to regulate the altruistic system. Each individual human is seen as possessing altruistic and cheating tendencies, the expression of which is sensitive to developmental variables that were selected to set the tendencies at a balance appropriate to the local social and ecological environment."
457,"information filtering and information retrieval two sides of the same coin",201597,"Information filtering and information retrieval: two sides of the same coin?","Information filtering systems are designed for unstructured or semistructured data, as opposed to database applications, which use very structured data. The systems also deal primarily with textual information, but they may also entail images, voice, video or other data types that are part of multimedia information systems. Information filtering systems also involve a large amount of data and streams of incoming data, whether broadcast from a remote source or sent directly by other sources. Filtering is based on descriptions of individual or group information preferences, or profiles, that typically represent long-term interests. Filtering also implies removal of data from an incoming stream rather than finding data in the stream; users see only the data that is extracted. Models of information retrieval and filtering, and lessons for filtering from retrieval research are presented."
458,"loopy belief propagation for approximate inference an empirical study",201704,"Loopy belief propagation for approximate inference: an empirical study","Recently, researchers have demonstrated that &amp;quot;loopy belief propagation &amp;quot;--- the use of Pearl&#039;s polytree algorithm in a Bayesian network with loops--- can perform well in the context of error-correcting codes. The most dramatic instance of this is the near Shannon-limit performance of &amp;quot;Turbo Codes &amp;quot;--- codes whose decoding algorithm is equivalent to loopy belief propagation in a chain-structured Bayesian network. In this paper we ask: is there something special about the error-correcting code context, or does loopy propagation work as an approximate inference scheme in a more general setting? We compare the marginals computed using loopy propagation to the exact ones in four Bayesian network architectures, including two real-world networks: ALARM and QMR. We find that the loopy beliefs often converge and when they do, they give a good approximation to the correct marginals. However, on the QMR network, the loopy beliefs oscillated and had no obvious relationship to the correct posteriors. We present some initial investigations into the cause of these oscillations, and show that some simple methods of preventing them lead to the wrong results. 1"
459,"the zipper",201777,"The Zipper","The main drawback to the purely applicative paradigm of programming is that many efficient algorithms use destructive operations in data structures such as bit vectors or character arrays or other mutable hierarchical classification structures, which are not immediately modeled as purely applicative data structures. A well known solution to this problem is called functional arrays (Paulson, 1991). For trees, this amounts to modifying an occurrence in a tree non-destructively by copying its path from the root of the tree. This is considered tolerable when the data structure is just an object local to some algorithm, the cost being logarithmic compared to the naive solution which copies all the tree. But when the data structure represents some global context, such as the buﬀer of a text editor, or the database of axioms and lemmas in a proof system, this technique is prohibitive. In this note, we explain a simple solution where tree editing is completely local, the handle on the data not being the original root of the tree, but rather the current position in the tree.The basic idea is simple: the tree is turned inside-out like a returned glove, pointers from the root to the current position being reversed in a path structure. The current location holds both the downward current subtree and the upward path. All navigation and modification primitives operate on the location structure. Going up and down in the structure is analogous to closing and opening a zipper in a piece of clothing, whence the name.The author coined this data-type when designing the core of a structured editor for use as a structure manager for a proof assistant. This simple idea must have been invented on numerous occasions by creative programmers, and the only justification for presenting what ought to be folklore is that it does not appear to have been published, or even to be well-known."
460,"measurement modeling and analysis of a peertopeer filesharing workload",201791,"Measurement, Modeling, and Analysis of a Peer-to-Peer File-Sharing Workload","Peer-to-peer (P2P) file sharing accounts for an astonishing volume of current Internet traffic. This paper probes deeply into modern P2P file sharing systems and the forces that drive them. By doing so, we seek to increase our understanding of P2P file sharing workloads and their implications for future multimedia workloads. Our research uses a three-tiered approach. First, we analyze a 200-day trace of over 20 terabytes of Kazaa P2P traffic collected at the University of Washington. Second, we develop a model of multimedia workloads that lets us isolate, vary, and explore the impact of key system parameters. Our model, which we parameterize with statistics from our trace, lets us confirm various hypotheses about file-sharing behavior observed in the trace. Third, we explore the potential impact of locality-awareness in Kazaa.Our results reveal dramatic differences between P2P file sharing and Web traffic. For example, we show how the immutability of Kazaa's multimedia objects leads clients to fetch objects at most once; in contrast, a World-Wide Web client may fetch a popular page (e.g., CNN or Google) thousands of times. Moreover, we demonstrate that: (1) this ""fetch-at-most-once"" behavior causes the Kazaa popularity distribution to deviate substantially from Zipf curves we see for the Web, and (2) this deviation has significant implications for the performance of multimedia file-sharing systems. Unlike the Web, whose workload is driven by document change, we demonstrate that clients' fetch-at-most-once behavior, the creation of new objects, and the addition of new clients to the system are the primary forces that drive multimedia workloads such as Kazaa. We also show that there is substantial untapped locality in the Kazaa workload. Finally, we quantify the potential bandwidth savings that locality-aware P2P file-sharing architectures would achieve."
461,"hybrid recommender systems survey and experiments",201820,"Hybrid Recommender Systems: Survey and Experiments","Recommender systems represent user preferences for the purpose of suggesting items to purchase or examine. They have become fundamental applications in electronic commerce and information access, providing suggestions that effectively prune large information spaces so that users are directed toward those items that best meet their needs and preferences. A variety of techniques have been proposed for performing recommendation, including content-based, collaborative, knowledge-based and other techniques. To improve performance, these methods have sometimes been combined in hybrid recommenders. This paper surveys the landscape of actual and possible hybrid recommenders, and introduces a novel hybrid, EntreeC, a system that combines knowledge-based recommendation and collaborative filtering to recommend restaurants. Further, we show that semantic ratings obtained from the knowledge-based part of the system enhance the effectiveness of collaborative filtering."
462,"recommendation as classification using social and contentbased information in recommendation",201830,"Recommendation as Classification: Using Social and Content-Based Information in Recommendation","Recommendation systems make suggestions about artifacts to a user. For instance, they may predict whether a user would be interested in seeing a particular movie. Social recomendation methods collect ratings of artifacts from many individuals and use nearest-neighbor techniques to make recommendations to a user concerning new artifacts. However, these methods do not use the significant amount of other information that is often available about the nature of each artifact --- such as cast lists or movie reviews, for example. This paper presents an inductive learning approach to recommendation that is able to use both ratings information and other forms of information about each artifact in predicting user preferences. We show that our method outperforms an existing social-filtering method in the domain of movie recommendations on a dataset of more than 45,000 movie ratings collected from a community of over 250 users.  Introduction  Recommendations are a part of everyday life. We usually..."
463,"designer gene networks towards fundamental cellular control",202252,"Designer gene networks: Towards fundamental cellular control.","The engineered control of cellular function through the design of synthetic genetic networks is becoming plausible. {H}ere we show how a naturally occurring network can be used as a parts list for artificial network design, and how model formulation leads to computational and analytical approaches relevant to nonlinear dynamics and statistical physics. {W}e first review the relevant work on synthetic gene networks, highlighting the important experimental findings with regard to genetic switches and oscillators. {W}e then present the derivation of a deterministic model describing the temporal evolution of the concentration of protein in a single-gene network. {B}istability in the steady-state protein concentration arises naturally as a consequence of autoregulatory feedback, and we focus on the hysteretic properties of the protein concentration as a function of the degradation rate. {W}e then formulate the effect of an external noise source which interacts with the protein degradation rate. {W}e demonstrate the utility of such a formulation by constructing a protein switch, whereby external noise pulses are used to switch the protein concentration between two values. {F}ollowing the lead of earlier work, we show how the addition of a second network component can be used to construct a relaxation oscillator, whereby the system is driven around the hysteresis loop. {W}e highlight the frequency dependence on the tunable parameter values, and discuss design plausibility. {W}e emphasize how the model equations can be used to develop design criteria for robust oscillations, and illustrate this point with parameter plots illuminating the oscillatory regions for given parameter values. {W}e then turn to the utilization of an intrinsic cellular process as a means of controlling the oscillations. {W}e consider a network design which exhibits self-sustained oscillations, and discuss the driving of the oscillator in the context of synchronization. {T}hen, as a second design, we consider a synthetic network with parameter values near, but outside, the oscillatory boundary. {I}n this case, we show how resonance can lead to the induction of oscillations and amplification of a cellular signal. {F}inally, we construct a toggle switch from positive regulatory elements, and compare the switching properties for this network with those of a network constructed using negative regulation. {O}ur results demonstrate the utility of model analysis in the construction of synthetic gene regulatory networks. (c) 2001 {A}merican {I}nstitute of {P}hysics."
464,"when old technologies were new thinking about electric communication in the late nineteenth century",202349,"When old technologies were new: thinking about electric communication in the late nineteenth century","{ In the history of electronic communication, the last quarter of the nineteenth century holds a special place, for it was during this period that the telephone, phonograph, electric light, wireless, and cinema were all invented.  In When old Technologies Were New, Carolyn Marvin explores how<br>two of these new inventions--the telephone and the electric light--were publicly envisioned at the end of the nineteenth century, as seen in specialized engineering journals and popular media.  Marvin pays particular attention to the telephone, describing how it disrupted established social<br>relations, unsettling customary ways of dividing the private person and family from the more public setting of the community. On the lighter side, she describes how people spoke louder when calling long distance, and how they worried about catching contagious diseases over the phone. A particularly<br>powerful chapter deals with telephonic precursors of radio broadcasting--the ""Telephone Herald"" in New York and the ""Telefon Hirmondo"" of Hungary--and the conflict between the technological development of broadcasting and the attempt to impose a homogenous, ethnocentric variant of Anglo-Saxon<br>culture on the public. While focusing on the way professionals in the electronics field tried to control the new media, Marvin also illuminates the broader social impact, presenting a wide-ranging, informative, and entertaining account of the early years of electronic media. }"
465,"user acceptance of computer technology a comparison of two theoretical models",202480,"User acceptance of computer technology: a comparison of two theoretical models","Computer systems cannot improve organizational performance if they aren't used. Unfortunately, resistance to end-user systems by managers and professionals is a widespread problem. To better predict, explain, and increase user acceptance, we need to better understand why people accept or reject computers. This research addresses the ability to predict peoples' computer acceptance from a measure of their intentions, and the ability to explain their intentions in terms of their attitudes, subjective norms, perceived usefulness, perceived ease of use, and related variables. In a longitudinal study of 107 users, intentions to use a specific system, measured after a one-hour introduction to the system, were correlated 0.35 with system use 14 weeks later. The intention-usage correlation was 0.63 at the end of this time period. Perceived usefulness strongly influenced peoples' intentions, explaining more than half of the variance in intentions at the end of 14 weeks. Perceived ease of use had a small but significant effect on intentions as well, although this effect subsided over time. Attitudes only partially mediated the effects of these beliefs on intentions. Subjective norms had no effect on intentions. These results suggest the possibility of simple but powerful models of the determinants of user acceptance, with practical value for evaluating systems and guiding managerial interventions aimed at reducing the problem of underutilized computer technology. 10.1287/mnsc.35.8.982"
466,"molecular dynamics simulations of biomolecules longrange electrostatic effects",202502,"Molecular dynamics simulations of biomolecules: long-range electrostatic effects.","Current computer simulations of biomolecules typically make use of classical molecular dynamics methods, as a very large number (tens to hundreds of thousands) of atoms are involved over timescales of many nanoseconds. The methodology for treating short-range bonded and van der Waals interactions has matured. However, long-range electrostatic interactions still represent a bottleneck in simulations. In this article, we introduce the basic issues for an accurate representation of the relevant electrostatic interactions. In spite of the huge computational time demanded by most biomolecular systems, it is no longer necessary to resort to uncontrolled approximations such as the use of cutoffs. In particular, we discuss the Ewald summation methods, the fast particle mesh methods, and the fast multipole methods. We also review recent efforts to understand the role of boundary conditions in systems with long-range interactions, and conclude with a short perspective on future trends."
467,"comparison of networkbased pathway analysis methods",203297,"Comparison of network-based pathway analysis methods","Network-based definitions of biochemical pathways have emerged in recent years. These pathway definitions insist on the balanced use of a whole network of biochemical reactions. Two such related definitions, elementary modes and extreme pathways, have generated novel hypotheses regarding biochemical network function. The relationship between these two approaches can be illustrated by comparing and contrasting the elementary modes and extreme pathways of previously published metabolic reconstructions of the human red blood cell {(RBC)} and the human pathogen Helicobacter pylori. Descriptions of network properties generated by using these two approaches in the analysis of realistic metabolic networks need careful interpretation."
468,"an empirical study of global software development distance and speed",203951,"An empirical study of global software development: distance and speed","Global software development is rapidly becoming the norm for technology companies. Previous qualitative research suggests that multi-site development may increase the development cycle time. We use both survey data and data from the source code change management system to model the extent of delay in a multi-site software development organization, and explore several possible mechanisms for this delay. We also measure differences in same-site and cross-site communication patterns, and analyze the relationship of these variables to delay. Our results show that, compared to same-site work, cross-site work takes much longer and requires more people for work of equal size and complexity. We also report a strong relationship between delay in cross-site work and the degree to which remote colleagues are perceived to help out when workloads are heavy. We discuss the implications of our findings for collaboration technology for distributed software development."
469,"pervasive computing vision and challenges",203956,"Pervasive computing: vision and challenges","This article discusses the challenges in computer systems research posed by the emerging field of pervasive computing. It first examines the relationship of this new field to its predecessors: distributed systems and mobile computing. It then identifies four new research thrusts: effective use of smart spaces, invisibility, localized scalability, and masking uneven conditioning. Next, it sketches a couple of hypothetical pervasive computing scenarios, and uses them to identify key capabilities missing from today's systems. The article closes with a discussion of the research necessary to develop these capabilities."
470,"prediction of the phenotypic effects of nonsynonymous single nucleotide polymorphisms using structural and evolutionary information",205022,"Prediction of the phenotypic effects of non-synonymous single nucleotide polymorphisms using structural and evolutionary information","Motivation: There has been great expectation that the knowledge of an individual's genotype will provide a basis for assessing susceptibility to diseases and designing individualized therapy. Non-synonymous single nucleotide polymorphisms (nsSNPs) that lead to an amino acid change in the protein product are of particular interest because they account for nearly half of the known genetic variations related to human inherited diseases. To facilitate the identification of disease-associated nsSNPs from a large number of neutral nsSNPs, it is important to develop computational tools to predict the phenotypic effects of nsSNPs.Results: We prepared a training set based on the variant phenotypic annotation of the Swiss-Prot database and focused our analysis on nsSNPs having homologous 3D structures. Structural environment parameters derived from the 3D homologous structure as well as evolutionary information derived from the multiple sequence alignment were used as predictors. Two machine learning methods, support vector machine and random forest, were trained and evaluated. We compared the performance of our method with that of the SIFT algorithm, which is one of the best predictive methods to date. An unbiased evaluation study shows that for nsSNPs with sufficient evolutionary information (with not <10 homologous sequences), the performance of our method is comparable with the SIFT algorithm, while for nsSNPs with insufficient evolutionary information (<10 homologous sequences), our method outperforms the SIFT algorithm significantly. These findings indicate that incorporating structural information is critical to achieving good prediction accuracy when sufficient evolutionary information is not available.Availability: The codes and curated dataset are available at http://compbio.utmem.edu/snp/dataset/Contact: ycui2@utmem.eduSupplementary information: The curated dataset is available at http://compbio.utmem.edu/snp/dataset/"
471,"engaging by design how engagement strategies in popular computer and video games can inform instructional design",206013,"Engaging By Design: How Engagement Strategies in Popular Computer and Video Games Can Inform Instructional Design","Computer and video games are a prevalent form of entertainment in which the purpose of the design is to engage players. Game designers incorporate a number of strategies and tactics for engaging players in ‘gameplay.’ These strategies and tactics may provide instructional designers with new methods for engaging learners. This investigation presents a review of game design strategies and the implications of appropriating these strategies for instructional design. Specifically, this study presents an overview of the trajectory of player positioning or point of view, the role of narrative, and methods of interactive design. A comparison of engagement strategies in popular games and characteristics of engaged learning is also presented to examine how strategies of game design might be integrated into the existing framework of engaged learning. [ABSTRACT FROM AUTHOR]"
472,"a comparison of mechanisms for improving tcp performance over wireless links",209097,"A comparison of mechanisms for improving TCP performance over wireless links","Wireless mesh networks (WMNs) have emerged as a key technology for next-generation wireless networking. Because of their advantages over other wireless mesh networks, WMNs are undergoing rapid progress and inspiring numerous applications. However, many technical issues still exist in this field. In order to provide a better understanding of the research challenges of WMNs, this article presents a detailed investigation of current state-of-the-art protocols and algorithms for WMNs. Open research issues in all protocol layers are also discussed, with an objective to spark new research interests in this field."
473,"dummynet a simple approach to the evaluation of network protocols",209184,"Dummynet: a simple approach to the evaluation of network protocols","Network protocols are usually tested in operational networks or in simulated environments. With the former approach it is not easy to set and control the various operational parameters such as bandwidth, delays, queue sizes. Simulators are easier to control, but they are often only an approximate model of the desired setting, especially for what regards the various traffic generators (both producers and consumers) and their interaction with the protocol itself.In this paper we show how a simple, yet flexible and accurate network simulator -  dummynet  - can be built with minimal modifications to an existing protocol stack, allowing experiments to be run on a standalone system.  dummynet  works by intercepting communications of the protocol layer under test and simulating the effects of finite queues, bandwidth limitations and communication delays. It runs in a fully operational system, hence allowing the use of real traffic generators and protocol implementations, while solving the problem of simulating unusual environments. With our tool, doing experiments with network protocols is as simple as running the desired set of applications on a workstation.A FreeBSD implementation of  dummynet , targeted to TCP, is available from the author. This implementation is highly portable and compatible with other BSD-derived systems, and takes less than 300 lines of kernel code."
474,"replaceing space the roles of place and space in collaborative systems",209472,"Re-place-ing space: the roles of place and space in collaborative systems","Many collaborative and communicative environments use notions of “space” and spatial organisation to facilitate and structure interaction. We argue that a focus on spatial models is misplaced. Drawing on understandings from architecture and urban design, as well as from our own research findings, we highlight the critical distinction between “space” and “place”. While designers use spatial models to support interaction, we show how it is actually a notion of “place” which frames interactive behaviour. This leads us to re-evaluate spatial systems, and discuss how “place”, rather than “space”, can support CSCW design."
475,"on agentbased software engineering",209491,"On Agent-based Software Engineering","Agent-based computing represents an exciting new synthesis both for Artificial Intelligence (AI) and, more generally, Computer Science. It has the potential to significantly improve the theory and the practice of modeling, designing, and implementing computer systems. Yet, to date, there has been little systematic analysis of what makes the agent-based approach such an appealing and powerful computational model. Moreover, even less effort has been devoted to discussing the inherent disadvantages that stem from adopting an agent-oriented view. Here both sets of issues are explored. The standpoint of this analysis is the role of agent-based software in solving complex, real-world problems. In particular, it will be argued that the development of robust and scalable software systems requires autonomous agents that can complete their objectives while situated in a dynamic and uncertain environment, that can engage in rich, high-level social interactions, and that can operate within flexible organisational structures."
476,"digital artifacts for remembering and storytelling posthistory and social network fragments",209629,"Digital Artifacts for Remembering and Storytelling: PostHistory and Social Network Fragments","As part of a long-term investigation into visualizing email, we have created two visualizations of email archives.  One highlights social networks while the other depicts the temporal rhythms of interactions with individuals. While interviewing users of these systems, it became clear that the applications triggered recall of many personal events. One of the most striking and not entirely expected outcomes was that the visualizations motivated retelling stories from the users’ pasts to others. In this paper, we discuss the motivation and design of these projects and analyze their use as catalysts for personal narrative and recall."
477,"what video games have to teach us about learning and literacy",209806,"What Video Games Have to Teach Us About Learning and Literacy","Review {'[Gee} is] a serious scholar who is taking a lead in an emerging {field.'{\\textendash}Scott} Carlson, Chronicle of Higher Education Review {'Gee} astutely points out that for video game makers, unlike schools, failing to engage children is not an {option.'{\\textendash}This} text refers to an out of print or unavailable edition of this title. Book Description A controversial look at the positive things that can be learned from video games by a well known professor of education. James Paul Gee begins his new book with {'I} want to talk about vide games{\\textendash}yes, even violent video games{\\textendash}and say some positive things about them.' With this simple but explosive beginning, one of America's most well-respected professors of education looks seriously at the good that can come from playing video games. Gee is interested in the cognitive development that can occur when someone is trying to escape a maze, find a hidden treasure and, even, blasting away an enemy with a high-powered rifle. Talking about his own video-gaming experience learning and using games as diverse as Lara Croft and Arcanum, Gee looks at major specific cognitive activities: * How individuals develop a sense of identity * How one grasps meaning * How one evaluates and follow a command * How one picks a role model * How one perceives the world This is a ground-breaking book that takes up a new electronic method of education and shows the positive upside it has for learning. About the Author James Paul Gee is one of the most well-known professors of education in the United States. He teaches at the University of Wisconsin, Madison and is the author of several books."
478,"less is more active learning with support vector machines",209845,"Less is More: Active Learning with Support Vector Machines","We describe a simple active learning heuristic which greatly enhances the generalization behav- ior of support vector machines (SVMs) on sev- eral practical document classiﬁcation tasks.  We observe a number of beneﬁts, the most surpris- ing of which is that a SVM trained on a well- chosen subset of the available corpus frequently performs better than one trained on all available data.  The heuristic for choosing this subset is simple to compute, and makes no use of infor- mation about the test set. Given that the training time of SVMs depends heavily on the training set size, our heuristic not only offers better per- formance with fewer data, it frequently does so in less time than the naive approach of training on all available data."
479,"a metrics suite for object oriented design",210399,"A metrics suite for object oriented design","Given the central role that software development plays in the delivery and application of information technology, managers are increasingly focusing on process improvement in the software development area. This demand has spurred the provision of a number of new and/or improved approaches to software development, with perhaps the most prominent being object-orientation (OO). In addition, the focus on process improvement has increased the demand for software measures, or metrics with which to manage the process. The need for such metrics is particularly acute when an organization is adopting a new technology for which established practices have yet to be developed. This research addresses these needs through the development and implementation of a new suite of metrics for OO design. Metrics developed in previous research, while contributing to the field's understanding of software development processes, have generally been subject to serious criticisms, including the lack of a theoretical base. Following Wand and Weber (1989), the theoretical base chosen for the metrics was the ontology of Bunge (1977). Six design metrics are developed, and then analytically evaluated against Weyuker's (1988) proposed set of measurement principles. An automated data collection tool was then developed and implemented to collect an empirical sample of these metrics at two field sites in order to demonstrate their feasibility and suggest ways in which managers may use these metrics for process improvement"
480,"comprehension of causeeffect relations in a toolusing task by chimpanzees pan troglodytes",211254,"Comprehension of Cause–Effect Relations in a Tool-Using Task by Chimpanzees (Pan troglodytes)","Five chimpanzees (Pan troglodytes) were tested to assess their understanding of causality in a tool task. The task consisted of a transparent tube with a trap-hole drilled in its middle. A reward was randomly placed on either side of the hole. Depending on which side the chimpanzee inserted the stick into, the candy was either pushed out of the tube or into the trap. In Experiment 1, the success rate of 2 chimpanzees rose highly above chance, but that of the other subjects did not. Results show that the 2 successful chimpanzees selected the correct side for insertion beforehand. Experiment 2 ruled out the possibility that their success was due to a distance-based associative rule, and the results favor an alternative hypothesis that relates success to an understanding of the causal relation between the tool-using action and its outcome."
481,"social bookmarking tools i",212480,"Social Bookmarking Tools (I)","This paper reviews some current initiatives, as of early 2005, in providing public link management applications on the Web – utilities that are often referred to under the general moniker of 'social bookmarking tools'. There are a couple of things going on here: 1) server-side software aimed specifically at managing links with, crucially, a strong, social networking flavour, and 2) an unabashedly open and unstructured approach to tagging, or user classification, of those links.  A number of such utilities are presented here, together with an emergent new class of tools that caters more to the academic communities and that stores not only user-supplied tags, but also structured citation metadata terms wherever it is possible to glean this information from service providers. This provision of rich, structured metadata means that the user is provided with an accurate third-party identification of a document, which could be used to retrieve that document, but is also free to search on user-supplied terms so that documents of interest (or rather, references to documents) can be made discoverable and aggregated with other similar descriptions either recorded by the user or by other users."
482,"mobile learning with a mobile game design and motivational effects",214145,"Mobile learning with a mobile game: design and motivational effects","Abstract Mobile technologies offer the opportunity to embed learning in a natural environment. This paper describes the design of the MobileGame prototype, exploring the opportunities to support learning through an orientation game in a university setting. The paper first introduces the scenario and then describes the general architecture of the prototype. The main part of the paper focuses on the evaluation of design issues and the effects observed in two trials. Design issues include: Supporting work on the move poses difficult interface questions, the accuracy of current outdoor, and indoor positioning systems is still problematic and the game requires near real-time response time. The evaluation of the effects shows that features such as ‘map-navigation’ and ‘hunting and hiding’ lead to excitement and fun. The participants immerse into a mixed reality that augments both physical and social space. The game success is based on the motivating design of the game itself. The paper concludes with open issues for future research, especially with the need to thoroughly evaluate the learning benefits."
483,"a simple transmit diversity technique for wireless communications",214373,"A simple transmit diversity technique for wireless communications","This paper presents a simple two-branch transmit diversity scheme. Using two transmit antennas and one receive antenna the scheme provides the same diversity order as maximal-ratio receiver combining (MRRC) with one transmit antenna, and two receive antennas. It is also shown that the scheme may easily be generalized to two transmit antennas and M receive antennas to provide a diversity order of 2M. The new scheme does not require any bandwidth expansion or any feedback from the receiver to the transmitter and its computation complexity is similar to MRRC"
484,"a generalized processor sharing approach to flow control in integrated services networks the singlenode case",214713,"A generalized processor sharing approach to flow control in integrated services networks: the single-node case","The problem of allocating network resources to the users of an integrated services network is investigated in the context of rate-based flow control. The network is assumed to be a virtual circuit, connection-based packet network. It is shown that the use of generalized processor sharing (GPS), when combined with leaky bucket admission control, allows the network to make a wide range of worst-case performance guarantees on throughput and delay. The scheme is flexible in that different users may be given widely different performance guarantees and is efficient in that each of the servers is work conserving. The authors present a practical packet-by-packet service discipline, PGPS that closely approximates GPS. This allows them to relate results for GPS to the packet-by-packet scheme in a precise manner. The performance of a single-server GPS system is analyzed exactly from the standpoint of worst-case packet delay and burstiness when the sources are constrained by leaky buckets. The worst-case session backlogs are also determined"
485,"web services been there done that",214716,"Web services: been there, done that?","Web services can be defined as loosely coupled, reusable software components that semantically encapsulate discrete functionality and are distributed and programmatically accessible over standard Internet protocols. Web services have received a lot of hype, the reasons for which are not easily determined. Some of their benefits might even seem to waste away, once we touch on the nitty-gritty details, because Web services per se do not offer a solution to underlying problems. The contributions included in this section delve into some of these issues, including: pitfalls of workflow issues; structuring procedural knowledge into problem-solving methods; discussing how a low initial entry barrier and simple technology are balanced against the long-term goal of easy integration; including semantics in a Web service modeling framework; and building on new kinds of applications such as grid enterprises."
486,"rascal calculation of graph similarity using maximum common edge subgraphs",215770,"RASCAL: Calculation of Graph Similarity using Maximum Common Edge Subgraphs","A new graph similarity calculation procedure is introduced for comparing labeled graphs. Given a minimum similarity threshold, the procedure consists of an initial screening process to determine whether it is possible for the measure of similarity between the two graphs to exceed the minimum threshold, followed by a rigorous maximum common edge subgraph (MCES) detection algorithm to compute the exact degree and composition of similarity. The proposed MCES algorithm is based on a maximum clique formulation of the problem and is a significant improvement over other published algorithms. It presents new approaches to both lower and upper bounding as well as vertex selection."
487,"user interactions with everyday applications as context for justintime information access",215987,"User interactions with everyday applications as context for just-in-time information access","Our central claim is that user interactions with everyday productivity applications (e.g., word processors, Web browsers, etc.) provide rich contextual information that can be leveraged to support just-in-time access to task-relevant information. We discuss the requirements for such systems, and develop a general architecture for systems of this type. As evidence for our claim, we present Watson, a system which gathers contextual information in the form of the text of the document the user is manipulating in order to proactively retrieve documents from distributed information repositories. We close by describing the results of several experiments with Watson, which show it consistently provides useful information to its users."
488,"optimizing search by showing results in context",215995,"Optimizing search by showing results in context","We developed and evaluated seven interfaces for integrating semantic category information with Web search results. List interfaces were based on the familiar ranked-listing of search results, sometimes augmented with a category name for each result. Category interfaces also showed page titles and/or category names, but re-organized the search results so that items in the same category were grouped together visually. Our user studies show that all Category interfaces were more effective than List interfaces even when lists were augmented with category names for each result. The best category performance was obtained when both category names and individual page titles were presented. Either alone is better than a list presentation, but both together provide the most effective means for allowing users to quickly examining search results. These results provide a better understanding of the perceptual and cognitive factors underlying the advantage of category groupings and provide some practical guidance to Web search interface designers."
489,"the metagenomics of soil",216962,"The metagenomics of soil","{Phylogenetic surveys of soil ecosystems have shown that the number of prokaryotic species found in a single sample exceeds that of known cultured prokaryotes. Soil metagenomics, which comprises isolation of soil DNA and the production and screening of clone libraries, can provide a cultivation-independent assessment of the largely untapped genetic reservoir of soil microbial communities. This approach has already led to the identification of novel biomolecules. However, owing to the complexity and heterogeneity of the biotic and abiotic components of soil ecosystems, the construction and screening of soil-based libraries is difficult and challenging. This review describes how to construct complex libraries from soil samples, and how to use these libraries to unravel functions of soil microbial communities.}"
490,"computer architecture a quantitative approach",218423,"Computer Architecture: A Quantitative Approach","An excellent successor to Hennessy and Patterson's _Computer Organization and Design_, this book presents computer architecture and design as something quantitative that can be studied in the context of real running systems rather than in an abstract format. The concepts are again grounded in real machine architectures and many of the examples are contemporary architectures, such as PowerPC chips and Intel 80x86. _Computer Architecture_ follows the same outline as its predecessor, but covers information in more depth, moving rapidly from introductory discussions to issues just shy of computer design research. The format again includes an excellent mix of exercises and historical background. This book is recommended for people with some experience in digital design--or people who have read and understood the authors' first text."
491,"stemming algorithms a case study for detailed evaluation",220515,"Stemming algorithms: A case study for detailed evaluation","The majority of information retrieval experiments are evaluated by measures such as average precision and average recall. Fundamental decisions about the superiority of one retrieval technique over another are made solely on the basis of these measures. We claim that average performance figures need to be validated with a careful statistical analysis and that there is a great deal of additional information that can be uncovered by looking closely at the results of individual queries. This article is a case study of stemming algorithms which describes a number of novel approaches to evaluation and demonstrates their value. &copy; 1996 John Wiley &amp; Sons, Inc."
492,"an algorithm for suffix stripping",220516,"An algorithm for suffix stripping","<B>Purpose</B> - The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. This work was originally published in <IT>Program</IT> in 1980 and is republished as part of a series of articles commemorating the 40th anniversary of the journal. <B>Design/methodology/approach</B> - An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. <B>Findings</B> - Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length. <B>Originality/value</B> - The piece provides a useful historical document on information retrieval."
493,"the unified software development process",220600,"{The Unified Software Development Process}","A software process defines the steps required to create software successfully. Written by the same authors who brought you the Unified Modelling Language (UML), _The Unified Software Development Process_ introduces a new standard for creating today's software that will certainly be useful for any software developer or manager who is acquainted with UML.  Early sections introduce four basic principles of the unified process: that software should stress use cases (which show how it interacts with users), that the process is architecture-centric and that it is iterative and incremental. The authors then apply these principles to their software process, which involves everything from gathering system requirements to analysis, design, implementation and testing. The use-case examples are excellent and include concrete examples drawn from such areas as banking and inventory control.  The authors point out the connection between UML document types (like use- cases, class diagrams and state transition diagrams) with various models used throughout the software process. They provide very short, real-world examples that illustrate how their ideas have been successfully applied. The straightforward tour of the new unified software process gets extra elaboration--along with some advice--in later chapters that further describe the author's ideas on design. With the weight of these three expert authors behind it, readers can expect _The Unified Software Development Process_ to be an important book and one that will be valuable to any working designer or manager. --_Richard Dragan_"
494,"efficient coding of natural scenes in the lateral geniculate nucleus experimental test of a computational theory",220695,"Efficient coding of natural scenes in the lateral geniculate nucleus: experimental test of a computational theory","A recent computational theory suggests that visual processing in the retina and the lateral geniculate nucleus (LGN) serves to recode information into an efficient form (Atick and Redlich, 1990). Information theoretic analysis showed that the representation of visual information at the level of the photoreceptors is inefficient, primarily attributable to a high degree of spatial and temporal correlation in natural scenes. It was predicted, therefore, that the retina and the LGN should recode this signal into a decorrelated form or, equivalently, into a signal with a ""white"" spatial and temporal power spectrum. In the present study, we tested directly the prediction that visual processing at the level of the LGN temporarily whitens the natural visual input. We recorded the responses of individual neurons in the LGN of the cat to natural, time-varying images (movies) and, as a control, to white-noise stimuli. Although there is substantial temporal correlation in natural inputs (Dong and Atick, 1995b), we found that the power spectra of LGN responses were essentially white. Between 3 and 15 Hz, the power of the responses had an average variation of only +/-10.3%. Thus, the signals that the LGN relays to visual cortex are temporarily decorrelated. Furthermore, the responses of X-cells to natural inputs can be well predicted from their responses to white-noise inputs. We therefore conclude that whitening of natural inputs can be explained largely by the linear filtering properties (Enroth-Cugell and Robson, 1966). Our results suggest that the early visual pathway is well adapted for efficient coding of information in the natural visual environment, in agreement with the prediction of the computational theory."
495,"determining optical flow",220700,"Determining Optical Flow","Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantified rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image."
496,"at home with ubiquitous computing seven challenges",220966,"At Home with Ubiquitous Computing: Seven Challenges","Abstract. The smart home offers a new opportunity to augment people&#039;s lives with ubiquitous computing technology that provides increased communications, awareness, and functionality. Recently, a number of trends have increased the likelihood that the aware home can soon become a reality. We examine a number of challenges from the technical, social, and pragmatic domains that we feel must be overcome before the vision of the smart home, posited by ubiquitous computing research, can become a reality. Our hope in raising these issues is to create a conversation among researchers in the varied disciplines that make up ubiquitous computing. In particular, we hope to raise awareness of the existing literature on the adoption, use, and history of domestic technologies, as well as the use of situated studies, and the benefits that these can bring to bear on the design and evaluation of technologies for the home"
497,"system support for pervasive applications",220970,"System support for pervasive applications","Pervasive computing provides an attractive vision for the future of computing. Computational power will be available everywhere. Mobile and stationary devices will dynamically connect and coordinate to seamlessly help people in accomplishing their tasks. For this vision to become a reality, developers must build applications that constantly adapt to a highly dynamic computing environment. To make the developers' task feasible, we present a system architecture for pervasive computing, called &#60;i>one.world&#60;/i>. Our architecture provides an integrated and comprehensive framework for building pervasive applications. It includes services, such as discovery and migration, that help to build applications and directly simplify the task of coping with constant change. We describe our architecture and its programming model and reflect on our own and others' experiences with using it."
498,"video blogging content to the max",221092,"Video blogging: content to the max","The lure of video blogging combines the ubiquitous, grassroots, Web-based journaling of blogging with the richness of expression available in multimedia. Some claim that video blogging is an important force in a future world of video journalism and a powerful technical adjunct to our existing televised news sources. Others point to the huge demands it imposes on networking resources, the lack of hard standards, and the poor usability of current video blogging systems as indicators that it's doomed to fail. Like any nascent technology, video blogging has many unsolved problems. The field, however, is vibrant, the goals are fairly clear, and the challenges they pose to multimedia researchers are exciting indeed. Developing the standards and technologies for video blogging requires a combination of approaches from various areas including media representation, information retrieval, multimedia content analysis, and video summarization. Like the development of the Web and text blogging before, video blogging only come about through open development and collaboration between engineers and researchers from diverse fields. Most strikingly, it is fueled by the passion and enthusiasm of those creating content - those who go to the trouble of recording their lives and opinions within the fledgling medium, shaping it as a lively and useful resource for generations of Internet users to come."
499,"functional genome annotation through phylogenomic mapping",221156,"Functional genome annotation through phylogenomic mapping","Accurate determination of functional interactions among proteins at the genome level remains a challenge for genomic research. Here we introduce a genome-scale approach to functional protein annotation{\^a}phylogenomic mapping{\^a}that requires only sequence data, can be applied equally well to both finished and unfinished genomes, and can be extended beyond single genomes to annotate multiple genomes simultaneously. We have developed and applied it to more than 200 sequenced bacterial genomes. Proteins with similar evolutionary histories were grouped together, placed on a three dimensional map and visualized as a topographical landscape. The resulting phylogenomic maps display thousands of proteins clustered in mountains on the basis of coinheritance, a strong indicator of shared function. In addition to systematic computational validation, we have experimentally confirmed the ability of phylogenomic maps to predict both mutant phenotype and gene function in the delta proteobacterium Myxococcus xanthus."
500,"the application of petri nets to workflow management",221328,"The Application of Petri Nets to Workflow Management","Workflow management promises a new solution to an age-old problem: controlling, monitoring, optimizing and supporting business processes. What is new about workflow management is the explicit representation of the business process logic which allows for computerized support. This paper discusses the use of Petri nets in the context of workflow management. Petri nets are an established tool for modeling and analyzing processes. On the one hand, Petri nets can be used as a design language for the specification of complex workflows. On the other hand, Petri net theory provides for powerful analysis techniques which can be used to verify the correctness of workflow procedures. This paper introduces workflow management  as an application domain for Petri nets, presents state-of-the-art results with respect to the verification of workflows, and highlights some Petri-net-based workflow tools.  1 Introduction  In former times, information systems were designed to support the execution of indiv..."
501,"basics of qualitative research techniques and procedures for developing grounded theory",221342,"Basics of Qualitative Research : Techniques and Procedures for Developing Grounded Theory","The **Third Edition **of the bestselling **Basics of Qualitative Research:Techniques and Procedures for Developing Grounded Theory **continues to offer immensely practical advice and technical expertise to aid researchers in making sense of their collected data. Authors Juliet Corbin and the late Anselm Strauss (co-creator of Grounded Theory) present methods that enable researchers to analyze and interpret their data, and ultimately build theory from it. Highly accessible in their approach, Corbin and Strauss provide a step-by-step guide to the research act--from the formation of the research question through several approaches to coding and analysis, to reporting on the research. Full of definitions and illustrative examples, this book concludes with chapters that present criteria for evaluating a study, as well as responses to common questions posed by students of qualitative research. Significantly revised, **Basics of Qualitative Research **remains a landmark volume in the study of qualitative methods.  **  Key Features of the Third Edition:**  * Allows for students to develop their critical thinking skills in the ""Critical Issues"" section at the end of each chapter.  * Shows the actual steps involved in data analysis (from description to grounded theory) and data gathering by means of theoretical sampling.  * Provides exercises for thinking, writing and group discussion that reinforces material presented in the text.  * Consists of a student companion Web site at www.sagepub.com/corbinstudysite that includes real data and practice with qualitative software such as MAXQDA, as well as student practice exercises."
502,"learning to control a brainmachine interface for reaching and grasping by primates",221552,"Learning to Control a Brain-Machine Interface for Reaching and Grasping by Primates","Reaching and grasping in primates depend on the coordination of neural activity in large frontoparietal ensembles. Here we demonstrate that primates can learn to reach and grasp virtual objects by controlling a robot arm through a closed-loop brain-machine interface (BMIc) that uses multiple mathematical models to extract several motor parameters (i.e., hand position, velocity, gripping force, and the EMGs of multiple arm muscles) from the electrical activity of frontoparietal neuronal ensembles. As single neurons typically contribute to the encoding of several motor parameters, we observed that high BMIc accuracy required recording from large neuronal ensembles. Continuous BMIc operation by monkeys led to significant improvements in both model predictions and behavioral performance. Using visual feedback, monkeys succeeded in producing robot reach-and-grasp movements even when their arms did not move. Learning to operate the BMIc was paralleled by functional reorganization in multiple cortical areas, suggesting that the dynamic properties of the BMIc were incorporated into motor and sensory cortical representations."
503,"ontology versioning in an ontology management framework",222732,"Ontology versioning in an ontology management framework","Ontologies have become ubiquitous in information systems. They constitute the semantic Web's backbone, facilitate e-commerce, and serve such diverse application fields as bioinformatics and medicine. As ontology development becomes increasingly widespread and collaborative, developers are creating ontologies using different tools and different languages. These ontologies cover unrelated or overlapping domains at different levels of detail and granularity. A uniform framework, which we present here, helps users manage multiple ontologies by leveraging data and algorithms developed for one tool in another. For example, by using an algorithm we developed for structural evaluation of ontology versions, this framework lets developers compare different ontologies and map similarities and differences among them. Multiple-ontology management includes these tasks: maintain ontology libraries, import and reuse ontologies, translate ontologies from one formalism to another, support ontology versioning, specify transformation rules between different ontologies and version, merge ontologies, align and map between ontologies, extract an ontology's self-contained parts, support inference across multiple ontologies, support query across multiple ontologies."
504,"a microrna polycistron as a potential human oncogene",222953,"A microRNA polycistron as a potential human oncogene","To date, more than 200 microRNAs have been described in humans; however, the precise functions of these regulatory, non-coding RNAs remains largely obscure. One cluster of microRNAs, the mir-17–92 polycistron, is located in a region of DNA that is amplified in human B-cell lymphomas1. Here we compared B-cell lymphoma samples and cell lines to normal tissues, and found that the levels of the primary or mature microRNAs derived from the mir-17–92 locus are often substantially increased in these cancers. Enforced expression of the mir-17–92 cluster acted with c-myc expression to accelerate tumour development in a mouse B-cell lymphoma model. Tumours derived from haematopoietic stem cells expressing a subset of the mir-17–92 cluster and c-myc could be distinguished by an absence of apoptosis that was otherwise prevalent in c-myc-induced lymphomas. Together, these studies indicate that non-coding RNAs, specifically microRNAs, can modulate tumour formation, and implicate the mir-17–92 cluster as a potential human oncogene."
505,"is there chaos in the brain i concepts of nonlinear dynamics and methods of investigation",223017,"Is there chaos in the brain? I. Concepts of nonlinear dynamics and methods of investigation.","In the light of results obtained during the last two decades in a number of laboratories, it appears that some of the tools of nonlinear dynamics, first developed and improved for the physical sciences and engineering, are well-suited for studies of biological phenomena. In particular it has become clear that the different regimes of activities undergone by nerve cells, neural assemblies and behavioural patterns, the linkage between them, and their modifications over time, cannot be fully understood in the context of even integrative physiology, without using these new techniques. This report, which is the first of two related papers, is aimed at introducing the non expert to the fundamental aspects of nonlinear dynamics, the most spectacular aspect of which is chaos theory. After a general history and definition of chaos the principles of analysis of time series in phase space and the general properties of chaotic trajectories will be described as will be the classical measures which allow a process to be classified as chaotic in ideal systems and models. We will then proceed to show how these methods need to be adapted for handling experimental time series; the dangers and pitfalls faced when dealing with non stationary and often noisy data will be stressed, and specific criteria for suspecting determinism in neuronal cells and/or assemblies will be described. We will finally address two fundamental questions, namely i) whether and how can one distinguish, deterministic patterns from stochastic ones, and, ii) what is the advantage of chaos over randomness: we will explain why and how the former can be controlled whereas, notoriously, the latter cannot be tamed. In the second paper of the series, results obtained at the level of single cells and their membrane conductances in real neuronal networks and in the study of higher brain functions, will be critically reviewed. It will be shown that the tools of nonlinear dynamics can be irreplaceable for revealing hidden mechanisms subserving, for example, neuronal synchronization and periodic oscillations. The benefits for the brain of adopting chaotic regimes with their wide range of potential behaviours and their aptitude to quickly react to changing conditions will also be considered."
506,"classifier fitness based on accuracy",224537,"Classifier Fitness Based on Accuracy","In many classifier systems, the classifier strength parameter serves as a predictor of future payoff and as the classifier's fitness for the genetic algorithm. We investigate a classifier system, XCS, in which each classifier maintains a prediction of expected payoff, but the classifier's fitness is given by a measure of the prediction's accuracy. The system executes the genetic algorithm in niches defined by the match sets, instead of panmictically. These aspects of XCS result in its population tending to form a complete and accurate mapping X x A -> P from inputs and actions to payoff predictions. Further, XCS tends to evolve classifiers that are maximally general, subject to an accuracy criterion. Besides introducing a new direction for classifier system research, these properties of XCS make it suitable for a wide range of reinforcement learning situations where generalization over states is desirable."
507,"modeling market mechanism with minority game",225130,"Modeling Market Mechanism with Minority Game","Using the Minority Game model we study a broad spectrum of problems of market mechanism. We study the role of different types of agents: producers, speculators as well as noise traders. The central issue here is the information flow : producers feed in the information whereas speculators make it away. How well each agent fares in the common game depends on the market conditions, as well as their sophistication. Sometimes there is much to gain with little effort, sometimes great effort virtually brings no more incremental gain. Market impact is shown to play also an important role, a strategy should be judged when it is actually used in play for its quality. Though the Minority Game is an extremely simplified market model, it allows to ask, analyze and answer many questions which arise in real markets."
508,"resource overbooking and application profiling in shared hosting platforms",225187,"Resource overbooking and application profiling in shared hosting platforms","In this paper, we present techniques for provisioning CPU and network resources in shared hosting platforms running potentially antagonistic third-party applications. The primary contribution of our work is to demonstrate the feasibility and benefits of overbooking resources in shared platforms, to maximize the platform yield: the revenue generated by the available resources. We do this by first deriving an accurate estimate of application resource needs by profiling applications on dedicated..."
509,"evolution of web services in bioinformatics",225734,"Evolution of web services in bioinformatics","Bioinformaticians have developed large collections of tools to make sense of the rapidly growing pool of molecular biological data. Biological systems tend to be complex and in order to understand them, it is often necessary to link many data sets and use more than one tool. Therefore, bioinformaticians have experimented with several strategies to try to integrate data sets and tools. Owing to the lack of standards for data sets and the interfaces of the tools this is not a trivial task. Over the past few years building services with web-based interfaces has become a popular way of sharing the data and tools that have resulted from many bioinformatics projects. This paper discusses the interoperability problem and how web services are being used to try to solve it, resulting in the evolution of tools with web interfaces from HTML/web form-based tools not suited for automatic workflow generation to a dynamic network of XML-based web services that can easily be used to create pipelines."
510,"stochastic reactiondiffusion simulation with mesord",225743,"Stochastic reaction-diffusion simulation with MesoRD","Summary: MesoRD is a tool for stochastic simulation of chemical reactions and diffusion. In particular, it is an implementation of the next subvolume method, which is an exact method to simulate the Markov process corresponding to the reaction-diffusion master equation.  Availability: MesoRD is free software, written in C++ and licensed under the GNU general public license (GPL). MesoRD runs on Linux, Mac OS X, NetBSD, Solaris and Windows XP. It can be downloaded from http://mesord.sourceforge.net.  Contact: johan.elf@icm.uu.se; johan.hattne@embl-hamburg.de  Supplementary information:  MesoRD User's Guide' and other documents are available at http://mesord.sourceforge.net. 10.1093/bioinformatics/bti431"
511,"acme an architecture description interchange language",226534,"Acme: an architecture description interchange language","Numerous architectural description languages (ADLs) have been developed, each providing complementary capabilities for architectural development and analysis. Unfortunately, each ADL and supporting toolset operates in isolation, making it difficult to integrate those tools and share architectural descriptions. Acme is being developed as a joint effort of the software architecture research community as a common interchange format for architecture design tools. Acme provides a structural framework for characterizing architectures, together with annotation facilities for additional ADL-specific information. This scheme permits subsets of ADL tools to share architectural information that is jointly understood, while tolerating the presence of information that falls outside their common vocabulary. In this paper we describe Acme's key features, rationale, and technical innovations."
512,"lineage retrieval for scientific data processing a survey",226864,"Lineage retrieval for scientific data processing: a survey","Scientific research relies as much on the dissemination and exchange of data sets as on the publication of conclusions. Accurately tracking the lineage (origin and subsequent processing history) of scientific data sets is thus imperative for the complete documentation of scientific work. Researchers are effectively prevented from determining, preserving, or providing the lineage of the computational data products they use and create, however, because of the lack of a definitive model for lineage retrieval and a poor fit between current data management tools and scientific software. Based on a comprehensive survey of lineage research and previous prototypes, we present a metamodel to help identify and assess the basic components of systems that provide lineage retrieval for scientific data products."
513,"support vector machine active learning with applications to text classification",226883,"Support Vector Machine Active Learning with Applications to Text Classification",". Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.  Keywords: Active Learning, Selective Sampling, Support Vector Machines, Classification, Relevance Feedback  Abbreviations: SVM -- Support Vector Machine; TSVM -- Transductive Support Vector Machine 1."
514,"preferential attachment in the protein network evolution",227176,"Preferential attachment in the protein network evolution.","The Saccharomyces cerevisiae protein-protein interaction map, as well as many natural and man-made networks, shares the scale-free topology. The preferential attachment model was suggested as a generic network evolution model that yields this universal topology. However, it is not clear that the model assumptions hold for the protein interaction network. Using a cross-genome comparison, we show that (a) the older a protein, the better connected it is, and (b) the number of interactions a protein gains during its evolution is proportional to its connectivity. Therefore, preferential attachment governs the protein network evolution. Evolutionary mechanisms leading to such preference and some implications are discussed."
515,"gaining confidence in highthroughput protein interaction networks",227185,"Gaining confidence in high-throughput protein interaction networks","Although genome-scale technologies have benefited from statistical measures of data quality, extracting biologically relevant pathways from high-throughput proteomics data remains a challenge. Here we develop a quantitative method for evaluating proteomics data. We present a logistic regression approach that uses statistical and topological descriptors to predict the biological relevance of protein-protein interactions obtained from high-throughput screens for yeast. Other sources of information, including mRNA expression, genetic interactions and database annotations, are subsequently used to validate the model predictions without bias or cross-pollution. Novel topological statistics show hierarchical organization of the network of high-confidence interactions: protein complex interactions extend one to two links, and genetic interactions represent an even finer scale of organization. Knowledge of the maximum number of links that indicates a significant correlation between protein pairs (correlation distance) enables the integrated analysis of proteomics data with data from genetics and gene expression. The type of analysis presented will be essential for analyzing the growing amount of genomic and proteomics data in model organisms and humans."
516,"a survey of eigenvector methods for web information retrieval",227604,"A Survey of Eigenvector Methods for Web Information Retrieval","Web information retrieval is  significantly more challenging  than traditional well-controlled, small document collection information retrieval.  One main difference between traditional information retrieval and Web information retrieval is the Web's hyperlink structure.  This structure has been exploited by several of today's leading Web search engines, particularly Google and Teoma.  In this survey paper, we focus on Web information retrieval methods that use eigenvector computations, presenting the three popular methods of HITS, PageRank, and SALSA."
517,"web services are not distributed objects",228031,"Web services are not distributed objects","Web services are frequently described as the latest incarnation of distributed object technology. This misconception, perpetuated by people from both industry and academia, seriously limits broader acceptance of the true Web services architecture. Although the architects of many distributed and Internet systems have been vocal about the differences between Web services and distributed objects, dispelling the myth that they are closely related appears difficult. Many believe that Web services is a distributed systems technology that relies on some form of distributed object technology. Unfortunately, this is not the only common misconception about Web services. We seek to clarify several widely held beliefs about the technology that are partially or completely wrong. Within the distributed technology world, it is probably more appropriate to associate Web services with messaging technologies because they share a common architectural view, although they address different application types. Web services technology will have a dramatic enabling effect on worldwide interoperable distributed computing once everyone recognizes that Web services are about interoperable document-centric computing, not distributed objects."
518,"comparing proteinligand docking programs is difficult",228353,"Comparing protein-ligand docking programs is difficult.","There is currently great interest in comparing protein-ligand docking programs. A review of recent comparisons shows that it is difficult to draw conclusions of general applicability. Statistical hypothesis testing is required to ensure that differences in pose-prediction success rates and enrichment rates are significant. Numerical measures such as root-mean-square deviation need careful interpretation and may profitably be supplemented by interaction-based measures and visual inspection of dockings. Test sets must be of appropriate diversity and of good experimental reliability. The effects of crystal-packing interactions may be important. The method used for generating starting ligand geometries and positions may have an appreciable effect on docking results. For fair comparison, programs must be given search problems of equal complexity (e.g. binding-site regions of the same size) and approximately equal time in which to solve them. Comparisons based on rescoring require local optimization of the ligand in the space of the new objective function. Re-implementations of published scoring functions may give significantly different results from the originals. Ostensibly minor details in methodology may have a profound influence on headline success rates. Proteins 2005. (c) 2005 Wiley-Liss, Inc."
519,"modeling tcp latency",228535,"Modeling TCP Latency","Several analytic models describe the steady-state throughput of bulk transfer TCP flows as a function of round trip time and packet loss rate. These models describe flows based on the assumption that they are long enough to sustain many packet losses. However, most TCP transfers across today's Internet are short enough to see few, if any, losses and consequently their performance is dominated by startup effects such as connection establishment and slow start. This paper extends the steady-state model proposed in Padhye et al. (1998), in order to capture these startup effects. The extended model characterizes the expected value and distribution of TCP connection establishment and data transfer latency as a function of transfer size, round trip time, and packet loss rate. Using simulations, controlled measurements of TCP transfers, and live Web measurements we show that, unlike earlier steady-state models for TCP performance, our extended model describes connection establishment and data transfer latency under a range of packet loss conditions, including no loss"
520,"codon bias and base composition are poor indicators of horizontally transferred genes",228643,"Codon bias and base composition are poor indicators of horizontally transferred genes.","Horizontal gene transfer is now recognized as an important mechanism of evolution. {S}everal methods to detect horizontally transferred genes have been suggested. {T}hese methods are based on either nucleotide composition or the failure to find a similar gene in closely related species. {G}enes that evolve vertically between closely related species can be divided into those that retain homologous chromosomal positions (positional orthologs) and those that do not. {B}y comparing open reading frames in the {E}scherichia coli and {S}almonella typhi genomes, we identified 2,728 positional orthologs since these species split 100 {MYA}. {A} group of 1,144 novel {E}. coli genes were unusually diverged from their {S}. typhi counterparts. {T}hese novel genes included those that had been horizontally transferred into {E}. coli, as well as members of gene pairs that had been rearranged or deleted. {P}ositional orthologs were used to investigate compositional methods of identifying horizontally transferred genes. {A} large number of {E}. coli genes with normal nucleotide composition have no apparent ortholog in {S}. typhi, and many genes of atypical composition do, in fact, have positional orthologs. {A} phylogenetic approach was employed to confirm selected examples of horizontal transmission among the novel groups of genes. {O}ur analysis of 80 {E}. coli genes determined that a number of genes previously classified as horizontally transferred based on base composition and codon bias were native, and genes previously classified as native appeared to be horizontally transferred. {H}ence, atypical nucleotide composition alone is not a reliable indicator of horizontal transmission."
521,"markov chain monte carlo without likelihoods",230052,"Markov chain Monte Carlo without likelihoods","Many stochastic simulation approaches for generating observations from a posterior distribution depend on knowing a likelihood function. However, for many complex probability models, such likelihoods are either impossible or computationally prohibitive to obtain. Here we present a Markov chain Monte Carlo method for generating observations from a posterior distribution without the use of likelihoods. It can also be used in frequentist applications, in particular for maximum-likelihood estimation. The approach is illustrated by an example of ancestral inference in population genetics. A number of open problems are highlighted in the discussion."
522,"theoretical neuroscience computational and mathematical modeling of neural systems",233773,"Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems","{Theoretical neuroscience provides a quantitative basis for describing what nervous systems do, determining how they function, and uncovering the general principles by which they operate. This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory.<br /> <br /> 	The book is divided into three parts. Part I discusses the relationship between sensory stimuli and neural responses, focusing on the representation of information by the spiking activity of neurons. Part II discusses the modeling of neurons and neural circuits on the basis of cellular and synaptic biophysics. Part III analyzes the role of plasticity in development and learning. An appendix covers the mathematical methods used, and exercises are available on the book's Web site.}"
523,"principles of neural science",233775,"Principles of Neural Science","{Now in resplendent color, the new edition continues to define the latest in the scientific understanding of the brain, the nervous system, and human behavior. Each chapter is thoroughly revised and includes the impact of molecular biology in the mechanisms underlying developmental processes and in the pathogenesis of disease. Important features to this edition include a new chapter - Genes and Behavior; a complete updating of development of the nervous system; the genetic basis of neurological and psychiatric disease; cognitive neuroscience of perception, planning, action, motivation and memory; ion channel mechanisms; and much more.}"
524,"prediction of protein function using proteinprotein interaction data",234370,"Prediction of protein function using protein-protein interaction data","Assigning functions to novel proteins is one of the most important problems in the postgenomic era. Several approaches have been applied to this problem, including the analysis of gene expression patterns, phylogenetic profiles, protein fusions, and protein-protein interactions. In this paper, we develop a novel approach that employs the theory of Markov random fields to infer a protein's functions using protein-protein interaction data and the functional annotations of protein's interaction partners. For each function of interest and protein, we predict the probability that the protein has such function using Bayesian approaches. Unlike other available approaches for protein annotation in which a protein has or does not have a function of interest, we give a probability for having the function. This probability indicates how confident we are about the prediction. We employ our method to predict protein functions based on ""biochemical function,"" ""subcellular location,"" and ""cellular role"" for yeast proteins defined in the Yeast Proteome Database (YPD, www.incyte.com), using the protein-protein interaction data from the Munich Information Center for Protein Sequences (MIPS, mips.gsf.de). We show that our approach outperforms other available methods for function prediction based on protein interaction data. The supplementary data is available at www-hto.usc.edu/~msms/ProteinFunction."
525,"the invasion of language emergence change and death",235238,"The invasion of language: emergence, change and death","Research into the emergence and evolution of human language has received unprecedented attention during the past 15 years. Efforts to better understand the processes of language emergence and evolution have proceeded in two main directions: from the top-down (linguists) and from the bottom-up (cognitive scientists). Language can be viewed as an invading process that has had profound impact on the human phenotype at all levels, from the structure of the brain to modes of cultural interaction. In our view, the most effective way to form a connection between the two efforts (essential if theories for language evolution are to reflect the constraints imposed on language by the brain) lies in computational modelling, an approach that enables numerous hypotheses to be explored and tested against objective criteria and which suggest productive paths for empirical researchers to then follow. Here, with the aim of promoting the cross-fertilization of ideas across disciplines, we review some of the recent research that has made use of computational methods in three principal areas of research into language evolution: language emergence, language change, and language death."
526,"network robustness and fragility percolation on random graphs",235618,"Network Robustness and Fragility: Percolation on Random Graphs","Recent work on the Internet, social networks, and the power grid has addressed the resilience of these networks to either random or targeted deletion of network nodes or links. Such deletions include, for example, the failure of Internet routers or power transmission lines. Percolation models on random graphs provide a simple representation of this process but have typically been limited to graphs with Poisson degree distribution at their vertices. Such graphs are quite unlike real-world networks, which often possess power-law or other highly skewed degree distributions. In this paper we study percolation on graphs with completely general degree distribution, giving exact solutions for a variety of cases, including site percolation, bond percolation, and models in which occupation probabilities depend on vertex degree. We discuss the application of our theory to the understanding of network resilience."
527,"lessons learned from real dsl experiments",235798,"Lessons learned from real DSL experiments","Over the years, our group, led by Bob Balzer, designed and implemented three domain-specific languages for use in real applications. Each was invented to ""showcase"" DSL language design and implementation technology that was the focus of our then-current research. Each of these was actually a prototype for what would have taken more time to engineer and polish before putting into practice. Although each effort was essentially successful, none of the languages was ever followed up with the subsequent engineering efforts that we expected or at least hoped for. Herein I elaborate where these language efforts succeeded and where they failed, gleaning lessons for others who take the somewhat risky step of committing to develop a DSL for a particular user community."
528,"modeling bounded rationality",235839,"Modeling Bounded Rationality","{The notion of bounded rationality was initiated in the 1950s by Herbert Simon; only recently has it influenced mainstream economics. In this book, Ariel Rubinstein defines models of bounded rationality as those in which elements of the process of choice are explicitly embedded. The book focuses on the challenges of modeling bounded rationality, rather than on substantial economic implications.<br /> <br /> In the first part of the book, the author considers the modeling of choice. After discussing some psychological findings, he proceeds to the modeling of procedural rationality, knowledge, memory, the choice of what to know, and group decisions.<br /> <br /> In the second part, he discusses the fundamental difficulties of modeling bounded rationality in games. He begins with the modeling of a game with procedural rational players and then surveys repeated games with complexity considerations. He ends with a discussion of computability constraints in games. The final chapter includes a critique by Herbert Simon of the author's methodology and the author's response.<br /> <br /> The Zeuthen Lecture Book series is sponsored by the Institute of Economics at the University of Copenhagen.}"
529,"a survey of decision tree classifier methodology",235888,"A Survey of Decision Tree Classifier Methodology","Decision Tree Classifiers (DTC's) are used successfully in many diverse areas such as radar signal classification, character recognition, remote sensing, medical diagnosis, expert systems, and speech recognition, to name only a few. Perhaps, the most important feature of DTC's is their capability to break down a complex decision-making process into a collection of simpler decisions, thus providing a solution which is often easier to interpret. This paper presents a survey of current methods for DTC designs and the various existing issues. After considering potential advantages of DTC's over single stage classifiers, the subjects of tree structure design, feature selection at each internal node, and decision and search strategies are discussed. Some remarks concerning the relation between decision trees and Neural Networks (NN) are also made."
530,"six degrees the science of a connected age",235931,"Six Degrees: the Science of a Connected Age","{You may be only six degrees away from Kevin Bacon, but would he let you  borrow his car? It depends on the structures within the network that  links you. When the power goes out, when we find that a stranger knows  someone we know, when dot-com stocks soar in price, networks are  evident. In <I>Six Degrees</I>, sociologist Duncan Watts examines  networks like these: what they are, how they're being studied, and what  we can use them for. To illustrate the often complicated mathematics  that describe such structures, Watts uses plenty of examples from  life, without which this book would quickly move beyond a general  science readership. Small chapters make each thought-provoking  conclusion easy to swallow, though some are hard to digest. For  instance, in a short bit on ""coercive externalities,"" Watts sums up  sociological research showing that: <p>  <blockquote>""Conversations concerning politics displayed a consistent  pattern .... On election day, the strongest predictor of electoral  success was not which party an individual privately supported but which  party he or she expected would win.""</blockquote> </p> <I>Six Degrees</I> attempts to help readers understand the new and  exciting field of networks and complexity. While considerably more  demanding than a general book like <I>The Tipping Point</I>,  it offers readers a snapshot of a riveting moment in science, when  understanding things like disease epidemics and the stock market seems  almost within our reach. <I>--Therese Littleton</I>} {The pioneering young scientist whose work on the structure of small worlds has triggered an avalanche of interest in networks.   In this remarkable book, Duncan Watts, one of the principal architects of network theory, sets out to explain the innovative research that he and other scientists are spearheading to create a blueprint of our connected planet. Whether they bind computers, economies, or terrorist organizations, networks are everywhere in the real world, yet only recently have scientists attempted to explain their mysterious workings.  <P>From epidemics of disease to outbreaks of market madness, from people searching for information to firms surviving crisis and change, from the structure of personal relationships to the technological and social choices of entire societies, Watts weaves together a network of discoveries across an array of disciplines to tell the story of an explosive new field of knowledge, the people who are building it, and his own peculiar path in forging this new science. 24 b/w illustrations.}"
531,"small worlds the dynamics of networks between order and randomness",235962,"Small Worlds : The Dynamics of Networks between Order and Randomness","Everyone knows the small-world phenomenon: soon after meeting a stranger, we are surprised to discover that we have a mutual friend, or we are connected through a short chain of acquaintances. In his book, Duncan Watts uses this intriguing phenomenon{\\textendash}colloquially called 'six degrees of separation'{\\textendash}as a prelude to a more general exploration: under what conditions can a small world arise in any kind of network? The networks of this story are everywhere: the brain is a network of neurons; organisations are people networks; the global economy is a network of national economies, which are networks of markets, which are in turn networks of interacting producers and consumers. Food webs, ecosystems, and the Internet can all be represented as networks, as can strategies for solving a problem, topics in a conversation, and even words in a language. Many of these networks, the author claims, will turn out to be small worlds. How do such networks matter? Simply put, local actions can have global consequences, and the relationship between local and global dynamics depends critically on the network's structure. Watts illustrates the subtleties of this relationship using a variety of simple models{\\textemdash}the spread of infectious disease through a structured population; the evolution of cooperation in game theory; the computational capacity of cellular automata; and the sychronisation of coupled phase-oscillators. Watts's novel approach is relevant to many problems that deal with network connectivity and complex systems' behaviour in general: How do diseases (or rumours) spread through social networks? How does cooperation evolve in large groups? How do cascading failures propagate through large power grids, or financial systems? What is the most efficient architecture for an organisation, or for a communications network? This fascinating exploration will be fruitful in a remarkable variety of fields, including physics and mathematics, as well as sociology, economics, and biology."
532,"emergence from chaos to order",235973,"Emergence: From Chaos to Order","{""Emergence"" is the notion that the whole is more than the sum of its parts. John Holland, a MacArthur Fellow known as the ""father of genetic algorithms,"" says this seemingly simple notion will be at the heart of the development of machines that can think for themselves. And while he claims that he'd rather do science than write about it, this is his second scientific philosophy book intended to increase public understanding of difficult concepts (his first was <I>Hidden Order: How Adaptation Builds Complexity</I>). One of the questions that Holland says emergence theory can help answer is: can we build systems from which more comes out than was put in? Think of the food replicators in the imaginary future of <I>Star Trek</I>--with some basic chemical building blocks and simple rules, those machines can produce everything from Klingon delicacies to Earl Grey tea. If scientists can understand and apply the knowledge they gather from studying emergent systems, we may soon witness the development of artificial intelligence, nanotech, biological machines, and other creations heretofore confined to science fiction. Using games, molecules, maps, and scientific theories as examples, Holland outlines how emergence works, emphasizing the interrelationships of simple rules and parts in generating a complex whole. Because of the theoretical depth, this book probably won't appeal to the casual reader of popular science, but those interested in delving a little deeper into the future of science and engineering will be fascinated. Holland's writing, while sometimes self-consciously precise, is clear, and he links his theoretical arguments to examples in the real world whenever possible. Emergence offers insight not just to scientific advancement, but across many areas of human endeavor--business, the arts, even the evolution of society and the generation of new ideas. <i>--Therese Littleton</i> } {'He's the man who taught computers how to have sex. And now, for an encore, he's working on a theory to explain the complexity of life and its myriad manifestations on planet earth.'     New York Times      In this book, one of today's most innovative thinkers, John H. Holland, explains the theory of emergenceDSa simple theory that the whole is greater than the sum of its parts. Emergence demonstrates that a small number of rules or laws can generate incredibly complex systems. From the checkers-playing computer that learnt to beat its creator again and again, to a fertilized egg that can program the development of a trillion-cell organism, to the ant colonies that build bridges over chasms and navigate leaf-boats on streams, this fascinating and groundbreaking book contains wide-ranging implications for science, business, and the arts.     'John Holland is an exceptionally imaginative person. Often surprising, and always engaging, he takes the reader on a journey from simplicity to complexity'     Sir Robert May}"
533,"holistic twig joins optimal xml pattern matching",236029,"Holistic twig joins: optimal xml pattern matching","XML employs a tree-structured data model, and, naturally, XML queries specify patterns of selection predicates on multiple elements related by a tree structure. Finding all occurrences of such a twig pattern in an XML database is a core operation for XML query processing. Prior work has typically decomposed the twig pattern into binary structural (parent-child and ancestor-descendant) relationships, and twig matching is achieved by: (i) using structural join algorithms to match the binary relationships against the XML database, and (ii) stitching together these basic matches. A limitation of this approach for matching twig patterns is that intermediate result sizes can get large, even when the input and output sizes are more manageable.In this paper, we propose a novel holistic twig join algorithm, TwigStack, for matching an XML query twig pattern. Our technique uses a chain of linked stacks to compactly represent partial results to root-to-leaf query paths, which are then composed to obtain matches for the twig pattern. When the twig pattern uses only ancestor-descendant relationships between elements, TwigStack is I/O and CPU optimal among all sequential algorithms that read the entire input: it is linear in the sum of sizes of the input lists and the final result list, but independent of the sizes of intermediate results. We then show how to use (a modification of) B-trees, along with TwigStack, to match query twig patterns in sub-linear time. Finally, we complement our analysis with experimental results on a range of real and synthetic data, and query twig patterns."
534,"probcons probabilistic consistencybased multiple sequence alignment",236100,"ProbCons: Probabilistic consistency-based multiple sequence alignment.","To study gene evolution across a wide range of organisms, biologists need accurate tools for multiple sequence alignment of protein families. Obtaining accurate alignments, however, is a difficult computational problem because of not only the high computational cost but also the lack of proper objective functions for measuring alignment quality. In this paper, we introduce probabilistic consistency, a novel scoring function for multiple sequence comparisons. We present ProbCons, a practical tool for progressive protein multiple sequence alignment based on probabilistic consistency, and evaluate its performance on several standard alignment benchmark data sets. On the BAliBASE, SABmark, and PREFAB benchmark alignment databases, ProbCons achieves statistically significant improvement over other leading methods while maintaining practical speed. ProbCons is publicly available as a Web resource."
535,"the functions of the orbitofrontal cortex",238820,"The functions of the orbitofrontal cortex.","The orbitofrontal cortex contains the secondary taste cortex, in which the reward value of taste is represented. It also contains the secondary and tertiary olfactory cortical areas, in which information about the identity and also about the reward value of odours is represented. The orbitofrontal cortex also receives information about the sight of objects from the temporal lobe cortical visual areas, and neurons in it learn and reverse the visual stimulus to which they respond when the association of the visual stimulus with a primary reinforcing stimulus (such as taste) is reversed. This is an example of stimulus-reinforcement association learning, and is a type of stimulus-stimulus association learning. More generally, the stimulus might be a visual or olfactory stimulus, and the primary (unlearned) positive or negative reinforcer a taste or touch. A somatosensory input is revealed by neurons that respond to the texture of food in the mouth, including a population that responds to the mouth feel of fat. In complementary neuroimaging studies in humans, it is being found that areas of the orbitofrontal cortex are activated by pleasant touch, by painful touch, by taste, by smell, and by more abstract reinforcers such as winning or losing money. Damage to the orbitofrontal cortex can impair the learning and reversal of stimulus-reinforcement associations, and thus the correction of behavioural responses when there are no longer appropriate because previous reinforcement contingencies change. The information which reaches the orbitofrontal cortex for these functions includes information about faces, and damage to the orbitofrontal cortex can impair face (and voice) expression identification. This evidence thus shows that the orbitofrontal cortex is involved in decoding and representing some primary reinforcers such as taste and touch; in learning and reversing associations of visual and other stimuli to these primary reinforcers; and in controlling and correcting reward-related and punishment-related behavior, and thus in emotion. The approach described here is aimed at providing a fundamental understanding of how the orbitofrontal cortex actually functions, and thus in how it is involved in motivational behavior such as feeding and drinking, in emotional behavior, and in social behavior. (C) 2003 Elsevier Inc. All rights reserved."
536,"the relationship between domain duplication and recombination",238866,"The relationship between domain duplication and recombination.","Protein domains represent the basic evolutionary units that form proteins. Domain duplication and shuffling by recombination are probably the most important forces driving protein evolution and hence the complexity of the proteome. While the duplication of whole genes as well as domain-encoding exons increases the abundance of domains in the proteome, domain shuffling increases versatility, i.e. the number of distinct contexts in which a domain can occur. Here, we describe a comprehensive, genome-wide analysis of the relationship between these two processes. We observe a strong and robust correlation between domain versatility and abundance: domains that occur more often also have many different combination partners. This supports the view that domain recombination occurs in a random way. However, we do not observe all the different combinations that are expected from a simple random recombination scenario, and this is due to frequent duplication of specific domain combinations. When we simulate the evolution of the protein repertoire considering stochastic recombination of domains followed by extensive duplication of the combinations, we approximate the observed data well. Our analyses are consistent with a stochastic process that governs domain recombination and thus protein divergence with respect to domains within a polypeptide chain. At the same time, they support a scenario in which domain combinations are formed only once during the evolution of the protein repertoire, and are then duplicated to various extents. The extent of duplication of different combinations varies widely and, in nature, will depend on selection for the domain combination based on its function. Some of the pair-wise domain combinations that are highly duplicated also recur frequently with other partner domains, and thus represent evolutionary units larger than single protein domains, which we term ""supra-domains""."
537,"the yeast coexpression network has a smallworld scalefree architecture and can be explained by a simple model",238884,"The yeast coexpression network has a small-world, scale-free architecture and can be explained by a simple model","We investigated the gene coexpression network in Saccharomyces cerevisiae, in which genes are linked when they are coregulated. This network is shown to have a scale-free, small-world architecture. Such architecture is typical of biological networks in which the nodes are connected when they are involved in the same biological process. Current models for the evolution of intracellular networks do not adequately reproduce the features that we observe in the network. We therefore derive a new model for its evolution based on the observation that there is a positive correlation between the sequence similarity of paralogues and their probability of coexpression or sharing of transcription factor binding sites (TFBSs). The simple, neutralist's model consists of (1) coduplication of genes with their TFBSs, (2) deletion and duplication of individual TFBSs and (3) gene loss. A network is constructed by connecting genes that share multiple TFBSs. Our model reproduces the scale-free, small-world architecture of the coregulation network and the homology relations between coregulated genes without the need for selection either at the level of the network structure or at the level of gene regulation. [Journal Article; In English; England]"
538,"integrative analysis of the cancer transcriptome",238896,"Integrative analysis of the cancer transcriptome.","DNA microarrays have been widely applied to the study of human cancer, delineating myriad molecular subtypes of cancer, many of which are associated with distinct biological underpinnings, disease progression and treatment response. These primary analyses have begun to decipher the molecular heterogeneity of cancer, but integrative analyses that evaluate cancer transcriptome data in the context of other data sources are often capable of extracting deeper biological insight from the data. Here we discuss several such integrative computational and analytical approaches, including meta-analysis, functional enrichment analysis, interactome analysis, transcriptional network analysis and integrative model system analysis."
539,"thermodynamic calculations in biological systems",238904,"Thermodynamic calculations in biological systems","The ability to compute intra- and inter-molecular interactions provides the opportunity to gain a deeper understanding of previously intractable problems in biochemistry and biophysics. This review presents three examples in which molecular dynamics calculations were used to gain insight into the atomic detail underlying important experimental observations. The three examples are the following: (1) Entropic contribution to rate acceleration that results from conformational constraints imposed on the reactants; (2) Mechanism of force unfolding of a small protein molecule by the application of a force that separates its N- and C-terminals; and (3) Loss of translational entropy experienced by small molecules when they bind to proteins."
540,"paradise a framework for evaluating spoken dialogue agents",239493,"PARADISE: a framework for evaluating spoken dialogue agents","This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a general framework for evaluating spoken dialogue agents. The framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normalizing for task complexity."
541,"surfing the p network",239628,"Surfing the p53 network","The p53 tumour-suppressor gene integrates numerous signals that control cell life and death. As when a highly connected node in the Internet breaks down, the disruption of p53 has severe consequences. Tumour-suppressor genes are needed to keep cells under control. Just as a car's brakes regulate its speed, properly functioning tumour-suppressor genes act as brakes to the cycle of cell growth, DNA replication and division into two new cells."
542,"the clustalx windows interface flexible strategies for multiple sequence alignment aided by quality analysis tools",240229,"The {ClustalX} windows interface: flexible strategies for multiple sequence alignment aided by quality analysis tools","CLUSTAL X is a new windows interface for the widely-used progressive multiple sequence alignment program CLUSTAL W, The new system is easy to use, providing an integrated system for performing multiple sequence and profile alignments and analysing the results, CLUSTAL X displays the sequence alignment in a window on the screen, A versatile sequence colouring scheme allows the user to highlight conserved features in the alignment, Pull-down menus provide all the options required for traditional multiple sequence and profile alignment, New features include: the ability to cut-and-paste sequences to change the order of the alignment, selection of a subset of the sequences to be realigned, and selection of a sub-range of the alignment to be realigned and inserted back into the original alignment, Alignment quality analysis can be performed and low-scoring segments or exceptional residues can be highlighted, Quality analysis and realignment of selected residue ranges provide the user with a powerful tool to improve and refine difficult alignments and to trap errors in input sequences, CLUSTAL X has been compiled on SUN Solaris, IRIX5.3 on Silicon Graphics, Digital UNIX on DECstations, Microsoft Windows (32 bit) for PCs, Linux ELF for x86 PCs, and Macintosh PowerMac."
543,"going digital a look at assumptions underlying digital libraries",240516,"Going digital: A look at assumptions underlying digital libraries","What are digital libraries, how should they be designed, how will they be used, and what relationship will they bear to what we now call “libraries”? Although we cannot hope to answer all these crucial questions in this short article, we do hope to encourage, and in some small measure to shape, the dialog among computer scientists, librarians, and other interested parties out of which answers may arise. Our contribution here is to make explicit, and to question, certain assumptions that underlie current digital library efforts. We will argue that current efforts are limited by a largely unexamined and unintended allegiance to an idealized view of what libraries have been, rather than what they actually are or could be. Since these limits come from current ways of thinking about the problem, rather than being inherent in the technology or in social practice, expanding our conception of digital libraries should serve to expand the scope and the utility of development efforts."
544,"automatic resource compilation by analyzing hyperlink structure and associated text",240622,"Automatic resource compilation by analyzing hyperlink structure and associated text","We describe the design, prototyping and evaluation of ARC, a system for automatically compiling a list of authoritative Web resources on any (sufficiently broad) topic. The goal of ARC is to compile resource lists similar to those provided by Yahoo! or Infoseek. The fundamental difference is that these services construct lists either manually or through a combination of human and automated effort, while ARC operates fully automatically. We describe the evaluation of ARC, Yahoo!, and Infoseek resource lists by a panel of human users. This evaluation suggests that the resources found by ARC frequently fare almost as well as, and sometimes better than, lists of resources that are manually compiled or classified into a topic. We also provide examples of ARC resource lists for the reader to examine."
545,"design patterns",240673,"Design Patterns","{<I>Design Patterns</I> is a modern classic in the literature of object-oriented development, offering timeless and elegant solutions to common problems in software design. It describes patterns for managing object creation, composing objects into larger structures, and coordinating control flow between objects. The book provides numerous examples where using composition rather than inheritance can improve the reusability and flexibility of code. Note, though, that it's not a tutorial but a catalog that you can use to find an object-oriented design pattern that's appropriate for the needs of your particular application--a selection for virtuoso programmers who appreciate (or require) consistent, well-engineered object-oriented designs.} {Now on CD, this internationally acclaimed bestseller is more valuable than ever! <P> Use the contents of the CD to create your own design documents and reusable components. The CD contains: 23 patterns you can cut and paste into your own design documents; sample code demonstrating pattern implementation; complete Design Patterns content in standard HTML format, with numerous hyperlinked cross-references; accessed through a standard web browser; Java-based dynamic search mechanism, enhancing online seach capabilities; graphical user environment, allowing ease of navigation. <P> First published in 1995, this landmark work on object-oriented software design presents a catalog of simple and succinct solutions to common design problems. Created by four experienced designers, the 23 patterns contained herein have become an essential resource for anyone developing reusable object-oriented software. In response to reader demand, the complete text and pattern catalog are now available on CD-ROM. This electronic version of <i>Design Patterns</i> enables programmers to install the book directly onto a computer or network for use as an online reference for creating reusable object-oriented software. <P> The authors first describe what patterns are and how they can help you in the design process. They then systematically name, explain, evaluate, and catalog recurring designs in object-oriented systems. All patterns are compiled from real-world examples and include code that demonstrates how they may be implemented in object-oriented programming languages such as C++ and Smalltalk. Readers who already own the book will want the CD to take advantage of its dynamic search mechanism and ready-to-install patterns.}"
546,"efficient crawling through url ordering",240731,"Efficient crawling through URL ordering","In this paper we study in what order a crawler should visit the URLs it has seen, in order to obtain more important pages first. Obtaining important pages rapidly can be very useful when a crawler cannot visit the entire Web in a reasonable amount of time. We define several importance metrics, ordering schemes, and performance evaluation measures for this problem. We also experimentally evaluate the ordering schemes on the Stanford University Web. Our results show that a crawler with a good..."
547,"strong regularities in world wide web surfing",240813,"Strong regularities in World Wide Web surfing","One of the most common modes of accessing information in the World Wide Web is surfing from one document to another along hyperlinks. Several large empirical studies have revealed common patterns of surfing behavior. A model that assumes that users make a sequence of decisions to proceed to another page, continuing as long as the value of the current page exceeds some threshold, yields the probability distribution for the number of pages that a user visits within a given Web site. This model was verified by comparing its predictions with detailed measurements of surfing patterns. The model also explains the observed Zipf-like distributions in page hits observed at Web sites."
548,"link analysis eigenvectors and stability",240878,"Link Analysis, Eigenvectors and Stability","The HITS and the PageRank algorithms are eigenvector methods for identifying “authoritative” or “influential” articles, given hyperlink or citation information. That such algorithms should give consistent answers is surely a desideratum, and in this paper, we address the question of when they can be expected to give stable rankings under small perturbations to the hyperlink patterns. Using tools from matrix perturbation theory and Markov chain theory, we provide conditions under which these methods are stable, and give specific examples of instability when these conditions are violated. We also briefly describe a modification to HITS that improves its stability."
549,"statistical significance for genomewide studies",241030,"Statistical significance for genomewide studies","With the increase in genomewide experiments and the sequencing of multiple genomes, the analysis of large data sets has become commonplace in biology. It is often the case that thousands of features in a genomewide data set are tested against some null hypothesis, where a number of features are expected to be significant. Here we propose an approach to measuring statistical significance in these genomewide studies based on the concept of the false discovery rate. This approach offers a sensible balance between the number of true and false positives that is automatically calibrated and easily interpreted. In doing so, a measure of statistical significance called the q value is associated with each tested feature. The q value is similar to the well known p value, except it is a measure of significance in terms of the false discovery rate rather than the false positive rate. Our approach avoids a flood of false positive results, while offering a more liberal criterion than what has been used in genome scans for linkage."
550,"multiple sequence alignment using partial order graphs",241036,"Multiple sequence alignment using partial order graphs","Motivation: Progressive Multiple Sequence Alignment (MSA) methods depend on reducing an MSA to a linear profile for each alignment step. However, this leads to loss of information needed for accurate alignment, and gap scoring artifacts. Results: We present a graph representation of an MSA that can itself be aligned directly by pairwise dynamic programming, eliminating the need to reduce the MSA to a profile. This enables our algorithm (Partial Order Alignment (POA)) to guarantee that the optimal alignment of each new sequence versus each sequence in the MSA will be considered. Moreover, this algorithm introduces a new edit operator, homologous recombination, important for multidomain sequences. The algorithm has improved speed (linear time complexity) over existing MSA algorithms, enabling construction of massive and complex alignments (e.g. an alignment of 5000 sequences in 4 h on a Pentium II). We demonstrate the utility of this algorithm on a family of multidomain SH2 proteins, and on EST assemblies containing alternative splicing and polymorphism. Availability: The partial order alignment program POA is available at http://www.bioinformatics.ucla.edu/poa. Contact: leec@mbi.ucla.edu"
551,"friction an enthography of global connection",241086,"Friction : An Enthography of Global Connection","{<p>A wheel turns because of its encounter with the surface of the road; spinning in the air it goes nowhere. Rubbing two sticks together produces heat and light; one stick alone is just a stick. In both cases, it is friction that produces movement, action, effect. Challenging the widespread view that globalization invariably signifies a ""clash"" of cultures, anthropologist Anna Tsing here develops friction in its place as a metaphor for the diverse and conflicting social interactions that make up our contemporary world. </p><p>She focuses on one particular ""zone of awkward engagement""--the rainforests of Indonesia--where in the 1980s and the 1990s capitalist interests increasingly reshaped the landscape not so much through corporate design as through awkward chains of legal and illegal entrepreneurs that wrested the land from previous claimants, creating resources for distant markets. In response, environmental movements arose to defend the rainforests and the communities of people who live in them. Not confined to a village, a province, or a nation, the social drama of the Indonesian rainforest includes local and national environmentalists, international science, North American investors, advocates for Brazilian rubber tappers, UN funding agencies, mountaineers, village elders, and urban students, among others--all combining in unpredictable, messy misunderstandings, but misunderstandings that sometimes work out.</p><p>Providing a portfolio of methods to study global interconnections, Tsing shows how curious and creative cultural differences are in the grip of worldly encounter, and how much is overlooked in contemporary theories of the global.</p>}"
552,"improved profile hmm performance by assessment of critical algorithmic features in sam and hmmer",241091,"Improved profile HMM performance by assessment of critical algorithmic features in SAM and HMMER.","BACKGROUND: Profile hidden Markov model (HMM) techniques are among the most powerful methods for protein homology detection. Yet, the critical features for successful modelling are not fully known. In the present work we approached this by using two of the most popular HMM packages: SAM and HMMER. The programs' abilities to build models and score sequences were compared on a SCOP/Pfam based test set. The comparison was done separately for local and global HMM scoring. RESULTS: Using default settings, SAM was overall more sensitive. SAM's model estimation was superior, while HMMER's model scoring was more accurate. Critical features for model building were then analysed by comparing the two packages' algorithmic choices and parameters. The weighting between prior probabilities and multiple alignment counts held the primary explanation why SAM's model building was superior. Our analysis suggests that HMMER gives too much weight to the sequence counts. SAM's emission prior probabilities were also shown to be more sensitive. The relative sequence weighting schemes are different in the two packages but performed equivalently. CONCLUSION: SAM model estimation was more sensitive, while HMMER model scoring was more accurate. By combining the best algorithmic features from both packages the accuracy was substantially improved compared to their default performance."
553,"on clusterings good bad and spectral",241096,"On clusterings - good, bad and spectral","We motivate and develop a new bicriteria measure for assessing the quality of a clustering which avoids the drawbacks of existing measures. A simple recursive heuristic has poly-logarithmic worst-case guarantees under the new measure. The main result of the paper is the analysis of a popular spectral algorithm. One variant of spectral clustering turns out to have eective worst-case guarantees; another nds a \good clustering if it exists.   Supported in part by NSF grant CCR-9820850."
554,"the anthropology of online communities",241241,"THE ANTHROPOLOGY OF ONLINE COMMUNITIES","▪ Abstract  Information and communication technologies based on the Internet have enabled the emergence of new sorts of communities and communicative practices—phenomena worthy of the attention of anthropological researchers. Despite early assessments of the revolutionary nature of the Internet and the enormous transformations it would bring about, the changes have been less dramatic and more embedded in existing practices and power relations of everyday life. This review explores researchers' questions, approaches, and insights within anthropology and some relevant related fields, and it seeks to identify promising new directions for study. The general conclusion is that the technologies comprising the Internet, and all the text and media that exist within it, are in themselves cultural products. Anthropology is thus well suited to the further investigation of these new, and not so new, phenomena."
555,"identification of hundreds of conserved and nonconserved human micrornas",241630,"Identification of hundreds of conserved and nonconserved human microRNAs","MicroRNAs are noncoding RNAs of 22 nucleotides that suppress translation of target genes by binding to their mRNA and thus have a central role in gene regulation in health and disease1, 2, 3, 4, 5. To date, 222 human microRNAs have been identified6, 86 by random cloning and sequencing, 43 by computational approaches and the rest as putative microRNAs homologous to microRNAs in other species. To prove our hypothesis that the total number of microRNAs may be much larger and that several have emerged only in primates, we developed an integrative approach combining bioinformatic predictions with microarray analysis and sequence-directed cloning. Here we report the use of this approach to clone and sequence 89 new human microRNAs (nearly doubling the current number of sequenced human microRNAs), 53 of which are not conserved beyond primates. These findings suggest that the total number of human microRNAs is at least 800."
556,"synthetic biology",241724,"Synthetic biology.","The projects and aims of synthetic biology raise various ethical questions, challenging some of our basic moral concepts. This chapter addresses these issues in three steps. First, we present an overview of different types of ethical issues related to synthetic biology by assigning them to three main categories: method-related, application-related, and distribution-related issues. The first category concerns the procedure and aims of synthetic biology, the second deals with certain planned applications of synthetic biology and the third with questions of distribution and access to procedures and products of this technology. Next, we address a statement often raised in the discussion about ethics of synthetic biology, namely that the ethical issues of synthetic biology have been discussed in previous debates and therefore do not need to be addressed again. We argue that past debates do not render the discussion of ethical issues superfluous because synthetic biology sets these issues in a new context and because the discussion of such issues fulfills in itself an important function, namely by stimulating thought about our relationship to technology and nature. Furthermore, given that synthetic biology’s aims go beyond those of previous technologies, we suggest that it does in fact raise novel ethical issues. Finally, we present opinions of European synthetic biologists on ethical issues in their field. At such an early stage of technological development, synthetic biologists play an important role in the assessment of their discipline, and are best placed to estimate the scientific potential of the field. In an attempt to capture the intuitions of the European synthetic biology community, we have carried out interviews, the results of which we briefly summarize in this last section. By presenting an overview of the various ethical issues and their actual and perceived importance, this chapter aims at providing a first outline for the agenda for an ethics of synthetic biology."
557,"irrational exuberance",241904,"Irrational Exuberance","{CNBC, day trading, the Motley Fool, Silicon Investor--not since the 1920s has there been such an intense fascination with the U.S. stock market. For an increasing number of Americans, logging on to Yahoo! Finance is a habit more precious than that morning cup of joe (as thousands of SBUX and YHOO shareholders know too well). Yet while the market continues to go higher, many of us can't get Alan Greenspan's famous line out of our heads. In <I>Irrational Exuberance</I>, Yale economics professor Robert J. Shiller examines this public fascination with stocks and sees a combination of factors that have driven stocks higher, including the rise of the Internet, 401(k) plans, increased coverage by the popular media of financial news, overly optimistic cheerleading by analysts and other pundits, the decline of inflation, and the rise of the mutual fund industry. He writes: ""Perceived long-term risk is down.... Emotions and heightened attention to the market create a desire to get into the game. Such is irrational exuberance today in the United States.""<p> By history's yardstick, Shiller believes this market is grossly overvalued, and the factors that have conspired to create and amplify this event--the baby-boom effect, the public infatuation with the Internet, and media interest--will most certainly abate. He fears that too many individuals and institutions have come to view stocks as their only investment vehicle, and that investors should consider looking beyond stocks as a way to diversify and hedge against the inevitable downturn. This is a serious and well-researched book that should read like a Stephen King novel to anyone who has staked his or her future on the market's continued success. --<I>Harry C. Edwards</I> } {In this timely and prescient update of his celebrated 2000 bestseller, Robert Shiller returns to the topic that gained him international fame: market volatility. Having predicted the stock market collapse that began just one month after the first edition was published, he now expands the book to cover other markets that have become volatile, particularly the recently red-hot housing market. He includes a full chapter on domestic and international housing prices in historical perspective.<br><br>Shiller amasses impressive evidence to support his argument that the recent housing market boom bears many similarities to the stock market bubble of the late 1990s, and may eventually be followed by declining home prices for years to come. After stocks plummeted when the bubble burst in 2000, investors moved their money into housing. This precipitated the inflated real estate prices not only in America but around the world, Shiller maintains. Hence, irrational exuberance did not disappear&#8212;it merely reappeared in other settings.<br><br>Building on the original edition, Shiller draws out the psychological origins of volatility in financial markets, this time folding real estate into his analysis. He broadens the evidence that investing in capital markets of all kinds in the modern free-market economy is inherently unstable&#8212;subject to the profoundly human influences captured in Alan Greenspan&#8217;s now-famous phrase, &#8220;irrational exuberance.&#8221; As was true of its predecessor, the second edition of <i>Irrational Exuberance</i> is destined to be widely read, discussed, and debated.} {In this bold and potentially urgent volume, Robert J. Shiller, a respected expert on market volatility, offers an unconventional interpretation of recent U.S. stock market highs and shows that Alan Greenspan's term ""irrational exuberance"" is a good description of the mood behind the market. He warns that poorer performance may be in the offing and tells us how we--as a country and individually--can respond.<p>Shiller credits an unprecedented confluence of events with driving stocks to uncharted heights. He analyzes the structural and psychological factors that explain why the Dow Jones Industrial Average tripled between 1994 and 1999, a level of growth not reflected in any other sector of the economy. In contrast to many analysts, Shiller stresses circumstances that alter investors' perceptions of the market.}"
558,"predicting functional gene links from phylogeneticstatistical analyses of whole genomes",244746,"Predicting Functional Gene Links from Phylogenetic-Statistical Analyses of Whole Genomes","An important element of the developing field of proteomics is to understand protein-protein interactions and other functional links amongst genes. Across-species correlation methods for detecting functional links work on the premise that functionally linked proteins will tend to show a common pattern of presence and absence across a range of genomes. We describe a maximum likelihood statistical model for predicting functional gene linkages. The method detects independent instances of the correlated gain or loss of pairs of proteins on phylogenetic trees, reducing the high rates of false positives observed in conventional across-species methods that do not explicitly incorporate a phylogeny. We show, in a dataset of 10,551 protein pairs, that the phylogenetic method improves by up to 35\% on across-species analyses at identifying known functionally linked proteins. The method shows that protein pairs with at least two to three correlated events of gain or loss are almost certainly functionally linked. Contingent evolution, in which one gene's presence or absence depends upon the presence of another, can also be detected phylogenetically, and may identify genes whose functional significance depends upon its interaction with other genes. Incorporating phylogenetic information improves the prediction of functional linkages. The improvement derives from having a lower rate of false positives and from detecting trends that across-species analyses miss. Phylogenetic methods can easily be incorporated into the screening of large-scale bioinformatics datasets to identify sets of protein links and to characterise gene networks."
559,"image quilting for texture synthesis and transfer",244765,"Image Quilting for Texture Synthesis and Transfer","We present a simple image-based method of generating novel visual appearance in which a new image is synthesized by stitching together small patches of existing images. We call this process  image quilting . First, we use quilting as a fast and very simple texture synthesis algorithm which produces surprisingly good results for a wide range of textures. Second, we extend the algorithm to perform texture transfer &mdash; rendering an object with a texture taken from a different object. More generally, we demonstrate how an image can be re-rendered in the style of a different image. The method works directly on the images and does not require 3D information."
560,"the mathematics of learning dealing with data",245758,"The mathematics of learning: dealing with data"," In statistical learning for regression problems, data is assumed to be an i.i.d. sample drawn from an unknown and arbitrary probability distribution $P$ over $\Bbb{R}^d\times\Bbb{R}$.  Given such a sample, the goal of the learner is to choose a low-risk function $h\colon \Bbb{R}^d\to\Bbb{R}$ from a fixed hypothesis space $\scr{H}$, that is, a function $h\in\scr{H}$ such that the expected value of $ l(h(X),Y)$, where $ l$ is a fixed real-valued loss function and the pair $(X,Y)\in\Bbb{R}^d\times\Bbb{R}$ is drawn from $P$, is as close as possible to its minimum value in $\scr{H}$. <P> In this paper, the authors provide an excellent survey of some recent results concerning the statistical performance of regularized learning algorithms.  In particular, the authors describe the regularized least-squares regressor with Gaussian kernels.  The hypothesis space associated to this algorithm is the family of hyperplanes in the reproducing kernel Hilbert space generated by the Gaussian kernel with a given variance parameter. This parameter plays a crucial role in balancing the approximation error term (the $L_2$ distance to the Bayes optimal regressor) and the risk term---recall that the sum of these two terms equals the expected square loss of the regressor. The analysis of the risk and the approximation error, reported in the paper, captures this tradeoff, opening the way to a principled choice of the kernel parameter. <P> The regularized learning algorithm can be naturally applied to problems other than regression. In particular, the authors show an application to classification problems describing the regularized least-squares algorithm, whose empirical performance is compared to that of support vector machines."
561,"a bayesian regression approach to the inference of regulatory networks from gene expression data",246028,"A Bayesian regression approach to the inference of regulatory networks from gene expression data","Motivation: There is currently much interest in reverse-engineering regulatory relationships between genes from microarray expression data. We propose a new algorithmic method for inferring such interactions between genes using data from gene knockout experiments. The algorithm we use is the Sparse Bayesian regression algorithm of Tipping and Faul. This method is highly suited to this problem as it does not require the data to be discretized, overcomes the need for an explicit topology search and, most importantly, requires no heuristic thresholding of the discovered connections.  Results: Using simulated expression data, we are able to show that this algorithm outperforms a recently published correlation-based approach. Crucially, it does this without the need to set any ad hoc threshold on possible connections.  Availability: Matlab code which allows all experimental results to be reproduced is available at http://www.dcs.gla.ac.uk/~srogers/reg_nets.html  Contact: srogers@dcs.gla.ac.uk  Supplementary information: Appendices and supplementary figures mentioned in the text can be found at http://www.dcs.gla.ac.uk/~srogers/reg_nets.html 10.1093/bioinformatics/bti487"
562,"elephants dont play chess",248185,"Elephants Don't Play Chess","Engineering and Computer Science at M.I.T. and a member of the Artificial Intelligence Laboratory where he leads the mobile robot group. He has authored two books, numerous scientific papers, and is the editor of the International Journal of Computer Vision. There is an alternative route to Artificial Intelligence that diverges from the directions pursued under that banner for the last thirty some years. The traditional approach has emphasized the abstract manipulation of symbols, whose grounding, in physical reality has. rarely been achieved. We explore a research methodology which emphasizes ongoing physical interaction with the environment as the primary source of constraint on the design of intelligent systems. We show how this methodology has recently had significant successes on a par with the most successful classical efforts. We outline plausible future work along these lines which can lead to vastly more ambitious systems. 1."
563,"blogs and wikis environments for online collaboration",248994,"Blogs and Wikis: Environments for On-line Collaboration","Language professionals have embraced the world of collaborative opportunities the Internet hasintroduced. Many tools -- e-mail, discussion forums, chat -- are by now familiar to many languageteachers. Recent innovations -- blogs, wikis, and RSS feeds -- may be less familiar but offer powerfulopportunities for online collaboration for both language professionals and learners. The underlyingtechnology of the new tools is XML (""extensible markup language"") which separates content fromformatting, encourages use of meta-data, and enables machine processing of Internet documents. Thelatter is key in the ability to link automatically disparate documents of interest to individuals or groups.The new collaborative opportunities this enables have led some to consider the growing importance ofXML as the signal of the arrival of the second-generation Web. (First paragraph)"
564,"orthomcl identification of ortholog groups for eukaryotic genomes",249006,"OrthoMCL: Identification of Ortholog Groups for Eukaryotic Genomes","The identification of orthologous groups is useful for genome annotation, studies on gene/protein evolution, comparative genomics, and the identification of taxonomically restricted sequences. Methods successfully exploited for prokaryotic genome analysis have proved difficult to apply to eukaryotes, however, as larger genomes may contain multiple paralogous genes, and sequence information is often incomplete. OrthoMCL provides a scalable method for constructing orthologous groups across multiple eukaryotic taxa, using a Markov Cluster algorithm to group (putative) orthologs and paralogs. This method performs similarly to the INPARANOID algorithm when applied to two genomes, but can be extended to cluster orthologs from multiple species. OrthoMCL clusters are coherent with groups identified by EGO, but improved recognition of ""recent"" paralogs permits overlapping EGO groups representing the same gene to be merged. Comparison with previously assigned EC annotations suggests a high degree of reliability, implying utility for automated eukaryotic genome annotation. OrthoMCL has been applied to the proteome data set from seven publicly available genomes (human, fly, worm, yeast, Arabidopsis, the malaria parasite Plasmodium falciparum, and Escherichia coli). A Web interface allows queries based on individual genes or user-defined phylogenetic patterns (http://www.cbil.upenn.edu/gene-family). Analysis of clusters incorporating P. falciparum genes identifies numerous enzymes that were incompletely annotated in first-pass annotation of the parasite genome."
565,"inference in belief networks a procedural guide",249102,"Inference in belief networks: A procedural guide","Belief networks are popular tools for encoding uncertainty in expert systems. These networks rely on inference algorithms to compute beliefs in the context of observed evidence. One established method for exact inference on belief networks is the Probability Propagation in Trees of Clusters (PPTC) algorithm, as developed by Lauritzen and Spiegelhalter and refined by Jensen et al. PPTC converts the belief network into a secondary structure, then computes probabilities by manipulating the secondary structure. In this document, we provide a self-contained, procedural guide to understanding and implementing PPTC. We synthesize various optimizations to PPTC that are scattered throughout the literature. We articulate undocumented, ""open secrets"" that are vital to that are vital to producing a robust and efficient implementation of PPTC. We hope that this document makes probabilistic inference more accessible and affordable to those without extensive prior exposure."
566,"mathematical and computational techniques to deduce complex biochemical reaction mechanisms",251558,"Mathematical and computational techniques to deduce complex biochemical reaction mechanisms.","Time series data can now be routinely collected for biochemical reaction pathways, and recently, several methods have been proposed to infer reaction mechanisms for metabolic pathways and networks. In this paper we provide a survey of mathematical techniques for determining reaction mechanisms for time series data on the concentration or abundance of different reacting components, with little prior information about the pathways involved."
567,"augmenting museums and art galleries",252074,"Augmenting Museums and Art Galleries","This paper is concerned with the application of context-aware computing to museums and art galleries. The paper reports three studies addressing this issue. The first study involved a survey of visitors to an art gallery and focused on visitor activity and information requirements. This led to the conclusion that one could define visitor types and relate these to the amount, and level of detail, of information for the visitor types. The second study involves a comparative evaluation of..."
568,"a fast level set method for propagating interfaces",254471,"A Fast Level Set Method for Propagating Interfaces","A method is introduced to decrease the computational labor of the standard level set method for propagating interfaces. The fast approach uses only points close to the curve at every time step. We describe this new algorithm and compare its efficiency and accuracy with the standard level set approach. 1 A Fast Level Set Implementation The level set technique was introduced in [9] to track moving interfaces in a wide variety of problems. It relies on the relation between propagating interfaces..."
569,"image processing analysis and machine vision",254902,"Image processing, analysis, and machine vision","{This robust text provides deep and wide coverage of the full range of topics encountered in the field of image processing and machine vision.  As a result, it can serve undergraduates, graduates, researchers, and professionals looking for a readable reference. The book's encyclopedic coverage of topics is wider than that found in any competing book, and it can be used in more than one course (both image processing and machine vision classes).  In addition, while advanced mathematics is not needed to understand basic concepts (making this a good choice for undergraduates), rigorous mathematical coverage is included for more advanced readers. This text is especially strong and up-to-date in its treatment of 3D vision, with many topics not covered at all in competing books. It is also distinguished by its easy-to-understand algorithm descriptions of difficult concepts, and a wealth of carefully selected problems and examples that can be worked with any general-purpose image processing software package or programming environment.}"
570,"foundations of human sociality economic experiments and ethnographic evidence from fifteen smallscale societies",255314,"Foundations of Human Sociality: Economic Experiments and Ethnographic Evidence from Fifteen Small-Scale Societies","What motives underlie the ways humans interact socially? Are these the same for all societies? Are these part of our nature, or influenced by our environments? Over the last decade, research in experimental economics has emphatically falsified the textbook representation of Homo economicus. Hundreds of experiments suggest that people care not only about their own material payoffs, but also about such things as fairness, equity, and reciprocity. However, this research left fundamental questions unanswered: Are such social preferences stable components of human nature, or are they modulated by economic, social, and cultural environments? Until now, experimental research could not address this question because virtually all subjects had been university students. Combining ethnographic and experimental approaches to fill this gap, this book breaks new ground in reporting the results of a large cross-cultural study aimed at determining the sources of social (non-selfish) preferences that underlie the diversity of human sociality. In this study, the same experiments carried out with university students were performed in fifteen small-scale societies exhibiting a wide variety of social, economic, and cultural conditions. The results show that the variation in behaviour is far greater than previously thought, and that the differences between societies in market integration and the importance of cooperation explain a substantial portion of this variation, which individual-level economic and demographic variables could not. The results also trace the extent to which experimental play mirrors patterns of interaction found in everyday life. The book includes a succinct but substantive introduction to the use of game theory as an analytical tool, and to its use in the social sciences for the rigorous testing of hypotheses about fundamental aspects of social behaviour outside artificially constructed laboratories. The editors also summarize the results of the fifteen case studies in a suggestive chapter about the scope of the project."
571,"the earth movers distance as a metric for image retrieval",255551,"The Earth Mover's Distance as a Metric for Image Retrieval","We investigate the properties of a metric between two distributions, the  Earth Mover's Distance  (EMD), for content-based image retrieval. The EMD is based on the minimal cost that must be paid to transform one distribution into the other, in a precise sense, and was first proposed for certain vision problems by Peleg, Werman, and Rom. For image retrieval, we combine this idea with a representation scheme for distributions that is based on vector quantization. This combination leads to an image comparison framework that often accounts for perceptual similarity better than other previously proposed methods. The EMD is based on a solution to the transportation problem from linear optimization, for which efficient algorithms are available, and also allows naturally for partial matching. It is more robust than histogram matching techniques, in that it can operate on variable-length representations of the distributions that avoid quantization and other binning problems typical of histograms. When used to compare distributions with the same overall mass, the EMD is a true metric. In this paper we focus on applications to color and texture, and we compare the retrieval performance of the EMD with that of other distances."
572,"time clocks and the ordering of events in a distributed system",256404,"Time, Clocks, and the Ordering of events in a Distributed System","The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become. Key Words and Phrases: distributed systems, computer networks, clock synchronization, multiprocess systems CR Categories: 4.32, 5.29"
573,"innate immune recognition",256918,"Innate Immune Recognition","▪ Abstract  The innate immune system is a universal and ancient form of host defense against infection. Innate immune recognition relies on a limited number of germline-encoded receptors. These receptors evolved to recognize conserved products of microbial metabolism produced by microbial pathogens, but not by the host. Recognition of these molecular structures allows the immune system to distinguish infectious nonself from noninfectious self. Toll-like receptors play a major role in pathogen recognition and initiation of inflammatory and immune responses. Stimulation of Toll-like receptors by microbial products leads to the activation of signaling pathways that result in the induction of antimicrobial genes and inflammatory cytokines. In addition, stimulation of Toll-like receptors triggers dendritic cell maturation and results in the induction of costimulatory molecules and increased antigen-presenting capacity. Thus, microbial recognition by Toll-like receptors helps to direct adaptive immune responses to antigens derived from microbial pathogens."
574,"imagined communities reflections on the origin and spread of nationalism",257982,"Imagined Communities: Reflections on the Origin and Spread of Nationalism","(From intro) In second appendix to 2nd edition, recognizes that had taken as facile irony what really strange: why new nations had imagined themselves as old... decides that lo que realmente importa es la alineacion estructural de la 'memoria' nacionalista posterior a 1820 con las premisas y convenciones internas de la biografia y la autobiografias modernas... 187: ""New"" London not as successor of ""Old"" London - as in places with ""New"" in SE Asia - but in sibling competition rather than inheritance... ...that new sychronic novelty only posible when groups of people could imagine themselves living in parallel to other groups of people: - from 1500-1800, una acumulacion de inovaciones tecnicas en los campos de la construccion de barcos, la navegacion, la relojeria y la cartografia, mediadas todas ellas por el capitalismo de imprenta... - necessary for distance between parallel groups to be large and newer to be substantial in size and settlement, while subordinated to older 1. Atlantic Ocean made gradual absorption of populations into larger units impossible (Scotland > UK, Las Espanas > Espana) 2. massive European migration to Americas by late eighteenth century: immigrant community maintained own cultural coherence and political ascendancy because of size and military/economy/technological power 3. metropole had military/bureaucratic resources to impose will for centuries on creoles > contrast to a) Chinese - e.g. early 15th century, voyages of Cheng-ho to enforce court monopoly of external trade with SE Asia against private merchants, but failed by mid-century, when Ming abandoned and tried to prevent any emigration from Middle Kingdom b) Arab migrations into SE Asia and E Africa: but lost ties with homeland > neither Arabs nor Chinese, though ventured overseas in large numbers, established coherent, wealthy, selfconsciously creole communities subordinated to great metropolitan core... Helps to explain why nationalism emerged first in New World rather than Old... ...and also features of 1776-1825 revolutionary wars: 1. creole revolutionaries not wanting to overthrow Old London, keeping empire intact, but shift metropole to New London: safeguard their continuing parallelism... 2. stakes were rather low for creoles: not fearing exterminatoin or reduction to servitude, because whites, Christians, Spanish/English-speakers, and economic intermediaries essential to empire > wars between kinsmen - and close ties maintained after period of acrimony Series of events that gave novelty of parallel existence new meaning: beginning with Declaration of Independence and its successful military defense... > 1811 - Venezuelan revolutionaries borrow verbatim: not seen as something North American but as universal truth and value ...and paralelled in Old world by French Revolution 268: difficult for us to imagine that idea of nation was then considered as something completely ""new"" (not just new nations) - no ""historical"" justification in US Constitution and French rev calendar: ""nation"" was itself ""revolutionary"" idea Declaration of Indepenence: no refernece to Columbus, et al. nor any historical grounds of antiquity of American people, etc.: American nation not even mentioned > French Revolutoin 1793: abolition of Christian calendar > new naming of years > Peru 1821: San Martin decrees that aborigines no longer to be called Indians but Peruvians > again, new system of naming Sense of rupture -  but not lasted long - for reasons that precipitated rupture in first place - last quarter of eighteenth century, Britain manufacturing massive amount of watches - serially published newspapers familiar to urban civilization - novel: with possiblities for representing simultaneous actions in empty homogeneous time > cosmic clocking which had made intellegible synrchronic transatlantic pairing felt to entail... ...wholly intramundane serial view of social causality >> less than two decades after proclamation of Year One- first academic chairs in History (1810 Berlin/1812 Sorbonne) >>> Year One makes way for 1792 AD [cf footnote: no longer Anno Domini proper] and revolutionary ruptures are embedded in historical series - and thus historical precedents and models Thus - for - ""second-generation"" nationalist movements (Europe 1815-1850 and American second-generation): no longer possible to recapture first sense of rupture > begin reading genealogically as historical traditoin of serial continuity > new nationalisms almost immediately began imagining as ""wakening from sleep"" - wholly foreign to Americas >> 1803 - Greek nationalist Koraes: ""Por vez primera la nacion [griega] contempla el horrible espectaculo de su ignorancia y tiembla al medir con los ojos la distancia que la separa de la gloria de sus antepasados"", then goes on to analyze: ""Por vez primera"" aun hace eco a las rupturas de 1776 a 1789, pero las dulces miradas de Koraes no se dirigen al futuro de San Martin [Indians > Peruvians], sino al pasado, temblando, a las glorias pasadas. Esta doblez exaltante no tardaria en desvanecerse, reemplaza por un modular despertar ""continuo"" de un sueno cronologicamente calculado, al estilo A. D.: un retorno garantizado a la esencia primigenia.... >>> and this doubleness of ""For the first time"" together with ""ancestors"" itself fades into modular, ""continuous"" awakening from a chronologically guaged, A.D.-style slumber: a guaranteed return to an aboriginal essence [once nations take revolutions as precedentes historicos - and in parallel to other revolutions - then logical next step was to keep going back: these intellectuals begin to ""discover"" nations that had ""forgotten"" rather than declaring as ""revolutionary"": different way of legitimizing, although  I'm not sure that necessarily LOGICAL next step from nations as ""revolutionary"" in name of just sovereignty] ...In Europe: 1. seemed to explain why nationalist movements appeared in civilized Old World before barbarous New World > awakening stimulated from afar... 2. provided link between languages and nationalisms... ...literate Europeans had thought of languages such as French/Spanish/English/German as languages of civilization, rather than associated with territorially-defined group > but emergence of print vernacular languages - like Atlantic Ocean, separating subjugated national communities from ancient dynastic realms: but this seemed strange at time, needing explanation - bourgeois turn to study language together with folklore, music, etc. [writing for - and reading together with - quite specific reading community: probably together in package with holding property, etc. as good respectable bourgeois qualifications] ...In America: similarly, nationalism had become inheritance by 1830's - but language shared with metropole (although attempts to define American English and Guarani as Paraguay language, etc.) ...creoles institutoinally committed (via schools, print media, administrative habits) to European languages Solution in both New and Old Worlds - >History - or rather History emplotted in particular ways >> White: Ranke, Michelet, Tocqueville, Marx, and Burckhardt all born within quarter century of Year One [NB White distinguishes between mode of emplotment of each] e.g. Michelet, self-appointed historian of the Revolution: first to write self-consciously on behalf of dead: Jamais dans ma carriere je n'ai pas perdu de vue ce devoir de l'historien. J'ai donne a beaucoup de morts trop oublies l'assistance dont moi-meme j'aurai besoin [NB]. Je les ai exhumes pour una seconde vie... Ils vivent maintenant avec nous qui nous sentons leurs parents, leurs amis. Ainsi se fait una famille, una cite commune entre les vivants et les morts... [community imagined as families and cities - geneaology and architecture]... Anderson notes that for Michelet, these dead were those who had contributed to the struggle... but had often not been aware of their role: re-awaking... [cf. Mormons] > not just random assemblage of anonymous dead: those whose sacrifices made possible rupture of 1789 and emergence of French nation - even when not understood as such by victims [cf. Davis] [Whiggish view of history: that rise of liberal nation-state was relentless unfolding of natural state in history] ...also in New World: Mexicans speaking in Spanish for pre-Columbian Indian civilizations whose language did not understand * Fermin de Vargas - need to extinguish [through biological mestizaje] living Indians vs. grandchildren: need to remember and speak for them ...precisely because by then often had been extinguished [and continues simultaneously - remember too that past-ing involved in ""recordar"" as much in ""olvidar""] e.g. Renan in 1882: not just need to remember and speak for victims in struggle of awakening - also need to ""forget"" fratricidal conflicts Or, l'essence d'une nation est que tous les individus aient beacoup de choses en comun et aussi que tous aient oublie bien des choses... Tout citoyen francais doit avoir oublie Saint-Barthelemy, les masacres du Midi au XIIIe siecle... * No need to explain what either was - French readers assumed to ""remember""... Saint-Barthelemy: anti-Huguenot pogrom launched 1572 by Valois dynast Charles IX and Florentine Mother les masacres du Midi: exterminatoin of Alibigensians in zone between Pyrenees and Southern Alps by Pope Innocent III > systematic historiographical campaign, deployed by state mainly through school system, to ""remind"" young Frenches of slaughters now inscribed as ""family history"": becomes characteristic device in later genealogies: Tener que ""haber olvidado ya"" unas tragedias que nos tienen que ""recordar"" incesentamente es un recurso caracteristico en la construccion ulterior de las genealogias nacionales... > ""remind"" readers that forgetting such tragedies was uno de los primeros deberes civicos contemporaneos (Resulta instructivo que Renan no diga que cada ciudadano frances ""debe haber olvidado"" la comuna de Paris. En 1882, su recuerdo aun real y no mitico, y lo bastante doloroso para que se dificultara leerlo bajo el signo de ""tranquilizadoramente fratricida"".) Similarly, vast pedagogical industry obliges: Americans to remember/forget hostilites of 1861-65 as fratricidal struggle [darkest hour - but in history of nation] English to remember Founding Father called William the Conqueror: Norman William and Saxon Harold meet at Hastings, if not as dancing partners, at least as brothers [cf. also Mary Queen of Scots - darkest hour] Not just icy calculations of state functionaries: deep reshaping of imagination of which state barely conscious... e.g. although in 1930's youths of many nationalities fought in Iberian peninsula, but Franco's Valley of the Fallen only celebrates those who triumphed against Bolshevism and atheism... ...but at state's margins, emergent ""memory"" of ""Spanish"" civil war - becomes official after 1975 e.g. class war fought 1918-1920 becomes our civil war in Soviet film and fiction, although state holds to orthodox Marxist reading e.g. Americas - although states were weak pedagogically and societies divided between ""whites"", ""blacks"", and ""natives"", still imagining of fraternity appears early - with some authentic popularity > 1840 James Fenmore Cooper ""The Pathfinder"" - love between white woodsman and noble Delaware chieftan - published in middle of war against Seminole Indians of Florida - but set in late British imperial rule > ""Americans"" fighting against French and their native allies, as well as against treacherous agents of George III > 1851 Herman Melville ""Moby Dick"" - Ishmael and Queequeg, depicted as George Washington, in bed together > 1881 Mark Twain ""Huckleberry Finn"" - picture of black Jim and white Huck as American ""brothers"" - again set in antebellum South in which black is a slave... >> Consciousness that arose - in midst of violent racial, class, and regional antagonisms, when no longer possible to experience nation as new at wave-top of moment of rupture... [Anderson's punto de partida was precisely why people die for their country as for their family] 285-86: La conciencia de estar formando parte de un tiempo secular, serial con todo lo que esto implica de continuidad, y sin embargo de ""olvidar"" [better ""haber olvidado""?] la experiencia de esta continuidad - producto de las rupturas de finales del siglo XVIII - da lugar a la necesidad de una narracion de ""identidad"". La tarea esta lista para el magistrado de Michelet. [i.e. this ""continuity"" was itself product of 18th century ruptures giving rise to nueva forma de conciencia includes notion of time as ""continuous"": by 1820, this new ""continuity"" had to be ""restored"" (or ""rediscovered"") as ""forgotten"", and then by 1882, urge to ""forget"" other episodes that are thus assumed to linger in ""memory""] ...but nations differ from persons in that no natural birth or death [emphasizes biografia moderna as otherwise fundamental], instead search for primordial origins (al hombre de Pekin, al hombre de Java, al rey Arturo)... ...and contrasts Braudel's anonymous deaths - defined within tasa de mortalidad in book that studiously ignores S. Bartolome's massacre - with suicidios ejemplares, martirios conmovedores, asesinatos, ejecuciones, guerras y holocaustos... and notes to conclude that estas muertes violentas deben ser olvidadas/recordadas como ""nuestras"" (286)...."
575,"swoogle a search and metadata engine for the semantic web",258855,"Swoogle: a search and metadata engine for the semantic web","Swoogle is a crawler-based indexing and retrieval system for the Semantic Web. It extracts metadata for each discovered document, and computes relations between documents. Discovered documents are also indexed by an information retrieval system which can use either character N-Gram or URIrefs as keywords to find relevant documents and to compute the similarity among a set of documents. One of the interesting properties we compute is <i>ontology rank</i>, a measure of the importance of a Semantic Web document."
576,"provat a tool for voronoi tessellation analysis of protein structures and complexes",259020,"PROVAT: a tool for Voronoi tessellation analysis of protein structures and complexes","10.1093/bioinformatics/bti523 Summary: Voronoi tessellation has proved to be a useful tool in protein structure analysis. We have developed PROVAT, a versatile public domain software that enables computation and visualization of Voronoi tessellations of proteins and protein complexes. It is a set of Python scripts that integrate freely available specialized software (Qhull, Pymol etc.) into a pipeline. The calculation component of the tool computes Voronoi tessellation of a given protein system in a way described by a user-supplied XML recipe and stores resulting neighbourhood information as text files with various styles. The Python pickle file generated in the process is used by the visualization component, a Pymol plug-in, that offers a GUI to explore the tessellation visually.Availability: PROVAT source code can be downloaded from http://raven.bioc.cam.ac.uk/~swanand/Provat1, which also provides a webserver for its calculation component, documentation and examples.Contact: swanand@cryst.bioc.cam.ac.uk"
577,"computational cluster validation in postgenomic data analysis",259037,"Computational cluster validation in post-genomic data analysis","Motivation: The discovery of novel biological knowledge from the ab initio analysis of post-genomic data relies upon the use of unsupervised processing methods, in particular clustering techniques. Much recent research in bioinformatics has therefore been focused on the transfer of clustering methods introduced in other scientific fields and on the development of novel algorithms specifically designed to tackle the challenges posed by post-genomic data. The partitions returned by a clustering algorithm are commonly validated using visual inspection and concordance with prior biological knowledge--whether the clusters actually correspond to the real structure in the data is somewhat less frequently considered. Suitable computational cluster validation techniques are available in the general data-mining literature, but have been given only a fraction of the same attention in bioinformatics. Results: This review paper aims to familiarize the reader with the battery of techniques available for the validation of clustering results, with a particular focus on their application to post-genomic data analysis. Synthetic and real biological datasets are used to demonstrate the benefits, and also some of the perils, of analytical clustervalidation. Availability: The software used in the experiments is available at http://dbkweb.ch.umist.ac.uk/handl/clustervalidation/ Contact: J.Handl@postgrad.manchester.ac.uk Supplementary information: Enlarged colour plots are provided in the Supplementary Material, which is available at http://dbkweb.ch.umist.ac.uk/handl/clustervalidation/"
578,"pocketlens toward a personal recommender system",260118,"PocketLens: Toward a personal recommender system","Recommender systems using collaborative filtering are a popular technique for reducing information overload and finding products to purchase. One limitation of current recommenders is that they are not portable. They can only run on large computers connected to the Internet. A second limitation is that they require the user to trust the owner of the recommender with personal preference data. Personal recommenders hold the promise of delivering high quality recommendations on palmtop computers, even when disconnected from the Internet. Further, they can protect the user's privacy by storing personal information locally, or by sharing it in encrypted form. In this article we present the new PocketLens collaborative filtering algorithm along with five peer-to-peer architectures for finding neighbors. We evaluate the architectures and algorithms in a series of offline experiments. These experiments show that Pocketlens can run on connected servers, on usually connected workstations, or on occasionally connected portable devices, and produce recommendations that are as good as the best published algorithms to date."
579,"generative communication in linda",260693,"Generative Communication in {L}inda","Generative communication is the basis of a new distributed programming langauge that is intended for systems programming in distributed settings generally and on integrated network computers in particular. It differs from previous interprocess communication models in specifying that messages be added in tuple-structured form to the computation environment, where they exist as named, independent entities until some process chooses to receive them. Generative communication results in a number of distinguishing properties in the new language, Linda, that is built around it. Linda is fully distributed in space and distributed in time; it allows distributed sharing, continuation passing, and structured naming. We discuss these properties and their implications, then give a series of examples. Linda presents novel implementation problems that we discuss in Part II. We are particularly concerned with implementation of the dynamic global name space that the generative communication model requires."
580,"widearea cooperative storage with cfs",260715,"Wide-area cooperative storage with {CFS}","The Cooperative File System (CFS) is a new peer-to-peer readonly storage system that provides provable guarantees for the efficiency, robustness, and load-balance of file storage and retrieval. CFS does this with a completely decentralized architecture that can scale to large systems. CFS servers provide a distributed hash table (DHash) for block storage. CFS clients interpret DHash blocks as a file system. DHash distributes and caches blocks at a fine granularity to achieve load balance, uses replication for robustness, and decreases latency with server selection. DHash finds blocks using the Chord location protocol, which operates in time logarithmic in the number of servers.  CFS is implemented using the SFS file system toolkit and runs on Linux, OpenBSD, and FreeBSD. Experience on a globally deployed prototype shows that CFS delivers data to clients as fast as FTP. Controlled tests show that CFS is scalable: with 4,096 servers, looking up a block of data involves contacting only seven servers. The tests also demonstrate nearly perfect robustness and unimpaired performance even when as many as half the servers fail.  1."
581,"farsite federated available and reliable storage for an incompletely trusted environment",260716,"FARSITE: Federated, Available, and Reliable Storage for an Incompletely Trusted Environment","Farsite is a serverless, distributed file system that does not assume mutual trust among the client computers on which it runs. Logically, the system functions as a central file server, but physically, there is no central server machine. Instead, a group of desktop client computers collaboratively establish a virtual file server that can be accessed by any of the clients. The system provides a global name space for files, location-transparent access to both private files and shared public files, and improved reliability relative to storing files on a desktop workstation. It does this by distributing multiple encrypted replicas of each file among a set of client machines. Files are referenced through a hierarchical directory structure that is maintained by a distributed directory service. Our broad objective is to figure out how to build highly available, reliable, and secure systems on a substrate of cooperating but mutually distrusting hosts. In the context of our distributed file system, we have identified three specific goals: * To provide high availability and reliability for file storage. * To provide security and resistance to Byzantine threats. * To have the system automatically configure and tune itself adaptively."
582,"spins security protocols for sensor networks",260912,"{SPINS}: Security Protocols for Sensor Networks","As sensor networks edge closer towards wide-spread deployment, security issues become a central concern. So far, much research has focused on making sensor networks feasible and useful, and has not concentrated on security. We present a suite of security building blocks optimized for resource-constrained environments and wireless communication. SPINS has two secure building blocks: SNEP and mTESLA. SNEP provides the following important baseline security primitives: Data confidentiality, two-party data authentication, and data freshness. A particularly hard problem is to provide efficient broadcast authentication, which is an important mechanism for sensor networks. mTESLA is a new protocol which provides authenticated broadcast for severely resource-constrained environments. We implemented the above protocols, and show that they are practical even on minimal hardware: the performance of the protocol suite easily matches the data rate of our network.Additionally, we demonstrate that the suite can be used for building higher level protocols."
583,"ontology based context modeling and reasoning using owl",260973,"Ontology Based Context Modeling and Reasoning using OWL","Here we propose an OWL encoded context ontology (CONON) for modeling context in pervasive computing environments, and for supporting logic-based context reasoning. CONON provides an upper context ontology that captures general concepts about basic context, and also provides extensibility for adding domain-specific ontology in a hierarchical manner. Based on this context ontology, we have studied the use of logic reasoning to check the consistency of context information, and to reason over low-level, explicit context to derive high-level, implicit context. By giving a performance study for our prototype, we quantitatively evaluate the feasibility of logic based context reasoning for nontime-critical applications in pervasive computing environments, where we always have to deal carefully with the limitation of computational resources."
584,"scribe a largescale and decentralized applicationlevel multicast infrastructure",261218,"{SCRIBE}: A large-scale and decentralized application-level multicast infrastructure","This paper presents Scribe, a scalable application-level multicast infrastructure. Scribe supports large numbers of groups, with a potentially large number of members per group. Scribe is built on top of Pastry, a generic peer-to-peer object location and routing substrate overlayed on the Internet, and leverages Pastry's reliability, self-organization, and locality properties. Pastry is used to create and manage groups and to build efficient multicast trees for the dissemination of messages to each group. Scribe provides best-effort reliability guarantees, and we outline how an application can extend Scribe to provide stronger reliability. Simulation results, based on a realistic network topology model, show that Scribe scales across a wide range of groups and group sizes. Also, it balances the load on the nodes while achieving acceptable delay and link stress when compared with Internet protocol multicast."
585,"network coding for large scale content distribution",261219,"Network Coding for large scale Content Distribution","We propose a new scheme for content distribution of large files that is based on network coding. With network coding, each node of the distribution network is able to generate and transmit encoded blocks of information. The randomization introduced by the coding process eases the scheduling of block propagation, and, thus, makes the distribution more efficient. This is particularly important in large unstructured overlay networks, where the nodes need to make block forwarding decisions based on local information only. We compare network coding to other schemes that transmit unencoded information (i.e. blocks of the original file) and, also, to schemes in which only the source is allowed to generate and transmit encoded packets. We study the performance of network coding in heterogeneous networks with dynamic node arrival and departure patterns, clustered topologies, and when incentive mechanisms to discourage free-riding are in place. We demonstrate through simulations of scenarios of practical interest that the expected file download time improves by more than 20-30\\ with network coding compared to coding at the server only and, by more than 2-3 times compared to sending unencoded information. Moreover, we show that network coding improves the robustness of the system and is able to smoothly handle extreme situations where the server and nodes leave the system."
586,"the complete genome sequence of escherichia coli k",261290,"The Complete Genome Sequence of Escherichia coli K-12","The 4,639,221âbase pair sequence of Escherichia coliK-12 is presented. Of 4288 protein-coding genes annotated, 38 percent have no attributed function. Comparison with five other sequenced microbes reveals ubiquitous as well as narrowly distributed gene families; many families of similar genes within E. coli are also evident. The largest family of paralogous proteins contains 80 ABC transporters. The genome as a whole is strikingly organized with respect to the local direction of replication; guanines, oligonucleotides possibly related to replication and recombination, and most genes are so oriented. The genome also contains insertion sequence (IS) elements, phage remnants, and many other patches of unusual composition indicating genome plasticity through horizontal transfer."
587,"pattern formation outside of equilibrium",262715,"Pattern formation outside of equilibrium","A comprehensive review of spatiotemporal pattern formation in systems driven away from equilibrium is presented, with emphasis on comparisons between theory and quantitative experiments. Examples include patterns in hydrodynamic systems such as thermal convection in pure fluids and binary mixtures, Taylor-Couette flow, parametric-wave instabilities, as well as patterns in solidification fronts, nonlinear optics, oscillatory chemical reactions and excitable biological media. The theoretical starting point is usually a set of deterministic equations of motion, typically in the form of nonlinear partial differential equations. These are sometimes supplemented by stochastic terms representing thermal or instrumental noise, but for macroscopic systems and carefully designed experiments the stochastic forces are often negligible. An aim of theory is to describe solutions of the deterministic equations that are likely to be reached starting from typical initial conditions and to persist at long times. A unified description is developed, based on the linear instabilities of a homogeneous state, which leads naturally to a classification of patterns in terms of the characteristic wave vector  q 0  and frequency  ω 0  of the instability. Type  I s  systems ( ω 0 =0 ,  q 0 ≠ 0 ) are stationary in time and periodic in space; type  III o  systems ( ω 0 ≠ 0 ,  q 0 =0 ) are periodic in time and uniform in space; and type  I o  systems ( ω 0 ≠ 0 ,  q 0 ≠ 0 ) are periodic in both space and time. Near a continuous (or supercritical) instability, the dynamics may be accurately described via ""amplitude equations,"" whose form is universal for each type of instability. The specifics of each system enter only through the nonuniversal coefficients. Far from the instability threshold a different universal description known as the ""phase equation"" may be derived, but it is restricted to slow distortions of an ideal pattern. For many systems appropriate starting equations are either not known or too complicated to analyze conveniently. It is thus useful to introduce phenomenological order-parameter models, which lead to the correct amplitude equations near threshold, and which may be solved analytically or numerically in the nonlinear regime away from the instability. The above theoretical methods are useful in analyzing ""real pattern effects"" such as the influence of external boundaries, or the formation and dynamics of defects in ideal structures. An important element in nonequilibrium systems is the appearance of deterministic chaos. A greal deal is known about systems with a small number of degrees of freedom displaying ""temporal chaos,"" where the structure of the phase space can be analyzed in detail. For spatially extended systems with many degrees of freedom, on the other hand, one is dealing with spatiotemporal chaos and appropriate methods of analysis need to be developed. In addition to the general features of nonequilibrium pattern formation discussed above, detailed reviews of theoretical and experimental work on many specific systems are presented. These include Rayleigh-Bénard convection in a pure fluid, convection in binary-fluid mixtures, electrohydrodynamic convection in nematic liquid crystals, Taylor-Couette flow between rotating cylinders, parametric surface waves, patterns in certain open flow systems, oscillatory chemical reactions, static and dynamic patterns in biological media, crystallization fronts, and patterns in nonlinear optics. A concluding section summarizes what has and has not been accomplished, and attempts to assess the prospects for the future."
588,"new directions on agile methods a comparative analysis",263024,"New directions on agile methods: a comparative analysis","Agile software development methods have caught the attention of software engineers and researchers worldwide. Scientific research is yet scarce. This paper reports results from a study, which aims to organize, analyze and make sense out of the dispersed field of agile software development methods. The comparative analysis is performed using the method’s life-cycle coverage, project management support, type of practical guidance, fitness-for-use and empirical evidence as the analytical lenses. The results show that agile software development methods, without rationalization, cover certain/different phases of the software development life-cycle and most of them do not offer adequate support for project management. Yet, many methods still attempt to strive for universal solutions (as opposed to situation appropriate) and the empirical evidence is still very limited. Based on the results, new directions are suggested. In principal, it is suggested to place emphasis on methodological quality - not method quantity."
589,"on lisp advanced techniques for common lisp",264058,"On LISP: Advanced Techniques for Common LISP","Starting in the 1980s, Lisp began to be used in several large systems, including Emacs, Autocad, and Interleaf. On Lisp explains the reasons behind Lisp's growing popularity as a mainstream programming language. On Lisp is a comprehensive study of advanced Lisp techniques, with bottom-up programming as the unifying theme. It gives the first complete description of macros and macro applications. The book also covers important subjects related to bottom-up programming, including functional programming, rapid prototyping, interactive development, and embedded languages. The final chapter takes a deeper look at object-oriented programming than previous Lisp books, showing the step-by-step construction of a working model of the Common Lisp Object System (CLOS). As well as an indispensable reference, On Lisp is a source of software. Its examples form a library of functions and macros that readers will be able to use in their own Lisp programs."
590,"distributed work",264330,"Distributed Work","{Technological advances and changes in the global economy are increasing the geographic distribution of work in industries as diverse as banking, wine production, and clothing design. Many workers communicate regularly with distant coworkers; some monitor and manipulate tools and objects at a distance. Work teams are spread across different cities or countries. Joint ventures and multiorganizational projects entail work in many locations. Two famous examples--the Hudson Bay Companys seventeenth-century fur trading empire and the electronic community that created the original Linux computer operating system--suggest that distributed work arrangements can be flexible, innovative, and highly successful. At the same time, distributed work complicates workers professional and personal lives. Distributed work alters how people communicate and how they organize themselves and their work, and it changes the nature of employee-employer relationships.<br /> <br /> This book takes a multidisciplinary approach to the study of distributed work groups and organizations, the challenges inherent in distributed work, and ways to make distributed work more effective. Specific topics include division of labor, incentives, managing group members, facilitating interaction among distant workers, and monitoring performance. The final chapters focus on distributed work in one domain, collaborative scientific research. The contributors include psychologists, cognitive scientists, sociologists, anthropologists, historians, economists, and computer scientists.}"
591,"multiple neural spike train data analysis stateoftheart and future challenges",264340,"Multiple neural spike train data analysis: state-of-the-art and future challenges","Multiple electrodes are anow a standard tool in neuroscience research that make it possible to study the simultaneous activity of several neurons in a given brain region or across different regions. The data from multi-electrode studies present important analysis challenges that must be resolved for optimal use of these neurophysiological measurements to answer questions about how the brain works. Here we review statistical methods for the analysis of multiple neural spike-train data and discuss future challenges for methodology research."
592,"artificial grammar learning by yearolds leads to specific and abstract knowledge",265615,"Artificial grammar learning by 1-year-olds leads to specific and abstract knowledge.","Four experiments used the head-turn preference procedure to assess whether infants could extract and remember information from auditory strings produced by a miniature artificial grammar. In all four experiments, infants generalized to new structure by discriminating new grammatical strings from ungrammatical ones after less than 2 min exposure to the grammar. Infants acquired specific information about the grammar as demonstrated by the ability to discriminate new grammatical strings from those with illegal endpoints (Experiment 1). Infants also discriminated new grammatical strings from those with string-internal pairwise violations (Experiments 2 and 3). Infants in Experiment 4 abstracted beyond specific word order as demonstrated by the ability to discriminate new strings produced by their training grammar from strings produced by another grammar despite a change in vocabulary between training and test. We discuss the implications of these findings for the study of language acquisition."
593,"information architecture blueprints for the web",266089,"Information Architecture: Blueprints for the Web","{<P>All web sites have an architecture, whether you design one or not-just as every building has an architecture, from the lowly shanty by the railroad track to Chicago's tallest skyscraper. Unfortunately, most web sites are shanties, not skyscrapers. Companies that hastily threw up a web site in the dot-com boom days were visited by building inspector Jakob Neilsen, who told them their site should be condemned. But now we are entering a time of rebuilding, and we've got a chance to get it right.</P> <P><I>Information Architecture: Blueprints for the Web</I> introduces the core concepts of information architecture: organizing web site content so that it can be found, designing web site interaction so that it's pleasant to use, and creating an interface that is easy to understand. This book will help designers, project managers, programmers, and other information architecture practitioners avoid the costly mistakes of the past by teaching the skills of information architecture swiftly and clearly. Use this book and you will pass the usability inspection with flying colors!</P>}"
594,"a tutorial on principal components analysis",266137,"A tutorial on Principal Components Analysis","This tutorial is designed to give the reader an understanding of Principal Components Analysis (PCA). PCA is a useful statistical technique that has found application in fields such as face recognition and image compression, and is a common technique for finding patterns in data of high dimension. Before getting to a description of PCA, this tutorial first introduces mathematical concepts that will be used in PCA. It covers standard deviation, covariance, eigenvectors and eigenvalues. This background knowledge is meant to make the PCA section very straightforward, but can be skipped if the concepts are already familiar. There are examples all the way through this tutorial that are meant to illustrate the concepts being discussed. If further information is required, the mathematics textbook “Elementary Linear Algebra 5e” by Howard Anton, Publisher John Wiley & Sons Inc, ISBN 0-471-85223-6 is a good source of information regarding the mathematical background."
595,"the experienced sense of a virtual community characteristics and processes",266206,"The experienced ""sense"" of a virtual community: characteristics and processes","E-commerce strategists advise companies to create virtual communities for their customers. But what does this involve? Research on face-to-face communities identifies the concept of ""sense of community:"" a characteristic of successful communities distinguished by members' helping behaviors and members' emotional attachment to the community and other members. Does a sense of virtual community exist in online settings, and what does it consist of? Answering these questions is key, if we are to provide guidance to businesses attempting to create virtual {communities.The} paper explores the concept of sense of virtual community in a newsgroup we call Multiple Sports Newsgroup {(MSN).} We first demonstrate that {MSN} does indeed have a sense of virtual community, but that the dimensions of the sense of community in {MSN} differ somewhat from those reported for physical communities. The nature of these differences is plausibly related to the differences between electronic and face-to-face communication. We next describe the behavioral processes that contribute to the sense of virtual community at {MSN-exchanging} support, creating identities and making identifications, and the production of trust. Again, these processes are similar to those found in non-virtual communities, but they are related to the challenges of electronic communication. Lastly, we consider the question of how sense of community may come about and discuss the implications for electronic business."
596,"protein folding and the organization of the protein topology universe",270356,"Protein folding and the organization of the protein topology universe.","The mechanism by which proteins fold to their native states has been the focus of intense research in recent years. The rate-limiting event in the folding reaction is the formation of a conformation in a set known as the transition-state ensemble. The structural features present within such ensembles have now been analysed for a series of proteins using data from a combination of biochemical and biophysical experiments together with computer-simulation methods. These studies show that the topology of the transition state is determined by a set of interactions involving a small number of key residues and, in addition, that the topology of the transition state is closer to that of the native state than to that of any other fold in the protein universe. Here, we review the evidence for these conclusions and suggest a molecular mechanism that rationalizes these findings by presenting a view of protein folds that is based on the topological features of the polypeptide backbone, rather than the conventional view that depends on the arrangement of different types of secondary-structure elements. By linking the folding process to the organization of the protein structure universe, we propose an explanation for the overwhelming importance of topology in the transition states for protein folding."
597,"the semantic learning organization",270721,"The semantic learning organization","Purpose --- The aim of this paper is introducing the concept of a ``semantic learning organization'' (SLO) as an extension of the concept of ``learning organization'' in the technological domain. Design/methodology/approach --- The paper takes existing definitions and conceptualizations of both learning organizations and Semantic Web technology to develop the new concept. Findings --- The main points in which Semantic Web technology can be applied to learning in organizations are identified, and ontological accounts of organizational earning behaviour are pointed out as the main open question to develop the concept of a SLO. Originality/value --- The paper provides a new conceptual framework for Semantic Web applications in organizational learning, which can be used as a roadmap for further research."
598,"handbook of action research participative inquiry and practice",270819,"Handbook of Action Research : Participative Inquiry and Practice","{<P>""A remarkable reframing of action research that engages the spirit as well as the mind, in inquiry that matters, shared among inquirers who matter. 'Validity as we once knew it will never be the same after these improvements. Wonderfully provocative!""<BR>  --Karl Weick, University of Michigan </P> <P>""This is truly a significant work. Not only has action research reached maturity, but in the context of the postmodern constructionist debates its scope has been dramatically expanded, its conceptual underpinnings deepened, and its forms of practice enormously enriched. The present confluence of humanism and pragmatism has inspired lively conversations between us; the work has the potential to transform the very idea of social science.""<BR>  --Kenneth J. Gergen, author of <B>An Invitation to Social Construction</B> and Mary Gergen, author of <B>Feminist Reconstructions in Psychology</B> </P> <P>""A wholly new kind of human inquiry is emerging. It is to do with taking our own, previously ignored, spontaneously responsive, living involvements with our surroundings seriously. Rather than with views and perspectives, rather than with a one-way manipulative understanding, gained by merely observing movements from a distance, it is concerned with quite a different kind of participatory, experiential understanding - the kind of understanding we have when playing a part in an activity which also to an extent 'plays' us. Workers are beginning to bring to light the many different knowledges present to us in the different practical ways in which we can be relationally involved with the others and othernesses around us. Everything changes when we get up close and personal. All that is solid melts into air! In this exciting <B>Handbook</B>, Reason and Bradbury have collected together a large number of the central workers in this new and developing sphere of participative inquiry. Overall, in the detailed explorations they conduct, just as in becoming familiar with a new and strange landscape, they help us get to know our `way about' in its rich and intricate 'landscape'. Literally, this is a landmark volume."" <BR> --John Shotter, University of New Hampshire </P> <P>""The <B>Handbook of Action Research</B> is truly a remarkable book. We are greatly indebted to the editors Peter Reason and Hilary Bradbury, who managed to avoid the usual tower of Babel, and succeeded to forge and orchestrate the somewhat incoherent mosaic of action research, with its many voices, into an intelligent comprehensive and logical whole. This handbook provides a much needed clarification of a critical transition in the social sciences."" <BR> --Hans van Beinum, International Journal of Action Research and Organizational Renewal </P> <P>The publication of the <B>Handbook of Action Research</B> is a publishing milestone, drawing together the different strands of action research, demonstrating their diverse applications and showing their interrelations. Far-reaching in scale and scope, the Handbook informs readers about the latest approaches, both quantitative and qualitative, in social inquiry, and moves the field forward with fresh insights and applications. Throughout, the contributing authors grapple with questions of how to integrate knowledge with action, how to collaborate with co-researchers in the field, and how to present the necessarily 'messy' components in a coherent fashion. The organization of the volume reflects the many different issues and levels of analysis represented. This volume is an essential resource for scholars and professionals engaged in social and political inquiry, organizational research and education. </P>}"
599,"refactoring to patterns",270986,"Refactoring to Patterns","{What Is This Book About? <BR>This book is about the marriage of refactoring&#151;the process of improving the design of existing code&#151;with patterns, the classic solutions to recurring design problems. Refactoring to Patterns suggests that using patterns to improve an existing design is better than using patterns early in a new design. This is true whether code is years old or minutes old. We improve designs with patterns by applying sequences of low-level design transformations, known as refactorings.    <P>What Are the Goals of This Book? <BR>This book was written to help you:   <P>Understand how to combine refactoring and patterns  <BR>Improve the design of existing code with pattern-directed refactorings  <BR>Identify areas of code in need of pattern-directed refactorings  <BR>Learn why using patterns to improve existing code is better than using patterns early in a new design  <BR>To achieve these goals, this book includes the following features:   <P>A catalog of 27 refactorings  <BR>Examples based on real-world code, not the toy stuff  <BR>Pattern descriptions, including real-world pattern examples  <BR>A collection of smells (i.e., problems) that indicate the need for pattern-directed refactorings  <BR>Examples of different ways to implement the same pattern  <BR>Advice for when to refactor to, towards, or away from patterns  <BR>To help individuals or groups learn the 27 refactorings in the book, you&#146;ll find a suggested study sequence on the inside back cover of the book.   <P>Who Should Read This Book?  <P>This book is for object-oriented programmers engaged in or interested in improving the design of existing code. Many of these programmers use patterns and/or practice refactoring but have never implemented patterns by refactoring; others know little about refactoring and patterns and would like to learn more.   <P>This book is useful for both greenfield development, in which you are writing a new system or feature from scratch, and legacy development, in which you are mostly maintaining a legacy system.   <P>What Background Do You Need? <BR>This book assumes you are familiar with design concepts like tight coupling and loose coupling as well as object-oriented concepts like inheritance, polymorphism, encapsulation, composition, interfaces, abstract and concrete classes, abstract and static methods, and so forth.   <P>I use Java examples in this book. I find that Java tends to be easy for most object-oriented programmers to read. I&#146;ve gone out of my way to not use fancy Java features, so whether you code in C++, C#, Visual Basic .NET, Python, Ruby, Smalltalk, or some other object-oriented language, you ought to be able to understand the Java code in this book.   <P>This book is closely tied to Martin Fowler&#146;s classic book Refactoring F. It contains references to low-level refactorings, such as:   <P>Extract Method  <BR>Extract Interface  <BR>Extract Superclass  <BR>Extract Subclass  <BR>Pull Up Method  <BR>Move Method  <BR>Rename Method   <P>Refactoring also contains references to more sophisticated refactorings, such as:   <P>Replace Inheritance with Delegation  <BR>Replace Conditional with Polymorphism  <BR>Replace Type Code with Subclasses   <P>To understand the pattern-directed refactorings in this book, you don&#146;t need to know every refactoring listed above. Instead, you can follow the example code that illustrates how the listed refactorings are implemented. However, if you want to get the most out of this book, I do recommend that you have Refactoring close by your side. It&#146;s an invaluable refactoring resource, as well as a useful aid for understanding this book.   <P>The patterns I write about come from the classic book Design Patterns DP, as well as from authors such as Kent Beck, Bobby Woolf, and myself. These are patterns that my colleagues and I have refactored to, towards, or away from on real-world projects. By learning the art of pattern-directed refactorings, you&#146;ll understand how to refactor to, towards, or away from patterns not mentioned in this book.   <P>You don&#146;t need expert knowledge of these patterns to read this book, though some knowledge of patterns is useful. To help you understand the patterns I&#146;ve written about, this book includes brief pattern summaries, UML sketches of patterns, and many example implementations of patterns. To get a more detailed understanding of the patterns, I recommend that you study this book in conjunction with the patterns literature I reference.   <P>This book uses UML 2.0 diagrams. If you don&#146;t know UML very well, you&#146;re in good company. I know the basics. While writing this book, I kept the third edition of Fowler&#146;s UML Distilled Fowler, UD close by my side and referred to it often.}"
600,"selfsimilarity in world wide web traffic evidence and possible causes",271442,"Self-similarity in World Wide Web traffic: evidence and possible causes","The notion of self-similarity has been shown to apply to wide-area and local-area network traffic. We show evidence that the subset of network traffic that is due to World Wide Web (WWW) transfers can show characteristics that are consistent with self-similarity, and we present a hypothesized explanation for that self-similarity. Using a set of traces of actual user executions of NCSA Mosaic, we examine the dependence structure of WWW traffic. First, we show evidence that WWW traffic exhibits behavior that is consistent with self-similar traffic models. Then we show that the self-similarity in such traffic can be explained based on the underlying distributions of WWW document sizes, the effects of caching and user preference in file transfer, the effect of user &ldquo;think time&rdquo;, and the superimposition of many such transfers in a local-area network. To do this, we rely on empirically measured distributions both from client traces and from data independently collected at WWW servers"
601,"a fast quantum mechanical algorithm for database search",272317,"A fast quantum mechanical algorithm for database search","Imagine a phone directory containing N names arranged in completely random order. In order to find someone's phone number with a 50% probability, any classical algorithm (whether deterministic or probabilistic) will need to look at a minimum of N/2 names. Quantum mechanical systems can be in a superposition of states and simultaneously examine multiple names. By properly adjusting the phases of various operations, successful computations reinforce each other while others interfere randomly. As a result, the desired phone number can be obtained in only O(sqrt(N)) steps. The algorithm is within a small constant factor of the fastest possible quantum mechanical algorithm."
602,"a survey of current work in biomedical text mining",272363,"A survey of current work in biomedical text mining.","The volume of published biomedical research, and therefore the underlying biomedical knowledge base, is expanding at an increasing rate. Among the tools that can aid researchers in coping with this information overload are text mining and knowledge extraction. Significant progress has been made in applying text mining to named entity recognition, text classification, terminology extraction, relationship extraction and hypothesis generation. Several research groups are constructing integrated flexible text-mining systems intended for multiple uses. The major challenge of biomedical text mining over the next 5-10 years is to make these systems useful to biomedical researchers. This will require enhanced access to full text, better understanding of the feature space of biomedical literature, better methods for measuring the usefulness of systems to users, and continued cooperation with the biomedical research community to ensure that their needs are addressed. 10.1093/bib/6.1.57"
603,"growth income distribution and democracy what the data say",273082,"Growth, Income Distribution and Democracy: What the Data Say","This paper investigates the relationship between income distribution, democratic institutions, and growth. It does so by addressing three main issues: the properties and reliability of the income distribution data, the robustness of the reduced form relationships between income distribution and growth estimated so far, and the specific channels through which income distribution affects growth. The main conclusion in this regard is that there is strong empirical support for two types of explanations, linking income distribution to sociopolitical instability and to the education/fertility decision. A third channel, based on the interplay of borrowing constraints and investment in human capital, also seems to receive some support by the data, although it is probably the hardest to test with the existing data. By contrast, there appears to be less empirical support for explanations based on the effects of income distribution on fiscal policy."
604,"survey of the state of the art in human language technology",273865,"Survey of the State of the Art in Human Language Technology","This book surveys the state of the art of human language technology. The goal of the survey is to provide an interested reader with an overview of the field---the main areas of work, the capabilities and limitations of current technology, and the technical challenges that must be overcome to realize the vision of graceful human computer interaction using natural communication skills. The book consists of thirteen chapters written by 97 different authors."
605,"what role can adaptive support play in an adaptable system",274045,"What role can adaptive support play in an adaptable system?","As computer applications become larger with every new version, there is a growing need to provide some way for users to manage the interface complexity. There are three different potential solutions to this problem: 1) an  adaptable  interface that allows users to customize the application to suit their needs; 2) an  adaptive  interface that performs the adaptation for the users; or 3) a combination of the  adaptive  and  adaptable  solutions, an approach that would be suitable in situations where users are not customizing effectively on their own. In this paper we examine what it means for users to engage in effective customization of a menu-based graphical user interface. We examine one aspect of effective customization, which is how characteristics of the users' tasks and customization behaviour affect their performance on those tasks. We do so by using a process model simulation based on cognitive modelling that generates quantitative predictions of user performance. Our results show that users can engage in customization behaviours that vary in efficiency. We use these results to suggest how adaptive support could be added to an adaptable interface to improve the effectiveness of the users' customization."
606,"annotation from paper books to the digital library",274117,"Annotation: from paper books to the digital library","Readers annotate paper books as a routine part of their engagement with the materials; it is a useful practice. manifested through a wide variety of markings made in service of very different purposes. This paper examines the practice of annotation in a particular situation: the markings students make in university-level textbooks. The study focuses on the form and function of these annotations, and their status within a community of fellow textbook readers. Using this study as a basis, I discuss issues and implications for the design of annotation tools for a digital library setting."
607,"wavelets in bioinformatics and computational biology state of art and perspectives",275186,"Wavelets in bioinformatics and computational biology: state of art and perspectives","Motivation: At a recent meeting{dagger}, the wavelet transform was depicted as a small child kicking back at its father, the Fourier transform. Wavelets are more efficient and faster than Fourier methods in capturing the essence of data. Nowadays there is a growing interest in using wavelets in the analysis of biological sequences and molecular biology-related signals.  Results: This review is intended to summarize the potential of state of the art wavelets, and in particular wavelet statistical methodology, in different areas of molecular biology: genome sequence, protein structure and microarray data analysis. I conclude by discussing the use of wavelets in modeling biological structures.  Contact: plio@hgmp.mrc.ac.uk 10.1093/bioinformatics/19.1.2"
608,"rewiring of the yeast transcriptional network through the evolution of motif usage",275498,"Rewiring of the Yeast Transcriptional Network Through the Evolution of Motif Usage","Recent experiments revealed large-scale differences in the transcription programs of related species, yet little is known about the genetic basis underlying the evolution of gene expression and its contribution to phenotypic diversity. Here we describe a large-scale modulation of the yeast transcription program that is connected to the emergence of the capacity for rapid anaerobic growth. Genes coding for mitochondrial and cytoplasmic ribosomal proteins display a strongly correlated expression pattern in Candida albicans, but this correlation is lost in the fermentative yeast Saccharomyces cerevisiae. We provide evidence that this change in gene expression is connected to the loss of a specific cis-regulatory element from dozens of genes following the apparent whole-genome duplication event. Our results shed new light on the genetic mechanisms underlying the large-scale evolution of transcriptional networks. 10.1126/science.1113833"
609,"digital formations it and new architectures in the global realm",276211,"Digital Formations : IT and New Architectures in the Global Realm","Review Comprehensive and insightful, Digital Formations will be greeted warmly in the fields that overlap its concerns. It addresses a most important set of questions concerning the relationship of information technologies to globalization. And this is an urgent topic for social science. Book Description Computer-centered networks and technologies are reshaping social relations and constituting new social domains on a global scale, from virtually borderless electronic markets and Internet-based large-scale conversations to worldwide open source software development communities, transnational corporate production systems, and the global knowledge-arenas associated with {NGO} networks. This book explores how such 'digital formations' emerge from the ever-changing intersection of computer-centered technologies and the broad range of social contexts that underlie much of what happens in cyberspace. While viewing technologies fundamentally in social rather than technical terms, Digital Formations nonetheless emphasizes the importance of recognizing the specific technical capacities of digital technologies. Importantly, it identifies digital formations as a new area of study in the social sciences and in thinking about globalization. The ten chapters, by leading scholars, examine key social, political, and economic developments associated with these new configurations of organization, space, and interaction. They address the operation of digital formations and their implications for the development of longstanding institutions and for their wider contexts and fields, and they consider the political, economic, and other forces shaping those formations and how the formations, in turn, are shaping such forces. Following a conceptual introduction by the editors are chapters by Hayward Alker, Jonathan Bach and David Stark, {Lars-Erik} Cederman and Peter A. Kraus, Dieter Ernst, D. Linda Garcia, Doug Guthrie, Robert Latham, Warren Sack, Saskia Sassen, and Steven Weber."
610,"making things public atmospheres of democracy",277054,"Making Things Public : Atmospheres of Democracy","{In this groundbreaking editorial and curatorial project, more than 100 writers, artists, and philosophers rethink what politics is about. In a time of political turmoil and anticlimax, this book redefines politics as operating in the realm of <i>things</i>. Politics is not just an arena, a profession, or a system, but a concern for things brought to the attention of the fluid and expansive constituency of the public. But how are things made public? What, we might ask, is a republic, a <i>res publica</i>, a public thing, if we do not know how to make things public? There are many other kinds of assemblies, which are not political in the usual sense, that gather a public around things -- scientific laboratories, supermarkets, churches, and disputes involving natural resources like rivers, landscapes, and air. The authors of <i>Making Things Public</i> -- and the ZKM show that the book accompanies -- ask what would happen if politics revolved around disputed things. Instead of looking for democracy only in the official sphere of professional politics, they examine the new atmospheric conditions -- technologies, interfaces, platforms, networks, and mediations that allow things to be made public. They show us that the old definition of politics is too narrow; there are many techniques of representation -- in politics, science, and art -- of which Parliaments and Congresses are only a part.<br /> <br /> The authors include such prominent thinkers as Richard Rorty, Simon Schaffer, Peter Galison, Richard Powers, Lorraine Daston, Richard Aczel, and Donna Haraway; their writings are accompanied by excerpts from John Dewey, Shakespeare, Swift, La Fontaine, and Melville. More than 500 color images document the new idea of what Bruno Latour and Peter Weibel call an ""object-oriented democracy.""}"
611,"personal information management",277591,"Personal information management","{In an ideal world, everyone would always have the right information, in the right form, with the right context, right when they needed it. Unfortunately, we do not live in an ideal world. This book looks at how people in the real world currently manage to store and process the massive amounts of information that overload their senses and their systems, and discusses how tools can help bring these real information interactions closer to the ideal. <P>Personal information management (PIM) is the study and practice of the activities people perform to acquire, organize, maintain, and retrieve information for everyday use. PIM is a growing area of interest as we all strive for better use of our limited personal resources of time, money, and energy, as well as greater workplace efficiency and productivity.  <P>Personal information is currently fragmented across electronic documents, email messages, paper documents, digital photographs, music, videos, instant messages, and so on. Each form of information is organized and used to complete different tasks and to fulfill disparate roles and responsibilities in an individual's life. Existing PIM tools are partly responsible for this fragmentation. They can also be part of the solution that brings information together again. A major contribution of this book is its integrative treatment of PIM-related research. <P>The book grows out of a workshop on PIM sponsored by the National Science Foundation, held in Seattle, Washington, in 2006. Scholars from major universities and researchers from companies such as Microsoft Research, Google, and IBM offer approaches to conceptual problems of information management. In doing so, they provide a framework for thinking about PIM as an area for future research and innovation.}"
612,"analysis of molecular variance inferred from metric distances among dna haplotypes application to human mitochondrial dna restriction data",277677,"Analysis of molecular variance inferred from metric distances among DNA haplotypes: application to human mitochondrial DNA restriction data","We present here a framework for the study of molecular variation within a single species. Information on DNA haplotype divergence is incorporated into an analysis of variance format, derived from a matrix of squared-distances among all pairs of haplotypes. This analysis of molecular variance (AMOVA) produces estimates of variance components and F-statistic analogs, designated here as phi-statistics, reflecting the correlation of haplotypic diversity at different levels of hierarchical subdivision. The method is flexible enough to accommodate several alternative input matrices, corresponding to different types of molecular data, as well as different types of evolutionary assumptions, without modifying the basic structure of the analysis. The significance of the variance components and phi-statistics is tested using a permutational approach, eliminating the normality assumption that is conventional for analysis of variance but inappropriate for molecular data. Application of AMOVA to human mitochondrial DNA haplotype data shows that population subdivisions are better resolved when some measure of molecular differences among haplotypes is introduced into the analysis. At the intraspecific level, however, the additional information provided by knowing the exact phylogenetic relations among haplotypes or by a nonlinear translation of restriction-site change into nucleotide diversity does not significantly modify the inferred population genetic structure. Monte Carlo studies show that site sampling does not fundamentally affect the significance of the molecular variance components. The AMOVA treatment is easily extended in several different directions and it constitutes a coherent and flexible framework for the statistical analysis of molecular data."
613,"lengths of chromosomal segments conserved since divergence of man and mouse",277764,"Lengths of Chromosomal Segments Conserved since Divergence of Man and Mouse","Linkage relationships of homologous loci in man and mouse were used to estimate the mean length of autosomal segments conserved during evolution. Comparison of the locations of greater than 83 homologous loci revealed 13 conserved segments. Map distances between the outermost markers of these 13 segments are known for the mouse and range from 1 to 24 centimorgans. Methods were developed for using this sample of conserved segments to estimate the mean length of all conserved autosomal segments in the genome. This mean length was estimated to be 8.1 +/- 1.6 centimorgans. Evidence is presented suggesting that chromosomal rearrangements that determine the lengths of these segments are randomly distributed within the genome. The estimated mean length of conserved segments was used to predict the probability that certain loci, such as peptidase-3 and renin, are linked in man given that homologous loci are chi centimorgans apart in the mouse. The mean length of conserved segments was also used to estimate the number of chromosomal rearrangements that have disrupted linkage since divergence of man and mouse. This estimate was shown to be 178 +/- 39 rearrangements."
614,"putting stress into words health linguistic and therapeutic implications",277967,"Putting stress into words: health, linguistic, and therapeutic implications.","When individuals are asked to write or talk about personally upsetting experiences, significant improvements in physical health are found. Analyses of subjects' writing about traumas indicate that those whose health improves most tend to use a higher proportion of negative emotion words than positive emotion words. Independent of verbal emotion expression, the increasing use of insight, causal, and associated cognitive words over several days of writing is linked to health improvement. That is, the construction of a coherent story together with the expression of negative emotions work together in therapeutic writing. Evidence of these processes are also seen in specific links between word production and immediate autonomic nervous system activity. Implications for therapy and for considering the mind and body as fluid, dynamic systems are discussed."
615,"statistical and structural approaches to texture",278151,"{Statistical and Structural Approaches to Texture}","In this survey we review the image processing literature on the various approaches and models investigators have used for texture. These include statistical approaches of autocorrelation function, optical transforms, digital transforms, textural edgeness, structural element, gray tone cooccurrence, run lengths, and autoregressive models. We discuss and generalize some structural approaches to texture based on more complex primitives than gray tone. We conclude with some structural-statistical generalizations which apply the statistical techniques to the structural primitives."
616,"improving search in peertopeer networks",279432,"Improving Search in Peer-to-Peer Networks","Peer-to-peer systems have emerged as a popular way to share huge volumes of data. The usability of these systems depends on effective techniques to find and retrieve data; however, current techniques used in existing P2P systems are often very inefficient. In this paper, we present three techniques for efficient search in P2P systems. We present the design of these techniques, and then evaluate them using a combination of analysis and experiments over Gnutella, the largest open P2P system in operation. We show that while our techniques maintain the same quality of results as currently used techniques, they use up to 5 times fewerresources. In addition, we designed our techniques to be simple, so that they can be easily incorporated into existing systems for immediate impact."
617,"the diameter of the world wide web",280464,"The diameter of the world wide web","Despite its increasing role in communication, the world wide web remains the least controlled medium: any individual or institution can create websites with unrestricted number of documents and links. While great efforts are made to map and characterize the Internet's infrastructure, little is known about the topology of the web. Here we take a first step to fill this gap: we use local connectivity measurements to construct a topological model of the world wide web, allowing us to explore and characterize its large scale properties."
618,"modeling gene and genome duplications in eukaryotes",280770,"Modeling gene and genome duplications in eukaryotes","10.1073/pnas.0501102102 Recent analysis of complete eukaryotic genome sequences has revealed that gene duplication has been rampant. Moreover, next to a continuous mode of gene duplication, in many eukaryotic organisms the complete genome has been duplicated in their evolutionary past. Such large-scale gene duplication events have been associated with important evolutionary transitions or major leaps in development and adaptive radiations of species. Here, we present an evolutionary model that simulates the duplication dynamics of genes, considering genome-wide duplication events and a continuous mode of gene duplication. Modeling the evolution of the different functional categories of genes assesses the importance of different duplication events for gene families involved in specific functions or processes. By applying our model to the  genome, for which there is compelling evidence for three whole-genome duplications, we show that gene loss is strikingly different for large-scale and small-scale duplication events and highly biased toward certain functional classes. We provide evidence that some categories of genes were almost exclusively expanded through large-scale gene duplication events. In particular, we show that the three whole-genome duplications in  have been directly responsible for >90% of the increase in transcription factors, signal transducers, and developmental genes in the last 350 million years. Our evolutionary model is widely applicable and can be used to evaluate different assumptions regarding small- or large-scale gene duplication events in eukaryotic genomes."
619,"mind wide open your brain and the neuroscience of everyday life",281947,"Mind Wide Open : Your Brain and the Neuroscience of Everyday Life","{Given the opportunity to watch the inner workings of his own brain, Steven Johnson jumps at the chance. He reveals the results in <I>Mind Wide Open</I>, an engaging and personal account of his foray into edgy brain science. In the 21st century, Johnson observes, we have become used to ideas such as ""adrenaline rushes"" and ""serotonin levels,"" without really recognizing that complex neurobiology has become a commonplace thing to talk about. He sees recent laboratory revelations about the brain as crucial for understanding ourselves and our psyches in new, post-Freudian ways. Readers shy about slapping electrodes on their own temples can get a vicarious scientific thrill as Johnson tries out empathy tests, neurofeedback, and fMRI scans. The results paint a distinct picture of the author, and uncover general brain secrets at the same time. Memory, fear, love, alertness--all the multitude of states housed in our brains are shown to be the results of chemical and electrical interactions constantly fed and changed by input from our senses. <I>Mind Wide Open</I> both satisfies curiosity and provokes more questions, leaving readers wondering about their own gray matter. <I>--Therese Littleton</I>} {<P>  In this nationally bestselling, compulsively readable account of what makes brain science a vital component of people's quest to know themselves, acclaimed science writer Steven Johnson subjects his own brain to a battery of tests to find out what's really going on inside. He asks:  <P>  <UL>  	<LI>How do we ""read"" other people?  	<LI><P>  	<LI>What is the neurochemistry behind love and sex?  	<LI><P>  	<LI>What does it mean that the brain is teeming with powerful chemicals closely related to recreational drugs?  	<LI><P>  	<LI>Why does music move us to tears?  	<LI><P>  	<LI>Where do breakthrough ideas come from?  </UL>  <P>  Johnson answers these and many more questions arising from the events of our everyday lives. You do not have to be a neuroscientist to wonder, for example, why do you smile? And why do you sometimes smile inappropriately, even if you don't want to? How do others read your inappropriate smile? How does such interplay occur neurochemically, and what, if anything, can you do about it?  <P>  Fascinating and rewarding, <I>Mind Wide Open</I> speaks to brain buffs, self-obsessed neurotics, barstool psychologists, mystified parents, grumpy spouses, exasperated managers, and anyone who enjoys speculating and gossiping about the motivations and behaviors of other human beings. Steven Johnson shows us the transformative power of understanding brain science and offers new modes of introspection and tools for better parenting, better relationships, and better living.} {""BRILLIANTLY EXPLORING TODAY'S CUTTING-EDGE BRAIN RESEARCH, MIND WIDE OPEN IS AN UNPRECEDENTED JOURNEY INTO THE ESSENCE OF HUMAN PERSONALITY, ALLOWING READERS TO UNDERSTAND THEMSELVES AND THE PEOPLE IN THEIR LIVES AS NEVER BEFORE.    Using a mix of experiential reportage, personal storytelling, and fresh scientific discovery, Steven Johnson describes how the brain works -- its chemicals, structures, and subroutines -- and how these systems connect to the day-to-day realities of individual lives. For a hundred years, he says, many of us have assumed that the most powerful route to self-knowledge took the form of lying on a couch, talking about our childhoods. The possibility entertained in this book is that you can follow another path, in which learning about the brain's mechanics can widen one's self-awareness as powerfully as any therapy or meditation or drug.   In Mind Wide Open, Johnson embarks on this path as his own test subject, participating in a battery of attention tests, learning to control video games by altering his brain waves, scanning his own brain with a $2 million fMRI machine, all in search of a modern answer to the oldest of questions: who am I?}"
620,"ownership types for safe programming preventing data races and deadlocks",282032,"Ownership types for safe programming: preventing data races and deadlocks","This paper presents a new static type system for multithreaded programs; well-typed programs in our system are guaranteed to be free of data races and deadlocks. Our type system allows programmers to partition the locks into a fixed number of equivalence classes and specify a partial order among the equivalence classes. The type checker then statically verifies that whenever a thread holds more than one lock, the thread acquires the locks in the descending order.Our system also allows programmers to use recursive tree-based data structures to describe the partial order. For example, programmers can specify that nodes in a tree must be locked in the  tree order . Our system allows mutations to the data structure that change the partial order at runtime. The type checker statically verifies that the mutations do not introduce cycles in the partial order, and that the changing of the partial order does not lead to deadlocks. We do not know of any other sound static system for preventing deadlocks that allows changes to the partial order at runtime.Our system uses a variant of ownership types to prevent data races and deadlocks. Ownership types provide a statically enforceable way of specifying object encapsulation. Ownership types are useful for preventing data races and deadlocks because the lock that protects an object can also protect its encapsulated objects. This paper describes how to use our type system to statically enforce object encapsulation as well as prevent data races and deadlocks. The paper also contains a detailed discussion of different ownership type systems and the encapsulation guarantees they provide."
621,"automatic acquisition of hyponyms from large text corpora",282193,"Automatic acquisition of hyponyms from large text corpora","We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested."
622,"subgraph centrality in complex networks",291875,"Subgraph centrality in complex networks.","We introduce a new centrality measure that characterizes the participation of each node in all subgraphs in a network. Smaller subgraphs are given more weight than larger ones, which makes this measure appropriate for characterizing network motifs. We show that the subgraph centrality [ C S ( i ) ] can be obtained mathematically from the spectra of the adjacency matrix of the network. This measure is better able to discriminate the nodes of a network than alternate measures such as degree, closeness, betweenness, and eigenvector centralities. We study eight real-world networks for which  C S ( i )  displays useful and desirable properties, such as clear ranking of nodes and scale-free characteristics. Compared with the number of links per node, the ranking introduced by  C S ( i )  (for the nodes in the protein interaction network of  S. cereviciae ) is more highly correlated with the lethality of individual proteins removed from the proteome."
623,"software architecture introducing ieee standard",296232,"Software Architecture: Introducing {IEEE} {Standard} 1471","IEEE Standard 1471 identifies sound practices to establish a framework and vocabulary for software architecture concepts.In 2000, the Computer Society approved IEEE Standard 1471, which documents a consensus on good architectural description practices. Five core concepts and relationships provide the foundation for the approved IEEE 1471 version: every system has an architecture, but an architecture is not a system; an architecture and an architecture description are not the same thing; architecture standards, descriptions, and development processes can differ and be developed separately; architecture descriptions are inherently multiviewed; and separating the concept of an object's view from its specification is an effective way to write architecture description standards. IEEE 1471 focuses on both software intensive systems and more general systems, such as information systems, embedded systems, systems-of-systems, product lines, and product families in which software plays a substantial role in development, operation, or evolution"
624,"software architecture the next step",296261,"Software Architecture: The Next Step","This position paper makes the following claims that, in our opinion, are worthwhile to discuss at the workshop. 1) The first phase of software architecture research, where the key concepts are components and connectors, has matured the technology to a level where industry adoption is wide-spread and few fundamental issues remain. 2) The traditional view on software architecture suffers from a number of key problems that cannot be solved without changing our perspective on the notion of software architecture. These problems include the lack of first-class representation of design decisions, the fact that these design decisions are cross-cutting and intertwined, that these problems lead to high maintenance cost, because of which design rules and constraints are easily violated and obsolete design decisions are not removed. 3) As a community, we need to take the next step and adopt the perspective that a software architecture is, fundamentally, a composition of architectural design decisions. These design decisions should be represented as first-class entities in the software architecture and it should, at least before system deployment, be possible to add, remove and change architectural design decisions against limited effort."
625,"a second generation force field for the simulation of proteins nucleic acids and organic molecules",297071,"A Second Generation Force Field for the Simulation of Proteins, Nucleic Acids, and Organic Molecules","We present the derivation of a new molecular mechanical force field for simulating the structures, conformational energies, and interaction energies of proteins, nucleic acids, and many related organic molecules in condensed phases. This effective two-body force field is the successor to the Weiner et al, force field and was developed with some of the same philosophies, such as the use of a simple diagonal potential function and electrostatic potential fit atom centered charges. The need for a 10-12 function for representing hydrogen bonds is no longer necessary due to the improved performance of the new charge model and new van der Waals parameters. These new charges are determined using a 6-31G basis set and restrained electrostatic potential (RESP) fitting and have been shown to reproduce interaction energies, free energies of solvation, and conformational energies of simple small molecules to a good degree of accuracy. Furthermore, the new RESP charges exhibit less variability as a function of the molecular conformation used in the charge determination. The new van der Waals parameters have been derived from liquid simulations and include hydrogen parameters which take into account the effects of any geminal electronegative atoms. The bonded parameters developed by Weiner et al. were modified as necessary to reproduce experimental vibrational frequencies and structures. Most of the simple dihedral parameters have been retained from Weiner et. al., but a complex set of phi and psi parameters which do a good job of reproducing the energies of the low-energy conformations of glycyl and alanyl dipeptides has been developed for the peptide backbone."
626,"interdisciplinary collaboration an emerging cognitive science",298192,"Interdisciplinary Collaboration:  An Emerging Cognitive Science","{Interdisciplinary Collaboration calls attention to a serious need to study the problems and processes of interdisciplinary inquiry, to reflect on the current state of scientific knowledge regarding interdisciplinary collaboration, and to encourage research that studies interdisciplinary cognition in relation to the ecological contexts in which it occurs. It contains reflections and research on interdisciplinarity found in a number of different contexts by practitioners and scientists from a number of disciplines and several chapters represent attempts by cognitive scientists to look critically at the cognitive science enterprise itself. Representing all of the seven disciplines listed in the official logo of the Cognitive Science Society and its journal--anthropology, artificial intelligence, education, linguistics, neuroscience, philosophy, and psychology--this book is divided into three parts: *Part I sets the stage by providing three broad overviews of literature and theory on interdisciplinary research and education. *Part II examines varied forms of interdisciplinarity in situ rather than the more traditional macrolevel interview or survey approaches to studying group work. *Part III consists of noted cognitive scientists who reflect on their experiences and turn the analytical lenses of their own discipline to the critical examination of cognitive science itself as a case study in interdisciplinary collaboration. Interdisciplinary Collaboration is intended for scholars at the graduate level and beyond in cognitive science and education.}"
627,"the complete atomic structure of the large ribosomal subunit at a resolution",298217,"The Complete Atomic Structure of the Large Ribosomal Subunit at 2.4 A Resolution","The large ribosomal subunit catalyzes peptide bond formation and binds initiation, termination, and elongation factors. We have determined the crystal structure of the large ribosomal subunit from Haloarcula marismortui at 2.4 angstrom resolution, and it includes 2833 of the subunit's 3045 nucleotides and 27 of its 31 proteins. The domains of its RNAs all have irregular shapes and fit together in the ribosome like the pieces of a three-dimensional jigsaw puzzle to form a large, monolithic structure. Proteins are abundant everywhere on its surface except in the active site where peptide bond formation occurs and where it contacts the small subunit. Most of the proteins stabilize the structure by interacting with several RNA domains, often using idiosyncratically folded extensions that reach into the subunit's interior."
628,"wiki web collaboration",299199,"Wiki : Web Collaboration","{Wikis are Web-based applications that allow all users not only to view pages but also to change them. The recent success of the Internet encyclopedia Wikipedia has drawn increasing attention from private users, small organizations and enterprises to the various possible uses of wikis. Their simple structure and straightforward operation make them a serious alternative to expensive content management systems and also provide a basis for many applications in the area of collaborative work. We show the practical use of wikis in carrying out projects for users as well as for maintainers. This includes a step-by-step introduction to wiki philosophy, social effects and functions, a survey of their controls and components, and the installation and configuration of the wiki clones MediaWiki and TWiki. In order to exemplify the possibilities of the software, we use it as a project tool for planning a conference.}"
629,"biomart and bioconductor a powerful link between biological databases and microarray data analysis",299529,"BioMart and Bioconductor: a powerful link between biological databases and microarray data analysis.","Summary: biomaRt is a new Bioconductor package that integrates BioMart data resources with data analysis software in Bioconductor. It can annotate a wide range of gene or gene product identifiers (e.g. Entrez-Gene and Affymetrix probe identifiers) with information such as gene symbol, chromosomal coordinates, Gene Ontology and OMIM annotation. Furthermore biomaRt enables retrieval of genomic sequences and single nucleotide polymorphism information, which can be used in data analysis. Fast and up-to-date data retrieval is possible as the package executes direct SQL queries to the BioMart databases (e.g. Ensembl). The biomaRt package provides a tight integration of large, public or locally installed BioMart databases with data analysis in Bioconductor creating a powerful environment for biological data mining. Availability: http://www.bioconductor.org. LGPL Contact: steffen.durinck@esat.kuleuven.ac.be"
630,"designing dccp congestion control without reliability",300210,"Designing DCCP: Congestion control without reliability","Fast-growing Internet applications like streaming media and telephony prefer timeliness to reliability, making TCP a poor fit. Unfortunately, UDP, the natural alternative, lacks congestion control. High-bandwidth UDP applications must implement congestion control themselves—a difficult task—or risk rendering congested networks unusable. We set out to ease the safe deployment of these applications by designing a congestion-controlled unreliable transport protocol. The outcome, the Datagram Congestion Control Protocol or DCCP, adds to a UDP-like foundation the minimum mechanisms necessary to support congestion control. We thought those mechanisms would resemble TCP’s, but without reliability and, especially, cumulative acknowledgements, we had to reconsider almost every aspect of TCP’s design. The resulting protocol sheds light on how congestion control interacts with unreliable transport, how modern network constraints impact protocol design, and how TCP’s reliable bytestream semantics intertwine with its other mechanisms, including congestion control."
631,"the design of browsing and berrypicking techniques for the online search interface",300569,"The Design of Browsing and Berrypicking Techniques for the Online Search Interface","First, a new model of searching in online and other information systems, called ""berrypicking,"" is discussed. This model, it is argued, is much closer to the real behavior of information searchers than the traditional model of information retrieval is, and, consequently, will guide our thinking better in the design of effective interfaces. Second, the research literature of manual information seeking behavior is drawn on for suggestions of capabilities that users might like to have in online systems. Third, based on the new model and the research on information seeking, suggestions are made for how new search capabilities could be incorporated into the design of search interfaces. Particular attention is given to the nature and types of browsing that can be facilitated."
632,"information seeking behavior of scientists in the electronic information age astronomers chemists mathematicians and physicists",300647,"Information seeking behavior of scientists in the electronic information age: Astronomers, chemists, mathematicians, and physicists","The information seeking behavior of astronomers, chemists, mathematicians, and physicists at the University of Oklahoma was assessed using an electronically distributed questionnaire. All of the scientists surveyed relied greatly on the journal literature to support their research and creative activities. The mathematicians surveyed indicated an additional reliance on monographs, preprints, and attendance at conferences and personal communication to support their research activities. Similarly, all scientists responding scanned the latest issues of journals to keep abreast of current developments in their fields, with the mathematicians again reporting attendance at conferences and personal communication. Despite an expression by the scientists for more electronic services, the majority preferred access to journal articles in a print, rather than an electronic, form. The primary deficit in library services appeared to be in access to electronic bibliographic databases. The data suggest that a primary goal of science libraries is to obtain access to as many appropriate electronic bibliographic finding aids and databases possible. Although the results imply the ultimate demise of the printed bibliographic reference tool, they underscore the continued importance to scientists of the printed peer-reviewed journal article."
633,"communities of practice foucault and actornetwork therory",302103,"Communities Of Practice, Foucault And Actor-Network Therory","			The paper discusses some of the main contributions to the theory of communities of practice (COP theory), especially as it relates to organizational learning. The paper does not attempt a full overview but concentrates on the notion of power relations. Early COP theory was formulated as part of situated learning theory, and promised to work on issues of social context and unequal power relations. Foucault&#146;s work and actor-network theory (ANT) is introduced and forms the basis of a constructive critique of COP theory. The paper argues that COP theory and ANT can enrich each other and together make a stronger contribution to our understanding of organizational learning. Specifically, these perspectives question the value in viewing organizations as formal, canonical entities as far as learning and change are concerned."
634,"advanced spectral methods for climatic time series",303461,"Advanced spectral methods for climatic time series","The analysis of univariate or multivariate time series provides crucial information to describe, understand, and predict climatic variability. The discovery and implementation of a number of novel methods for extracting useful information from time series has recently revitalized this classical field of study. Considerable progress has also been made in interpreting the information so obtained in terms of dynamical systems theory. In this review we describe the connections between time series analysis and nonlinear dynamics, discuss signal-to-noise enhancement, and present some of the novel methods for spectral analysis. The various steps, as well as the advantages and disadvantages of these methods, are illustrated by their application to an important climatic time series, the Southern Oscillation Index. This index captures major features of interannual climate variability and is used extensively in its prediction. Regional and global sea surface temperature data sets are used to illustrate multivariate spectral methods. Open questions and further prospects conclude the review."
635,"could information theory provide an ecological theory of sensory processing",304987,"Could information theory provide an ecological theory of sensory processing?","The sensory pathways of animals are well adapted to processing a special class of signals, namely stimuli from the animal's environment. An important fact about natural stimuli is that they are typically very redundant and hence the sampled representation of these signals formed by the array of sensory cells is inefficient. One could argue for some animals and pathways, as the author does in this review, that efficiency of information representation in the nervous system has several evolutionary advantages. Consequently, one might expect that much of the processing in the early levels of these sensory pathways could be dedicated towards recording incoming signals into a more efficient form. The author explores the principle of efficiency of information representation as a design principle for sensory processing. He gives a preliminary discussion on how this principle could be applied in general to predict neural processing and then discuss concretely some neural systems where it recently has been shown to be successful. In particular, he examines the fly's LMC coding strategy and the mammalian retinal coding in the spatial, temporal and chromatic domains."
636,"fast and loose reasoning is morally correct",305371,"Fast and Loose Reasoning Is Morally Correct","Functional programmers often reason about programs as if they were written in a total language, expecting the results to carry over to non-total (partial) languages. We justify such reasoning.Two languages are defined, one total and one partial, with identical syntax. The semantics of the partial language includes partial and infinite values, and all types are lifted, including the function spaces. A partial equivalence relation (PER) is then defined, the domain of which is the total subset of the partial language. For types not containing function spaces the PER relates equal values, and functions are related if they map related values to related values.It is proved that if two closed terms have the same semantics in the total language, then they have related semantics in the partial language. It is also shown that the PER gives rise to a bicartesian closed category which can be used to reason about values in the domain of the relation."
637,"gdel escher bach an eternal golden braid",305936,"Gödel, Escher, Bach: An Eternal Golden Braid","{Twenty years after it topped the bestseller charts, Douglas R. Hofstadter's <I>G&ouml;del, Escher, Bach: An Eternal Golden Braid</I> is still something of a marvel. Besides being a profound and entertaining meditation on human thought and creativity, this book looks at the surprising points of contact between the music of Bach, the artwork of Escher, and the mathematics of G&ouml;del. It also looks at the prospects for computers and artificial intelligence (AI) for mimicking human thought. For the general reader and the computer techie alike, this book still sets a standard for thinking about the future of computers and their relation to the way we think.<p> Hofstadter's great achievement in <I>G&ouml;del, Escher, Bach</I> was making abstruse mathematical topics (like undecidability, recursion, and 'strange loops') accessible and remarkably entertaining. Borrowing a page from Lewis Carroll (who might well have been a fan of this book), each chapter presents dialogue between the Tortoise and Achilles, as well as other characters who dramatize concepts discussed later in more detail. Allusions to Bach's music (centering on his <I>Musical Offering</I>) and Escher's continually paradoxical artwork are plentiful here. This more approachable material lets the author delve into serious number theory (concentrating on the ramifications of G&ouml;del's Theorem of Incompleteness) while stopping along the way to ponder the work of a host of other mathematicians, artists, and thinkers.<p> The world has moved on since 1979, of course. The book predicted that computers probably won't ever beat humans in chess, though Deep Blue beat Garry Kasparov in 1997. And the vinyl record, which serves for some of Hofstadter's best analogies, is now left to collectors. Sections on recursion and the graphs of certain functions from physics look tantalizing, like the fractals of recent chaos theory. And AI has moved on, of course, with mixed results. Yet <I>G&ouml;del, Escher, Bach</I> remains a remarkable achievement. Its intellectual range and ability to let us visualize difficult mathematical concepts help make it one of this century's best for anyone who's interested in computers and their potential for <I>real</I> intelligence. <I>--Richard Dragan</I><p> <B>Topics Covered</B>: J.S. Bach, M.C. Escher, Kurt G&ouml;del: biographical information and work, artificial intelligence (AI) history and theories, strange loops and tangled hierarchies, formal and informal systems, number theory, form in mathematics, figure and ground, consistency, completeness, Euclidean and non-Euclidean geometry, recursive structures, theories of meaning, propositional calculus, typographical number theory, Zen and mathematics, levels of description and computers; theory of mind: neurons, minds and thoughts; undecidability; self-reference and self-representation; Turing test for machine intelligence.} {Winner of the Pulitzer Prize, this book applies Godel's seminal contribution to modern mathematics to the study of the human mind and the development of artificial intelligence.}"
638,"dynamic causal modelling",306080,"Dynamic causal modelling.","In this paper we present an approach to the identification of nonlinear input–state–output systems. By using a bilinear approximation to the dynamics of interactions among states, the parameters of the implicit causal model reduce to three sets. These comprise (1) parameters that mediate the influence of extrinsic inputs on the states, (2) parameters that mediate intrinsic coupling among the states, and (3) [bilinear] parameters that allow the inputs to modulate that coupling. Identification proceeds in a Bayesian framework given known, deterministic inputs and the observed responses of the system. We developed this approach for the analysis of effective connectivity using experimentally designed inputs and fMRI responses. In this context, the coupling parameters correspond to effective connectivity and the bilinear parameters reflect the changes in connectivity induced by inputs. The ensuing framework allows one to characterise fMRI experiments, conceptually, as an experimental manipulation of integration among brain regions (by contextual or trial-free inputs, like time or attentional set) that is revealed using evoked responses (to perturbations or trial-bound inputs, like stimuli). As with previous analyses of effective connectivity, the focus is on experimentally induced changes in coupling (cf., psychophysiologic interactions). However, unlike previous approaches in neuroimaging, the causal model ascribes responses to designed deterministic inputs, as opposed to treating inputs as unknown and stochastic."
639,"a delay tolerant network architecture for challenged internets",307311,"A Delay Tolerant Network Architecture for Challenged Internets","The highly successful architecture and protocols of today's Internet may operate poorly in environments characterized by very long delay paths and frequent network partitions. These problems are exacerbated by end nodes with limited power or memory resources. Often deployed in mobile and extreme environments lacking continuous connectivity, many such networks have their own specialized protocols, and do not utilize IP. To achieve interoperability between them, we propose a network architecture..."
640,"how practice matters a relational view of knowledge sharing",307623,"How practice matters: a relational view of knowledge sharing","This paper addresses the issue of knowledge sharing practices in complex organizations. The authors propose that a refined understanding of the relational thinking underpinning practice theories is required if we want to further our comprehension of knowledge sharing and distinguish existing approaches. Knowledge sharing, we argue, is defined by the specific differences and dependencies in practices existing within or across communities. Changes in those differences and dependencies leads to the formation of new knowledge. Specifying the differences, dependencies and changes provides the first analytical step in understanding knowledge sharing as it takes shape in and across communities of practice. The authors apply this relational perspective to probe the discrepancies and complementarities among three seminal approaches to knowing within and across communities of practice."
641,"drawing graphs nicely using simulated annealing",307877,"Drawing graphs nicely using simulated annealing","The paradigm of simulated annealing is applied to the problem of drawing graphs &ldquo;nicely.&rdquo; Our algorithm deals with general undirected graphs with straight-line edges, and employs several simple criteria for the aesthetic quality of the result. The algorithm is flexible, in that the relative weights of the criteria can be changed. For graphs of modest size it produces good results, competitive with those produced by other methods, notably, the &ldquo;spring method&rdquo; and its variants."
642,"a new structurally nonredundant diverse data set of proteinprotein interfaces and its implications",308885,"A new, structurally nonredundant, diverse data set of protein-protein interfaces and its implications.","Here, we present a diverse, structurally nonredundant data set of two-chain protein-protein interfaces derived from the PDB. Using a sequence order-independent structural comparison algorithm and hierarchical clustering, 3799 interface clusters are obtained. These yield 103 clusters with at least five nonhomologous members. We divide the clusters into three types. In Type I clusters, the global structures of the chains from which the interfaces are derived are also similar. This cluster type is expected because, in general, related proteins associate in similar ways. In Type II, the interfaces are similar; however, remarkably, the overall structures and functions of the chains are different. The functional spectrum is broad, from enzymes/inhibitors to immunoglobulins and toxins. The fact that structurally different monomers associate in similar ways, suggests ""good"" binding architectures. This observation extends a paradigm in protein science: It has been well known that proteins with similar structures may have different functions. Here, we show that it extends to interfaces. In Type III clusters, only one side of the interface is similar across the cluster. This structurally nonredundant data set provides rich data for studies of protein-protein interactions and recognition, cellular networks and drug design. In particular, it may be useful in addressing the difficult question of what are the favorable ways for proteins to interact. (The data set is available at http://protein3d.ncifcrf.gov/~keskino/ and http://home.ku.edu.tr/~okeskin/INTERFACE/INTERFACES.html.)"
643,"ecosystem size determines foodchain length in lakes",309243,"Ecosystem size determines food-chain length in lakes","Food-chain length is an important characteristic of ecological communities(1) : it influences community structure(2), ecosystem functions(1-4) and contaminant concentrations in top predators(5,6). Since Elton(7) first noted that food-chain length was variable among natural systems, ecologists have considered many explanatory hypotheses(1,4,8,9), but few are supported by empirical evidence(4,10,11). Here we test three hypotheses that predict food-chain length to be determined by productivity alone (productivity hypothesis)(4,10,12,13), ecosystem size alone (ecosystem-size hypothesis)(14,15) or a combination of productivity and ecosystem size (productive-space hypothesis)(7,16-18). The productivity and productive-space hypotheses propose that food-chain length should increase with increasing resource availability; however, the productivity hypothesis does not include ecosystem size as a determinant of resource availability. The ecosystem-size hypothesis is based on the relationship between ecosystem size and species diversity, habitat availability and habitat heterogeneity(14,15). We find that food-chain length increases with ecosystem size, but that the length of the food chain is not related to productivity. Our results support the hypothesis that ecosystem size, and not resource availability, determines food-chain length in these natural ecosystems."
644,"prokaryotes the unseen majority",309283,"Prokaryotes: The unseen majority","The number of prokaryotes and the total amount of their cellular carbon on earth are estimated to be 4–6 × 1030 cells and 350–550 Pg of C (1 Pg = 1015 g), respectively. Thus, the total amount of prokaryotic carbon is 60–100% of the estimated total carbon in plants, and inclusion of prokaryotic carbon in global models will almost double estimates of the amount of carbon stored in living organisms. In addition, the earth's prokaryotes contain 85–130 Pg of N and 9–14 Pg of P, or about 10-fold more of these nutrients than do plants, and represent the largest pool of these nutrients in living organisms. Most of the earth's prokaryotes occur in the open ocean, in soil, and in oceanic and terrestrial subsurfaces, where the numbers of cells are 1.2 × 1029, 2.6 × 1029, 3.5 × 1030, and 0.25–2.5 × 1030, respectively. The numbers of heterotrophic prokaryotes in the upper 200 m of the open ocean, the ocean below 200 m, and soil are consistent with average turnover times of 6–25 days, 0.8 yr, and 2.5 yr, respectively. Although subject to a great deal of uncertainty, the estimate for the average turnover time of prokaryotes in the subsurface is on the order of 1–2 × 103 yr. The cellular production rate for all prokaryotes on earth is estimated at 1.7 × 1030 cells/yr and is highest in the open ocean. The large population size and rapid growth of prokaryotes provides an enormous capacity for genetic diversity."
645,"boostexter a boostingbased system for text categorization",309498,"{BoosTexter}: A Boosting-based System for Text Categorization","This work focuses on algorithms which learn from examples to perform multiclass text and speech categorization tasks. Our approach is based on a new and improved family of boosting algorithms. We describe in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks. We present results comparing the performance of BoosTexter and a number of other text-categorization algorithms on a variety of tasks. We conclude by describing the application of our system to automatic call-type identification from unconstrained spoken customer responses."
646,"coevolution of gene expression among interacting proteins",310608,"Coevolution of gene expression among interacting proteins","10.1073/pnas.0402591101 Physically interacting proteins or parts of proteins are expected to evolve in a coordinated manner that preserves proper interactions. Such coevolution at the amino acid-sequence level is well documented and has been used to predict interacting proteins, domains, and amino acids. Interacting proteins are also often precisely coexpressed with one another, presumably to maintain proper stoichiometry among interacting components. Here, we show that the expression levels of physically interacting proteins coevolve. We estimate average expression levels of genes from four closely related fungi of the genus  using the codon adaptation index and show that expression levels of interacting proteins exhibit coordinated changes in these different species. We find that this coevolution of expression is a more powerful predictor of physical interaction than is coevolution of amino acid sequence. These results demonstrate that gene expression levels can coevolve, adding another dimension to the study of the coevolution of interacting proteins and underscoring the importance of maintaining coexpression of interacting proteins over evolutionary time. Our results also suggest that expression coevolution can be used for computational prediction of protein-protein interactions."
647,"the evolution of social and economic networks",310706,"The Evolution of Social and Economic Networks","We examine the dynamic formation and stochastic evolution of networks connecting individuals. The payoff to an individual from an economic or social activity depends on the network of connections among individuals. Over time individuals form and sever links connecting themselves to other individuals based on the improvement that the resulting network offers them relative to the current network. We call such sequences of networks, `improving paths,’ and show that such sequences can include cycles and study conditions on underlying allocation rules that characterize cycles. Building on the concept of improving paths, we consider a stochastic evolutionary process where in addition to intended changes in the network there is a small probability of unintended changes or errors. Predictions can be made regarding the relative likelihood that the stochastic process will lead to any given network at some time, and the evolutionary process selects from among the statically stable networks and cycles. We apply these results to a series of models including the Gale-Shapley marriage market. In some cases, the evolutionary process selects only inefficient networks even though there are efficient networks that are statically stable, and moreover even if interventions that reallocate value are admissible."
648,"on the variability of manual spike sorting",311031,"On the variability of manual spike sorting.","The analysis of action potentials, or spikes, is central to systems neuroscience research. Spikes are typically identified from raw waveforms manually for off-line analysis or automatically by human-configured algorithms for on-line applications. The variability of manual spike ?sorting? is studied and its implications for neural prostheses discussed. Waveforms were recorded using a micro-electrode array and were used to construct a statistically similar synthetic dataset. Results showed wide variability in the number of neurons and spikes detected in real data. Additionally, average error rates of 23\\% false positive and 30\\% false negative were found for synthetic data."
649,"efficiency and ambiguity in an adaptive neural code",311226,"Efficiency and ambiguity in an adaptive neural code","We examine the dynamics of a neural code in the context of stimuli whose statistical properties are themselves evolving dynamically. Adaptation to these statistics occurs over a wide range of timescales—from tens of milliseconds to minutes. Rapid components of adaptation serve to optimize the information that action potentials carry about rapid stimulus variations within the local statistical ensemble, while changes in the rate and statistics of action-potential firing encode information about the ensemble itself, thus resolving potential ambiguities. The speed with which information is optimized and ambiguities are resolved approaches the physical limit imposed by statistical sampling and noise."
650,"information flow in social groups",311564,"Information Flow in Social Groups","We present a study of information flow that takes into account the observation that an item relevant to one person is more likely to be of interest to individuals in the same social circle than those outside of it. This is due to the fact that the similarity of node attributes in social networks decreases as a function of the graph distance. An epidemic model on a scale-free network with this property has a finite threshold, implying that the spread of information is limited. We tested our predictions by measuring the spread of messages in an organization and also by numerical experiments that take into consideration the organizational distance among individuals."
651,"automated reasoning on feature models",312040,"Automated Reasoning on Feature Models","Software Product Line (SPL) Engineering has proved to be an effective method for software production. However, in the SPL community it is well recognized that variability in SPLs is increasing by the thousands. Hence, an automatic support is needed to deal with variability in SPL. Most of the current proposals for automatic reasoning on SPL are not devised to cope with extra–functional features. In this paper we introduce a proposal to model and reason on an SPL using constraint programming. We take into account functional and extra–functional features, improve current proposals and present a running, yet feasible implementation."
652,"formalizing cardinalitybased feature models and their specialization",312061,"Formalizing cardinality-based feature models and their specialization.","Feature modeling is an important approach to capture the commonalities and variabilities in system families and product lines. Cardinality-based feature modeling integrates a number of existing extensions of the original feature-modeling notation from Feature-Oriented Domain Analysis. Staged configuration is a process that allows the incremental configuration of cardinality-based feature models. It can be achieved by performing a step-wise specialization of the feature model. In this article, we argue that cardinality-based feature models can be interpreted as a special class of context-free grammars. We make this precise by specifying a translation from a feature model into a context-free grammar. Consequently, we provide a semantic interpretation for cardinality-based feature models by assigning an appropriate semantics to the language recognized by the corresponding grammar. Finally, we give an account on how feature model specialization can be formalized as transformations on the grammar equivalent of feature models. Copyright © 2005 John Wiley & Sons, Ltd."
653,"domainspecific language design requires feature descriptions",312063,"Domain--Specific Language Design Requires Feature Descriptions","A domain-specific language (DSL) provides a notation tailored towards an application domain and is based on the relevant concepts and features of that domain. As such, a DSL is a means to describe and generate members of a family of programs in the domain. A prerequisite for the design of a DSL is a detailed analysis and structuring of the application domain. Graphical feature diagrams have been proposed to organize the dependencies between such features, and to indicate which ones are common to all family members and which ones vary. In this paper, we study feature diagrams in more details, as well as their relationship to domain-specific languages. We propose the Feature Description Language (FDL), a textual language to describe features. We explore automated manipulation of feature descriptions such as normalization, expansion to disjunctive normal form, variability computation and constraint satisfaction. Feature descriptions can be directly mapped to UML diagrams which in their turn can be used for Java code generation. The value of FDL is assessed via a case study in the use and expressiveness of feature descriptions for the area of documentation generators."
654,"integrating feature modeling with the rseb",312073,"Integrating Feature Modeling with the {RSEB}","We have integrated the feature modeling of Feature-Oriented Domain Analysis (FODA) into the processes and work products of the Reuse-Driven Software Engineering Business (RSEB). The RSEB is a use case driven systematic reuse process: architecture and reusable subsystems are first described by use cases and then transformed into object models that are traceable to these use cases. Variability in the RSEB is captured by structuring use case and object models using explicit variation points and variants. Traditional domain engineering steps have been distributed into the steps of the architectural and component system development methods of the RSEB. But the RSEB prescribes no explicit models of the essential features that characterize the different versions. Building on our experience in applying FODA and RSEB to the telecom domain, we have added explicit domain engineering steps and an explicit feature model to the RSEB to support domain engineering and component reuse. These additions provide an effective reuse oriented model as a `catalog' capability to link use cases, variation points, reusable components and configured applications"
655,"programming with constraints an introduction",312090,"Programming with Constraints: An Introduction","The job of the constraint programmer is to use mathematical constraints to model real world constraints and objects. In this book, Kim Marriott and Peter Stuckey provide the first comprehensive introduction to the discipline of constraint programming and, in particular, constraint logic programming. The book covers the necessary background material from artificial intelligence, logic programming, operations research, and mathematical programming. Topics discussed range from constraint-solving techniques to programming methodologies for constraint programming languages. Because there is not yet a universally used syntax for constraint logic programming languages, the authors present the programs in a way that is independent of any existing programming language. Practical exercises cover how to use the book with a number of existing constraint languages."
656,"foundations of constraint satisfaction",312115,"Foundations of Constraint Satisfaction","Constraint satisfaction is a general problem in which the goal is to find values for a set of variables that will satisfy a given set of constraints. It is the core of many applications in artificial intelligence, and has found its application in many areas, such as planning and scheduling. Because of its generality, most AI researchers should be able to benefit from having good knowledge of techniques in this field. This book is the most comprehensive book on the field of constraint satisfaction so far. It covers both the theoretical and the implemenatation aspects of the subject. It provides a framework for studying this field, relates different research, and resolves ambiguity in a number of concepts and algorithms in the literature. This book provides a solid foundation for researchers in this field. It is also an invaluable text for graduate and research level students in cognitive science and artificial intelligence."
657,"improving recommendation lists through topic diversification",312817,"Improving recommendation lists through topic diversification","In this work we present topic diversi&#64257;cation, a novel method designed to balance and diversify personalized recommenda- tion lists in order to re&#64258;ect the user?s complete spectrum of interests. Though being detrimental to average accuracy, we show that our method improves user satisfaction with rec- ommendation lists, in particular for lists generated using the common item-based collaborative &#64257;ltering algorithm. Our work builds upon prior research on recommender sys- tems, looking at properties of recommendation lists as en- tities in their own right rather than speci&#64257;cally focusing on the accuracy of individual recommendations. We introduce the intra-list similarity metric to assess the topical diver- sity of recommendation lists and the topic diversi&#64257;cation approach for decreasing the intra-list similarity. We evalu- ate our method using book recommendation data, including o&#64260;ine analysis on 361, 349 ratings and an online study in- volving more than 2, 100 subjects."
658,"probabilistic and statistical properties of words an overview",315482,"Probabilistic and Statistical Properties of Words: An Overview","In the following, an overview is given on statistical and probabilistic properties of words, as occurring in the analysis of biological sequences. Counts of occurrence, counts of clumps, and renewal counts are distinguished, and exact distributions as well as normal approximations, Poisson process approximations, and compound Poisson approximations are derived. Here, a sequence is modelled as a stationary ergodic Markov chain; a test for determining the appropriate order of the Markov chain is described. The convergence results take the error made by estimating the Markovian transition probabilities into account. The main tools involved are moment generating functions, martingales, Stein's method, and the Chen-Stein method. Similar results are given for occurrences of multiple patterns, and, as an example, the problem of unique recoverability of a sequence from SBH chip data is discussed. Special emphasis lies on disentangling the complicated dependence structure between word occurrences, due to self-overlap as well as due to overlap between words. The results can be used to derive approximate, and conservative, confidence intervals for tests."
659,"a simple method for displaying the hydropathic character of a protein",315520,"{A simple method for displaying the hydropathic character of a protein.}","A computer program that progressively evaluates the hydrophilicity and hydrophobicity of a protein along its amino acid sequence has been devised. For this purpose, a hydropathy scale has been composed wherein the hydrophilic and hydrophobic properties of each of the 20 amino acid side-chains is taken into consideration. The scale is based on an amalgam of experimental observations derived from the literature. The program uses a moving-segment approach that continuously determines the average hydropathy within a segment of predetermined length as it advances through the sequence. The consecutive scores are plotted from the amino to the carboxy terminus. At the same time, a midpoint line is printed that corresponds to the grand average of the hydropathy of the amino acid compositions found in most of the sequenced proteins. In the case of soluble, globular proteins there is a remarkable correspondence between the interior portions of their sequence and the regions appearing on the hydrophobic side of the midpoint line, as well as the exterior portions and the regions on the hydrophilic side. The correlation was demonstrated by comparisons between the plotted values and known structures determined by crystallography. In the case of membrane-bound proteins, the portions of their sequences that are located within the lipid bilayer are also clearly delineated by large uninterrupted areas on the hydrophobic side of the midpoint line. As such, the membrane-spanning segments of these proteins can be identified by this procedure. Although the method is not unique and embodies principles that have long been appreciated, its simplicity and its graphic nature make it a very useful tool for the evaluation of protein structures."
660,"methods for assessing the statistical significance of molecular sequence features by using general scoring schemes",315523,"Methods for assessing the statistical significance of molecular sequence features by using general scoring schemes.","An unusual pattern in a nucleic acid or protein sequence or a region of strong similarity shared by two or more sequences may have biological significance. It is therefore desirable to know whether such a pattern can have arisen simply by chance. To identify interesting sequence patterns, appropriate scoring values can be assigned to the individual residues of a single sequence or to sets of residues when several sequences are compared. For single sequences, such scores can reflect biophysical properties such as charge, volume, hydrophobicity, or secondary structure potential; for multiple sequences, they can reflect nucleotide or amino acid similarity measured in a wide variety of ways. Using an appropriate random model, we present a theory that provides precise numerical formulas for assessing the statistical significance of any region with high aggregate score. A second class of results describes the composition of high-scoring segments. In certain contexts, these permit the choice of scoring systems which are ""optimal"" for distinguishing biologically relevant patterns. Examples are given of applications of the theory to a variety of protein sequences, highlighting segments with unusual biological features. These include distinctive charge regions in transcription factors and protooncogene products, pronounced hydrophobic segments in various receptor and transport proteins, and statistically significant subalignments involving the recently characterized cystic fibrosis gene."
661,"versatile and open software for comparing large genomes",315559,"{Versatile and open software for comparing large genomes.}","The newest version of MUMmer easily handles comparisons of large eukaryotic genomes at varying evolutionary distances, as demonstrated by applications to multiple genomes. Two new graphical viewing tools provide alternative ways to analyze genome alignments. The new system is the first version of MUMmer to be released as open-source software. This allows other developers to contribute to the code base and freely redistribute the code. The MUMmer sources are available at http://www.tigr.org/software/mummer webcite."
662,"query recommendation using query logs in search engines",317978,"Query Recommendation using Query Logs in Search Engines","In this paper we propose a method that, given a query submitted to a search engine, suggests a list of related queries. The related queries are based in previously issued queries, and can be issued by the user to the search engine to tune or redirect the search process. The method proposed is based on a query clustering process in which groups of semantically similar queries are identified. The clustering process uses the content of historical preferences of users registered in the query log of the search engine. The method not only discovers the related queries, but also ranks them according to a relevance criterion. Finally, we show with experiments over the query log of a search engine the effectiveness of the method."
663,"correlations without synchrony",319962,"Correlations Without Synchrony","Peaks in spike train correlograms are usually taken as indicative of spike timing synchronization between neurons. Strictly speaking, however, a peak merely indicates that the two spike trains were not independent. Two biologically plausible ways of departing from independence that are capable of generating peaks very similar to spike timing peaks are described here: covariations over trials in response latency and covariations over trials in neuronal excitability. Since peaks due to these interactions can be similar to spike timing peaks, interpreting a correlogram may be a problem with ambiguous solutions. What peak shapes do latency or excitability interactions generate? When are they similar to spike timing peaks? When can they be ruled out from having caused an observed correlogram peak? These are the questions addressed here. The previous article in this issue proposes quantitative methods to tell cases apart when latency or excitability covariations cannot be ruled out."
664,"a multilevel analysis of sociability usability and community dynamics in an online health community",320258,"A multilevel analysis of sociability, usability, and community dynamics in an online health community","The aim of this research is to develop an in-depth understanding of the dynamics of online group interaction and the relationship between the participation in an online community and an individual's off-line life. The 2&half;-year study of a thriving online health support community (Bob's ACL WWWBoard) used a broad fieldwork approach, guided by the ethnographic research techniques of observation, interviewing, and archival research in combination with analysis of the group's dynamics during a one-week period. Research tools from the social sciences were used to develop a thick, rich description of the group. The significant findings of this study include: dependable and reliable technology is more important than state-of-the-art technology in this community; strong community development exists despite little differentiation of the community space provided by the software; members reported that participation in the community positively influenced their offline lives; strong group norms of support and reciprocity made externally-driven governance unnecessary; tools used to assess group dynamics in face-to-face groups provide meaningful information about online group dynamics; and, membership patterns in the community and strong subgroups actively contributed to the community's stability and vitality."
665,"calculation of conformational ensembles from potentials of mean force an approach to the knowledgebased prediction of local structures in globular proteins",321328,"Calculation of conformational ensembles from potentials of mean force. An approach to the knowledge-based prediction of local structures in globular proteins.","We present a prototype of a new approach to the folding problem of polypeptide chains. This approach is based on the analysis of known protein structures. It derives the energy potentials for the atomic interactions of all amino acid residue pairs as a function of the distance between the involved atoms. These potentials are then used to calculate the energies of all conformations that exist in the data base with respect to a given sequence. Then, by using only the most stable conformations, clusters of the most probable conformations for the given sequence are obtained. To discuss the results properly we introduce a new classification of segments based on their conformational stability. Special care is taken to allow for sparse data sets. The use of the method is demonstrated in the discussion of the identical oligopeptide sequences found in different conformations in unrelated proteins. {VNTFV,} for example, adopts a beta-strand in ribonuclease but it is found in an alpha-helical conformation in erythrocruorin. In the case of {VNTFV} the ensemble obtained consists of a single cluster of beta-strand conformations, indicating that this may be the preferred conformation for the pentapeptide. When the flanking residues are included in the calculation the hepapeptide {P-VNTFV-H} (ribonuclease) again yields an ensemble of beta-strands. However, in the ensemble of {D-VNTFV-A} (erythrocruorin) the major cluster is of alpha-helical type. In the present study we concentrate on the local aspects of protein conformations. However, the theory presented is quite general and not restricted to oligopeptides. We indicate extensions of the approach to the calculation of global conformations of proteins as well as conceivable applications to a number of molecular systems."
666,"how buildings learn what happens after theyre built",323050,"How Buildings Learn: What Happens After They're Built","{All buildings are forced to adapt over time because of physical deterioration, changing surroundings and the life within--yet very few buildings adapt gracefully, according to Brand. Houses, he notes, respond to families' tastes, ideas, annoyance and growth; and institutional buildings change with expensive reluctance and delay; while commercial structures have to adapt quickly because of intense competitive pressures. Creator of The Whole Earth Catalog and founder of CoEvolution Quarterly (now Whole Earth Review ), Brand splices a conversational text with hundreds of extensively captioned photographs and drawings juxtaposing buildings that age well with those that age poorly. He buttresses his critique with insights gleaned from facilities managers, planners, preservationists, building historians and futurists. This informative, innovative handbook sets forth a strategy for constructing adaptive buildings that incorporates a conservationist approach to design, use of traditional materials, attention to local vernacular styles and budgeting to allow for continuous adjustment and maintenance.}"
667,"fundamentals of cdna microarray data analysis",325139,"Fundamentals of cDNA microarray data analysis.","Microarray technology is a powerful approach for genomics research. The multi-step, data-intensive nature of this technology has created an unprecedented informatics and analytical challenge. It is important to understand the crucial steps that can affect the outcome of the analysis. In this review, we provide an overview of the contemporary trend on various main analysis steps in the microarray data analysis process, which includes experimental design, data standardization, image acquisition and analysis, normalization, statistical significance inference, exploratory data analysis, class prediction and pathway analysis, as well as various considerations relevant to their implementation."
668,"distributed realtime computation of community preferences",325817,"Distributed, real-time computation of community preferences","We describe the integration of smart digital objects with Hebbian learning to create a distributed, real-time, scalable approach to adapting to a community's preferences. We designed an experiment using popular music as the subject matter. Each digital object corresponded to a music album and contained links to other music albums. By dynamically generating links among digital objects according to user traversal patterns, then hierarchically organizing these links according to shared metadata values, we created a network of digital objects that self-organized in real-time according to the preferences of the user community. Furthermore, the similarity between user preferences and generated link structure was more pronounced between collections of objects aggregated by shared metadata values."
669,"comparative analysis of algorithms for identifying amplifications and deletions in array cgh data",326502,"Comparative analysis of algorithms for identifying amplifications and deletions in array CGH data.","MOTIVATION: Array Comparative Genomic Hybridization (CGH) can reveal chromosomal aberrations in the genomic DNA. These amplifications and deletions at the DNA level are important in the pathogenesis of cancer and other diseases. While a large number of approaches have been proposed for analyzing the large array CGH datasets, the relative merits of these methods in practice are not clear. RESULTS: We compare 11 different algorithms for analyzing array CGH data. These include both segment detection methods and smoothing methods, based on diverse techniques such as mixture models, Hidden Markov Models, maximum likelihood, regression, wavelets and genetic algorithms. We compute the Receiver Operating Characteristic (ROC) curves using simulated data to quantify sensitivity and specificity for various levels of signal-to-noise ratio and different sizes of abnormalities. We also characterize their performance on chromosomal regions of interest in a real dataset obtained from patients with Glioblastoma Multiforme. While comparisons of this type are difficult due to possibly sub-optimal choice of parameters in the methods, they nevertheless reveal general characteristics that are helpful to the biological investigator."
670,"a physical basis for protein secondary structure",326547,"{A physical basis for protein secondary structure}","A physical theory of protein secondary structure is proposed and tested by performing exceedingly simple Monte Carlo simulations. In essence, secondary structure propensities are predominantly a consequence of two competing local effects, one favoring hydrogen bond formation in helices and turns; the other opposing the attendant reduction in sidechain conformational entropy on helix and turn formation. These sequence specific biases are densely dispersed throughout the unfolded polypeptide chain, where they serve to preorganize the folding process and largely, but imperfectly, anticipate the native secondary structure. [Journal Article; 55 Refs; In English; Summary in English]"
671,"evolution strategies a comprehensive introduction",326870,"Evolution strategies - A comprehensive introduction","This article gives a comprehensive introduction into one of the main branches of evolutionary computation – the evolution strategies (ES) the history of which dates back to the 1960s in Germany. Starting from a survey of history the philosophical background is explained in order to make understandable why ES are realized in the way they are. Basic ES algorithms and design principles for variation and selection operators as well as theoretical issues are presented, and future branches of ES research are discussed."
672,"lattice boltzmann method for fluid flows",327100,"Lattice Boltzmann method for fluid flows","▪ Abstract  We present an overview of the lattice Boltzmann method (LBM), a parallel and efficient algorithm for simulating single-phase and multiphase fluid flows and for incorporating additional physical complexities. The LBM is especially useful for modeling complicated boundary conditions and multiphase interfaces. Recent extensions of this method are described, including simulations of fluid turbulence, suspension flows, and reaction diffusion systems."
673,"cost models for future software life cycle processes cocomo",327306,"Cost Models for Future Software Life Cycle Processes: COCOMO 2.0","Current software cost estimation models, such as the 1981 Constructive Cost Model (COCOMO) for software cost estimation and its 1987 Ada COCOMO update, have been experiencing increasing difficulties in estimating the costs of software developed to new life cycle processes and capabilities. These include non-sequential and rapid-development process models; reuse-driven approaches involving commercial off-the-shelf (COTS) packages, re-engineering, applications composition, and applications generation capabilities; object-oriented approaches supported by distributed middleware; and software process maturity initiatives. This paper summarizes research in deriving a baseline COCOMO 2.0 model tailored to these new forms of software development, including rationale for the model decisions. The major new modeling capabilities of COCOMO 2.0 are a tailorable family of software sizing models, involving Object Points, Function Points, and Source Lines of Code; nonlinear models for software reuse and re-engineering; an exponentdriver approach for modeling relative software diseconomies of scale; and several additions, deletions and updates to previous COCOMO effort-multiplier cost drivers. This model is serving as a framework for an extensive current data collection and analysis effort to further refine and calibrate the model's estimation capabilities."
674,"swrl a semantic web rule language combining owl and ruleml",327800,"SWRL: A Semantic Web Rule Language Combining OWL and RuleML","This document contains a proposal for a Semantic Web Rule Language (SWRL) based on a combination of the OWL DL and OWL Lite sublanguages of the OWL Web Ontology Language with the Unary/Binary Datalog RuleML sublanguages of the Rule Markup Language. SWRL includes a high-level abstract syntax for Horn-like rules in both the OWL DL and OWL Lite sublanguages of OWL. A model-theoretic semantics is given to provide the formal meaning for OWL ontologies including rules written in this abstract syntax. An XML syntax based on RuleML and the OWL XML Presentation Syntax as well as an RDF concrete syntax based on the OWL RDF/XML exchange syntax are also given, along with several examples."
675,"statistical signals in bioinformatics",327974,"Statistical signals in bioinformatics.","Contributed by Samuel Karlin, July 7, 2005The Arthur M. Sackler Colloquium of the National Academy of Sciences, ""Frontiers in Bioinformatics: Unsolved Problems and Challenges,"" organized by David Eisenberg, Russ Altman, and myself, was held October 15-17, 2004, to provide a forum for discussing concepts and methods in bioinformatics serving the biological and medical sciences. The deluge of genomic and proteomic data in the last two decades has driven the creation of tools that search and analyze biomolecular sequences and structures. Bioinformatics is highly interdisciplinary, using knowledge from mathematics, statistics, computer science, biology, medicine, physics, chemistry, and engineering."
676,"search and replication in unstructured peertopeer networks",328154,"Search and Replication in Unstructured Peer-to-Peer networks","Decentralized and unstructured peer-to-peer networks such as Gnutella are attractive for certain applications because they require no centralized directories and no precise control over network topology or data placement. However, the flooding-based query algorithm used in Gnutella does not scale; each query generates a large amount of traffic and large systems quickly become overwhelmed by the query-induced load. This paper explores, through simulation, various alternatives to Gnutella's query algorithm, data replication strategy, and network topology. We propose a query algorithm based on multiple random walks that resolves queries almost as quickly as Gnutella's flooding method while reducing the network traffic by two orders of magnitude in many cases. We also present simulation results on a distributed replication strategy proposed in [8]. Finally, we find that among the various network topologies we consider, uniform random graphs yield the best performance."
677,"random walks in peertopeer networks",328165,"Random Walks in Peer-to-Peer Networks","We quantify the effectiveness of random walks for searching and construction of unstructured peer-to-peer (P2P) networks. We have identified two cases where the use of random walks for searching achieves better results than flooding: a) when the overlay topology is clustered, and h) when a client re-issues the same query while its horizon does not change much. For construction, we argue that an expander can he maintained dynamically with constant operations per addition. The key technical ingredient of our approach is a deep result of stochastic processes indicating that samples taken from consecutive steps of a random walk can achieve statistical properties similar to independent sampling (if the second eigenvalue of the transition matrix is hounded away from 1, which translates to good expansion of the network; such connectivity is desired, and believed to hold, in every reasonable network and network model). This property has been previously used in complexity theory for construction of pseudorandom number generators. We reveal another facet of this theory and translate savings in random bits to savings in processing overhead."
678,"phenotypic diversity population growth and information in fluctuating environments",331510,"Phenotypic diversity, population growth, and information in fluctuating environments.","Organisms in fluctuating environments must constantly adapt their behavior to survive. In clonal populations, this may be achieved through sensing followed by response or through the generation of diversity by stochastic phenotype switching. Here we show that stochastic switching can be favored over sensing when the environment changes infrequently. The optimal switching rates then mimic the statistics of environmental changes. We derive a relation between the long-term growth rate of the organism and the information available about its fluctuating environment. 10.1126/science.1114383"
679,"evolutionarily conserved pathways of energetic connectivity in protein families",332173,"Evolutionarily Conserved Pathways of Energetic Connectivity in Protein Families","For mapping energetic interactions in proteins, a technique was developed that uses evolutionary data for a protein family to measure statistical interactions between amino acid positions. {F}or the {PDZ} domain family, this analysis predicted a set of energetically coupled positions for a binding site residue that includes unexpected long-range interactions. {M}utational studies confirm these predictions, demonstrating that the statistical energy function is a good indicator of thermodynamic coupling in proteins. {S}ets of interacting residues form connected pathways through the protein fold that may be the basis for efficient energy conduction within proteins."
680,"microrna identification based on sequence and structure alignment",333232,"MicroRNA identification based on sequence and structure alignment","Motivation: MicroRNAs (miRNA) are [~]22 nt long non-coding RNAs that are derived from larger hairpin RNA precursors and play important regulatory roles in both animals and plants. The short length of the miRNA sequences and relatively low conservation of pre-miRNA sequences restrict the conventional sequence-alignment-based methods to finding only relatively close homologs. On the other hand, it has been reported that miRNA genes are more conserved in the secondary structure rather than in primary sequences. Therefore, secondary structural features should be more fully exploited in the homologue search for new miRNA genes.  Results: In this paper, we present a novel genome-wide computational approach to detect miRNAs in animals based on both sequence and structure alignment. Experiments show this approach has higher sensitivity and comparable specificity than other reported homologue searching methods. We applied this method on Anopheles gambiae and detected 59 new miRNA genes.  Availability: This program is available at http://bioinfo.au.tsinghua.edu.cn/miralign  Contact: daulyd@tsinghua.edu.cn  Supplementary information: Supplementary information is available at http://bioinfo.au.tsinghua.edu.cn/miralign/supplementary.htm 10.1093/bioinformatics/bti562"
681,"link mining a new data mining challenge",333373,"Link Mining: A New Data Mining Challenge","A key challenge for data mining is tackling the problem of mining richly structured datasets, where the objects are linked in some way. Links among the objects may demonstrate certain patterns, which can be helpful for many data mining tasks and are usually hard to capture with traditional statistical models. Recently there has been a surge of interest in this area, fueled largely by interest in web and hypertext mining, but also by interest in mining social networks, security and law enforcement data, bibliographic citations and epidemiological records."
682,"modelbased adaptation for selfhealing systems",333377,"Model-based adaptation for self-healing systems","Traditional mechanisms that allow a system to detect and recover from errors are typically wired into applications at the level of code where they are hard to change, reuse, or analyze. An alternative approach is to use externalized adaptation: one or more models of a system are maintained at run time and external to the application as a basis for identifying problems and resolving them. In this paper we provide an overview of recent research in which we use architectural models as the basis for such problem diagnosis and repair. These models can be specialized to the particular style of the system, the quality of interest, and the dimensions of run time adaptation that are permitted by the running system."
683,"multicast routing in datagram internetworks and extended lans",333928,"Multicast Routing in Datagram Internetworks and Extended LANs","Multicasting, the transmission of a packet to a  group  of hosts, is an important service for improving the efficiency and robustness of distributed systems and applications. Although multicast capability is available and widely used in local area networks, when those LANs are interconnected by store-and-forward routers, the multicast service is usually not offered across the resulting  internetwork . To address this limitation, we specify extensions to two common internetwork routing algorithms distance-vector routing and link-state routing to support low-delay datagram multicasting beyond a single LAN. We also describe modifications to the single-spanning-tree routing algorithm commonly used by link-layer bridges, to reduce the costs of multicasting in large extended LANs. Finally, we discuss how the use of multicast scope control and hierarchical multicast routing allows the multicast service to scale up to large internetworks."
684,"agile applicationaware adaptation for mobility",334010,"Agile Application-Aware Adaptation for Mobility","In this paper we show that application-aware adaptation, a collaborative partnership between the operating system and applications, offers the most general and effective approach to mobile information access. We describe the design of Odyssey, a prototype implementing this approach, and show how it supports concurrent execution of diverse mobile applications. We identify agility as a key attribute of adaptive systems, and describe how to quantify and measure it. We present the results of our..."
685,"service capacity of peer to peer networks",334077,"Service Capacity of Peer to Peer Networks","We study the 'service capacity' of peer to peer (P2P) file sharing applications. We begin by considering a transient regime which is key to capturing the ability of such systems to handle bursty traffic, e.g., flash crowds. In this context our models, based on age dependent branching processes, exhibit exponential growth in service capacity, and permit the study of sensitivity of this growth to system policies and parameters. Then we consider a model for such systems in steady state and show how the average delay seen by peers would scale in the offered load and rate at which peers exit the system. We find that the average delays scale well in the offered load. In particular the delays are upper bounded by some constant given any offered load and even decrease in the offered load if peers exit the system slowly. We validate many of our findings by analyzing traces obtained from a second generation P2P application called BitTorrent."
686,"towards a proteomescale map of the human proteinprotein interaction network",334264,"Towards a proteome-scale map of the human protein-protein interaction network.","Systematic mapping of protein-protein interactions, or 'interactome' mapping, was initiated in model organisms, starting with defined biological processes and then expanding to the scale of the proteome. Although far from complete, such maps have revealed global topological and dynamic features of interactome networks that relate to known biological properties, suggesting that a human interactome map will provide insight into development and disease mechanisms at a systems level. Here we describe an initial version of a proteome-scale map of human binary protein-protein interactions. Using a stringent, high-throughput yeast two-hybrid system, we tested pairwise interactions among the products of approximately 8,100 currently available Gateway-cloned open reading frames and detected approximately 2,800 interactions. This data set, called CCSB-HI1, has a verification rate of approximately 78% as revealed by an independent co-affinity purification assay, and correlates significantly with other biological attributes. The CCSB-HI1 data set increases by approximately 70% the set of available binary interactions within the tested space and reveals more than 300 new connections to over 100 disease-associated proteins. This work represents an important step towards a systematic and comprehensive human interactome project."
687,"whats real about virtual reality",334436,"What's Real About Virtual Reality?","The author presents a personal assessment of the state of the art of VR. In 1994, he surveyed the field of VR. His assessment then was that VR almost worked, but that we were not yet there. There were lots of demos and pilot systems, but except for vehicle simulators and entertainment applications, VR was not yet in production use doing real work. This year he was invited to do an up-to-date assessment of VR, with funding to visit major centers in North America and Europe. Every one of the component technologies has made big strides. Moreover, I found that there now exist some VR applications routinely operated for the results they produce."
688,"neurophysiological investigation of the basis of the fmri signal",335441,"Neurophysiological investigation of the basis of the fMRI signal","Functional magnetic resonance imaging (fMRI) is widely used to study the operational organization of the human brain, but the exact relationship between the measured fMRI signal and the underlying neural activity is unclear. Here we present simultaneous intracortical recordings of neural signals and fMRI responses. We compared local field potentials (LFPs), single- and multi-unit spiking activity with highly spatio-temporally resolved blood-oxygen-level-dependent (BOLD) fMRI responses from the visual cortex of monkeys. The largest magnitude changes were observed in LFPs, which at recording sites characterized by transient responses were the only signal that significantly correlated with the haemodynamic response. Linear systems analysis on a trial-by-trial basis showed that the impulse response of the neurovascular system is both animal- and site-specific, and that LFPs yield a better estimate of BOLD responses than the multi-unit responses. These findings suggest that the BOLD contrast mechanism reflects the input and intracortical processing of a given area rather than its spiking output."
689,"design and evaluation of an autonomic workflow engine",335719,"Design and Evaluation of an Autonomic Workflow Engine","In this paper we present the design and evaluate the performance of an autonomic workflow execution engine. Although there exist many distributed workflow engines, in practice, it remains a difficult problem to deploy such systems in an optimal configuration. Furthermore, when facing an unpredictable workload with high variability, manual reconfiguration is not an option. Thanks to its autonomic controller, the engine features self-configuration, self-tuning and self-healing properties. The engine runs on a cluster of computers using a tuple space to coordinate its various components. Its autonomic controller monitors its performance and responds to workload variations by altering the configuration. In case failures occur, the controller can recover the workflow execution state from persistent storage and migrate it to a different node of the cluster. Such interventions are carried out without any human supervision. As part of the results of our performance evaluation, we compare different autonomic control strategies and discuss how they can automatically tune the system."
690,"an architectural approach to autonomic computing",335723,"An architectural approach to autonomic computing","We describe an architectural approach to achieving the goals of autonomic computing. The architecture that we outline describes interfaces and behavioral requirements for individual system components, describes how interactions among components are established, and recommends design patterns that engender the desired system-level properties of self-configuration, self-optimization, self-healing and self-protection. We have validated many of these ideas in two prototype autonomic computing systems."
691,"is there a difference between leads and drugs a historical perspective",336740,"Is there a difference between leads and drugs? {A} historical perspective","To be considered for further development, lead structures should display the following properties: (1) simple chemical features, amenable for chemistry optimization; (2) membership to an established SAR series; (3) favorable patent situation; and (4) good absorption, distribution, metabolism, and excretion (ADME) properties. There are two distinct categories of leads: those that lack any therapeutic use (i.e., ""pure"" leads), and those that are marketed drugs themselves but have been altered to yield novel drugs. We have previously analyzed the design of leadlike combinatorial libraries starting from 18 lead and drug pairs of structures (S. J. Teague et al. Angew. Chem., Int. Ed. Engl. 1999, 38, 3743-3748). Here, we report results based on an extended dataset of 96 lead-drug pairs, of which 62 are lead structures that are not marketed as drugs, and 75 are drugs that are not presumably used as leads. We examined the following properties: MW (molecular weight), CMR (the calculated molecular refractivity), RNG (the number of rings), RTB (the number of rotatable bonds), the number of hydrogen bond donors (HDO) and acceptors (HAC), the calculated logarithm of the n-octanol/water partition (CLogP), the calculated logarithm of the distribution coefficient at pH 7.4 (LogD(74)), the Daylight-fingerprint druglike score (DFPS), and the property and pharmacophore features score (PPFS). The following differences were observed between the medians of drugs and leads: DeltaMW = 69; DeltaCMR = 1.8; DeltaRNG = DeltaHAC =1; DeltaRTB = 2; DeltaCLogP = 0.43; DeltaLogD(74) = 0.97; DeltaHDO = 0; DeltaDFPS = 0.15; DeltaPPFS = 0.12. Lead structures exhibit, on the average, less molecular complexity (less MW, less number of rings and rotatable bonds), are less hydrophobic (lower CLogP and LogD(74)), and less druglike (lower druglike scores). These findings indicate that the process of optimizing a lead into a drug results in more complex structures. This information should be used in the design of novel combinatorial libraries that are aimed at lead discovery."
692,"the gender similarities hypothesis",336990,"The Gender Similarities Hypothesis","The differences model, which argues that males and females are vastly different psychologically, dominates the popular media. Here, the author advances a very different view, the gender similarities hypothesis, which holds that males and females are similar on most, but not all, psychological variables. Results from a review of 46 meta-analyses support the gender similarities hypothesis. Gender differences can vary substantially in magnitude at different ages and depend on the context in which measurement occurs. Overinflated claims of gender differences carry substantial costs in areas such as the workplace and relationships."
693,"learning by googling",337170,"Learning by Googling","The goal of giving a well-defined meaning to information is currently shared by endeavors such as the Semantic Web as well as by current trends within Knowledge Management. They all depend on the large-scale formalization of knowledge and on the availability of formal metadata about information resources. However, the question how to provide the necessary formal metadata in an effective and efficient way is still not solved to a satisfactory extent. Certainly, the most effective way to provide such metadata as well as formalized knowledge is to let humans encode them directly into the system, but this is neither efficient nor feasible. Furthermore, as current social studies show, individual knowledge is often less powerful than the collective knowledge of a certain community.As a potential way out of the  knowledge acquisition bottleneck , we present a novel methodology that acquires collective knowledge from the World Wide Web using the Google TM  API. In particular, we present PANKOW, a concrete instantiation of this methodology which is evaluated in two experiments: one with the aim of classifying novel instances with regard to an existing ontology and one with the aim of learning sub-/superconcept relations."
694,"reconfigurable computing architectures and design methods",337765,"Reconfigurable computing: architectures and design methods","Reconfigurable computing is becoming increasingly attractive for many applications. This survey covers two aspects of reconfigurable computing: architectures and design methods. The paper includes recent advances in reconfigurable architectures, such as the Alters Stratix II and Xilinx Virtex 4 FPGA devices. The authors identify major trends in general-purpose and special-purpose design methods. It is shown that reconfigurable computing designs are capable of achieving up to 500 times speedup and 70% energy savings over microprocessor implementations for specific applications."
695,"nonrigid registration using freeform deformations application to breast mr images",338247,"Nonrigid registration using free-form deformations: application to breast MR images","In this paper the authors present a new approach for the nonrigid registration of contrast-enhanced breast {MRI.} A hierarchical transformation model of the motion of the breast has been developed. The global motion of the breast is modeled by an affine transformation while the local breast motion is described by a free-form deformation {(FFD)} based on B-splines. Normalized mutual information is used as a voxel-based similarity measure which is insensitive to intensity changes as a result of the contrast enhancement. Registration is achieved by minimizing a cost function, which represents a combination of the cost associated with the smoothness of the transformation and the cost associated with the image similarity. The algorithm has been applied to the fully automated registration of three-dimensional {(3-D)} breast {MRI} in volunteers and patients. In particular, the authors have compared the results of the proposed nonrigid registration algorithm to those obtained using rigid and affine registration techniques. The results clearly indicate that the nonrigid registration algorithm is much better able to recover the motion and deformation of the breast than rigid or affine registration algorithms."
696,"the sprite network operating system",339286,"The Sprite Network Operating System","A description is given of Sprite, an experimental network operating system under development at the University of California at Berkeley. It is part of a larger research project, SPUR, for the design and construction of a high-performance multiprocessor workstation with special hardware support of Lisp applications. Sprite implements a set of kernel calls that provide sharing, flexibility, and high performance to networked workstations. The discussion covers: the application interface: the basic kernel structure; management of the file name space and file data, virtual memory; and process migration."
697,"pagerank hits and a unified framework for link analysis",339334,"PageRank, {HITS} and a Unified Framework for Link Analysis","Two popular webpage ranking algorithms are HITS and PageRank. HITS emphasizes mutual reinforcement between authority and hub webpages, while PageRank emphasizes hyperlink weight normalization and web surfing based on random walk models. We systematically generalize /combine these concepts into a unified framework. The ranking framework contains a large algorithm space; HITS and PageRank are two extreme ends in this space. We study several normalized ranking algorithms which are intermediate between HITS and PageRank, and obtain closed-form solutions. We show that, to first order approximation, all ranking algorithms in this framework, including PageRank and HITS, lead to same ranking which is highly correlated with ranking by indegree. Rankings of webgraphs of different sizes and queries are presented to illustrate our analysis."
698,"a surprising simplicity to protein folding",339382,"A surprising simplicity to protein folding","The polypeptide chains that make up proteins have thousands of atoms and hence millions of possible inter-atomic interactions. It might be supposed that the resulting complexity would make prediction of protein structure and protein-folding mechanisms nearly impossible. But the fundamental physics underlying folding may be much simpler than this complexity would lead us to expect: folding rates and mechanisms appear to be largely determined by the topology of the native (folded) state, and new methods have shown great promise in predicting protein-folding mechanisms and the three-dimensional structures of proteins."
699,"processing and visualization for diffusion tensor mri",339630,"Processing and visualization for diffusion tensor MRI","This paper presents processing and visualization techniques for Diffusion Tensor Magnetic Resonance Imaging (DT-MRI). In DT-MRI, each voxel is assigned a tensor that describes local water diffusion. The geometric nature of diffusion tensors enables us to quantitatively characterize the local structure in tissues such as bone, muscle, and white matter of the brain. This makes DT-MRI an interesting modality for image analysis. In this paper we present a novel analytical solution to the Stejskal-Tanner diffusion equation system whereby a dual tensor basis, derived from the diffusion sensitizing gradient configuration, eliminates the need to solve this equation for each voxel. We further describe decomposition of the diffusion tensor based on its symmetrical properties, which in turn describe the geometry of the diffusion ellipsoid. A simple anisotropy measure follows naturally from this analysis. We describe how the geometry or shape of the tensor can be visualized using a coloring scheme based on the derived shape measures. In addition, we demonstrate that human brain tensor data when filtered can effectively describe macrostructural diffusion, which is important in the assessment of fiber-tract organization. We also describe how white matter pathways can be monitored with the methods introduced in this paper. DT-MRI tractography is useful for demonstrating neural connectivity (in vivo) in healthy and diseased brain tissue. (C) 2002 Elsevier Science B.V. All rights reserved."
700,"what makes us tick functional and neural mechanisms of interval timing",339850,"What makes us tick? Functional and neural mechanisms of interval timing.","Time is a fundamental dimension of life. It is crucial for decisions about quantity, speed of movement and rate of return, as well as for motor control in walking, speech, playing or appreciating music, and participating in sports. Traditionally, the way in which time is perceived, represented and estimated has been explained using a pacemaker–accumulator model that is not only straightforward, but also surprisingly powerful in explaining behavioural and biological data. However, recent advances have challenged this traditional view. It is now proposed that the brain represents time in a distributed manner and tells the time by detecting the coincidental activation of different neural populations."
701,"contextual cueing of visual attention",339999,"Contextual cueing of visual attention","{Visual context information constrains what to expect and where to look, facilitating search for and recognition of objects embedded in complex displays. This article reviews a new paradigm called contextual cueing, which presents well-defined, novel visual contexts and aims to understand how contextual information is learned and how It guides the deployment of visual attention. In addition, the contextual cueing task is well suited to the study of the neural substrate of contextual learning. For example, amnesic patient, with hippocampal damage are impaired in their learning of novel contextual information, even though learning in the contextual cueing task does not appear to rely on conscious retrieval of contextual memory traces. We argue that contextual information is important because it embodies invariant properties of the visual environment such as stable spatial layout information as well as object covariation information. Sensitivity to these statistical regularities allows us to interact more effectively with the visual world.}"
702,"requirements for digital preservation systems a bottomup approach",340705,"Requirements for Digital Preservation Systems: A Bottom-Up Approach","The field of digital preservation is being defined by a set of standards developed top-down, starting with an abstract reference model (OAIS) and gradually adding more specific detail. Systems claiming conformance to these standards are entering production use. Work is underway to certify that systems conform to requirements derived from OAIS.   We complement these requirements derived top-down by presenting an alternate, bottom-up view of the field. The fundamental goal of these systems is to ensure that the information they contain remains accessible for the long term. We develop a parallel set of requirements based on observations of how existing systems handle this task, and on an analysis of the threats to achieving the goal. On this basis we suggest disclosures that systems should provide as to how they satisfy their goals."
703,"parameter control in evolutionary algorithms",341384,"Parameter Control in {E}volutionary {A}lgorithms","The issue of controlling values of various parameters of an evolutionary algorithm is one of the most important and promising areas of research in evolutionary computation: it has a potential of adjusting the algorithm to the problem while solving the problem. In the paper we: 1) revise the terminology, which is unclear and confusing, thereby providing a classification of such control mechanisms, and 2) survey various forms of control which have been studied by the evolutionary computation community in recent years. Our classification covers the major forms of parameter control in evolutionary computation and suggests some directions for further research"
704,"discovery of regulatory elements in vertebrates through comparative genomics",342965,"Discovery of regulatory elements in vertebrates through comparative genomics","We have analyzed issues of reliability in studies in which comparative genomic approaches have been applied to the discovery of regulatory elements at a genome-wide level in vertebrates. We point out some potential problems with such studies, including difficulties in accurately identifying orthologous promoter regions. Many of these subtle analytical problems have become apparent only when studying the more complex vertebrate genomes. By determining motif reliability, we compared existing tools when applied to the discovery of vertebrate regulatory elements. We then used a statistical clustering method to produce a computational catalog of high quality putative regulatory elements from vertebrates, some of which are widely conserved among vertebrates and many of which are novel regulatory elements. The results provide a glimpse into the wealth of information that comparative genomics can yield and suggest the need for further improvement of genome-wide comparative computational techniques."
705,"modeling the heartfrom genes to cells to the whole organ",343691,"Modeling the heart-from genes to cells to the whole organ","Successful physiological analysis requires an understanding of the functional interactions between the key components of cells, organs, and systems, as well as how these interactions change in disease states. This information resides neither in the genome nor even in the individual proteins that genes code for. It lies at the level of protein interactions within the context of subcellular, cellular, tissue, organ, and system structures. There is therefore no alternative to copying nature and computing these interactions to determine the logic of healthy and diseased states. The rapid growth in biological databases; models of cells, tissues, and organs; and the development of powerful computing hardware and algorithms have made it possible to explore functionality in a quantitative manner all the way from the level of genes to the physiological function of whole organs and regulatory systems. This review illustrates this development in the case of the heart. Systems physiology of the 21st century is set to become highly quantitative and, therefore, one of the most computer-intensive disciplines."
706,"ecell software environment for wholecell simulation",343769,"E-{CELL}: software environment for whole-cell simulation","MOTIVATION: Genome sequencing projects and further systematic functional analyses of complete gene sets are producing an unprecedented mass of molecular information for a wide range of model organisms. This provides us with a detailed account of the cell with which we may begin to build models for simulating intracellular molecular processes to predict the dynamic behavior of living cells. Previous work in biochemical and genetic simulation has isolated well-characterized pathways for detailed analysis, but methods for building integrative models of the cell that incorporate gene regulation, metabolism and signaling have not been established. We, therefore, were motivated to develop a software environment for building such integrative models based on gene sets, and running simulations to conduct experiments in silico. RESULTS: E-CELL, a modeling and simulation environment for biochemical and genetic processes, has been developed. The E-CELL system allows a user to define functions of proteins, protein-protein interactions, protein-DNA interactions, regulation of gene expression and other features of cellular metabolism, as a set of reaction rules. E-CELL simulates cell behavior by numerically integrating the differential equations described implicitly in these reaction rules. The user can observe, through a computer display, dynamic changes in concentrations of proteins, protein complexes and other chemical compounds in the cell. Using this software, we constructed a model of a hypothetical cell with only 127 genes sufficient for transcription, translation, energy production and phospholipid synthesis. Most of the genes are taken from Mycoplasma genitalium, the organism having the smallest known chromosome, whose complete 580 kb genome sequence was determined at TIGR in 1995. We discuss future applications of the E-CELL system with special respect to genome engineering. AVAILABILITY: The E-CELL software is available upon request. SUPPLEMENTARY INFORMATION: The complete list of rules of the developed cell model with kinetic parameters can be obtained via our web site at: http://e-cell.org/."
707,"gepasi a software package for modelling the dynamics steady states and control of biochemical and other systems",343795,"{GEPASI}: A software package for modelling the dynamics, steady states and control of biochemical and other systems","GEPASI is a software system for modelling chemical and biochemical reaction networks on computers running Microsoft Windows. For any system of up to 45 metabolites and 45 reactions, each with any user-defined or one of 35 predefined rate equations, one can produce trajectories of the metabolite concentrations and obtain a steady state (if it does exist). When steady-state solutions are produced, elasticity and control coefficients, as defined in metabolic control analysis, are calculated. GEPASI also allows the automatic generation of a sequence of simulations with different combinations of parameter values, effectively scanning a hyper-solid in parameter space. Together with the ability to produce user-defined columnar data files, these features allow for both very quick and systematic study of biochemical pathway models. The source code (in C) is available on request from the author, and while the user interface is dependent on having MS-Windows as the operating system, the numerical part is portable to other operating systems. GEPASI is suitable both for research and educational purposes. Although GEPASI was written with biochemical pathways in mind, it can equally be used to simulate other dynamical systems. 10.1093/bioinformatics/9.5.563"
708,"compucell a multimodel framework for simulation of morphogenesis",343798,"{COMPUCELL}, a multi-model framework for simulation of morphogenesis","Motivation: COMPUCELL is a multi-model software framework for simulation of the development of multicellular organisms known as morphogenesis. It models the interaction of the gene regulatory network with generic cellular mechanisms, such as cell adhesion, division, haptotaxis and chemotaxis. A combination of a state automaton with stochastic local rules and a set of differential equations, including subcellular ordinary differential equations and extracellular reaction{\^a}diffusion partial differential equations, model gene regulation. This automaton in turn controls the differentiation of the cells, and cell{\^a}cell and cell{\^a}extracellular matrix interactions that give rise to cell rearrangements and pattern formation, e.g. mesenchymal condensation. The cellular Potts model, a stochastic model that accurately reproduces cell movement and rearrangement, models cell dynamics. All these models couple in a controllable way, resulting in a powerful and flexible computational environment for morphogenesis, which allows for simultaneous incorporation of growth and spatial patterning. Results: We use COMPUCELL to simulate the formation of the skeletal architecture in the avian limb bud. Availability: Binaries and source code for Microsoft Windows, Linux and Solaris are available for download from http://sourceforge.net/projects/compucell/ Contact: compucell@cse.nd.edu"
709,"being there putting brain body and world together again",343921,"Being there : {P}utting {B}rain, {B}ody, and {W}orld {T}ogether {A}gain","{Brain, body, and world are united in a complex dance of circular causation and extended computational activity. In <i>Being There</i>, Andy Clark weaves these several threads into a pleasing whole and goes on to address foundational questions concerning the new tools and techniques needed to make sense of the emerging sciences of the embodied mind. Clark brings together ideas and techniques from robotics, neuroscience, infant psychology, and artificial intelligence. He addresses a broad range of adaptive behaviors, from cockroach locomotion to the role of linguistic artifacts in higher-level thought.}"
710,"latent semantic models for collaborative filtering",344216,"Latent semantic models for collaborative filtering","Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, that is, a database of available user preferences. In this article, we describe a new family of model-based algorithms designed for this task. These algorithms rely on a statistical modelling technique that introduces latent class variables in a mixture model setting to discover user communities and prototypical interest profiles. We investigate several variations to deal with discrete and continuous response variables as well as with different objective functions. The main advantages of this technique over standard memory-based methods are higher accuracy, constant time prediction, and an explicit and compact model representation. The latter can also be used to mine for user communitites. The experimental evaluation shows that substantial improvements in accucracy over existing methods and published results can be obtained."
711,"developing a contextaware electronic tourist guide some issues and experiences",344278,"{Developing a Context-Aware Electronic Tourist Guide: Some Issues and Experiences}","In this paper, we describe our experiences of developing and evaluating GUIDE, an intelligent electronic tourist guide. The GUIDE system has been built to overcome many of the limitations of the traditional information and navigation tools available to city visitors. For example, group-based tours are inherently inflexible with fixed starting times and fixed durations and (like most guidebooks) are constrained by the need to satisfy the interests of the majority rather than the specific interests of individuals. Following a period of requirements capture, involving experts in the field of tourism, we developed and installed a system for use by visitors to Lancaster. The system combines mobile computing technologies with a wireless infrastructure to present city visitors with information tailored to both their personal and environmental contexts. In this paper we present an evaluation of GUIDE, focusing on the quality of the visitor's experience when using the system."
712,"privacy by design principles of privacyaware ubiquitous systems",344318,"{Privacy by Design -- Principles of Privacy-Aware Ubiquitous Systems}","This paper tries to serve as an introductory reading to privacy issues in the field of ubiquitous computing. It develops six principles for guiding system design, based on a set of fair information practices common in most privacy legislation in use today: notice, choice and consent, proximity and locality, anonymity and pseudonymity, security, and access and recourse. A brief look at the history of privacy protection, its legal status, and its expected utility is provided as a background."
713,"interprocedural slicing using dependence graphs",344542,"Interprocedural Slicing Using Dependence Graphs","The notion of a program slice, originally introduced by Mark Weiser, is useful in program debugging, automatic parallelization, and program integration. A slice of a program is taken with respect to a program point p and a variable x; the slice consists of all statements of the program that might affect the value of x at point p. This paper concerns the problem of interprocedural slicing generating a slice of an entire program, where the slice crosses the boundaries of procedure calls. To solve this problem, we introduce a new kind of graph to represent programs, called a system dependence graph, which extends previous dependence representations to incorporate collections of procedures (with procedure calls) rather than just monolithic programs. Our main result is an algorithm for interprocedural slicing that uses the new representation. (It should be noted that our work concerns a somewhat restricted kind of slice: rather than permitting a program to b e sliced with respect to program point p and an arbitrary variable, a slice must be taken with respect to a variable that is defined or used at p.) The chief difficulty in interprocedural slicing is correctly accounting for the calling context of a called procedure. To handle this problem, system dependence graphs include some data dependence edges that represent transitive dependences due to the effects of procedure calls, in addition to the conventional direct-dependence edges. These edges are constructed with the aid of an auxiliary structure that represents calling and parameter-linkage relationships. This structure takes the form of an attribute grammar. The step of computing the required transitive-dependence edges is reduced to the construction of the subordinate characteristic graphs for the grammar's nonterminals."
714,"objectoriented design heuristics",344613,"Object-Oriented Design Heuristics","Object-Oriented Design Heuristics offers insight into object-oriented design improvement. The more than sixty guidelines presented in this book are language-independent and allow you to rate the integrity of a software design. The heuristics are not written as hard and fast rules; they are meant to serve as warning mechanisms which allow the flexibility of ignoring the heuristic as necessary. This tutorial-based approach, born out of the author's extensive experience developing software, teaching thousands of students, and critiquing designs in a variety of domains, allows you to apply the guidelines in a personalized manner. The heuristics cover important topics ranging from classes and objects (with emphasis on their relationships including association, uses, containment, and both single and multiple inheritance) to physical object-oriented design. You will gain an understanding of the synergy that exists between design heuristics and the popular concept of design patterns; heuristics can highlight a problem in one facet of a design while patterns can provide the solution. Programmers of all levels will find value in this book. The newcomer will discover a fast track to understanding the concepts of object-oriented programming. At the same time, experienced programmers seeking to strengthen their object-oriented development efforts will appreciate the insightful analysis. In short, with Object-Oriented Design Heuristics as your guide, you have the tools to become a better software developer."
715,"concern graphs finding and describing concerns using structural program dependencies",344617,"Concern Graphs: Finding and Describing Concerns Using Structural Program Dependencies","Many maintenance tasks address concerns, or features, that are not well modularized in the source code comprising a system. Existing approaches available to help software developers locate and manage scattered concerns use a representation based on lines of source code, complicating the analysis of the concerns. In this paper, we introduce the Concern Graph representation that abstracts the implementation details of a concern and makes explicit the relationships between different parts of the concern. The abstraction used in a Concern Graph has been designed to allow an obvious and inexpensive mapping back to the corresponding source code. To investigate the practical tradeoffs related to this approach, we have built the Feature Exploration and Analysis tool (FEAT) that allows a developer to manipulate a concern representation extracted from a Java system, and to analyze the relationships of that concern to the code base. We have used this tool to find and describe concerns related to software change tasks. We have performed case studies to evaluate the feasibility, usability, and scalability of the approach. Our results indicate that Concern Graphs can be used to document a concern for change, that developers unfamiliar with Concern Graphs can use them effectively, and that the underlying technology scales to industrial-sized programs."
716,"an integrated experimental environment for distributed systems and networks",345172,"An Integrated Experimental Environment for Distributed Systems and Networks","Three experimental environments traditionally support network and distributed systems research: network emulators, network simulators, and live networks. The continued use of multiple approaches highlights both the value and inadequacy of each. Netbed, a descendant of Emulab, provides an experimentation facility that integrates these approaches, allowing researchers to configure and access networks composed of emulated, simulated, and wide-area nodes and links. Netbed's primary goals are  ease of use, control , and  realism , achieved through consistent use of virtualization and abstraction.By providing operating system-like services, such as resource allocation and scheduling, and by virtualizing heterogeneous resources, Netbed acts as a virtual machine for network experimentation. This paper presents Netbed's overall design and implementation and demonstrates its ability to improve experimental automation and efficiency. These, in turn, lead to new methods of experimentation, including automated parameter-space studies within emulation and straightforward comparisons of simulated, emulated, and wide-area scenarios."
717,"dynamic properties of network motifs contribute to biological network organization",345209,"Dynamic properties of network motifs contribute to biological network organization.","Biological networks, such as those describing gene regulation, signal transduction, and neural synapses, are representations of large-scale dynamic systems. Discovery of organizing principles of biological networks can be enhanced by embracing the notion that there is a deep interplay between network structure and system dynamics. Recently, many structural characteristics of these non-random networks have been identified, but dynamical implications of the features have not been explored comprehensively. We demonstrate by exhaustive computational analysis that a dynamical property&#8212;stability or robustness to small perturbations&#8212;is highly correlated with the relative abundance of small subnetworks (network motifs) in several previously determined biological networks. We propose that robust dynamical stability is an influential property that can determine the non-random structure of biological networks"
718,"condensation conditional density propagation for visual tracking",347146,"Condensation -- conditional density propagation for visual tracking","The problem of tracking curves in dense visual clutter is challenging. Kalman filtering is inadequate because it is based on Gaussian densities which, being unimodal, cannot represent simultaneous alternative hypotheses. The Condensation algorithm uses ""factored sampling"", previously applied to the interpretation of static images, in which the probability distribution of possible interpretations is represented by a randomly generated set. Condensation uses learned dynamical models, together..."
719,"simulationbased comparisons of tahoe reno and sack tcp",347359,"Simulation-based comparisons of Tahoe, Reno and SACK TCP","This paper uses simulations to explore the benefits of adding selective acknowledgments (SACK) and selective repeat to TCP. We compare Tahoe and Reno TCP, the two most common reference implementations for TCP, with two modified versions of Reno TCP. The first version is New-Reno TCP, a modified version of TCP without SACK that avoids some of Reno TCP's performance problems when multiple packets are dropped from a window of data. The second version is SACK TCP, a conservative extension of Reno TCP modified to use the SACK option being proposed in the Internet Engineering Task Force (IETF). We describe the congestion control algorithms in our simulated implementation of SACK TCP and show that while selective acknowledgments are not required to solve Reno TCP's performance problems when multiple packets are dropped, the absence of selective acknowledgments does impose limits to TCP's ultimate performance. In particular, we show that without selective acknowledgments, TCP implementations are constrained to either retransmit at most one dropped packet per round-trip time, or to retransmit packets that might have already been successfully delivered."
720,"a simple fast and accurate algorithm to estimate large phylogenies by maximum likelihood",347869,"A simple, fast, and accurate algorithm to estimate large phylogenies by maximum likelihood.","The increase in the number of large data sets and the complexity of current probabilistic sequence evolution models necessitates fast and reliable phylogeny reconstruction methods. We describe a new approach, based on the maximum- likelihood principle, which clearly satisfies these requirements. The core of this method is a simple hill-climbing algorithm that adjusts tree topology and branch lengths simultaneously. This algorithm starts from an initial tree built by a fast distance-based method and modifies this tree to improve its likelihood at each iteration. Due to this simultaneous adjustment of the topology and branch lengths, only a few iterations are sufficient to reach an optimum. We used extensive and realistic computer simulations to show that the topological accuracy of this new method is at least as high as that of the existing maximum-likelihood programs and much higher than the performance of distance-based and parsimony approaches. The reduction of computing time is dramatic in comparison with other maximum-likelihood packages, while the likelihood maximization ability tends to be higher. For example, only 12 min were required on a standard personal computer to analyze a data set consisting of 500 rbcL sequences with 1,428 base pairs from plant plastids, thus reaching a speed of the same order as some popular distance-based and parsimony algorithms. This new method is implemented in the PHYML program, which is freely available on our web page: http://www.lirmm.fr/w3ifa/MAAS/."
721,"perceived selfefficacy in cognitive development and functioning",348334,"Perceived Self-Efficacy in Cognitive Development and Functioning","In this article, I review the diverse ways in which perceived self-efficacy contributes to cognitive development and functioning. Perceived self-efficacy exerts its influence through four major processes. They include cognitive, motivational, affective, and selection processes. There are three different levels at which perceived self-efficacy operates as an important contributor to academic development. Students' beliefs in their efficacy to regulate their own learning and to master academic activities determine their aspirations, level of motivation, and academic accomplishments. Teachers' beliefs in their personal efficacy to motivate and promote learning affect the types of learning environments they create and the level of academic progress their students achieve. Faculties' beliefs in their collective instructional efficacy contribute significantly to their schools' level of academic achievement. Student body characteristics influence school-level achievement more strongly by altering faculties' beliefs in their collective efficacy than through direct affects on school achievement."
722,"remote sensing and image interpretation",348659,"Remote Sensing and Image Interpretation","From recent developments in digital image processing to the next generation of satellite systems, this book provides a comprehensive introduction to the field of remote sensing and image interpretation. This book is discipline neutral, so readers in any field of study can gain a clear understanding of these systems and their virtually unlimited applications.<br> * The authors underscore close interactions among the related areas of remote sensing, GIS, GPS, digital image processing, and environmental modeling.<br> * Appendices include material on sources of remote sensing data and information, remote sensing periodicals, online glossaries, and online tutorials."
723,"dopamine learning and motivation",349172,"Dopamine, learning and motivation","Brain dopamine has been linked to both motor and motivational functions. Several motivational hypotheses have been challenged and found inadequate, but it remains clear that dopamine is vital for the 'stamping-in' of stimulus–reward and response–reward associations. Stimulus–reward associations are, in turn, crucial for the subsequent motivation in a previous-reward situation. Response habits are triggered by environmental stimuli that have been previously associated with reward, and the initiation of such response habits is not dependent on immediate dopamine function. If repeated with dopamine function blocked, however, the old stimulus–reward associations are extinguished and response motivation progressively weakens. While the motivational effectiveness of reward-associated stimuli does not require immediate dopamine function, phasic dopamine elevations can nonetheless amplify stimulus effectiveness. This amplification is thought to be a dopamine function in the nucleus accumbens. The role of dopamine in the stamping-in of reward associations might be much less localized. Dopamine seems to have important roles in the consolidation of memory in various structures — structures that are linked to different kinds of learning or to the learning of different things. A full appreciation of the role of dopamine in motivation must be on the basis of an understanding of not only the role of dopamine in immediate behavioural arousal, but also its role in the learning and memory of learned motivational stimuli."
724,"effects of computerbased clinical decision support systems on physician performance and patient outcomes a systematic review",349634,"Effects of computer-based clinical decision support systems on physician performance and patient outcomes: a systematic review.","Context.-- Many computer software developers and vendors claim that their systems can directly improve clinical decisions. As for other health care interventions, such claims should be based on careful trials that assess their effects on clinical performance and, preferably, patient outcomes.  Objective.-- To systematically review controlled clinical trials assessing the effects of computer-based clinical decision support systems (CDSSs) on physician performance and patient outcomes.  Data Sources.-- We updated earlier reviews covering 1974 to 1992 by searching the MEDLINE, EMBASE, INSPEC, SCISEARCH, and the Cochrane Library bibliographic databases from 1992 to March 1998. Reference lists and conference proceedings were reviewed and evaluators of CDSSs were contacted.  Study Selection.-- Studies were included if they involved the use of a CDSS in a clinical setting by a health care practitioner and assessed the effects of the system prospectively with a concurrent control.  Data Extraction.-- The validity of each relevant study (scored from 0-10) was evaluated in duplicate. Data on setting, subjects, computer systems, and outcomes were abstracted and a power analysis was done on studies with negative findings.  Data Synthesis.-- A total of 68 controlled trials met our criteria, 40 of which were published since 1992. Quality scores ranged from 2 to10, with more recent trials rating higher (mean, 7.7) than earlier studies (mean, 6.4) (P<.001). Effects on physician performance were assessed in 65 studies and 43 found a benefit (66%). These included 9 of 15 studies on drug dosing systems, 1 of 5 studies on diagnostic aids, 14 of 19 preventive care systems, and 19 of 26 studies evaluating CDSSs for other medical care. Six of 14 studies assessing patient outcomes found a benefit. Of the remaining 8 studies, only 3 had a power of greater than 80% to detect a clinically important effect.  Conclusions.-- Published studies of CDSSs are increasing rapidly, and their quality is improving. The CDSSs can enhance clinical performance for drug dosing, preventive care, and other aspects of medical care, but not convincingly for diagnosis. The effects of CDSSs on patient outcomes have been insufficiently studied. 10.1001/jama.280.15.1339"
725,"interactions between frontal cortex and basal ganglia in working memory a computational model",349688,"Interactions between frontal cortex and basal ganglia in working memory: A computational model","The frontal cortex and the basal ganglia interact via a relatively well understood and elaborate system of interconnections. In the context of motor function, these interconnections can be understood as disinhibiting, or ""releasing the brakes,"" on frontal motor action plans: The basal ganglia detect appropriate contexts for performing motor actions and enable the frontal cortex to execute such actions at the appropriate time. We build on this idea in the domain of working memory through the use of computational neural network models of this circuit. In our model, the frontal cortex exhibits robust active maintenance, whereas the basal ganglia contribute a selective, dynamic gating function that enables frontal memory representations to be rapidly updated in a task-relevant manner. We apply the model to a novel version of the continuous performance task that requires subroutine-like selective working memory updating and compare and contrast our model with other existing models and theories of frontal-cortex-basal-ganglia interactions."
726,"ten thousand interactions for the molecular biologist",349730,"Ten thousand interactions for the molecular biologist","Previous studies have suggested that nature is restricted to about 1,000 protein folds to perform a great diversity of functions. Here, we use protein interaction data from different sources and three-dimensional structures to suggest that the total number of interaction types is also limited, and estimate that most interactions in nature will conform to one of about 10,000 types. We currently know fewer than 2,000, and at the present rate of structure determination, it will be more than 20 years before we know a full representative set."
727,"bioinformatics for wholegenome shotgun sequencing of microbial communities",349734,"Bioinformatics for whole-genome shotgun sequencing of microbial communities.","The application of whole-genome shotgun sequencing to microbial communities represents a major development in metagenomics, the study of uncultured microbes via the tools of modern genomic analysis. In the past year, whole-genome shotgun sequencing projects of prokaryotic communities from an acid mine biofilm, the Sargasso Sea, Minnesota farm soil, three deep-sea whale falls, and deep-sea sediments have been reported, adding to previously published work on viral communities from marine and fecal samples. The interpretation of this new kind of data poses a wide variety of exciting and difficult bioinformatics problems. The aim of this review is to introduce the bioinformatics community to this emerging field by surveying existing techniques and promising new approaches for several of the most interesting of these computational problems."
728,"formal methods state of the art and future directions",350137,"Formal methods: state of the art and future directions","this report assesses the state of the art in specification and verification. For verification, we highlight advances in model checking and theorem proving. In the three sections on specification, model checking, and theorem proving, we explain what we mean by the general technique and briefly describe some successful case studies and well-known tools. The second part of this report outlines future directions in fundamental concepts, new methods and tools, integration of methods, and education and technology transfer. We close with summary remarks and pointers to resources for more information"
729,"core transcriptional regulatory circuitry in human embryonic stem cells",350303,"Core transcriptional regulatory circuitry in human embryonic stem cells.","The transcription factors OCT4, SOX2, and NANOG have essential roles in early development and are required for the propagation of undifferentiated embryonic stem (ES) cells in culture. To gain insights into transcriptional regulation of human ES cells, we have identified OCT4, SOX2, and NANOG target genes using genome-scale location analysis. We found, surprisingly, that OCT4, SOX2, and NANOG co-occupy a substantial portion of their target genes. These target genes frequently encode transcription factors, many of which are developmentally important homeodomain proteins. Our data also indicate that OCT4, SOX2, and NANOG collaborate to form regulatory circuitry consisting of autoregulatory and feedforward loops. These results provide new insights into the transcriptional regulation of stem cells and reveal how OCT4, SOX2, and NANOG contribute to pluripotency and self-renewal."
730,"why arent operating systems getting faster as fast as hardware",350522,"Why Aren't Operating Systems Getting Faster As Fast as Hardware?","This note evaluates several hardware platforms and operating systems using a set of benchmarks that test memory bandwidth and various operating system features such as kernel entry/exit and file systems. The overall conclusion is that operating system performance does not seem to be improving at the same rate as the base speed of the underlying hardware. Copyright Ó 1989 Digital Equipment Corporation d i g i t a l Western Research Laboratory 100 Hamilton Avenue Palo Alto, California 94301 USA..."
731,"optimal experimental design for eventrelated fmri",352058,"Optimal experimental design for event-related fMRI.","An important challenge in the design and analysis of event-related or single-trial functional magnetic resonance imaging (fMRI) experiments is to optimize statistical efficiency, i.e., the accuracy with which the event-related hemodynamic response to different stimuli can be estimated for a given amount of imaging time. Several studies have suggested that using a fixed inter-stimulus-interval (ISI) of at least 15 sec results in optimal statistical efficiency or power and that using shorter ISIs results in a severe loss of power. In contrast, recent studies have demonstrated the feasibility of using ISIs as short as 500 ms while still maintaining considerable efficiency or power. Here, we attempt to resolve this apparent contradiction by a quantitative analysis of the relative efficiency afforded by different event-related experimental designs. This analysis shows that statistical efficiency falls off dramatically as the ISI gets sufficiently short, if the ISI is kept fixed for all trials. However, if the ISI is properly jittered or randomized from trial to trial, the efficiency improves monotonically with decreasing mean ISI. Importantly, the efficiency afforded by such variable ISI designs can be more than 10 times greater than that which can be achieved by fixed ISI designs. These results further demonstrate the feasibility of using identical experimental designs with fMRI and electro-/magnetoencephalography (EEG/MEG) without sacrificing statistical power or efficiency of either technique, thereby facilitating comparison and integration across imaging modalities. Hum. Brain Mapping 8:109-114, 1999. © 1999 Wiley-Liss, Inc."
732,"naive bayes at forty the independence assumption in information retrieval",352260,"Naive (Bayes) at forty: The independence assumption in information retrieval.",". The naive Bayes classifier, currently experiencing a renaissance in machine learning, has long been a core technique in information retrieval. We review some of the variations of naive Bayes models used for text retrieval and classification, focusing on the distributional assumptions made about word occurrences in documents. 1 Introduction The naive Bayes classifier, long a favorite punching bag of new classification techniques, has recently emerged as a focus of research itself in machine..."
733,"an invitation to d vision",352535,"An Invitation to 3-D Vision","{This book gives senior undergraduate and beginning graduate students and researchers in computer vision, applied mathematics, computer graphics, and robotics a self-contained introduction to the geometry of 3D vision; that is the reconstruction of 3D models of objects from a collection of 2D images. Following a brief introduction, Part I provides background materials for the rest of the book. The two fundamental transformations, namely rigid body motion and perspective projection are introduced and image formation and feature extraction discussed. Part II covers the classic theory of two view geometry based on the so-called epipolar constraint. Part III shows that a more proper tool for studying the geometry of multiple views is the so- called rank considtion on the multiple view matrix. Part IV develops practical reconstruction algorithms step by step as well as discusses possible extensions of the theory. Exercises are provided at the end of each chapter. Software for examples and algorithms are available on the author's website.}"
734,"a survey on sensor networks",352713,"A survey on sensor networks","The advancement in wireless communications and electronics has enabled the development of low-cost sensor networks. The sensor networks can be used for various application areas (e.g., health, military, home). For different application areas, there are different technical issues that researchers are currently resolving. The current state of the art of sensor networks is captured in this article, where solutions are discussed under their related protocol stack layer sections. This article also points out the open research issues and intends to spark new interests and developments in this field."
735,"conservation and evolvability in regulatory networks the evolution of ribosomal regulation in yeast",352715,"Conservation and evolvability in regulatory networks: The evolution of ribosomal regulation in yeast","Transcriptional modules of coregulated genes play a key role in regulatory networks. Comparative studies show that modules of coexpressed genes are conserved across taxa. However, little is known about the mechanisms underlying the evolution of module regulation. Here, we explore the evolution of cis-regulatory programs associated with conserved modules by integrating expression profiles for two yeast species and sequence data for a total of 17 fungal genomes. We show that although the cis-elements accompanying certain conserved modules are strictly conserved, those of other conserved modules are remarkably diverged. In particular, we infer the evolutionary history of the regulatory program governing ribosomal modules. We show how a cis-element emerged concurrently in dozens of promoters of ribosomal protein genes, followed by the loss of a more ancient cis-element. We suggest that this formation of an intermediate redundant regulatory program allows conserved transcriptional modules to gradually switch from one regulatory mechanism to another while maintaining their functionality. Our work provides a general framework for the study of the dynamics of promoter evolution at the level of transcriptional modules and may help in understanding the evolvability and increased redundancy of transcriptional regulation in higher organisms."
736,"statistical pattern recognition",353833,"Statistical pattern recognition","Statistical pattern recognition is a very active area of study and research, which has seen many advances in recent years. New and emerging applications - such as data mining, web searching, multimedia data retrieval, face recognition, and cursive handwriting recognition - require robust and efficient pattern recognition techniques. Statistical decision making and estimation are regarded as fundamental to the study of pattern recognition. Statistical Pattern Recognition, Second Edition has been fully updated with new methods, applications and references. It provides a comprehensive introduction to this vibrant area - with material drawn from engineering, statistics, computer science and the social sciences - and covers many application areas, such as database design, artificial neural networks, and decision support systems. * Provides a self-contained introduction to statistical pattern recognition. * Each technique described is illustrated by real examples. * Covers Bayesian methods, neural networks, support vector machines, and unsupervised classification. * Each section concludes with a description of the applications that have been addressed and with further developments of the theory. * Includes background material on dissimilarity, parameter estimation, data, linear algebra and probability. * Features a variety of exercises, from 'open-book' questions to more lengthy projects. The book is aimed primarily at senior undergraduate and graduate students studying statistical pattern recognition, pattern processing, neural networks, and data mining, in both statistics and engineering departments. It is also an excellent source of reference for technical professionals working in advanced information development environments."
737,"total functional programming",354229,"Total Functional Programming","The driving idea of functional programming is to make programming more closely related to mathematics. A program in a functional language such as Haskell or Miranda consists of equations which are both computation rules and a basis for simple algebraic reasoning about the functions and data structures they deﬁne. The existing model of functional programming, although elegant and powerful, is compromised to a greater extent than is commonly recognised by the presence of partial functions. We consider a simple discipline of total functional programming designed to exclude the possibility of non-termination. Among other things this requires a type distinction between data, which is ﬁnite, and codata, which is potentially inﬁnite."
738,"applied multivariate statistical analysis",354778,"Applied Multivariate Statistical Analysis","Most of the observable phenomena in the empirical sciences are of a multivariate nature.In financial studies, assets in stock markets are observed simultaneously and their joint development is analyzed to better understand general tendencies and to track indices. In medicine recorded observations of subjects in different locations are the basis of reliable diagnoses and medication. In quantitative marketing consumer preferences are collected in order to construct models of consumer behavior. The underlying theoretical structure of these and many other quantitative studies of applied sciences is multivariate. Focussing on applications this book presents the tools and concepts of multivariate data analysis in a way that is understandable for non-mathematicians and practitioners who face statistical data analysis.  In this second edition a wider scope of methods and applications of multivariate statistical analysis is introduced. All quantlets have been translated into the R and Matlab language and are made available online."
739,"discriminant analysis by gaussian mixtures",354781,"Discriminant Analysis by Gaussian Mixtures","Fisher-Rao linear discriminant analysis (LDA) is a valuable tool for multigroup classification. LDA is equivalent to maximum likelihood classification assuming Gaussian distributions for each class. In this paper, we fit Gaussian mixtures to each class to facilitate effective classification in non-normal settings, especially when the classes are clustered. Low dimensional views are an important by-product of LDA--our new techniques inherit this feature. We can control the within-class spread of the subclass centres relative to the between-class spread. Our technique for fitting these models permits a natural blend with nonparametric versions of LDA."
740,"introduction to automata theory languages and computation",354787,"Introduction to Automata Theory, Languages, and Computation","This book is a rigorous exposition of formal languages and models of computation, with an introduction to computational complexity. The authors present the theory in a concise and straightforward manner, with an eye out for the practical applications. Exercises at the end of each chapter, including some that have been solved, help readers confirm and enhance their understanding of the material. This book is appropriate for upper-level computer science undergraduates who are comfortable with mathematical arguments."
741,"towards a theory of scalefree graphs definition properties and implications extended version",355384,"Towards a Theory of Scale-Free Graphs: Definition, Properties, and Implications (Extended Version)","Although the “scale-free” literature is large and growing, it gives neither a precise definition of scale-free graphs nor rigorous proofs of many of their claimed properties. In fact, it is easily shown that the existing theory has many inherent contradictions and verifiably false claims. In this paper, we propose a new, mathematically precise, and structural definition of the extent to which a graph is scale-free, and prove a series of results that recover many of the claimed properties while suggesting the potential for a rich and interesting theory. With this definition, scale-free (or its opposite, scale-rich) is closely related to other structural graph properties such as various notions of self-similarity (or respectively, self-dissimilarity). Scale-free graphs are also shown to be the likely outcome of random construction processes, consistent with the heuristic definitions implicit in existing random graph approaches. Our approach clarifies much of the confusion surrounding the sensational qualitative claims in the scale-free literature, and offers rigorous and quantitative alternatives."
742,"the topological theory of defects in ordered media",355644,"The topological theory of defects in ordered media","Aspects of the theory of homotopy groups are described in a mathematical style closer to that of condensed matter physics than that of topology. The aim is to make more readily accessible to physicists the recent applications of homotopy theory to the study of defects in ordered media. Although many physical examples are woven into the development of the subject, the focus is on mathematical pedagogy rather than on a systematic review of applications."
743,"telling more than we can know verbal reports on mental processes",356505,"Telling More Than We Can Know: Verbal Reports on Mental Processes","Reviews evidence which suggests that there may be little or no direct introspective access to higher order cognitive processes. Ss are sometimes (a) unaware of the existence of a stimulus that importantly influenced a response, (b) unaware of the existence of the response, and (c) unaware that the stimulus has affected the response. It is proposed that when people attempt to report on their cognitive processes, that is, on the processes mediating the effects of a stimulus on a response, they do not do so on the basis of any true introspection. Instead, their reports are based on a priori, implicit causal theories, or judgments about the extent to which a particular stimulus is a plausible cause of a given response. This suggests that though people may not be able to observe directly their cognitive processes, they will sometimes be able to report accurately about them. Accurate reports will occur when influential stimuli are salient and are plausible causes of the responses they produce, and will not occur when stimuli are not salient or are not plausible causes. (86 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved)"
744,"beyond the interface encountering artifacts in use",359106,"Beyond the interface: encountering artifacts in use","In this paper, we provide a brief overview and critique of the descriptions and concepts that are currently used in the HCI area coming primarily from the cognitive science tradition, as they seem to embed within them certain assumptions which are overly limiting. Then we look at some recent arguments for re-organizing our conception of the field, or extending the field, coming primarily from within the field itself (as presently constituted). Section 4 then presents a more elaborated ""activity-theoretical"" framework as one possible alternative, or perhaps complementary, framework that may give a richer depiction of the HCI field. In Section 5 we return to look more specifically at the different theoretical viewpoints, especially in regard to their re-framing of issues in the field, relating the different emphases to another field, software engineering. This section tries to summarize some of the main form that can serve as a basis for future discussion."
745,"on the move with a magic thing role playing in concept design of mobile services and devices",359369,"On the Move with a Magic Thing: Role Playing in Concept Design of Mobile Services and Devices.","Designing concepts for new mobile services and devices, poses several challenges to the design. We consider user participation as a way to address part of the challenges. We show how our effort relates to current and past research. In particular, PD (Participatory Design) has inspired us in developing two participatory techniques. The two techniques are organized around situations either staged or real where users and designers can envision and enact future scenarios: a role-playing game with toys, and SPES (Situated and Participative Enactment of Scenarios). They were developed in an industry-funded project that investigates services for the nomadic Internet user of the future. We then discuss how the techniques help in facing the design challenges."
746,"cybercode designing augmented reality environments with visual tags",359525,"{CyberCode}: designing augmented reality environments with visual tags","The CyberCode is a visual tagging system based on a 2D-barcode technology and provides several features not provided by other tagging systems. CyberCode tags can be recognized by the low-cost CMOS or CCD cameras found in more and more mobile devices, and it can also be used to determine the 3D position of the tagged object as well as its ID number. This paper describes examples of augmented reality applications based on CyberCode, and discusses some key characteristics of tagging technologies that must be taken into account when designing augmented reality environments."
747,"tasser an automated method for the prediction of protein tertiary structures in casp",361471,"TASSER: An automated method for the prediction of protein tertiary structures in CASP6.","The recently developed {TASSER} {(Threading/ASSembly/Refinement)} method is applied to predict the tertiary structures of all {CASP6} targets. {TASSER} is a hierarchical approach that consists of template identification by the threading program {PROSPECTOR_3,} followed by tertiary structure assembly via rearranging continuous template fragments. Assembly occurs using parallel hyperbolic Monte Carlo sampling under the guide of an optimized, reduced force field that includes knowledge-based statistical potentials and spatial restraints extracted from threading alignments. Models are automatically selected from the Monte Carlo trajectories in the low-temperature replicas using the clustering program {SPICKER.} For all 90 {CASP} targets/domains, {PROSPECTOR_3} generates initial alignments with an average root-mean-square deviation {(RMSD)} to native of 8.4 ý with 79% coverage. After {TASSER} reassembly, the average {RMSD} decreases to 5.4 ý over the same aligned residues; the overall cumulative {TM-score} increases from 39.44 to 52.53. Despite significant improvements over the {PROSPECTOR_3} template alignment observed in all target categories, the overall quality of the final models is essentially dictated by the quality of threading templates: The average {TM-scores} of {TASSER} models in the three categories are, respectively, 0.79 [comparative modeling {(CM),} 43 targets/domains], 0.47 [fold recognition {(FR),} 37 targets/domains], and 0.30 [new fold {(NF),} 10 targets/domains]. This highlights the need to develop novel (or improved) approaches to identify very distant targets as well as better {NF} algorithms. Proteins {2005;Suppl} 7:91-98. ý 2005 {Wiley-Liss,} Inc."
748,"the physiology of the grid an open grid services architecture for distributed systems integration",361809,"The Physiology of the Grid: An Open Grid Services Architecture for Distributed Systems Integration","In both e-business and e-science, we often need to integrate services across distributed, heterogeneous, dynamic &#034;virtual organizations&#034; formed from the disparate resources within a single enterprise and/or from external resource sharing and service provider relationships. This integration can be technically challenging because of the need to achieve various qualities of service when running on top of different native platforms. We present an Open Grid Services Architecture that addresses these challenges. Building on concepts and technologies from the Grid and Web services communities, this architecture defines a uniform exposed service semantics (the Grid service); defines standard mechanisms for creating, naming, and discovering transient Grid service instances; provides location transparency and multiple protocol bindings for service instances; and supports integration with underlying native platform facilities. The Open Grid Services Architecture also defines, in terms of Web Services Description Language (WSDL) interfaces and associated conventions, mechanisms required for creating and composing sophisticated distributed systems, including lifetime management, change management, and notification. Service bindings can support reliable invocation, authentication, authorization, and delegation, if required. Our presentation complements an earlier foundational article, &#034;The Anatomy of the Grid,&#034; by describing how Grid mechanisms can implement a service-oriented architecture, explaining how Grid functionality can be incorporated into a Web services framework, and illustrating how our architecture can be applied within commercial computing as a basis for distributed system integration--within and across organizational domains."
749,"metrics based refactoring",361881,"Metrics Based Refactoring","Refactoring is one key issue to increase internal software quality during the whole software lifecycle. Since identifying structures where refactorings should be applied often is explained with subjective perceptions like &ldquo;bad taste&rdquo; or &ldquo;bad smell&rdquo;, an automatic refactoring location finder seems difficult. We show that a special kind of metrics can support these subjective perceptions and thus can be used as an effective and efficient way to get support for the decision of where to apply refactoring. Due to the fact that the software developer is the last authority, we provide powerful and metrics based software visualisation to support the developers in judging their products. The authors demonstrate this approach for four typical refactorings and present both a tool supporting the identification and case studies of its application"
750,"evolution of function in protein superfamilies from a structural perspective",363536,"{Evolution of function in protein superfamilies, from a structural perspective.}","The recent growth in protein databases has revealed the functional diversity of many protein superfamilies. We have assessed the functional variation of homologous enzyme superfamilies containing two or more enzymes, as defined by the {CATH} protein structure classification, by way of the Enzyme Commission {(EC)} scheme. Combining sequence and structure information to identify relatives, the majority of superfamilies display variation in enzyme function, with 25 % of superfamilies in the {PDB} having members of different enzyme types. We determined the extent of functional similarity at different levels of sequence identity for 486,000 homologous pairs (enzyme/enzyme and enzyme/non-enzyme), with structural and sequence relatives included. For single and multi-domain proteins, variation in {EC} number is rare above 40 % sequence identity, and above 30 %, the first three digits may be predicted with an accuracy of at least 90 %. For more distantly related proteins sharing less than 30 % sequence identity, functional variation is significant, and below this threshold, structural data are essential for understanding the molecular basis of observed functional differences. To explore the mechanisms for generating functional diversity during evolution, we have studied in detail 31 diverse structural enzyme superfamilies for which structural data are available. A large number of variations and peculiarities are observed, at the atomic level through to gross structural rearrangements. Almost all superfamilies exhibit functional diversity generated by local sequence variation and domain shuffling. Commonly, substrate specificity is diverse across a superfamily, whilst the reaction chemistry is maintained. In many superfamilies, the position of catalytic residues may vary despite playing equivalent functional roles in related proteins. The implications of functional diversity within supefamilies for the structural genomics projects are discussed. More detailed information on these superfamilies is available at {http://www.biochem.ucl.ac.uk/bsm/FAM-EC/.}"
751,"economic mechanism design for computerized agents",364210,"Economic Mechanism Design for computerized Agents","The field of economic mechanism design has been an active area of research in economics for at least 20 years. This field uses the tools of economics and game theory to design ‘‘rules of interaction’ ’ for economic transactions that will, in principle, yield some desired outcome. In this paper I provide an overview of this subject for an audience interested in applications to electronic commerce and discuss some special problems that arise in this context. 1 Mechanism design As an example of mechanism design in action, let us consider the case of designing an auction to award an item to one of n individuals. Each individual i has a ‘‘maximum willingness to pay’ ’ or ‘‘value’ ’ for the item that we denote by vi. We assume that this value is private information known only by person i. Our goal is to design an auction that will award the item to the person with the highest value."
752,"the analysis of visual motion a comparison of neuronal and psychophysical performance",366148,"The analysis of visual motion: a comparison of neuronal and psychophysical performance.","We compared the ability of psychophysical observers and single cortical neurons to discriminate weak motion signals in a stochastic visual display. All data were obtained from rhesus monkeys trained to perform a direction discrimination task near psychophysical threshold. The conditions for such a comparison were ideal in that both psychophysical and physiological data were obtained in the same animals, on the same sets of trials, and using the same visual display. In addition, the psychophysical task was tailored in each experiment to the physiological properties of the neuron under study; the visual display was matched to each neuron's preference for size, speed, and direction of motion. Under these conditions, the sensitivity of most MT neurons was very similar to the psychophysical sensitivity of the animal observers. In fact, the responses of single neurons typically provided a satisfactory account of both absolute psychophysical threshold and the shape of the psychometric function relating performance to the strength of the motion signal. Thus, psychophysical decisions in our task are likely to be based upon a relatively small number of neural signals. These signals could be carried by a small number of neurons if the responses of the pooled neurons are statistically independent. Alternatively, the signals may be carried by a much larger pool of neurons if their responses are partially intercorrelated."
753,"mapping determinants of human gene expression by regional and genomewide association",366475,"Mapping determinants of human gene expression by regional and genome-wide association","To study the genetic basis of natural variation in gene expression, we previously carried out genome-wide linkage analysis and mapped the determinants of similar to 1,000 expression phenotypes(1). In the present study, we carried out association analysis with dense sets of single-nucleotide polymorphism ( SNP) markers from the International HapMap Project(2). For 374 phenotypes, the association study was performed with markers only from regions with strong linkage evidence; these regions all mapped close to the expressed gene. For a subset of 27 phenotypes, analysis of genome-wide association was performed with > 770,000 markers. The association analysis with markers under the linkage peaks confirmed the linkage results and narrowed the candidate regulatory regions for many phenotypes with strong linkage evidence. The genome-wide association analysis yielded highly significant results that point to the same locations as the genome scans for about 50% of the phenotypes. For one candidate determinant, we carried out functional analyses and confirmed the variation in cis-acting regulatory activity. Our findings suggest that association studies with dense SNP maps will identify susceptibility loci or other determinants for some complex traits or diseases."
754,"scalability fidelity and containment in the potemkin virtual honeyfarm",366673,"Scalability, fidelity, and containment in the potemkin virtual honeyfarm","The rapid evolution of large-scale worms, viruses and bot-nets have made Internet malware a pressing concern. Such infections are at the root of modern scourges including DDoS extortion, on-line identity theft, SPAM, phishing, and piracy. However, the most widely used tools for gathering intelligence on new malware -- network honeypots -- have forced investigators to choose between monitoring activity at a large scale or capturing behavior with high fidelity. In this paper, we describe an approach to minimize this tension and improve honeypot scalability by up to six orders of magnitude while still closely emulating the execution behavior of individual Internet hosts. We have built a prototype honeyfarm system, called  Potemkin , that exploits virtual machines, aggressive memory sharing, and late binding of resources to achieve this goal. While still an immature implementation, Potemkin has emulated over 64,000 Internet honeypots in live test runs, using only a handful of physical servers."
755,"stateful intrusion detection for highspeed networks",366727,"Stateful intrusion detection for high-speed network's","As networks become faster there is an emerging need for security, analysis techniques that can keep up with the increased network throughput. Existing network-based intrusion detection sensors can barely, keep up with bandwidths of a few hundred Mbps. Analysis tools that can deal with higher throughput are unable to maintain state between different steps of an attack or they are limited to the analysis of packet headers. We propose a partitioning approach to network security, analysis that supports in-depth, stateful intrusion detection on high-speed links. The approach is centered around a slicing mechanism that divides the overall network traffic into subsets of manageable size. The traffic partitioning is done so that a single slice contains all the evidence necessary to detect a specific attack, making sensor-to-sensor interactions unnecessary. This paper describes the approach and presents a first experimental evaluation of its effectiveness."
756,"teacher professional development technology and communities of practice are we putting the cart before the horse",368224,"Teacher professional development, technology, and communities of practice: Are we putting the cart before the horse?","Over the past decade, education reform and teacher training projects have spent a great deal of effort to create and support sustainable, scalable online communities of education professionals. For the most part, those communities have been created in isolation from the existing local professional communities within which the teachers practice. We argue that focusing on online technology solely as a mechanism to deliver training and/or create online networks places the cart before the horse by ignoring the Internet's even greater potential to help support and strengthen local communities of practice within which teachers work. In this article we seek guideposts to help education technologists understand the nature of local K-12 education communities of practice--specifically their reciprocal relationship with teacher professional development and instructional improvement interventions--as a prerequisite to designing online sociotechnical infrastructure that supports the professional growth of education professionals."
757,"the episodic buffer a new component of working memory",368247,"The episodic buffer: a new component of working memory?","In 1974, Baddeley and Hitch proposed a three-component model of working memory. Over the years, this has been successful in giving an integrated account not only of data from normal adults, but also neuropsychological, developmental and neuroimaging data. There are, however, a number of phenomena that are not readily captured by the original model. These are outlined here and a fourth component to the model, the episodic buffer, is proposed. It comprises a limited capacity system that provides temporary storage of information held in a multimodal code, which is capable of binding information from the subsidiary systems, and from long-term memory, into a unitary episodic representation. Conscious awareness is assumed to be the principal mode of retrieval from the buffer. The revised model differs from the old principally in focussing attention on the processes of integrating information, rather than on the isolation of the subsystems. In doing so, it provides a better basis for tackling the more complex aspects of executive control in working memory."
758,"planet of slums",369239,"Planet of Slums","{<B>Celebrated urban theorist lifts the lid on the effects of a global explosion of disenfranchised slum-dwellers.</B><BR><BR>According to the United Nations, more than one billion people now live in the slums of the cities of the South. In this brilliant and ambitious book, Mike Davis explores the future of a radically unequal and explosively unstable urban world. <BR><BR>From the sprawling <I>barricadas</I> of Lima to the garbage hills of Manila, urbanization has been disconnected from industrialization, even economic growth. Davis portrays a vast humanity warehoused in shantytowns and exiled from the formal world economy. He argues that the rise of this informal urban proletariat is a wholly original development unforeseen by either classical Marxism or neoliberal theory.<BR><BR>Are the great slums, as a terrified Victorian middle class once imagined, volcanoes waiting to erupt? Davis provides the first global overview of the diverse religious, ethnic, and political movements competing for the souls of the new urban poor. He surveys Hindu fundamentalism in Bombay, the Islamist resistance in Casablanca and Cairo, street gangs in Cape Town and San Salvador, Pentecostalism in Kinshasa and Rio de Janeiro, and revolutionary populism in Caracas and La Paz.<I>Planet of Slums</I> ends with a provocative meditation on the ""war on terrorism"" as an incipient world war between the American empire and the slum poor.}"
759,"unified segmentation",373277,"Unified segmentation","A probabilistic framework is presented that enables image registration, tissue classification, and bias correction to be combined within the same generative model. A derivation of a log-likelihood objective function for the unified model is provided. The model is based on a mixture of Gaussians and is extended to incorporate a smooth intensity variation and nonlinear registration with tissue probability maps. A strategy for optimising the model parameters is described, along with the requisite partial derivatives of the objective function."
760,"a single determinant dominates the rate of yeast protein evolution",373611,"A single determinant dominates the rate of yeast protein evolution.","A gene's rate of sequence evolution is among the most fundamental evolutionary quantities in common use, but what determines evolutionary rates has remained unclear. Here, we carry out the first combined analysis of seven predictors (gene expression level, dispensability, protein abundance, codon adaptation index, gene length, number of protein-protein interactions, and the gene's centrality in the interaction network) previously reported to have independent influences on protein evolutionary rates. Strikingly, our analysis reveals a single dominant variable linked to the number of translation events which explains 40-fold more variation in evolutionary rate than any other, suggesting that protein evolutionary rate has a single major determinant among the seven predictors. The dominant variable explains nearly half the variation in the rate of synonymous and protein evolution. We show that the two most commonly used methods to disentangle the determinants of evolutionary rate, partial correlation analysis and ordinary multivariate regression, produce misleading or spurious results when applied to noisy biological data. We overcome these difficulties by employing principal component regression, a multivariate regression of evolutionary rate against the principal components of the predictor variables. Our results support the hypothesis that translational selection governs the rate of synonymous and protein sequence evolution in yeast. 10.1093/molbev/msj038"
761,"estimating continuous distributions in bayesian classifiers",373647,"Estimating Continuous Distributions in Bayesian Classifiers","{When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables. Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparametric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann Publishers, San Mateo, 1995 1 Introduction In rec...}"
762,"a graphbased recommender system for digital library",373702,"A Graph-based Recommender System for Digital Library","Research shows that recommendations comprise a valuable service for users of a digital library [11]. While most existing recommender systems rely either on a content-based approach or a collaborative approach to make recommendations, there is potential to improve recommendation quality by using a combination of both approaches (a hybrid approach). In this paper, we report how we tested the idea of using a graph-based recommender system that naturally combines the content-based and collaborative approaches. Due to the similarity between our problem and a concept retrieval task, a Hopfield net algorithm was used to exploit high-degree book-book, useruser and book-user associations. Sample hold-out testing and preliminary subject testing were conducted to evaluate the system, by which it was found that the system gained improvement with respect to both precision and recall by combining content-based and collaborative approaches. However, no significant improvement was observed by exploiting high-degree associations."
763,"formalising trust as a computational concept",373720,"Formalising Trust as a Computational Concept","Trust is a judgement of unquestionable utility — as humans we use it every day of our lives. However, trust has suffered from an imperfect understanding, a plethora of definitions, and informal use in the literature and in everyday life. It is common to say “I trust you, ” but what does that mean? This thesis provides a clarification of trust. We present a formalism for trust which provides us with a tool for precise discussion. The formalism is implementable: it can be embedded in an artificial agent, enabling the agent to make trust-based decisions. Its applicability in the domain of Distributed Artificial Intelligence (DAI) is raised. The thesis presents a testbed populated by simple trusting agents which substantiates the utility of the formalism. The formalism provides a step in the direction of a proper understanding and definition of human trust. A contribution of the thesis is its detailed exploration of the possibilities of future work in the area. Summary 1. Overview This thesis presents an overview of trust as a social phenomenon and discusses it formally. It argues that trust is: • A means for understanding and adapting to the complexity of the environment. • A means of providing added robustness to independent agents. • A useful judgement in the light of experience of the behaviour of others. • Applicable to inanimate others. The thesis argues these points from the point of view of artificial agents. Trust in an artificial agent is a means of providing an additional tool for the consideration of other agents and the environment in which it exists. Moreover, a formalisation of trust enables the embedding of the concept into an artificial agent. This has been done, and is documented in the thesis. 2. Exposition There are places in the thesis where it is necessary to give a broad outline before going deeper. In consequence it may seem that the subject is not receiving a thorough treatment, or that too much is being discussed at one time! (This is particularly apparent in the first and second chapters.) To present a thorough understanding of trust, we have proceeded breadth first in the introductory chapters. Chapter 3 expands, depth first, presenting critical views of established researchers."
764,"soft updates a solution to the metadata update problem in file systems",376684,"Soft updates: a solution to the metadata update problem in file systems","Metadata updates, such as file creation and block allocation, have consistently been identified as a source of performance, integrity, security, and availability problems for file systems.  Soft updates  is an implementation technique for low-cost sequencing of fine-grained updates to write-back cache blocks. Using soft updates to track and enforce metadata update dependencies, a file system can safely use delayed writes for almost all file operations. This article describes soft updates, their incorporation into the 4.4BSD fast file system, and the resulting effects on the sytem. We show that a disk-based file system using soft updates achieves memory-based file system performance while providing stronger integrity and security guarantees than most disk-based file  systems. For workloads that frequently perform updates on metadata (such as creating and deleting files), this improves performance by more than a factor of two and up to a factor of 20 when compared to the conventional synchronous write approach and by 4-19% when compared to an aggressive write-ahead logging approach. In addition, soft updates can improve file system availablity by relegating crash-recovery assistance (e.g., the  fsck  utility) to an optional and background role, reducing file system recovery time to less than one second."
765,"finescale phylogenetic architecture of a complex bacterial community",377043,"Fine-scale phylogenetic architecture of a complex bacterial community","Although molecular data have revealed the vast scope of microbial diversity, two fundamental questions remain unanswered even for well-defined natural microbial communities: how many bacterial types co-exist, and are such types naturally organized into phylogenetically discrete units of potential ecological significance? It has been argued that without such information, the environmental function, population biology and biogeography of microorganisms cannot be rigorously explored. Here we address these questions by comprehensive sampling of two large 16S ribosomal RNA clone libraries from a coastal bacterioplankton community. We show that compensation for artefacts generated by common library construction techniques reveals fine-scale patterns of community composition. At least 516 ribotypes (unique rRNA sequences) were detected in the sample and, by statistical extrapolation, at least 1,633 co-existing ribotypes in the sampled population. More than 50% of the ribotypes fall into discrete clusters containing less than 1% sequence divergence. This pattern cannot be accounted for by interoperon variation, indicating a large predominance of closely related taxa in this community. We propose that such microdiverse clusters arise by selective sweeps and persist because competitive mechanisms are too weak to purge diversity from within them."
766,"a constructivist alternative to the representational view of mind in mathematics education",377816,"A Constructivist Alternative to the Representational View of Mind in Mathematics Education","The representational view of mind in mathematics education is evidenced by theories that characterize learning as a process in which students modify their internal mental representations to construct mathematical relationships or structures that mirror those embodied in external instructional representations. It is argued that, psychologically, this view falls prey to the learning paradox, that, anthropologically, it fails to consider the social and cultural nature of mathematical activity and that, pedagogically, it leads to recommendations that are at odds with the espoused goal of encouraging learning with understanding. These difficulties are seen to arise from the dualism created between mathematics in students' heads and mathematics in their environment. An alternative view is then outlined and illustrated that attempts to transcend this dualism by treating mathematics as both an individual, constructive activity and as a communal, social practice. It is suggested that such an approach might make it possible to explain how students construct mathematical meanings and practices that, historically, took several thousand years to evolve without attributing to students the ability to peek around their internal representations and glimpse a mathematically prestructured environment. In addition, it is argued that this approach might offer a way to go beyond the traditional tripartite scheme of the teacher, the student, and mathematics that has traditionally guided reform efforts in mathematics education."
767,"the new higher level classification of eukaryotes with emphasis on the taxonomy of protists",378422,"The new higher level classification of eukaryotes with emphasis on the taxonomy of protists.","Abstract. This revision of the classification of unicellular eukaryotes updates that of Levine et al. (1980) for the protozoa and expands it to include other protists. Whereas the previous revision was primarily to incorporate the results of ultrastructural studies, this revision incorporates results from both ultrastructural research since 1980 and molecular phylogenetic studies. We propose a scheme that is based on nameless ranked systematics. The vocabulary of the taxonomy is updated, particularly to clarify the naming of groups that have been repositioned. We recognize six clusters of eukaryotes that may represent the basic groupings similar to traditional ``kingdoms.'' The multicellular lineages emerged from within monophyletic protist lineages: animals and fungi from Opisthokonta, plants from Archaeplastida, and brown algae from Stramenopiles."
768,"computational design and experimental validation of oligonucleotidesensing allosteric ribozymes",381128,"Computational design and experimental validation of oligonucleotide-sensing allosteric ribozymes","Allosteric RNAs operate as molecular switches that alter folding and function in response to ligand binding. A common type of natural allosteric RNAs is the riboswitch; designer RNAs with similar properties can be created by RNA engineering. We describe a computational approach for designing allosteric ribozymes triggered by binding oligonucleotides. Four universal types of RNA switches possessing AND, OR, YES and NOT Boolean logic functions were created in modular form, which allows ligand specificity to be changed without altering the catalytic core of the ribozyme. All computationally designed allosteric ribozymes were synthesized and experimentally tested in vitro. Engineered ribozymes exhibit >1,000-fold activation, demonstrate precise ligand specificity and function in molecular circuits in which the self-cleavage product of one RNA triggers the action of a second. This engineering approach provides a rapid and inexpensive way to create allosteric RNAs for constructing complex molecular circuits, nucleic acid detection systems and gene control elements."
769,"collaborating around collections informing the continued development of photoware",381880,"Collaborating around collections: informing the continued development of photoware","This paper explores the embodied interactional ways in which people naturally collaborate around and share collections of photographs. We employ ethnographic studies of paper-based photograph use to consider requirements for distributed collaboration around digital photographs. Distributed sharing is currently limited to the 'passing on' of photographs to others, by email, webpages, or mobile phones. To move beyond this, a fundamental challenge for photoware consists of developing support for the practical achievement of sharing 'at a distance'. Specifically, this entails augmenting the natural production of accounts or 'photo-talk' to support the distributed achievement of sharing."
770,"technology as experience",381881,"Technology as Experience","{In <i>Technology as Experience</i>, John McCarthy and Peter Wright argue that any account of what is often called the user experience must take into consideration the emotional, intellectual, and sensual aspects of our interactions with technology. We don't just use technology, they point out; we live with it. They offer a new approach to understanding human-computer interaction through examining the felt experience of technology. Drawing on the pragmatism of such philosophers as John Dewey and Mikhail Bakhtin, they provide a framework for a clearer analysis of technology as experience.<br /> <br /> Just as Dewey, in <i>Art as Experience</i>, argued that art is part of everyday lived experience and not isolated in a museum, McCarthy and Wright show how technology is deeply embedded in everyday life. The ""zestful integration"" or transcendent nature of the aesthetic experience, they say, is a model of what human experience with technology might become.<br /> <br /> McCarthy and Wright illustrate their theoretical framework with real-world examples that range from online shopping to ambulance dispatch. Their approach to understanding human computer interaction -- seeing it as creative, open, and relational, part of felt experience -- is a measure of the fullness of technology's potential to be more than merely functional.}"
771,"an ethnographic study of copy and paste programming practices in oopl",381884,"An Ethnographic Study of Copy and Paste Programming Practices in OOPL","Although programmers frequently copy and paste  code when they develop software, implications of  common copy and paste (C&P) usage patterns have  not been studied previously. We have conducted an  ethnographic study in order to understand  programmers' C&P programming practices and  discover opportunities to assist common C&P usage  patterns. We observed programmers using an  instrumented Eclipse IDE and then analyzed why and  how they use C&P operations. Based on our analysis,  we constructed a taxonomy of C&P usage patterns.  This paper presents our taxonomy of C&P usage  patterns and discusses our insights with examples  drawn from our observations. From our insights, we  propose a set of tools that both can reduce software  maintenance problems incurred by C&P and can  better support the intents of commonly used C&P  scenarios."
772,"dynamic class loading in the java virtual machine",382687,"Dynamic class loading in the Java virtual machine","Class loaders are a powerful mechanism for dynamically loading software components on the Java platform. They are unusual in supporting all of the following features: laziness, type-safe linkage, user-defined extensibility, and multiple communicating namespaces.  We present the notion of class loaders and demonstrate some of their interesting uses. In addition, we discuss how to maintain type safety in the presence of user-defined dynamic class loading.  1 Introduction  In this paper, we..."
773,"selfsimilar scalefree networks and disassortativity",382953,"Self-similar Scale-free Networks and Disassortativity","Self-similar networks with scale-free degree distribution have recently attracted much attention, since these apparently incompatible properties were reconciled in a paper by Song et al. by an appropriate box-counting method that enters the measurement of the fractal dimension. We study two genetic regulatory networks ({\it Saccharomyces cerevisiae} and {\it Escherichai coli} and show their self-similar and scale-free features, in extension to the datasets studied by Song et al. Moreover, by a number of numerical results we support the conjecture that self-similar scale-free networks are not assortative. From our simulations so far these networks seem to be disassortative instead. We also find that the qualitative feature of disassortativity is scale-invariant under renormalization, but it appears as an intrinsic feature of the renormalization prescription, as even assortative networks become disassortative after a sufficient number of renormalization steps."
774,"transductive inference for text classification using support vector machines",384511,"Transductive Inference for Text Classification using Support Vector Machines","This paper introduces Transductive Support Vector Machines (TSVMs) for text classifi­ cation. While regular Support Vector Ma­ chines (SVMs) try to induce a general deci­ sion function for a learning task, Transduc­ tive Support Vector Machines take into ac­ count a particular test set and try to mini­ mize misclassifications of just those particu­ lar examples. The paper presents an anal­ ysis of why TSVMs are well suited for text classification. These theoretical findings are supported by experiments on three test col­ lections. The experiments show substantial improvements over inductive methods, espe­ cially for small training sets, cutting the num­ ber of labeled training examples down to a twentieth on some tasks. This work also pro­ poses an algorithm for training TSVMs effi­ ciently, handling 10,000 examples and more."
775,"the social construction of facts and artefacts or how the sociology of science and the sociology of technology might benefit each other",386078,"The Social Construction of Facts and Artefacts: Or How the Sociology of Science and the Sociology of Technology Might Benefit Each Other","The need for an integrated social constructivist approach towards the study of science and technology is outlined. Within such a programme both scientific facts and technological artefacts are to be understood as social constructs. Literature on the sociology of science, the science-technology relationship, and technology studies is reviewed. The empirical programme of relativism within the sociology of scientific knowledge and a recent study of the social construction of technological artefacts are combined to produce the new approach. The concepts of `interpretative flexibility' and `closure mechanism', and the notion of `social group' are developed and illustrated by reference to a study of solar physics and a study of the development of the bicycle. The paper concludes by setting out some of the terrain to be explored in future studies. 10.1177/030631284014003004"
776,"functional programming with overloading and higherorder polymorphism",386121,"Functional Programming with Overloading and Higher-Order Polymorphism",". The Hindley/Milner type system has been widely adopted as  a basis for statically typed functional languages. One of the main reasons  for this is that it provides an elegant compromise between flexibility, allowing  a single value to be used in different ways, and practicality, freeing  the programmer from the need to supply explicit type information.  Focusing on practical applications rather than implementation or theoretical  details, these notes examine a range of extensions that ..."
777,"constrained geometric simulation of diffusive motion in proteins",386174,"Constrained geometric simulation of diffusive motion in proteins","We describe a new computational method, FRODA (framework rigidity optimized dynamic algorithm), for exploring the internal mobility of proteins. The rigid regions in the protein are first determined, and then replaced by ghost templates which are used to guide the movements of the atoms in the protein. Using random moves, the available conformational phase space of a 100 residue protein can be well explored in approximately 10-100 min of computer time using a single processor. All of the covalent, hydrophobic and hydrogen bond constraints are maintained, and van der Waals overlaps are avoided, throughout the simulation. We illustrate the results of a FRODA simulation on barnase, and show that good agreement is obtained with nuclear magnetic resonance experiments. We additionally show how FRODA can be used to find a pathway from one conformation to another. This directed dynamics is illustrated with the protein dihydrofolate reductase."
778,"an insight into domain combinations",388727,"An insight into domain combinations.","Domains are the building blocks of all globular proteins, and are units of compact three-dimensional structure as well as  evolutionary units. There is a limited repertoire of domain families, so that these domain families are duplicated and combined in different ways to form the set of proteins in a genome. Proteins are gene products. The processes that produce new genes are duplication and recombination as well as gene fusion and fission. We attempt to gain an overview of these processes by studying the structural domains in the proteins of seven genomes from the three kingdoms of life: Eubacteria, Archaea and Eukaryota. We use here the domain and superfamily definitions in Structural Classification of Proteins Database (SCOP) in order to map pairs of adjacent domains in genome  sequences in terms of their superfamily combinations. We find 624 out of the 764 superfamilies in SCOP in these genomes, and the 624 families occur in 585 pairwise combinations. Most families are observed in combination with one or two other families, while a few families are very versatile in their combinatorial behaviour. This type of pattern can be described by a scale-free network. Finally, we study domain repeats and we compare the set of the domain combinations in the genomes to those in PDB, and discuss the implications for structural genomics.  Contact: apic@mrc-lmb.cam.ac.uk 10.1093/bioinformatics/17.suppl_1.S83"
779,"using latent semantic analysis to identify similarities in source code to support program understanding",391336,"Using latent semantic analysis to identify similarities in source code to support program understanding","The paper describes the results of applying Latent Semantic Analysis (LSA), an advanced information retrieval method, to program source code and associated documentation. Latent semantic analysis is a corpus based statistical method for inducing and representing aspects of the meanings of words and passages (of natural language) reflective in their usage. This methodology is assessed for application to the domain of software components (i.e., source code and its accompanying documentation). Here LSA is used as the basis to cluster software components. This clustering is used to assist in the understanding of a nontrivial software system, namely a version of Mosaic. Applying latent semantic analysis to the domain of source code and internal documentation for the support of program understanding is a new application of this method and a departure from the normal application domain of natural language"
780,"borders of multiple visual areas in humans revealed by functional magnetic resonance imaging",392247,"Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging.","The borders of human visual areas V1, V2, VP, V3, and V4 were precisely and noninvasively determined. Functional magnetic resonance images were recorded during phase-encoded retinal stimulation. This volume data set was then sampled with a cortical surface reconstruction, making it possible to calculate the local visual field sign (mirror image versus non-mirror image representation). This method automatically and objectively outlines area borders because adjacent areas often have the opposite field sign. Cortical magnification factor curves for striate and extrastriate cortical areas were determined, which showed that human visual areas have a greater emphasis on the center-of-gaze than their counterparts in monkeys. Retinotopically organized visual areas in humans extend anteriorly to overlap several areas previously shown to be activated by written words. 10.1126/science.7754376"
781,"ancient and recent positive selection transformed opioid cisregulation in humans",392495,"Ancient and Recent Positive Selection Transformed Opioid cis-Regulation in Humans","Changes in the cis -regulation of neural genes likely contributed to the evolution of our species' unique attributes, but evidence of a role for natural selection has been lacking. We found that positive natural selection altered the cis -regulation of human prodynorphin, the precursor molecule for a suite of endogenous opioids and neuropeptides with critical roles in regulating perception, behavior, and memory. Independent lines of phylogenetic and population genetic evidence support a history of selective sweeps driving the evolution of the human prodynorphin promoter. In experimental assays of chimpanzeeâ€“human hybrid promoters, the selected sequence increases transcriptional inducibility. The evidence for a change in the response of the brain's natural opioids to inductive stimuli points to potential human-specific characteristics favored during evolution. In addition, the pattern of linked nucleotide and microsatellite variation among and within modern human populations suggests that recent selection, subsequent to the fixation of the human-specific mutations and the peopling of the globe, has favored different prodynorphin cis -regulatory alleles in different parts of the world."
782,"educating the net generation",392815,"Educating the Net Generation","The Net Generation has grown up with information technology. The aptitudes, attitudes, expectations, and learning styles of Net Gen students reflect the environment in which they were raised -- one that is decidedly different from that which existed when faculty and administrators were growing up.  This collection explores the Net Gen and the implications for institutions in areas such as teaching, service, learning space design, faculty development, and curriculum. Contributions by educators and students are included."
783,"the topological relationship between the largescale attributes and local interaction patterns of complex networks",393074,"The topological relationship between the large-scale attributes and local interaction patterns of complex networks.","Recent evidence indicates that the abundance of recurring elementary interaction patterns in complex networks, often called subgraphs or motifs, carry significant information about their function and overall organization. Yet, the underlying reasons for the variable quantity of different subgraph types, their propensity to form clusters, and their relationship with the networks' global organization remain poorly understood. Here we show that a network's large-scale topological organization and its local subgraph structure mutually define and predict each other, as confirmed by direct measurements in five well studied cellular networks. We also demonstrate the inherent existence of two distinct classes of subgraphs, and show that, in contrast to the low-density type II subgraphs, the highly abundant type I subgraphs cannot exist in isolation but must naturally aggregate into subgraph clusters. The identified topological framework may have important implications for our understanding of the origin and function of subgraphs in all complex networks."
784,"colloidosomes selectively permeable capsules composed of colloidal particles",393136,"Colloidosomes: Selectively Permeable Capsules Composed of Colloidal Particles","We present an approach to fabricate solid capsules with precise control of size, permeability, mechanical strength, and compatibility. The capsules are fabricated by the self-assembly of colloidal particles onto the interface of emulsion droplets. After the particles are locked together to form elastic shells, the emulsion droplets are transferred to a fresh continuous-phase fluid that is the same as that inside the droplets. The resultant structures, which we call ""colloidosomes,"" are hollow, elastic shells whose permeability and elasticity can be precisely controlled. The generality and robustness of these structures and their potential for cellular immunoisolation are demonstrated by the use of a variety of solvents, particles, and contents."
785,"group and topic discovery from relations and text",393513,"Group and Topic Discovery from Relations and Text","We present a probabilistic generative model of entity relationships and textual attributes that simultaneously discover groups among the entities and topics among the corresponding text. Block-models of relationship data have been studied in social network analysis for some time. Here we simultaneously cluster in several modalities at once, incorporating the words associated with certain relationships. Significantly, joint inference allows the discovery of groups to be guided by the emerging topics, and vice-versa. We present experimental results on two large data sets: sixteen years of bills put before the U.S. Senate, comprising their corresponding text and voting records, and 43 years of similar data from the United Nations. We show that in comparison with traditional, separate latent-variable models for words or Blockstructures for votes, the Group-Topic model's joint inference improves both the groups and the topics discovered."
786,"gene expression profiling of nrfmediated protection against oxidative injury",398140,"Gene expression profiling of NRF2-mediated protection against oxidative injury.","Nuclear factor E2 p45-related factor 2 (NRF2) contributes to cellular protection against oxidative insults and chemical carcinogens via transcriptional activation of antioxidant/detoxifying enzymes. To understand the molecular basis of NRF2-mediated protection against oxidative lung injury, pulmonary gene expression profiles were characterized in Nrf2-disrupted (Nrf2(-/-)) and wild-type (Nrf2(+/+)) mice exposed to hyperoxia or air. Genes expressed constitutively higher in Nrf2(+/+) mice than in Nrf2(-/-) mice included antioxidant defense enzyme and immune cell receptor genes. Higher basal expression of heat shock protein and structural genes was detected in Nrf2(-/-) mice relative to Nrf2(+/+) mice. Hyperoxia enhanced expression of 175 genes (> or = twofold) and decreased expression of 100 genes (> or =50%) in wild-type mice. Hyperoxia-induced upregulation of many well-known/new antioxidant/defense genes (e.g., Txnrd1, Ex, Cp-2) and other novel genes (e.g., Pkc-alpha, Tcf-3, Ppar-gamma) was markedly greater in Nrf2(+/+) mice than in Nrf2(-/-) mice. In contrast, induced expression of genes encoding extracellular matrix and cytoskeletal proteins was higher in Nrf2(-/-) mice than in Nrf2(+/+) mice. These NRF2-dependent gene products might have key roles in protection against hyperoxic lung injury. Results from our global gene expression profiles provide putative downstream molecular mechanisms of oxygen tissue toxicity."
787,"sampling searchengine results",398285,"Sampling search-engine results","We consider the problem of efficiently sampling Web search engine query results. In turn, using a small random sample instead of the full set of results leads to efficient approximate algorithms for several applications, such as:  Determining the set of categories in a given taxonomy spanned by the search results; Finding the range of metadata values associated to the result set in order to enable ""multi-faceted search;"" Estimating the size of the result set; Data mining associations to the query terms. We present and analyze an efficient algorithm for obtaining uniform random samples applicable to any search engine based on posting lists and document-at-a-time evaluation. (To our knowledge, all popular Web search engines, e.g. Google, Inktomi, AltaVista, AllTheWeb, belong to this class.)Furthermore, our algorithm can be modified to follow the modern object-oriented approach whereby posting lists are viewed as streams equipped with a  next  method, and the  next  method for Boolean and other complex queries is built from the  next  method for primitive terms. In our case we show how to construct a basic  next(p)  method that samples term posting lists with probability p, and show how to construct  next(p)  methods for Boolean operators (AND, OR, WAND) from primitive methods.Finally, we test the efficiency and quality of our approach on both synthetic and real-world data."
788,"prediction of mammalian microrna targets",398542,"Prediction of mammalian microRNA targets.","1Department of Biology, Massachusetts Institute of Technology, Cambridge, MA 02139 USA 2Whitehead Institute for Biomedical Research, 9 Cambridge Center, Cambridge, MA 02142 USA MicroRNAs (miRNAs) can play important gene regulatory roles in nematodes, insects, and plants by basepairing to mRNAs to specify posttranscriptional repression of these messages. However, the mRNAs regulated by vertebrate miRNAs are all unknown. Here we predict more than 400 regulatory target genes for the conserved vertebrate miRNAs by identifying mRNAs with conserved pairing to the 5′ region of the miRNA and evaluating the number and quality of these complementary sites. Rigorous tests using shuffled miRNA controls supported a majority of these predictions, with the fraction of false positives estimated at 31% for targets identified in human, mouse, and rat and 22% for targets identified in pufferfish as well as mammals. Eleven predicted targets (out of 15 tested) were supported experimentally using a HeLa cell reporter system. The predicted regulatory targets of mammalian miRNAs were enriched for genes involved in transcriptional regulation but also encompassed an unexpectedly broad range of other functions.   MicroRNA genes are one of the more abundant classes of regulatory genes in animals, estimated to comprise between 0.5 and 1 percent of the predicted genes in worms, flies, and humans, raising the prospect that they could have many more regulatory functions than those uncovered to date (Lagos-Quintana et al. 2001), (Lau et al. 2001), (Lee and Ambros 2001), (Lai et al. 2003), (Lim et al. 2003a) and (Lim et al. 2003b). The regulatory roles of the vertebrate miRNAs in particular remain unknown. The possibility that many mammalian miRNAs play important roles during development and other processes is supported by their tissue-specific or developmental stage-specific expression patterns as well as their evolutionary conservation, which is very strong within mammals and often extends to invertebrate homologs (Pasquinelli et al. 2000), (Aravin et al. 2001), (Lagos-Quintana et al. 2001), (Lagos-Quintana et al. 2002), (Lagos-Quintana et al. 2003), (Lau et al. 2001), (Lee and Ambros 2001), (Ambros et al. 2003b), (Dostie et al. 2003), (Houbaviy et al. 2003), (Krichevsky et al. 2003), (Lai et al. 2003), (Lim et al. 2003a), (Lim et al. 2003b) and (Moss and Tang 2003). Indeed, miR-181, one of the many miRNAs conserved among vertebrates, is preferentially expressed in the B lymphocytes of mouse bone marrow, and the ectopic expression of this miRNA in hematopoietic stem/progenitor cells modulates blood cell development such that the proportion of B lymphocytes increases (Chen et al., 2003). However, regulatory targets have not been established or even confidently predicted for any of the vertebrate miRNAs, which has slowed progress toward understanding the functions of these tiny noncoding RNAs in humans and other vertebrates. Finding regulatory targets is much easier for the plant miRNAs. In a systematic search for the targets of 13 Arabidopsis miRNA families, 49 unique targets were found with a signal-to-noise ratio exceeding 10:1, simply by looking for Arabidopsis messages with near-perfect complementarity to the miRNAs (Rhoades et al., 2002). Confidence in many of these predictions was bolstered by the observation that the complementarity is conserved among rice orthologs of the miRNAs and messages (Rhoades et al., 2002), and many of the 49 have since been confirmed experimentally (Llave et al. 2002), (Emery et al. 2003), (Kasschau et al. 2003) and (Tang et al. 2003). These predicted targets were greatly enriched in transcription factors involved in developmental patterning or stem cell maintenance and identity, suggesting that many plant miRNAs function during cellular differentiation to clear regulatory gene transcripts from daughter cell lineages, perhaps enabling more rapid differentiation without having to depend on regulatory genes having constitutively unstable messages (Rhoades et al., 2002). An analogous search for near-perfect pairing between the miRNAs and messages of C. elegans and Drosophila genes did not uncover more hits than would be expected by chance (Rhoades et al., 2002). More sophisticated methods for predicting targets of insect miRNAs have recently been published (Stark et al., 2003) or submitted (Enright et al. http://genomebiology.com/2003/4/11/P8). The method of Stark et al. (2003) provides lists of candidate target genes that when used in combination with additional biological criteria, including functional relationships shared among predicted targets of individual miRNAs, led to validation of six targets for two Drosophila miRNAs (Stark et al., 2003). The current Drosophila analyses do not include estimates of false positive rates, leaving open the question of the accuracy of these methods in cases where predicted targets of a miRNA do not have clear functional relatedness. In the present study, we describe an approach that predicts hundreds of mammalian miRNA targets and provide computational and experimental evidence that most are authentic, allowing us to begin to explore fundamental questions about miRNA:target relationships in animals. Pairing to the 5′ portion of the miRNA, particularly nucleotides 2–8, appears to be most important for target recognition by vertebrate miRNAs. As seen previously for plant miRNAs, the predicted regulatory targets of mammalian miRNAs are enriched for genes involved in transcriptional regulation. In addition, the predicted mammalian regulatory targets encompass an unexpectedly broad range of other functions. Indeed, several lines of evidence imply that the targets identified in this initial analysis are only a fraction of the total, supporting the possibility that miRNAs regulate the expression of a large portion of the mammalian transcriptome. To identify the targets of vertebrate miRNAs, we developed an algorithm called TargetScan (the TargetScan software is available for download at http://genes.mit.edu/targetscan), which combines thermodynamics-based modeling of RNA:RNA duplex interactions with comparative sequence analysis to predict miRNA targets conserved across multiple genomes (Figure 1). Given an miRNA that is conserved in multiple organisms and a set of orthologous 3′ UTR sequences from these organisms, TargetScan (1) searches the UTRs in the first organism for segments of perfect Watson-Crick complementarity to bases 2–8 of the miRNA (numbered from the 5′ end)—we refer to this 7 nt segment of the miRNA as the “miRNA seed” and UTR heptamers with perfect Watson-Crick complementarity to the seed as “seed matches”; (2) extends each seed match with additional base pairs to the miRNA as far as possible in each direction, allowing G:U pairs, but stopping at mismatches; (3) optimizes basepairing of the remaining 3′ portion of the miRNA to the 35 bases of the UTR immediately 5′ of each seed match using the RNAfold program (Hofacker et al., 1994), thus extending each seed match to a longer “target site”; (4) assigns a folding free energy G to each such miRNA:target site interaction (ignoring initiation free energy) using RNAeval (Hofacker et al., 1994); (5) assigns a Z score to each UTR, defined as: , where n is the number of seed matches in the UTR, Gk is the free energy of the miRNA:target site interaction (kcal/mol) for the kth target site evaluated in the previous step, and T is a parameter described below (UTRs that have no seed match are assigned a Z score of 1.0); (6) sorts the UTRs in this organism by Z score and assigns a rank Ri to each; (7) repeats this process for the set of UTRs from each organism; and (8) predicts as targets those genes for which both Zi ≥ ZC and Ri ≤ RC for an orthologous UTR sequence in each organism, where ZC and RC are pre-chosen Z score and rank cutoffs. The only free parameters in this protocol are RC and ZC, and the T parameter in the formula relating predicted free energy to Z score. The value of the T parameter influences the relative weighting of UTRs with fewer high-affinity target sites to those with larger numbers of low-affinity target sites, and in this sense is analogous to temperature. However, there is no thermodynamic meaning to the T parameter or the Z scores used in this analysis; they merely provide a convenient means of weighting and summing predicted folding free energies. Suitable values for RC, ZC, and T were assigned by optimization over a range of reasonable values using separate training and test sets of miRNAs. TargetScan was initially applied using two sets of miRNAs: a nonredundant pan-mammalian set of 79 miRNAs that have homologs in human, mouse, and pufferfish and identical sequence in human and mouse, but not necessarily pufferfish, and a nonredundant pan-vertebrate set of 55 miRNAs that have identical sequence in human, mouse, and pufferfish (Lagos-Quintana et al. 2001), (Lagos-Quintana et al. 2002), (Lagos-Quintana et al. 2003), (Mourelatos et al. 2002), (Dostie et al. 2003) and (Lim et al. 2003a). These sets, referred to as nrMamm and nrVert, respectively (Supplemental Table S1 at http://www.cell.com/cgi/content/full/115/7/787/DC1), are nonredundant in that when multiple miRNAs had identical seed heptamers, a single representative was chosen. The initial use of miRNAs that were both nonredundant and perfectly conserved among the queried species simplified the analysis of signal to noise. To predict mammalian miRNA targets, the nrMamm set of miRNAs was searched against orthologous human, mouse, and rat 3′ UTRs derived from the Ensembl classification of orthologous genes. Using RC = 200, ZC = 4.5, and T = 20, TargetScan identified 451 putative miRNA:target interactions (representing 400 distinct genes), an average of 5.7 targets per miRNA (Figure 2A). This number of predicted targets (the “signal”) was compared to the number of targets predicted for cohorts of shuffled (i.e., randomly permuted) miRNAs (the “noise”). As described below, these shuffled sequences were carefully screened to ensure that our estimates of noise were as accurate as possible and not artifactually low. An average of only 1.8 targets were identified per shuffled miRNA sequence, for a signal:noise ratio of 3.2:1. This ratio was higher than the roughly 2:1 ratio observed for targets of the nrMamm miRNA set predicted using only the human and mouse UTRs (Figure 2A), underscoring the importance of evolutionary conservation across multiple genomes in our approach. The signal:noise ratio improved to 4.6:1 when conservation was required additionally in the fourth and most divergent species, Fugu rubripes, using the nrVert set of miRNAs (Figure 2A). Although the signal:noise ratio improved as more genomes were included, the number of predicted targets per miRNA decreased—even though RC and ZC were relaxed to 350 and 4.5, respectively, and the value T = 10 was used for the four-species analysis (Figure 2A). Several factors might contribute to this effect, including the increased chance that an orthologous gene will be missing from the annotations of one genome as the number of organisms is increased. For example, the number of ortholog pairs available in human-mouse, 17166, decreased to 14539 ortholog sets in human-mouse-rat and 10276 ortholog sets in human-mouse-rat-Fugu. In addition, some miRNA:target interactions might not be conserved between mammals and fish. Another likely factor is that some features used by TargetScan to achieve an acceptable signal:noise ratio might not be strictly required for miRNA regulation. For example, although most known invertebrate miRNA target sites have 7 nt Watson-Crick seed matches (or longer matches), some do not, such as lin-41, a target of the C. elegans let-7 miRNA (Lee et al. 1993), (Wightman et al. 1993), (Moss et al. 1997), (Reinhart et al. 2000), (Abrahante et al. 2003), (Brennecke et al. 2003) and (Lin et al. 2003). Thus, increasing the number of species increases the probability that the orthologous UTR of one or more species harbors functional sites that fail to satisfy the criteria required for TargetScan detection. Nonetheless, in 115 cases involving the UTRs of 107 genes, the predicted target sites were sufficiently conserved to be detected by TargetScan in orthologous UTRs from all four vertebrates (details of these predictions are given in Supplemental Table S5 and Figure S1A on the Cell website). It is of utmost importance in this type of bioinformatic analysis to ensure that the shuffled control sequences preserve all relevant compositional features of the authentic miRNAs. For example, when compared to the seeds of shuffled cohorts that had not been screened to control for the expected number of target sites and the expected strength of miRNA:target site interactions, the seeds of vertebrate miRNAs have approximately 1.4 times as many seed matches in vertebrate UTRs. Specifically, the seeds of vertebrate miRNAs each had an average of about 2100 perfect-complement matches in masked vertebrate UTR regions whereas random heptamers with the same base composition averaged only about 1500 matches. The high number of additional matches seen for the miRNA seed (and also for the antisense of the seed) argues strongly against the biological significance of most of these matches. Instead, these excess matches appear to be the consequence of dinucleotide composition biases shared between vertebrate miRNAs and UTRs, which must be controlled for in order to avoid artificially high estimates of TargetScan signal:noise ratios (particularly in an algorithm that looks for multiple matches). Therefore, it was important to ensure that the shuffled miRNA controls matched the corresponding miRNAs closely in all sequence properties that impact the expected number and quality of TargetScan target sites. The properties we considered were (1) the expected frequency of seed matches in the UTR dataset; (2) the expected frequency of matching to the 3′ end of the miRNA; (3) the observed count of seed matches in the UTR dataset; and (4) the predicted free energy of a seed:seed match duplex. A miRNA shuffling protocol, MiRshuffle, was developed to generate randomized control sequences that possess all of these properties. For a given miRNA sequence, MiRshuffle generates a series of random permutations with the same length and base composition as the miRNA, until a shuffled sequence is found that matches the parent miRNA closely in each of the four criteria listed above. The MiRshuffle procedure calculated expected frequencies using a first-order Markov model of 3′ UTR composition that accounts for the long-recognized impact of dinucleotide frequency biases on the counts of longer oligonucleotides (Nussinov, 1981). As an additional control, another shuffling protocol was developed, DiMiRshuffle, which preserves the precise dinucleotide composition of both the seed and the 3′ end of the miRNA, as well as the seed match count and seed:seed match folding free energy. This protocol is less general than MiRshuffle in that not every oligonucleotide can be randomized while preserving exact dinucleotide composition—e.g., the only heptamer with the same dinucleotide composition as the miR-100 seed, ACCCGUA, is ACCCGUA itself. Nevertheless, it was possible to generate DiMiRShuffled controls for 47 of the 79 nrMamm miRNAs, and a signal:noise ratio of 3.5 was observed using this control in the three-mammal analysis (data not shown), comparable to the value obtained for MiRshuffled controls. Because of its wider applicability, MiRshuffle was used in all reported experiments. In summary, even when the shuffled control sequences were carefully selected to closely match the corresponding miRNAs in all sequence properties expected to influence the number and quality of target sites, these shuffled controls yielded far fewer targets than did the authentic miRNA sequences. This difference results from an increased propensity of vertebrate UTRs to contain multiple conserved regions of complementarity to authentic miRNAs. We conclude that this propensity reflects a functional relationship between the miRNAs and the identified UTRs—that is, to the extent that the signal exceeds the noise, these identified UTRs are the regulatory targets of the miRNAs. Correcting for the estimated rate of false positives, TargetScan appears to have identified an average of 5.7 − 1.8 = 3.9 true targets conserved across mammals per miRNA (Figure 2A). A number of factors limit the sensitivity of our method, including (1) the incompleteness of orthologous gene annotations; (2) the possibility that some targets do not meet our stringent seed matching, Z score, or rank criteria; (3) the possibility that some mammalian target sites lie outside the 3′ UTR, as often observed for plant miRNAs (Rhoades et al., 2002); (4) the requirement that targets be conserved in the complete set of organisms; and (5) the limitation that our method does not model the simultaneous interaction of multiple miRNA species with the same UTR. Thus, the actual number of target genes regulated by each miRNA is likely to be substantially higher. TargetScan treats the 5′ and 3′ ends of miRNAs differently, with perfect basepairing required for the seed at the 5′ end, but no such requirement at the 3′ end. The importance of complementarity to the 5′ portion of invertebrate miRNAs has been suspected since the observation that complementary sites within the lin-14 mRNA have “core elements” of complementarity to the 5′ segment of the lin-4 miRNA (Wightman et al., 1993) and has been corroborated with the observation that the 5′ segments of numerous invertebrate miRNAs are perfectly complementary to 3′ UTR elements that mediate posttranscriptional regulation or are known miRNA targets (Lai 2002) and (Stark et al. 2003). Moreover, the 5′ ends of related miRNAs tend to be better conserved than the 3′ ends (Lim et al., 2003b), further supporting the hypothesis that these segments are most critical for mRNA recognition. To explore this hypothesis, TargetScan was applied to predict targets of the nrVert miRNA set conserved between human, mouse, and rat using versions of the algorithm differing in the miRNA heptamer defined as the seed in step 1 (Figure 2B). Consistent with residues at the 5′ end of miRNAs being most important for target recognition, the highest signal:noise ratio was observed when the seed was positioned at or near the extreme 5′ end of the miRNA, with signal:noise values of 2.7, 3.4, and 1.6 observed for seeds at segments 1..7, 2..8, and 3..9, respectively, and signal:noise ratios of 1.3 or less at other seed positions. We suggest that the critical importance of pairing to segment 2..8 for target identification in silico reflects its importance for target recognition in vivo and speculate that this segment nucleates pairing between miRNAs and mRNAs. Those seed positions that had the highest signal:noise ratios in the sliding seed analysis (Figure 2B) also had the highest degree of heptamer conservation in paralogous human miRNAs (Figure 2C). This observation strengthens the assertion that the signal seen above noise in our analysis reflects a functional relationship between the miRNAs and the identified UTRs because otherwise it would be difficult to explain why the most conserved portions of the miRNA and not other miRNA segments have the greatest propensity to match multiple conserved segments in UTRs. The set of target genes predicted using conservation of miRNA complementarity across the three mammals was most suitable in size and quality for systematic analysis of gene function. To obtain as large a set of targets as possible, we searched our set of orthologous mammalian 3′ UTRs using an expanded set of 121 conserved mammalian miRNAs (rMamm, Supplemental Table S1 on Cell website) that includes miRNAs that were excluded from the nrMamm set because they had redundant seeds, yielding a total of 854 predicted miRNA:UTR pairs conserved across human, mouse, and rat (Supplemental Figure S1B). This number of predicted targets (854) represents an 89% increase over the 451 targets predicted for the nrMamm miRNAs, even though the number of miRNAs used increased by only 53% from 79 to 121. This discrepancy prompted us to ask whether membership in a multi-miRNA gene family influenced the abundance of targets. Indeed, we found that the 27 miRNAs in nrMamm that were members of paralogous miRNA families, i.e., families with variant miRNAs that have the same seed, had an average of 8.7 predicted targets per miRNA, more than twice the average of 4.2 seen for the remaining 52 nrMamm miRNAs, although the difference in signal:noise between these two sets was not as pronounced. When initially expanding our list of mammalian miRNAs, we found that the set of 19 mammalian miRNAs that were conserved between human and rodents but for which a Fugu homolog was not found gave an unacceptably low signal:noise ratio of 1.2:1, even though the analysis did not extend to the Fugu UTRs. Accordingly, the rMamm set was restricted to those miRNAs with recognized Fugu homologs. The higher signal seen for the more broadly conserved miRNAs can be explained by the idea that miRNAs with larger numbers of targets would be under greater selective constraint, and therefore less likely to change during the course of evolution. Thus, more broadly conserved miRNAs would be likely to have more targets and consequently a higher TargetScan signal. This observation again supports the conclusion that TargetScan is detecting authentic targets because otherwise it would be difficult to explain the observed difference in signal:noise for broadly conserved miRNAs relative to that of less broadly conserved miRNAs. The 854 miRNA:UTR pairs represented UTRs of just 442 distinct genes because many genes were hit by multiple miRNAs. In these cases, the miRNAs were usually, but not always, from the same paralogous miRNA family, often with the same seed heptamer. In those cases where the same UTR was hit by multiple miRNAs from different families (54 genes), the target sites generally did not overlap, consistent with simultaneous binding and regulation of some target genes by combinations of miRNAs. A complete list of the 442 target genes and the corresponding miRNAs is provided (Supplemental Figure S1B and Table S2 on the Cell website). An abbreviated list appears as Table 1, where genes were chosen on the basis of high biological interest. Genes involved in transcription, signal transduction, and cell-cell signaling dominate this list, including a number of human disease genes such as the tumor suppressor gene PTEN and the protooncogenes E2F-1, N-MYC, C-KIT, FLI-1, and LIF. Reporter assays were used to test 15 predicted targets of mammalian miRNAs in HeLa cells. The 15 targets selected for these experiments all had known biological functions but resembled the complete set of predictions in other respects, e.g., there was no significant difference in the average Z score, rank, or number of target sites per mRNA between the tested targets and the complete set of predicted targets. In only one case did the tested targets of a miRNA have obvious functional relatedness (NOTCH1, a receptor for DELTA1, both predicted targets of miR-34). Three of the 15 genes, SMAD-1, BRN-3b, and Notch1, were also in the set of predicted targets conserved to Fugu. Eight genes were predicted targets of miRNAs that had been cloned from HeLa cells (Lagos-Quintana et al. 2001) and (Mourelatos et al. 2002), and three genes were predicted targets of miR-34, which is also expressed in HeLa cells, based on Northern analysis (data not shown). For these 11 genes, a 100 to 1200 nt 3′ UTR segment that included miRNA target sites was inserted downstream of a firefly luciferase ORF, and luciferase activity was compared to that of an analogous reporter with point substitutions disrupting the target sites (as illustrated for SMAD-1, Figure 3A). Of these 11 UTRs, mutations in eight (SMAD-1, SDF-1, BRN-3b, ENX-1, N-MYC, PTEN, Delta1, and Notch1, but not HOX-A5, MECP-2, or VAMP-2) significantly enhanced expression (p < 0.001), as expected if the endogenous miRNAs in the HeLa cells were specifying the repression of reporter gene expression by pairing to the predicted target sites (Figure 3B). Significantly enhanced expression was also observed when the analogous experiment was performed using either the full-length C. elegans lin-41 3′ UTR or a 124 nt segment of the UTR containing the two previously proposed let-7 miRNA target sites (Reinhart et al., 2000), indicating that at least some of the repression of lin-41 observed in C. elegans can be recapitulated by HeLa let-7 miRNA in this heterologous reporter assay (Figure 3B). For all eight predicted human targets of endogenous HeLa miRNAs that responded to mutations, the increase in expression seen when disrupting the pairing to the miRNA seed was at least as high as that seen for mutations in the let-7 target sites of lin-41 (Figure 3B). Four tested genes (G6PD, BDNF, MCSF, and LDLR) were predicted targets of miR-1 and miR-130, two miRNAs that had not been cloned from HeLa cells and were not detected by Northern analysis. Initially, reporters containing UTR segments from these four genes were examined for response to transfected miRNAs (Doench et al., 2003) (data not shown). Of the four, G6PD, BDNF, and MCSF responded to the transfected miRNAs. To further validate these targets, we used a second assay resembling the one described for targets of miRNAs expressed in HeLa cells, except that it took advantage of HeLa cell lines ectopically expressing either human miR-1 or human miR-130. Mutations in the miRNA target sites of all three of the genes that had responded to transfected miRNAs led to significantly increased reporter output in the lines expressing the cognate miRNAs, but not in the parental lines lacking the miRNAs (Figure 3B), as expected if these genes were authentic targets of the respective miRNAs. The levels of ectopically expressed miR-1 and miR-130 were comparable to those of endogenous miRNAs, as judged by Northern blot analysis (Lim et al., 2003b). For miR-1, Northern analysis with a synthetic miR-1 standard allowed accurate quantitation, revealing an average expression of 500 miR-1 molecules per cell. In sum, for 11 of the 15 cases tested, the sites identified by TargetScan influenced expression of an upstream ORF when expressed in the same cells as the corresponding miRNAs. Additional experiments in animals will be needed to address the particular biological consequences of these regulatory interactions, but the evolutionary conservation of the pairings suggests that they are important. All four of the remaining genes might not be true targets; our statistical analysis using shuffled controls indicated that about 30% of predicted mammalian targets are likely to be false positives (Figure 2). Alternatively, some might still be authentic targets whose regulation was not detected in our assays. Regulation would be missed in cases for which cell type-specific factors were required that were not expressed in HeLa cells, or in cases for which additional mRNA elements were required but were not included in the UTR segments used in our reporters. One limitation of the existing sequence databases that complicates the systematic identification of miRNA targets is that UTR annotations are often absent or incomplete. In order to compensate for this limitation, we had extended each annotated 3′ UTR with 2 kb of 3′ flanking sequence. Using extended UTRs substantially increased the number of predicted targets, with signal-to-noise ratios at least as high as they were for unextended UTRs, suggesting that extension of the annotated UTRs allows detection of many additional authentic target genes. One consequence of using this UTR-extension protocol is that for some genes, all predicted target sites will fall outside of annotated UTRs. Manual inspection of the 15 UTR regions tested in our reporter assays revealed that in all but one of these cases the tested target sites were contained within regions whose status as UTRs was supported by known ESTs and predicted polyadenylation sites, even though some of these regions are not yet annotated as human UTRs. For the single exception, the Notch1 gene, the tested target sites were all located downstream of the annotated 3′ UTR of the human gene, and the end of the annotated Notch1 3′ UTR was supported by a predicted polyadenylation site and alignment of multiple ESTs. However, Notch1 might have additional 3′ UTR isoforms; many human genes—perhaps as many as 50% or more of the genes in the genome—have alternative polyadenylation sites (Iseli et al., 2002). In order to investigate the potential expression of the tested Notch1 target sites, which gave a positive result in our assay for miRNA regulation (Figure 3), an RT-PCR assay was used with polyA-selected RNA from a pool of human tissues. Consistent with the possibility that these sites lie within an alternative UTR isoform of Notch1, an RT-dependent product of the correct size and sequence was observed (data not shown). The TargetScan set of predicted mammalian target genes (Supplemental Table S1B on the Cell website) undoubtedly contains other examples for which the target sites all lie outside of the UTR regions supported by available data; some of these will be false positives, but others might point to the miRNA regulation of alternative mRNA isoforms. The finding that a sizable fraction of the tested UTR segments were sensitive to mutations disrupting their target sites supports the assertion that most of the predicted targets are authentic. For many, the pairing outside the seed was less extensive than that previously proposed for miRNA targets (Supplemental Figures S1A and S1B). Perhaps TargetScan is identifying mRNA elements that are necessary but not sufficient for miRNA regulation. Alternatively, these elements might be sufficient, in which case their low information content raises the possibility that miRNAs modulate the utilization of a substantial fraction of the mammalian mRNAs. In none of the 15 cases tested was there evidence of miRNA-mediated activation of reporter expression; changes either were not statistically significant or were in the direction of miRNA-directed repression. This result suggests that mammalian miRNAs are generally negative regulators of gene expression, as has been observed for the known examples in invertebrates and plants (Lai 2003) and (Bartel 2004). To assess target gene functions, we evaluated the frequency of specific gene ontology (GO) molecular function classifications (The Gene Ontology Consortium, 2001) among the predicted targets of the nrMamm miRNAs and their shuffled control sequences (Table 2). Predicted miRNA targets populated many major GO functional categories, and for each of these categories, the number of targets for the real miRNAs greatly exceeded the average for the shuffled cohorts. Therefore, despite the presence of false positives among our predictions, the data in Table 2 strongly indicate that mammalian miRNAs are involved in regulation of target genes with a wide spectrum of molecular functions. We also compared the proportion of genes that fell in each of the GO molecular function and GO biological process categories for the predicted targets of miRNAs, for targets of shuffled control sequences, and for the initial set of orthologous genes (Table 2 and Supplemental Table S4 on Cell website). The targets of the shuffled cohorts were enriched relative to the initial set of orthologous genes in certain GO biological process categories such as development (14% versus 8%) and transcription (13% versus 9%) (Table S4) and in molecular function categories such as nucleic acid binding (21% versus 14%), DNA binding (15% versus 10%), and transcriptional regulator activity (10% versus 6%) (Table 2). The biases seen for the shuffled cohorts are likely to result primarily from the TargetScan requirement for conserved segments in the 3′ UTRs of predicted targets and may reflect differences in the occurrence of 3′ UTR regulatory elements in different classes of genes. In the GO biological process classifications, the predicted regulatory targets of authentic miRNA genes were enriched in the development category but no more than the targets of shuffled controls and were substantially more enriched for genes involved in transcription (21% of miRNA targets versus 13% of shuffled targets versus 9% of the initial dataset) and regulation of transcription (21% versus 12% versus 8%) (Supplemental Table S4). In terms of the GO molecular function classifications, targets of authentic miRNAs were enriched in the categories DNA binding (20% versus 15% versus 10%), transcription regulatory activity (14% versus 10% versus 6%), and nucleotide binding (13% versus 8% versus 8%) (Table 2). The differing numbers of predicted targets in the similar-sounding categories “regulation of transcription” (GO biological process classification) and “transcription regulatory activity” (GO molecular function classification) prompted us to investigate the gene content of these two categories. Inspection of the lists of genes showed that all but two of the predicted target genes in the “transcription regulatory activity” category were also included in the larger “regulation of transcription category,” but that the latter category also contained more than two dozen additional target genes, the annotation of which generally supported a role in control of transcription. The GO process category “regulation of transcription” (Supplemental Table S4) therefore appears to provide a more complete listing of known and putative transcription factors. The proportion of the predicted mammalian miRNA target genes involved in the GO process categories “transcription” and “regulation of transcription” was significantly higher than that seen for either shuffled targets or for the initial gene set (p < 0.001). Nonetheless, this bias was much lower in magnitude than that seen in plants: of the 49 targets predicted in a systematic search for complementarity to plant miRNAs, 69% were members of transcription factor gene families (Rhoades et al., 2002). Examples of other types of predicted mammalian targets include translational regulators (e.g., COP9 subunit 6, ERF1), regulators of mRNA stability (e.g., HU-Antigen D), structural proteins (e.g., collagen), and enzymes (e.g., G6PD). The set of predicted miRNA targets conserved across all four vertebrates (Supplemental Table S5 online) was also somewhat biased toward genes involved in transcription, but had annotated functions consistent with the broad array of biological activities seen for the larger mammalian target set. We conclude that although mammalian miRNAs are sometimes at the center of gene regulatory networks, where they regulate genes, such as transcription factors, that regulate other genes, they are more likely than plant miRNAs to be at the periphery of the regulatory networks, where they regulate genes with a variety of molecular functions. The predicted mammalian targets also differ from the plant targets with respect to biological function. Nearly all of the transcription factors (TFs) predicted to be plant miRNA targets have known or implied roles in plant development, as do several of the other predicted plant targets (Rhoades et al., 2002). By comparison, only 13% of predicted mammalian miRNA targets were involved in development according to the GO biological process categories (Supplemental Table S4). An important caveat to this analysis is that gene annotation and GO categories are still evolving. Nonetheless, our data suggest that mammalian miRNAs are not exclusively, or even primarily, involved in the traditional miRNA role of developmental control. Instead, we find evidence for miRNA regulation of a very broad diversity of biological processes. Human and mouse miRNA sequences that satisfy established criteria (Ambros et al., 2003a) were downloaded from the Rfam website (http://www.sanger.ac.uk/Software/Rfam). Human miRNAs that lacked annotated mouse orthologs and mouse miRNAs that lacked annotated human orthologs were searched against the mouse and human genomes, respectively, with BLASTN (Altschul et al., 1997) and MiRscan (Lim et al. 2003a) and (Lim et al. 2003b). To identify Fugu homologs, the human miRNAs were searched against the Fugu genome using BLASTN and MiRscan, and the 121 human miRNAs with perfectly homologous miRNAs in mouse and clear homologous miRNAs in Fugu were assigned to rMamm. For sets of human miRNAs in rMamm with identical seed heptamers, a single representative was chosen, yielding 79 human miRNAs (nrMamm). The choice was based on conservation to Fugu and C. elegans miRNAs when possible (i.e., the sequence most broadly conserved was chosen), but was otherwise essentially arbitrary (the miRNA with the lowest mir-# was generally chosen). The subset of 55 miRNAs from nrMamm that had perfect conservation to Fugu were assigned to nrVert. 3′ UTR sequences for all human genes, and all mouse, rat, and Fugu genes associated with a human ortholog, were retrieved using EnsMart version 15.1 (http://www.ensembl.org/EnsMart). Annotated 3′ UTR sequences were available for only 45% of rat genes in this set and for none of the Fugu genes. Moreover, 14% of annotated rat 3′ UTR sequences were less than 50 nucleotides in length. Therefore, we extended each annotated 3′ UTR with 2 kb of 3′ flanking sequence. Repetitive elements were masked in these sequences using RepeatMasker (Smit, A.F.A. and Green, P., http://repeatmasker.genome.washington.edu/cgi-bin/RM2_req.pl) with repeat libraries for primates, rodents, or vertebrates, as appropriate. The 3′ UTR sequences were searched for antisense matches to the designated seed region of each miRNA (e.g., bases 2..8 starting from the 5′ end). Our choice of a 7 nt seed was motivated by the observation that shorter seeds gave substantially lower signal:noise ratios, while longer seeds reduced the number of predicted targets at comparable signal:noise ratios. Because changing the size of the seed has a large effect on the noise as well as the signal, these observations are much more difficult to interpret in terms of potential mechanistic implications than the “sliding seed” data of Figure 2B. For seeds located on the 5′ portion of the miRNA, 35 nt flanking the seed match on the 5′ end and 5 nt flanking the seed match on the 3′ end were retrieved (a “mirror” version of this algorithm was used for 3′ seeds in the experiment described in Figure 2B). Target sites in which the 35 nt flanking region contained masked bases or the seed match occurred less than 20 nt downstream of a previous seed match were discarded. Basepairing between the miRNA seed and UTR was extended with additional flanking basepairs as far as possible in both directions, allowing G:U pairs but disallowing gaps. The basepairing pattern of the remaining 3′ end (or in the case of a 3′ seed, the remaining 5′ end) was predicted by running RNAfold on a foldback sequence consisting of an artificial stemloop (5′- GGGCCCGGGULLLLLLACCCGGGCCC-3′, where “L” is an anonymous unpaired loop character, and all other bases are paired to a complementary base on the opposite side of the stem) attached to the extended seed match. RNAfold optimization was constrained so that all basepairs found in previous steps were fixed, the structure of the artificial stem was fixed, and bases in the miRNA and UTR were allowed to pair only with bases in the UTR and miRNA, respectively. The stemloop was removed, and RNAeval was used to estimate the energy of the miRNA:UTR duplex formed by the basepairs determined in the previous steps. Training sets were constructed with 40 randomly chosen miRNAs from nrMamm and 27 randomly chosen miRNAs from nrVert. The remaining microRNAs were assigned to the nrMamm and nrVert reference sets. TargetScan was tested on the training sets with various parameter values: T was varied from 5 to 25 in increments of 5, ZC was varied between 1 and 10 in increments of 0.5, and RC was varied between 50 and 1000 in increments of 50. The parameters T = 20, ZC = 4.5, RC = 200 were found to give an optimal signal:noise of 3.4:1 for the nrMamm training set. When RC was raised to 300 or ZC was lowered to 4, the signal:noise decreased only moderately to 3:1. The parameters T = 10, ZC = 4.5, RC = 350 were found to give an optimal signal:noise of 4.6:1 for the nrVert training set used with UTR sets from all four genomes. For both the nrMamm and nrVert sets, the signal:noise ratios obtained using the training sets did not differ significantly from the corresponding signal:noise ratios obtained using the reference sets, and thus results from the two sets were merged. For each miRNA in nrMamm, randomly permuted sequences with the same starting base, length, and base composition as the real miRNA were generated until four sequences were found that deviated from the original miRNA by less than 15% in the following properties: (1) E(SM), the 1st order Markov probability of the seed match, (2) E(TM), the 1st order Markov probability of the antisense of the 3′ end of the miRNA (or the 5′ end in the case of a 3′ miRNA seed), (3) O(SM), the observed count of seed matches in the UTR dataset, and (4) the predicted folding free energy of a seed:seed match duplex. For a miRNA (or shuffled miRNA) with the initial sequence S1,S2,S3,S4,S5,S6,S7,S8, and the seed designated as bases 2..8, E(SM) was equal to (PS1,S2·PS2,S3·PS3,S4·PS4,S5·PS5,S6·PS6,S7·PS7,S8) where PSk,Sk+1 was the conditional frequency of the nucleotide Sk+1 given Sk at the previous position in the set of inverse complements of the UTRs in the UTR database. E(TM) was the analogous quantity calculated for the remainder of the sequence (i.e., for bases 9, 10, 11, … to the end of the miRNA or shuffled miRNA). O(SM) was determined directly from heptamer counts in the UTR dataset. The predicted folding free energy of a seed:seed match duplex was determined using RNAeval. The DiMirShuffle program generated shuffled controls for a given miRNA sequence by shuffling the dinucleotides of the specified miRNA seed (e.g., bases 2..8 of the miRNA). The firefly luciferase vector was modified from pGL3 Control Vector (Promega), such that a short sequence containing multiple cloning sites (5′-AGCTCTATACGCGTCTCAAGCTTACTGCTAGCGT-3′) was inserted into the XbaI site immediately downstream from the stop codon. 3′UTR segments of the target genes were amplified by PCR from human genomic DNA and inserted into the modified pGL3 vector between SacI and NheI sites. PCR with the appropriate primers also generated inserts with point substitutions in the miRNA complementary sites. Wild-type and mutant inserts were confirmed by sequencing and are listed (Supplemental Figure S2 online). Adherent HeLa S3 cells were grown in 10% FBS in DMEM, supplemented with glutamine in the presence of antibiotics, to 80%–90% confluency in 24-well plates. Cells were transfected with 0.4 μg of the firefly luciferase reporter vector and 0.08 μg of the control vector containing Renilla luciferase, pRL-TK (Promega), in a final volume of 0.5 ml using Lipofectamine 2000 (Invitrogen). Firefly and Renilla luciferase activities were measured consecutively using the Dual-luciferase assays (Promega) 30 hr after transfection. Each firefly plasmid was tested in 12–15 transfections (four or five independent experiments, each with three culture replicates) involving two independent plasmid preparations (six to nine transfections each). A HeLa cell line that constitutively expressed miR-1 from a pol-II promoter was created using a derivative of the retroviral vector pRevTRE (Clontech) containing a 500 bp fragment of human mir-1d gene. A HeLa S3 cell line that constitutively expressed miR-130 from the H1 pol-III promoter was constructed using a retroviral vector containing a 330 nt fragment of the human mir-130 gene and a GFP gene under the murine 3-phosphoglycerate kinase promoter, which served as an infection marker (Chen, et al., 2003). Cells expressing GFP following infection were enriched to 95% purity by FACS. Gene ontologies were assigned to human genes from the Ensembl database by crossreferencing Ensembl identifiers with GO identifiers using EnsMart version 15.1 (http://www.ensembl.org/EnsMart). The Gene Ontology Consortium database was retrieved from http://www.geneontology.org and function and process ontologies were compiled for all predicted target genes. In addition to the assigned categories, each gene was considered as having all more general (“parent”) categories within the “Molecular Function” and “Biological Process” ontologies. In Tables 2 and S4, sets of GO categories were selected that were both broad enough to contain a significant fraction of the predicted targets and specific enough to be meaningful. Because the GO descriptions are not mutually exclusive, the sum of the percentages in these tables is not interpretable. GO categories were also used to produce the categories in Table 1. To be included in a category, a gene had to be annotated with at least one out of a set of GO categories. The sets of GO categories used were: regulation of transcription/DNA binding (GO:0003700, GO:0003713,GO:0003714, GO:0016563, or GO:0045449) and signal transduction/cell-cell signaling (GO:0004871, GO:0004872, GO:0007154, GO:0007165, GO:0007267 or GO:0008083).     Figure S1B   Figure S2   The inclusion of each miRNA in the three subsets used in this study (rMamm, nrMamm, and nrVert) is indicated by Y (Yes) or N (No). For those miRNAs in nrMamm, the sequences of the four shuffled variants generated my MiRhuffle are listed on the next four lines (labeled miR-X_sh0, miR-X_sh1, etc.). The 19 miRNAs not in rMamm are those for which Fuguhomologs could not be identified. For completeness, we list here a few additional human miRNAs that were not used in this study for various reasons, such as imperfect conservation between human and mouse: miR-17, miR-103b, miR-129, miR-134, miR-150, miR-151, miR-189 miR-200a, miR-217.   The 442 genes in this set were predicted as targets of rMamm miRNAs by TargetScan in human, mouse, and rat orthologs, as described in the text. MiRNAs with identical seeds that are predicted to target the same gene are shown in a single row of the table. MiRNAs with different seeds that target the same gene are listed on separate lines.   The 558 shuffled sequence:UTR pairs found human, mouse, and rat that were predicted by TargetScan for any of the four cohorts of MiRshuffled variants of nrMamm miRNAs are shown.   The number and percentage of UTRs annotated in various Gene Ontology biological process categories are shown for the 400 predicted miRNA-UTR pairs predicted by TargetScan for nrMamm miRNAs; the miRNA-UTR pairs predicted with randomized miRNAs (average of 4 cohorts); and for the total set of orthologous genes conserved between human, mouse, and rat. For cases in which GO categories with a parent-child relationship are shown, the child is indented. Note that the GO categories are not mutually exclusive.   The orthologous genes for this set scored highly as targets of nrVert miRNAs in all four organisms studied. MicroRNAs with different seeds that target the same gene are listed on separate lines. We thank W.K. Johnston for technical assistance, C-Z. Chen and L.P. Lim for helpful discussions, H.F. Lodish for use of facilities and equipment, N.C. Lau for the miR-1-expressing cell line, and G. Ruvkun for plasmids used to construct the lin-41 reporters. Supported by grants from the N.I.H (D.P.B. and C.B.B.), the Searle Scholars Program (C.B.B.), and the Alexander and Margaret Stewart Trust (D.P.B.), and fellowships from the DOE (B.P.L.) and the Cancer Research Institute (I.S.)."
789,"the modern molecular clock",400050,"THE MODERN MOLECULAR CLOCK","The discovery of the molecular clock — a relatively constant rate of molecular evolution — provided an insight into the mechanisms of molecular evolution, and created one of the most useful new tools in biology. The unexpected constancy of rate was explained by assuming that most changes to genes are effectively neutral. Theory predicts several sources of variation in the rate of molecular evolution. However, even an approximate clock allows time estimates of events in evolutionary history, which provides a method for testing a wide range of biological hypotheses ranging from the origins of the animal kingdom to the emergence of new viral epidemics."
790,"fading channels informationtheoretic and communications aspects",400549,"Fading channels: information-theoretic and communications aspects","In this paper we review the most peculiar and interesting information- theoretic and communications features of fading channels. We first describe the statistical models of fading channels which are frequently used in the analysis and design of communication systems. Next, we focus on the information theory of fading channels, by emphasizing capacity as the most important performance measure. Both single-user and multiuser transmission are examined. Further, we describe how the structure of fading channels impacts code design, and finally overview equalization of fading multipath channels"
791,"dialign improvement of the segmenttosegment approach to multiple sequence alignment",400626,"{DIALIGN 2: improvement of the segment-to-segment approach to multiple sequence alignment}","MOTIVATION: The performance and time complexity of an improved version of the segment-to-segment approach to multiple sequence alignment is discussed. In this approach, alignments are composed from gap-free segment pairs, and the score of an alignment is defined as the sum of so-called weights of these segment pairs. RESULTS: A modification of the weight function used in the original version of the alignment program DIALIGN has two important advantages: it can be applied to both globally and locally related sequence sets, and the running time of the program is considerably improved. The time complexity of the algorithm is discussed theoretically, and the program running time is reported for various test examples. AVAILABILITY: The program is available on-line at the Bielefeld University Bioinformatics Server (BiBiServ) http://bibiserv.TechFak.Uni-Bielefeld.DE/dial ign/"
792,"what is dynamic programming",400642,"What is dynamic programming?","Sequence alignment methods often use something called a 'dynamic programming' algorithm. What is dynamic programming and how does it work? Dynamic programming algorithms are a good place to start understanding what's really going on inside computational biology software. The heart of many well-known programs is a dynamic programming algorithm, or a fast approximation of one, including sequence database search programs like BLAST and FASTA, multiple sequence alignment programs like CLUSTALW, profile search programs like HMMER, gene finding programs like GENSCAN and even RNA-folding programs like MFOLD and phylogenetic inference programs like PHYLIP."
793,"a systematic approach to dynamic programming in bioinformatics",400676,"A systematic approach to dynamic programming in Bioinformatics","Motivation: Dynamic programming is probably the most popular programming method in bioinformatics. Sequence comparison, gene recognition, RNA structure prediction and hundreds of other problems are solved by ever new variants of dynamic programming. Currently, the development of a successful dynamic programming algorithm is a matter of experience, talent and luck. The typical matrix recurrence relations that make up a dynamic programming algorithm are intricate to construct, and difficult to implement reliably. No general problem independent guidance is available. Results: This article introduces a systematic method for constructing dynamic programming solutions to problems in biosequence analysis. By a conceptual splitting of the algorithm into a recognition and an evaluation phase, algorithm development is simplified considerably, and correct recurrences can be derived systematically. Without additional effort, the method produces an early, executable prototype expressed in a functional programming language. The method is quite generally applicable, and, while programming effort decreases, no overhead in terms of ultimate program efficiency is incurred. Contact: robert@techfak.uni-bielefeld.de"
794,"significant improvement in accuracy of multiple protein sequence alignments by iterative refinement as assessed by reference to structural alignments",400687,"{Significant improvement in accuracy of multiple protein sequence alignments by iterative refinement as assessed by reference to structural alignments}","The relative performances of four strategies for aligning a large number of protein sequences were assessed by referring to corresponding structural alignments of 54 independent families. Multiple sequence alignment of a family was constructed by a given method from the sequences of known structures and their homologues, and the subset consisting of the sequences of known structures was extracted from the whole alignment and compared with the structural counterpart in a residue-to-residue fashion. Gap-opening and -extension penalties were optimized for each family and method. Each of the four multiple alignment methods gave significantly more accurate alignments than the conventional pairwise method. In addition, a clear difference in performance was detected among three of the four multiple alignment methods examined. The currently most popular progressive method ranked worst among the four, and the randomized iterative strategy that optimizes the sum-of-pairs score ranked next worst. The two best-performing strategies, one of which was newly developed, both pursue an optimal weighted sum-of-pairs score, where the pair weights were introduced to correct for uneven representations of subgroups in a family. The new method uses doubly nested iterations to make alignment, phylogenetic tree and pair weights mutually consistent. Most importantly, the improvement in accuracy of alignments obtained by these iterative methods over pairwise or progressive method tends to increase with decreasing average sequence identity, implying that iterative refinement is more effective for the generally difficult alignment of remotely related sequences. Four well-known amino acid substitution matrices were also tested in combination with the various methods. However, the effects of substitution matrices were found to be minor in the framework of multiple alignment, and the same order of relative performance of the alignment methods was observed with any of the matrices."
795,"the accuracy of ribosomal rna comparative structure models",400695,"The accuracy of ribosomal {RNA} comparative structure models","The determination of the 16S and 23S rRNA secondary structure models was initiated shortly after the first complete 16S and 23S rRNA sequences were determined in the late 1970s. The structures that are common to all 16S rRNAs and all 23S rRNAs were determined using comparative methods from the analysis of thousands of rRNA sequences. Twenty-plus years later, the 16S and 23S rRNA comparative structure models have been evaluated against the recently determined high-resolution crystal structures of the 30S and 50S ribosomal subunits. Nearly all of the predicted covariation-based base pairs, including the regular base pairs and helices, and the irregular base pairs and tertiary interactions, were present in the 30S and 50S crystal structures."
796,"evolutionary hmms a bayesian approach to multiple alignment",400698,"{Evolutionary HMMs: a Bayesian approach to multiple alignment}","Motivation: We review proposed syntheses of probabilistic sequence alignment, profiling and phylogeny. We develop a multiple alignment algorithm for Bayesian inference in the links model proposed by Thorne et al. (1991, J. Mol. Evol. , 33, 114â124). The algorithm, described in detail in Section 3, samples from and/or maximizes the posterior distribution over multiple alignments for any number of DNA or protein sequences, conditioned on a phylogenetic tree. The individual sampling and maximization steps of the algorithm require no more computational resources than pairwise alignment.Methods: We present a software implementation (Handel) of our algorithm and report test results on (i) simulated data sets and (ii) the structurally informed protein alignments of BAliBASE (Thompson et al. , 1999, Nucleic Acids Res. , 27, 2682â2690).Results: We find that the mean sum-of-pairs score (a measure of residue-pair correspondence) for the BAliBASE alignments is only 13% lower for Handelthan for CLUSTALW(Thompson et al. , 1994, Nucleic Acids Res. , 22, 4673â4680), despite the relative simplicity of the links model (CLUSTALW uses affine gap scores and increased penalties for indels in hydrophobic regions). With reference to these benchmarks, we discuss potential improvements to the links model and implications for Bayesian multiple alignment and phylogenetic profiling.Availability: The source code to Handelis freely distributed on the Internet at http://www.biowiki.org/Handel under the terms of the GNU Public License (GPL, 2000, http://www.fsf.org./copyleft/gpl.html).Contact: ihh@fruitfly.org"
797,"alignment of rna base pairing probability matrices",400712,"{Alignment of RNA base pairing probability matrices}","Motivation: Many classes of functional RNA molecules are characterized by highly conserved secondary structures but little detectable sequence similarity. Reliable multiple alignments can therefore be constructed only when the shared structural features are taken into account. Since multiple alignments are used as input for many subsequent methods of data analysis, structure-based alignments are an indispensable necessity in RNA bioinformatics.  Results: We present here a method to compute pairwise and progressive multiple alignments from the direct comparison of base pairing probability matrices. Instead of attempting to solve the folding and the alignment problem simultaneously as in the classical Sankoff's algorithm, we use McCaskill's approach to compute base pairing probability matrices which effectively incorporate the information on the energetics of each sequences. A novel, simplified variant of Sankoff's algorithms can then be employed to extract the maximum-weight common secondary structure and an associated alignment.  Availability: The programs pmcomp and pmmulti described in this contribution are implemented in Perl and can be downloaded together with the example datasets from http://www.tbi.univie.ac.at/RNA/PMcomp/. A web server is available at http://rna.tbi.univie.ac.at/cgi-bin/pmcgi.pl 10.1093/bioinformatics/bth229"
798,"accelerated probabilistic inference of rna structure evolution",400727,"{Accelerated probabilistic inference of RNA structure evolution}","BACKGROUND:Pairwise stochastic context-free grammars (Pair SCFGs) are powerful tools for evolutionary analysis of RNA, including simultaneous RNA sequence alignment and secondary structure prediction, but the associated algorithms are intensive in both CPU and memory usage. The same problem is faced by other RNA alignment-and-folding algorithms based on Sankoff's 1985 algorithm. It is therefore desirable to constrain such algorithms, by pre-processing the sequences and using this first pass to limit the range of structures and/or alignments that can be considered.RESULTS:We demonstrate how flexible classes of constraint can be imposed, greatly reducing the computational costs while maintaining a high quality of structural homology prediction. Any score-attributed context-free grammar (e.g. energy-based scoring schemes, or conditionally normalized Pair SCFGs) is amenable to this treatment. It is now possible to combine independent structural and alignment constraints of unprecedented general flexibility in Pair SCFG alignment algorithms. We outline several applications to the bioinformatics of RNA sequence and structure, including Waterman-Eggert N-best alignments and progressive multiple alignment. We evaluate the performance of the algorithm on test examples from the RFAM database.CONCLUSION:A program, Stemloc, that implements these algorithms for efficient RNA sequence alignment and structure prediction is available under the GNU General Public License."
799,"rsearch finding homologs of single structured rna sequences",400750,"{RSEARCH: finding homologs of single structured RNA sequences}","BACKGROUND: For many RNA molecules, secondary structure rather than primary sequence is the evolutionarily conserved feature. No programs have yet been published that allow searching a sequence database for homologs of a single RNA molecule on the basis of secondary structure. RESULTS: We have developed a program, RSEARCH, that takes a single RNA sequence with its secondary structure and utilizes a local alignment algorithm to search a database for homologous RNAs. For this purpose, we have developed a series of base pair and single nucleotide substitution matrices for RNA sequences called RIBOSUM matrices. RSEARCH reports the statistical confidence for each hit as well as the structural alignment of the hit. We show several examples in which RSEARCH outperforms the primary sequence search programs BLAST and SSEARCH. The primary drawback of the program is that it is slow. The C code for RSEARCH is freely available from our lab's website. CONCLUSION: RSEARCH outperforms primary sequence programs in finding homologs of structured RNA sequences."
800,"mafft a novel method for rapid multiple sequence alignment based on fast fourier transform",400777,"{MAFFT: a novel method for rapid multiple sequence alignment based on fast Fourier transform}","A multiple sequence alignment program, MAFFT, has been developed. The CPU time is drastically reduced as compared with existing methods. MAFFT includes two novel techniques. (i) Homo logous regions are rapidly identified by the fast Fourier transform (FFT), in which an amino acid sequence is converted to a sequence composed of volume and polarity values of each amino acid residue. (ii) We propose a simplified scoring system that performs well for reducing CPU time and increasing the accuracy of alignments even for sequences having large insertions or extensions as well as distantly related sequences of similar length. Two different heuristics, the progressive method (FFT-NS-2) and the iterative refinement method (FFT-NS-i), are implemented in MAFFT. The performances of FFT-NS-2 and FFT-NS-i were compared with other methods by computer simulations and benchmark tests; the CPU time of FFT-NS-2 is drastically reduced as compared with CLUSTALW with comparable accuracy. FFT-NS-i is over 100 times faster than T-COFFEE, when the number of input sequences exceeds 60, without sacrificing the accuracy."
801,"incorporating chemical modification constraints into a dynamic programming algorithm for prediction of rna secondary structure",400784,"Incorporating chemical modification constraints into a dynamic programming algorithm for prediction of {RNA} secondary structure","A dynamic programming algorithm for prediction of RNA secondary structure has been revised to accommodate folding constraints determined by chemical modification and to include free energy increments for coaxial stacking of helices when they are either adjacent or separated by a single mismatch. Furthermore, free energy parameters are revised to account for recent experimental results for terminal mismatches and hairpin, bulge, internal, and multibranch loops. To demonstrate the applicability of this method, in vivo modification was performed on 5S rRNA in both Escherichia coli and Candida albicans with 1-cyclohexyl-3-(2-morpholinoethyl) carbodiimide metho-p-toluene sulfonate, dimethyl sulfate, and kethoxal. The percentage of known base pairs in the predicted structure increased from 26.3% to 86.8% for the E. coli sequence by using modification constraints. For C. albicans, the accuracy remained 87.5% both with and without modification data. On average, for these sequences and a set of 14 sequences with known secondary structure and chemical modification data taken from the literature, accuracy improves from 67% to 76%. This enhancement primarily reflects improvement for three sequences that are predicted with <40% accuracy on the basis of energetics alone. For these sequences, inclusion of chemical modification constraints improves the average accuracy from 28% to 78%. For the 11 sequences with <6% pseudoknotted base pairs, structures predicted with constraints from chemical modification contain on average 84% of known canonical base pairs."
802,"sequence logos a new way to display consensus sequences",400850,"Sequence Logos: A New Way to Display Consensus Sequences","A graphical method is presented for displaying the patterns in a set of aligned sequences. The characters representing the sequence are stacked on top of each other for each position in the aligned sequences. The height of each letter is made proportional to Its frequency, and the letters are sorted so the most common one is on top. The height of the entire stack is then adjusted to signify the information content of the sequences at that position. From these  sequence logos', one can determine not only the consensus sequence but also the relative frequency of bases and the information content (measured In bits) at every position in a site or sequence. The logo displays both significant residues and subtle sequence patterns. 10.1093/nar/18.20.6097"
803,"the major transitions in evolution",400861,"The Major Transitions in Evolution","{Over the history of life there have been several major changes in the way genetic information is organized and transmitted from one generation to the next. These transitions include the origin of life itself, the first eukaryotic cells, reproduction by sexual means, the appearance of<br>multicellular plants and animals, the emergence of cooperation and of animal societies, and the unique language ability of humans. This ambitious book provides the first unified discussion of the full range of these transitions. The authors highlight the similarities between different<br>transitions--between the union of replicating molecules to form chromosomes and of cells to form multicellular organisms, for example--and show how understanding one transition sheds light on others. They trace a common theme throughout the history of evolution: after a major transition some<br>entities lose the ability to replicate independently, becoming able to reproduce only as part of a larger whole. The authors investigate this pattern and why selection between entities at a lower level does not disrupt selection at more complex levels. Their explanation encompasses a compelling<br>theory of the evolution of cooperation at all levels of complexity. Engagingly written and filled with numerous illustrations, this book can be read with enjoyment by anyone with an undergraduate training in biology. It is ideal for advanced discussion groups on evolution and includes accessible<br>discussions of a wide range of topics, from molecular biology and linguistics to insect societies.}"
804,"a comprehensive comparison of multiple sequence alignment programs",400878,"{A comprehensive comparison of multiple sequence alignment programs}","{In recent years improvements to existing programs and the introduction of new iterative algorithms have changed the state-of-the-art in protein sequence alignment. This paper presents the first systematic study of the most commonly used alignment programs using BAliBASE benchmark alignments as test cases. Even below the 'twilight zone' at 10-20\% residue identity, the best programs were capable of correctly aligning on average 47\% of the residues. We show that iterative algorithms often offer improved alignment accuracy though at the expense of computation time. A notable exception was the effect of introducing a single divergent sequence into a set of closely related sequences, causing the iteration to diverge away from the best alignment. Global alignment programs generally performed better than local methods, except in the presence of large N/C-terminal extensions and internal insertions. In these cases, a local algorithm was more successful in identifying the most conserved motifs. This study enables us to propose appropriate alignment strategies, depending on the nature of a particular set of sequences. The employment of more than one program based on different alignment techniques should significantly improve the quality of automatic protein sequence alignment methods. The results also indicate guidelines for improvement of alignment algorithms.}"
805,"systematic evolution of ligands by exponential enrichment rna ligands to bacteriophage t dna polymerase",400885,"Systematic evolution of ligands by exponential enrichment: {RNA} ligands to bacteriophage {T4 DNA} polymerase","High-affinity nucleic acid ligands for a protein were isolated by a procedure that depends on alternate cycles of ligand selection from pools of variant sequences and amplification of the bound species. Multiple rounds exponentially enrich the population for the highest affinity species that can be clonally isolated and characterized. In particular one eight-base region of an RNA that interacts with the T4 DNA polymerase was chosen and randomized. Two different sequences were selected by this procedure from the calculated pool of 65,536 species. One is the wild-type sequence found in the bacteriophage mRNA; one is varied from wild type at four positions. The binding constants of these two RNA's to T4 DNA polymerase are equivalent. These protocols with minimal modification can yield high-affinity ligands for any protein that binds nucleic acids as part of its function; high-affinity ligands could conceivably be developed for any target molecule. 10.1126/science.2200121"
806,"simple fast algorithms for the editing distance between trees and related problems",400913,"Simple fast algorithms for the editing distance between trees and related problems","Ordered labeled trees are trees in which the left-to-right order among siblings is significant. The distance between two ordered trees is considered to be the weighted number of edit operations (insert, delete, and modify) to transform one tree to another. The problem of approximate tree matching is also considered. Specifically, algorithms are designed to answer the following kinds of questions:1. What is the distance between two trees? 2. What is the minimum distance between \\$T_1 \\$ and \\$T_2 \\$ when zero or more subtrees can be removed from \\$T_2 \\$? 3. Let the pruning of a tree at node \\$n\\$ mean removing all the descendants of node \\$n\\$. The analogous question for prunings as for subtrees is answered.A dynamic programming algorithm is presented to solve the three questions in sequential time \\$O( T_1   \\times  T_2   \\times \\min ({\\textit{depth}}(T_1 ),{\\textit{leaves}}(T_1 )) \\times \\min ({\\textit{depth}}(T_2 ),{\\textit{leaves}}(T_2 )))\\$ and space \\$O( T_1   \\times  T_2  )\\$ compared with \\$O( T_1   \\times  T_2   \\times ({\\textit{depth}}(T_1 ))^2 \\times ({\\textit{depth}}(T_2 ))^2 )\\$ for the best previous published algorithm due to Tai [J. Assoc. Comput. Mach., 26 (1979), pp, 422-433]. Further, the algorithm presented here can be parallelized to give time \\$O( T_1   \\times  T_2  )\\$."
807,"calculating nucleic acid secondary structure",400914,"Calculating nucleic acid secondary structure","New results for calculating nucleic acid secondary structure by free energy minimization and phylogenetic comparisons have recently been reported. A complete set of DNA energy parameters is now available and the RNA parameters have been improved. Although databases of RNA secondary structures are still derived and expanded using computer-assisted, ad hoc comparative analysis, a number of new computer algorithms combine covariation analysis with energy methods."
808,"wholegenome shotgun assembly and analysis of the genome of fugu rubripes",400921,"{Whole-genome shotgun assembly and analysis of the genome of Fugu rubripes}","The compact genome of Fugu rubripes has been sequenced to over 95% coverage, and more than 80% of the assembly is in multigene-sized scaffolds. In this 365-megabase vertebrate genome, repetitive DNA accounts for less than one-sixth of the sequence, and gene loci occupy about one-third of the genome. As with the human genome, gene loci are not evenly distributed, but are clustered into sparse and dense regions. Some ""giant"" genes were observed that had average coding sequence sizes but were spread over genomic lengths significantly larger than those of their human orthologs. Although three-quarters of predicted human proteins have a strong match to Fugu, approximately a quarter of the human proteins had highly diverged from or had no pufferfish homologs, highlighting the extent of protein evolution in the 450 million years since teleosts and mammals diverged. Conserved linkages between Fugu and human genes indicate the preservation of chromosomal segments from the common vertebrate ancestor, but with considerable scrambling of gene order."
809,"genome sequence of the brown norway rat yields insights into mammalian evolution",400952,"{Genome sequence of the Brown Norway rat yields insights into mammalian evolution}","{The laboratory rat (Rattus norvegicus) is an indispensable tool in experimental medicine and drug development, having made inestimable contributions to human health. We report here the genome sequence of the Brown Norway (BN) rat strain. The sequence represents a high-quality 'draft' covering over 90\% of the genome. The BN rat sequence is the third complete mammalian genome to be deciphered, and three-way comparisons with the human and mouse genomes resolve details of mammalian evolution. This first comprehensive analysis includes genes and proteins and their relation to human disease, repeated sequences, comparative genome-wide studies of mammalian orthologous chromosomal regions and rearrangement breakpoints, reconstruction of ancestral karyotypes and the events leading to existing species, rates of variation, and lineage-specific and lineage-independent evolutionary events such as expansion of gene families, orthology relations and protein evolution.}"
810,"comparative recombination rates in the rat mouse and human genomes",400963,"{Comparative recombination rates in the rat, mouse, and human genomes}","10.1101/gr.1970304 Levels of recombination vary among species, among chromosomes within species, and among regions within chromosomes in mammals. This heterogeneity may affect levels of diversity, efficiency of selection, and genome composition, as well as have practical consequences for the genetic mapping of traits. We compared the genetic maps to the genome sequence assemblies of rat, mouse, and human to estimate local recombination rates across these genomes. Humans have greater overall levels of recombination, as well as greater variance. In rat and mouse, the size of the chromosome and proximity to telomere have less effect on local recombination rate than in human. At the chromosome level, rat and mouse X chromosomes have the lowest recombination rates, whereas human chromosome X does not show the same pattern. In all species, local recombination rate is significantly correlated with several sequence variables, including GC%, CpG density, repetitive elements, and the neutral mutation rate, with some pronounced differences between species. Recombination rate in one species is not strongly correlated with the rate in another, when comparing homologous syntenic blocks of the genome. This comparative approach provides additional insight into the causes and consequences of genomic heterogeneity in recombination."
811,"evolutions cauldron duplication deletion and rearrangement in the mouse and human genomes",400971,"{Evolution's cauldron: duplication, deletion, and rearrangement in the mouse and human genomes}","This study examines genomic duplications, deletions, and rearrangements that have happened at scales ranging from a single base to complete chromosomes by comparing the mouse and human genomes. From whole-genome sequence alignments, 344 large (>100-kb) blocks of conserved synteny are evident, but these are further fragmented by smaller-scale evolutionary events. Excluding transposon insertions, on average in each megabase of genomic alignment we observe two inversions, 17 duplications (five tandem or nearly tandem), seven transpositions, and 200 deletions of 100 bases or more. This includes 160 inversions and 75 duplications or transpositions of length >100 kb. The frequencies of these smaller events are not substantially higher in finished portions in the assembly. Many of the smaller transpositions are processed pseudogenes; we define a âsyntenicâ subset of the alignments that excludes these and other small-scale transpositions. These alignments provide evidence that â2% of the genes in the human/mouse common ancestor have been deleted or partially deleted in the mouse. There also appears to be slightly less nontransposon-induced genome duplication in the mouse than in the human lineage. Although some of the events we detect are possibly due to misassemblies or missing data in the current genome sequence or to the limitations of our methods, most are likely to represent genuine evolutionary events. To make these observations, we developed new alignment techniques that can handle large gaps in a robust fashion and discriminate between orthologous and paralogous alignments."
812,"benchmarking tools for the alignment of functional noncoding dna",400984,"{Benchmarking tools for the alignment of functional noncoding DNA}","{BACKGROUND:} Numerous tools have been developed to align genomic sequences. However, their relative performance in specific applications remains poorly characterized. Alignments of protein-coding sequences typically have been benchmarked against correct alignments inferred from structural data. For noncoding sequences, where such independent validation is lacking, simulation provides an effective means to generate correct alignments with which to benchmark alignment tools. {RESULTS:} Using rates of noncoding sequence evolution estimated from the genus Drosophila, we simulated alignments over a range of divergence times under varying models incorporating point substitution, insertion/deletion events, and short blocks of constrained sequences such as those found in cis-regulatory regions. We then compared correct alignments generated by a modified version of the {ROSE} simulation platform to alignments of the simulated derived sequences produced by eight pairwise alignment tools {(Avid,} {BlastZ,} Chaos, {ClustalW,} {DiAlign,} Lagan, Needle, and {WABA)} to determine the off-the-shelf performance of each tool. As expected, the ability to align noncoding sequences accurately decreases with increasing divergence for all tools, and declines faster in the presence of insertion/deletion evolution. Global alignment tools {(Avid,} {ClustalW,} Lagan, and Needle) typically have higher sensitivity over entire noncoding sequences as well as in constrained sequences. Local tools {(BlastZ,} Chaos, and {WABA)} have lower overall sensitivity as a consequence of incomplete coverage, but have high specificity to detect constrained sequences as well as high sensitivity within the subset of sequences they align. Tools such as {DiAlign,} which generate both local and global outputs, produce alignments of constrained sequences with both high sensitivity and specificity for divergence distances in the range of 1.25-3.0 substitutions per site. {CONCLUSION:} For species with genomic properties similar to Drosophila, we conclude that a single pair of optimally diverged species analyzed with a high performance alignment tool can yield accurate and specific alignments of functionally constrained noncoding sequences. Further algorithm development, optimization of alignment parameters, and benchmarking studies will be necessary to extract the maximal biological information from alignments of functional noncoding {DNA.}"
813,"pfam multiple sequence alignments and hmmprofiles of protein domains",401001,"{Pfam: multiple sequence alignments and HMM-profiles of protein domains}","{Pfam contains multiple alignments and hidden Markov model based profiles (HMM-profiles) of complete protein domains. The definition of domain boundaries, family members and alignment is done semi-automatically based on expert knowledge, sequence similarity, other protein family databases and the ability of HMM-profiles to correctly identify and align the members. Release 2.0 of Pfam contains 527 manually verified families which are available for browsing and on-line searching via the World Wide Web in the UK at http://www.sanger.ac.uk/Pfam/ and in the US at http://genome.wustl. edu/Pfam/ Pfam 2.0 matches one or more domains in 50\% of Swissprot-34 sequences, and 25\% of a large sample of predicted proteins from the Caenorhabditis elegans genome.}"
814,"haskell language and libraries the revised report",402517,"{Haskell~98} Language and Libraries, The Revised Report","Haskell is the world’s leading lazy functional programming language, widely used for teaching, research, and applications. The language continues to develop rapidly, but in 1998 the community decided to capture a stable snapshot of the language: Haskell 98. All Haskell compilers support Haskell 98, so practitioners and educators alike have a stable base for their work. This book constitutes the agreed definition of the Haskell 98, both the language itself and its supporting libraries. It has been considerably revised and refined since the original definition, and appears in print for the first time. It should be a standard reference work for anyone involved in research, teaching, or application of Haskell."
815,"software metrics a rigorous and practical approach",403648,"Software Metrics: {A} Rigorous and Practical Approach","From the Publisher: The Second Edition of  Software Metrics  provides an up-to-date, coherent, and rigorous framework for controlling, managing, and predicting software development processes. With an emphasis on real-world applications, Fenton and Pfleeger apply basic ideas in measurement theory to quantify software development resources, processes, and products.  The book offers an accessible and comprehensive introduction to software metrics, now an essential component of software engineering for both classroom and industry.  Software Metrics  features extensive case studies from Hewlett Packard, IBM, the U.S. Department of Defense, Motorola, and others, in addition to worked examples and exercises. The Second Edition includes up-to-date material on process maturity and measurement, goal-question-metric, planning a metrics program, measurement in practice, experimentation, empirical studies, ISO9216, and metric tools."
816,"seven myths of formal methods",403658,"Seven Myths of Formal Methods","Seven widely held conceptions about formal methods are challenged. These beliefs are variants of the following: formal methods can guarantee that software is perfect; they work by proving that the programs are correct; only highly critical systems benefit from their use; they involve complex mathematics; they increase the cost of development; they are incomprehensible to clients; and nobody uses them for real projects. The arguments are based on the author's experiences. They address the bounds of formal methods, identify the central role of specifications in the development process, and cover education and training."
817,"a history of instructional design and technology part ii a history of instructional design",404740,"A History of Instructional Design and Technology: Part II: A History of Instructional Design","This second of a two-part article focuses on the history of instructional design in the United States. Starting with its origins during World War II, major events in the development of instructional design are described. Factors affecting the field over the last two decades, including increasing interest in cognitive psychology, microcomputers, performance technology, and constructivism, are also described. (Contains 105 references.) (Author/AEF)"
818,"nexus small worlds and the groundbreaking theory of networks",405773,"Nexus: Small Worlds and the Groundbreaking Theory of Networks","As _Chaos_ explained the science of disorder, _Nexus_ reveals the new science of connection and the odd logic of six degrees of separation.  ""If you ever wanted to know how many links connect you and the Pope, or why when the U.S. Federal Reserve Bank sneezes the global economy catches cold, read this book,"" writes John L. Casti (Santa Fe Institute). This ""cogent and engaging"" (_Nature_) work presents the fundamental principles of the emerging field of ""small-worlds"" theory—the idea that a hidden pattern is the key to how networks interact and exchange information, whether that network is the information highway or the firing of neurons in the brain. Mathematicians, physicists, computer scientists, and social scientists are working to decipher this complex organizational system, for it may yield a blueprint of dynamic interactions within our physical as well as social worlds.  Highlighting groundbreaking research behind network theory, ""Mark Buchanan's graceful, lucid, nontechnical and entertaining prose"" (Mark Granovetter) documents the mounting support among various disciplines for the small-worlds idea and demonstrates its practical applications to diverse problems—from the volatile global economy or the Human Genome Project to the spread of infectious disease or ecological damage. _Nexus_ is an exciting introduction to the hidden geometry that weaves our lives so inextricably together."
819,"the next generation of literature analysis integration of genomic analysis into text mining",405843,"The next generation of literature analysis: integration of genomic analysis into text mining.","Text-mining systems are indispensable tools to reduce the increasing flux of information in scientific literature to topics pertinent to a particular interest in focus. Most of the scientific literature is published as unstructured free text, complicating the development of data processing tools, which rely on structured information. To overcome the problems of free text analysis, structured, hand-curated information derived from literature is integrated in text-mining systems to improve precision and recall. In this paper several text-mining approaches are reviewed and the next step in development of text-mining systems, which is based on a concept of multiple lines of evidence, is described: results from literature analysis are combined with evidence from experiments and genome analysis to improve the accuracy of results and to generate additional knowledge beyond what is known solely from literature."
820,"option pricing a simplified approach",406295,"Option pricing: A simplified approach","This paper presents a simple discrete-time model for valuing options. The fundamental economic principles of option pricing by arbitrage methods are particularly clear in this setting. Its development requires only elementary mathematics, yet it contains as a special limiting case the celebrated Black-Scholes model, which has previously been derived only by much more difficult methods. The basic model readily lends itself to generalization in many ways. Moreover, by its very construction, it gives rise to a simple and efficient numerical procedure for valuing options for which premature exercise may be optimal."
821,"foundations for engineering biology",406529,"Foundations for engineering biology","Engineered biological systems have been used to manipulate information, construct materials, process chemicals, produce energy, provide food, and help maintain or enhance human health and our environment. Unfortunately, our ability to quickly and reliably engineer biological systems that behave as expected remains quite limited. Foundational technologies that make routine the engineering of biology are needed. Vibrant, open research communities and strategic leadership are necessary to ensure that the development and application of biological technologies remains overwhelmingly constructive."
822,"the effects of common ground and perspective on domains of referential interpretation",406673,"The effects of common ground and perspective on domains of referential interpretation","(from the journal abstract) Addressees' eye movements were tracked as they followed instructions given by a confederate speaker hidden from view. Experiment 1 used objects in common ground (known to both participants) or privileged ground (known to the addressee). Although privileged objects interfered with reference to an identical object in common ground, addressees were always more likely to look at an object in common ground than privileged ground. Experiment 2 used definite and indefinite referring expressions with early or late points of disambiguation, depending on the uniqueness of the display objects. The speaker's and addressee's perspectives matched when the speaker was accurately informed about the display, and mismatched when the speaker was misinformed. When perspectives matched, addressees identified the target faster with early than with late disambiguation displays. When perspectives mismatched, addressees still identified the target quickly, showing an ability to use the speaker's perspective. These experiments demonstrate that although addressees cannot completely ignore information in privileged ground, common ground and perspective each have immediate effects on reference resolution. (PsycINFO Database Record (c) 2004 APA, all rights reserved) reference resolution; common ground; eye movements; perspective match; referential interpretation; speaker; conversation; communication; privileged ground; response speed; disambiguation"
823,"trustguard countering vulnerabilities in reputation management for decentralized overlay networks",406703,"TrustGuard: Countering Vulnerabilities in Reputation Management for Decentralized Overlay Networks","Reputation systems have been popular in estimating the trustworthiness and predicting the future behavior of nodes in a large-scale distributed system where nodes may transact with one another without prior knowledge or experience. One of the fundamental challenges in distributed reputation management is to understand vulnerabilities and develop mechanisms that can minimize the potential damages to a system by malicious nodes. In this paper, we identify three vulnerabilities that are detrimental to decentralized reputation management and propose TrustGuard - a  safeguard  framework for providing a highly dependable and yet efficient reputation system. First, we provide a dependable trust model and a set of formal methods to handle strategic malicious nodes that continuously change their behavior to gain unfair advantages in the system. Second, a transaction based reputation system must cope with the vulnerability that malicious nodes may misuse the system by flooding feedbacks with fake transactions. Third, but not least, we identify the importance of filtering out dishonest feedbacks when computing reputation-based trust of a node, including the feedbacks filed by malicious nodes through collusion. Our experiments show that, comparing with existing reputation systems, our framework is highly dependable and effective in countering malicious nodes regarding strategic oscillating behavior, flooding malevolent feedbacks with fake transactions, and dishonest feedbacks."
824,"mass spectrometrybased proteomics",407408,"Mass spectrometry-based proteomics","Recent successes illustrate the role of mass spectrometry-based proteomics as an indispensable tool for molecular and cellular biology and for the emerging field of systems biology. These include the study of protein−protein interactions via affinity-based isolations on a small and proteome-wide scale, the mapping of numerous organelles, the concurrent description of the malaria parasite genome and proteome, and the generation of quantitative protein profiles from diverse species. The ability of mass spectrometry to identify and, increasingly, to precisely quantify thousands of proteins from complex samples can be expected to impact broadly on biology and medicine."
825,"from words to literature in structural proteomics",407474,"From words to literature in structural proteomics","Technical advances on several frontiers have expanded the applicability of existing methods in structural biology and helped close the resolution gaps between them. As a result, we are now poised to integrate structural information gathered at multiple levels of the biological hierarchy — from atoms to cells — into a common framework. The goal is a comprehensive description of the multitude of interactions between molecular entities, which in turn is a prerequisite for the discovery of general structural principles that underlie all cellular processes."
826,"selforganized stigmergic document maps environment as mechanism for context learning",407633,"Self-Organized Stigmergic Document Maps: Environment As Mechanism for Context Learning","Social insect societies and more specifically ant colonies, are distributed systems that, in spite of the simplicity of their individuals, present a highly structured social organization. As a result of this organization, ant colonies can accomplish complex tasks that in some cases exceed the individual capabilities of a single ant. The study of ant colonies behavior and of their self-organizing capabilities is of interest to knowledge retrieval/ management and decision support systems..."
827,"less is more genetic optimisation of nearest neighbour classifiers",407635,"Less is More - Genetic Optimisation of Nearest Neighbour Classifiers","The present paper deals with optimisation of Nearest Neighbour rule Classifiers via Genetic Algorithms. The methodology consists on implement a Genetic Algorithm capable of search the input feature space used by the NNR classifier. Results show that is adequate to perform feature reduction and simultaneous improve the Recognition Rate. Some practical examples prove that is possible to Recognise Portuguese Granites in 100%, with only 3 morphological features (from an original set of 117..."
828,"map segmentation by colour cube genetic kmean clustering",407636,"Map Segmentation by Colour Cube Genetic K-Mean Clustering","Segmentation of a colour image composed of different kinds of texture regions can be a hard problem, namely to compute for an exact texture fields and a decision of the optimum number of segmentation areas in an image when it contains similar and/or unstationary texture fields. In this work, a method is described for evolving adaptive procedures for these problems. In many real world applications data clustering constitutes a fundamental issue whenever behavioural or feature domains can be ..."
829,"the biological concept of neoteny in evolutionary colour image segmentation simple experiments in simple nonmemetic genetic algorithms",407638,"The Biological Concept of Neoteny in Evolutionary Colour Image Segmentation: Simple Experiments in Simple Non-Memetic Genetic Algorithms","Neoteny, also spelled Paedomorphosis, can be defined in biological terms as the retention by an organism of juvenile or even larval traits into later life. In some species, all morphological development is retarded; the organism is juvenilized but sexually mature. Such shifts of reproductive capability would appear to have adaptive significance to organisms that exhibit it. In terms of evolutionary theory, the process of paedomorphosis suggests that larval stages and developmental phases of..."
830,"artificial ant colonies in digital image habitats a mass behaviour effect study on pattern recognition",407639,"Artificial Ant Colonies in Digital Image Habitats - A Mass Behaviour Effect Study on Pattern Recognition","Some recent studies have pointed that, the self-organization of neurons into brain-like structures, and the self-organization of ants into a swarm are similar in many respects. If possible to implement, these features could lead to important developments in pattern recognition systems, where perceptive capabilities can emerge and evolve from the interaction of many simple local rules. The principle of the method is inspired by the work of Chialvo and Millonas who developed the first numerical simulation in which swarm cognitive map formation could be explained. From this point, an extended model is presented in order to deal with digital image habitats, in which artificial ants could be able to react to the environment and perceive it. Evolution of pheromone fields point that artificial ant colonies could react and adapt appropriately to any type of digital habitat. KEYWORDS: Swarm Intelligence, Self-Organization, Stigmergy, Artificial Ant Systems, Pattern Recognition and Perception, Image Segmentation, Gestalt Perception Theory, Distributed Computation."
831,"swarms on continuous data",407641,"Swarms on Continuous Data","While being it extremely important, many Exploratory Data Analysis (EDA) systems have the inhability to perform classification and visualization in a continuous basis or to self-organize new data-items into the older ones (evenmore into new labels if necessary), which can be crucial in KDD - Knowledge Discovery, Retrieval and Data Mining Systems (interactive and online forms of Web Applications are just one example). This disadvantge is also present in more recent approaches using Self-Organizing Maps. On the present work, and exploiting past sucesses in recently proposed Stigmergic Ant Systems a robust online classifier is presented, which produces class decisions on a continuous stream data, allowing for continuous mappings. Results show that increasingly better results are achieved, as demonstraded by other authors in different areas. KEYWORDS: Swarm Intelligence, Ant Systems, Stigmergy, Data-Mining, Exploratory Data Analysis, Image Retrieval, Continuous Classification."
832,"image colour segmentation by genetic algorithms",407642,"Image Colour Segmentation by Genetic Algorithms","Segmentation of a colour image composed of different kinds of texture regions can be a hard problem, namely to compute for an exact texture fields and a decision of the optimum number of segmentation areas in an image when it contains similar and/or unstationary texture fields. In this work, a method is described for evolving adaptive procedures for these problems. In many real world applications data clustering constitutes a fundamental issue whenever behavioural or feature domains can be..."
833,"from feature extraction to classification a multidisciplinary approach applied to portuguese granites",407643,"From Feature Extraction to Classification: A Multidisciplinary Approach applied to Portuguese Granites","The purpose of this paper is to present a complete methodology based on a multidisciplinary approach, that goes from the extraction of features till the classification of a set of different portuguese granites. The set of tools to extract the features that characterise polished surfaces of the granites is mainly based on mathematical morphology. The classification methodology is based on a genetic algorithm capable of search the input feature space used by the nearest neighbour rule classifier. ..."
834,"on image filtering noise and morphological size intensity diagrams",407644,"On Image Filtering, Noise and Morphological Size Intensity Diagrams","In the absence of a pure noise-free image it is hard to define what noise is, in any original noisy image, and as a consequence also where it is, and in what amount. In fact, the definition of noise depends largely on our own aim in the whole image analysis process, and (perhaps more important) in our self-perception of noise. For instance, when we perceive noise as disconnected and small it is normal to use MMAF filters to treat it. There is two evidences of this. First, in many instances..."
835,"artificial neoteny in evolutionary image segmentation",407645,"Artificial Neoteny In Evolutionary Image Segmentation","Neoteny, also spelled Paedomorphosis, can be defined in biological terms as the retention by an organism of juvenile or even larval traits into later life. In some species, all morphological development is retarded; the organism is juvenilized but sexually mature. Such shifts of reproductive capability would appear to have adaptive significance to organisms that exhibit it. In terms of evolutionary theory, the process of paedomorphosis suggests that larval stages and developmental phases of..."
836,"intrusion detection systems using adaptive regression splines",407678,"Intrusion Detection Systems using Adaptive Regression Splines","Past few years have witnessed a growing recognition of soft computing technologies for the construction of intelligent and reliable intrusion detection systems. Due to increasing incidents of cyber attacks, building effective intrusion detection systems (IDSs) are essential for protecting information systems security, and yet it remains an elusive goal and a great challenge. In this paper, we report a performance analysis between Multivariate Adaptive Regression Splines (MARS), neural networks and support vector machines. The MARS procedure builds flexible regression models by fitting separate splines to distinct intervals of the predictor variables. A brief comparison of different neural network learning algorithms is also given."
837,"swarming around shellfish larvae images",407679,"Swarming around Shellfish Larvae Images","The collection of wild larvae seed as a source of raw material is a major sub industry of shellfish aquaculture. To predict when, where and in what quantities wild seed will be available, it is necessary to track the appearance and growth of planktonic larvae. One of the most difficult groups to identify, particularly at the species level are the Bivalvia. This difficulty arises from the fact that fundamentally all bivalve larvae have a similar shape and colour. Identification based on gross morphological appearance is limited by the time-consuming nature of the microscopic examination and by the limited availability of expertise in this field. Molecular and immunological methods are also being studied. We describe the application of computational pattern recognition methods to the automated identification and size analysis of scallop larvae. For identification, the shape features used are binary invariant moments; that is, the features are invariant to shift (position within the image), scale (induced either by growth or differential image magnification) and rotation. Images of a sample of scallop and non-scallop larvae covering a range of maturities have been analysed. In order to overcome the automatic identification, as well as to allow the system to receive new unknown samples at any moment, a self-organized and unsupervised ant-like clustering algorithm based on Swarm Intelligence is proposed, followed by simple k-NNR nearest neighbour classification on the final map. Results achieve a full recognition rate of 100% under several situations (k =1 or 3)."
838,"antids selforganized antbased clustering model for intrusion detection system",407681,"ANTIDS: Self-Organized Ant-based Clustering Model for Intrusion Detection System","Security of computers and the networks that connect them is increasingly becoming of great significance. Computer security is defined as the protection of computing systems against threats to confidentiality, integrity, and availability. There are two types of intruders: the external intruders who are unauthorized users of the machines they attack, and internal intruders, who have permission to access the system with some restrictions. Due to the fact that it is more and more improbable to a system administrator to recognize and manually intervene to stop an attack, there is an increasing recognition that ID systems should have a lot to earn on following its basic principles on the behavior of complex natural systems, namely in what refers to self-organization, allowing for a real distributed and collective perception of this phenomena. With that aim in mind, the present work presents a self-organized ant colony based intrusion detection system (ANTIDS) to detect intrusions in a network infrastructure. The performance is compared among conventional soft computing paradigms like Decision Trees, Support Vector Machines and Linear Genetic Programming to model fast, online and efficient intrusion detection systems."
839,"exploiting and evolving rn mathematical morphology feature spaces",407688,"Exploiting and Evolving Rn Mathematical Morphology Feature Spaces","A multidisciplinary methodology that goes from the extraction of features till the classification of a set of different portuguese granites is presented in this paper. The set of tools to extract the features that characterise the polished surfaces of granites is mainly based on mathematical morphology. The classification methodology is based on a genetic algorithm capable of search for the input feature space used by the nearest neighbour rule classifier. Results show that is adequate to perform feature reduction and simultaneous improve the recognition rate. Moreover, the present methodology represents a robust strategy to understand the proper nature of the textures studied and their discriminant features."
840,"social cognitive maps swarm collective perception and distributed search on dynamic landscapes",407689,"Social Cognitive Maps, Swarm Collective Perception and Distributed Search on Dynamic Landscapes","Swarm Intelligence (SI) is the property of a system whereby the collective behaviors of (unsophisticated) entities interacting locally with their environment cause coherent functional global patterns to emerge. SI provides a basis with wich it is possible to explore collective (or distributed) problem solving without centralized control or the provision of a global model. To tackle the formation of a coherent social collective intelligence from individual behaviors, we discuss several concepts related to Self-Organization, Stigmergy and Social Foraging in animals. Then, in a more abstract level we suggest and stress the role played not only by the environmental media as a driving force for societal learning, as well as by positive and negative feedbacks produced by the many interactions among agents. Finally, presenting a simple model based on the above features, we will adress the collective adaptation of a social community to a cultural (environmenatl, contextual) or media informational dynamical landscape, represented here - for the purpose of different experiments - by several three-dimensional mathematical functions that suddenly change over time. Results indicate that the collective intelligence is able to cope and quickly adapt to unforseen situations even when over the same cooperative foraging period, the community is requested to deal with two different and contradictory purposes."
841,"varying the population size of artificial foraging swarms on time varying landscapes",407690,"Varying the Population Size of Artificial Foraging Swarms on Time Varying Landscapes","{Swarm Intelligence (SI) is the property of a system whereby the collective behaviors of (unsophisticated) entities interacting locally with their environment cause coherent functional global patterns to emerge. SI provides a basis with wich it is possible to explore collective (or distributed) problem solving without centralized control or the provision of a global model. In this paper we present a Swarm Search Algorithm with varying population of agents. The swarm is based on a previous model with fixed population which proved its effectiveness on several computation problems. We will show that the variation of the population size provides the swarm with mechanisms that improves its self-adaptability and causes the emergence of a more robust self-organized behavior, resulting in a higher efficiency on searching peaks and valleys over dynamic search landscapes represented here - for the purpose of different experiments - by several three-dimensional mathematical functions that suddenly change over time. We will also show that the present swarm, for each function, self-adapts towards an optimal population size, thus self-regulating.}"
842,"stock market prediction using multi expression programming",407710,"Stock Market Prediction using Multi Expression Programming","The use of intelligent systems for stock market predictions has been widely established. In this paper we introduce a genetic programming technique (called Multi-Expression programming) for the prediction of two stock indices. The performance is then compared with an artifcial neural network trained using Levenberg-Marquardt algorithm, support vector machine, Takagi-Sugeno neuro-fuzzy model, a difference boosting neural network. We considered Nasdaq-100 index of Nasdaq Stock MarketSM and the S&P CNX NIFTY stock index as test data."
843,"on ants bacteria and dynamic environments",407711,"On Ants, Bacteria and Dynamic Environments","Wasps, bees, ants and termites all make effective use of their environment and resources by displaying collective “swarm” intelligence. Termite colonies - for instance - build nests with a complexity far beyond the comprehension of the individual termite, while ant colonies dynamically allocate labor to various vital tasks such as foraging or defense without any central decision-making ability. Recent research suggests that microbial life can be even richer: highly social, intricately networked, and teeming with interactions, as found in bacteria. What strikes from these observations is that both ant colonies and bacteria have similar natural mechanisms based on Stigmergy and Self-Organization in order to emerge coherent and sophisticated patterns of global behaviour. Keeping in mind the above characteristics we will present a simple model to tackle the collective adaptation of a social swarm based on real ant colony behaviors (SSA algorithm) for tracking extrema in dynamic environments and highly multimodal complex functions described in the well-know De Jong test suite. Then, for the purpose of comparison, a recent model of artificial bacterial foraging (BFOA algorithm) based on similar stigmergic features is described and analyzed. Final results indicate that the SSA collective intelligence is able to cope and quickly adapt to unforeseen situations even when over the same cooperative foraging period, the community is requested to deal with two different and contradictory purposes, while outperforming BFOA in adaptive speed."
844,"selfregulated artificial ant colonies on digital image habitats",407712,"Self-Regulated Artificial Ant Colonies on Digital Image Habitats","Artificial life models, swarm intelligent and evolutionary computation algorithms are usually built on fixed size populations. Some studies indicate however that varying the population size can increase the adaptability of these systems and their capability to react to changing environments. In this paper we present an extended model of an artificial ant colony system designed to evolve on digital image habitats. We will show that the present swarm can adapt the size of the population according to the type of image on which it is evolving and reacting faster to changing images, thus converging more rapidly to the new desired regions, regulating the number of his image foraging agents. Finally, we will show evidences that the model can be associated with the Mathematical Morphology Watershed algorithm to improve the segmentation of digital grey-scale images."
845,"societal implicit memory and his speed on tracking extrema over dynamic environments using selfregulatory swarms",407741,"Societal Implicit Memory and his Speed on Tracking Extrema over Dynamic Environments using Self-Regulatory Swarms","In order to overcome difficult dynamic optimization and environment extrema tracking problems, we propose a Self-Regulated Swarm (SRS) algorithm which hybridizes the advantageous characteristics of Swarm Intelligence as the emergence of a societal environmental memory or cognitive map via collective pheromone laying in the landscape (properly balancing the exploration/exploitation nature of the search strategy), with a simple Evolutionary mechanism that trough a direct reproduction procedure linked to local environmental features is able to self-regulate the above exploratory swarm population, speeding it up globally. In order to test his adaptive response and robustness, we have recurred to different dynamic multimodal complex functions as well as to Dynamic Optimization Control (DOC) problems. Measures were made for different dynamic settings and parameters such as, environmental upgrade frequencies, landscape changing speed severity, type of dynamic (linear or circular), and to dramatic changes on the algorithmic search purpose over each test environment (e.g. shifting the extrema). Finally, comparisons were made with traditional Genetic Algorithms (GA), Bacterial Foraging Optimization Algorithms (BFOA), as well as with more recently proposed Co-Evolutionary approaches. SRS, were able to demonstrate quick adaptive responses, while outperforming the results obtained by the other approaches. Additionally, some successful behaviors were found: SRS was able not only to achieve quick adaptive responses, as to maintaining a number of different solutions, while adapting to new unforeseen extrema; the possibility to spontaneously create and maintain different subpopulations on different peaks, emerging different exploratory corridors with intelligent path planning capabilities; the ability to request for new agents over dramatic changing periods, and economizing those foraging resources over periods of stabilization. Finally, results prove that the present SRS collective swarm of bio-inspired agents is able to track about 65% of moving peaks traveling up to ten times faster than the velocity of a single individual composing that precise swarm tracking system. This emerged behavior is probably one of the most interesting ones achieved by the present work."
846,"expression pattern and surprisingly gene length shape codon usage in caenorhabditis drosophila and arabidopsis",408095,"Expression pattern and, surprisingly, gene length shape codon usage in Caenorhabditis, Drosophila, and Arabidopsis","We measured the expression pattern and analyzed codon usage in 8,133, 1,550, and 2,917 genes, respectively, from Caenorhabditis elegans, Drosophila melanogaster, and Arabidopsis thaliana. In those three species, we observed a clear correlation between codon usage and gene expression levels and showed that this correlation is not due to a mutational bias. This provides direct evidence for selection on silent sites in those three distantly related multicellular eukaryotes. Surprisingly, there is a strong negative correlation between codon usage and protein length. This effect is not due to a smaller size of highly expressed proteins. Thus, for a same-expression pattern, the selective pressure on codon usage appears to be lower in genes encoding long rather than short proteins. This puzzling observation is not predicted by any of the current models of selection on codon usage and thus raises the question of how translation efficiency affects fitness in multicellular organisms."
847,"entanglement and the foundations of statistical mechanics",409394,"Entanglement and the Foundations of Statistical Mechanics","Statistical mechanics is one of the most successful areas of physics. Yet, almost 150 years since its inception, its foundations and basic postulates are still the subject of debate. Here we suggest that the main postulate of statistical mechanics, the equal a priori probability postulate, should be abandoned as misleading and unnecessary. We argue that it should be replaced by a general canonical principle, whose physical content is fundamentally different from the postulate it replaces: it refers to individual states, rather than to ensemble or time averages. Furthermore, whereas the original postulate is an unprovable assumption, the principle we propose is mathematically proven. The key element in this proof is the quantum entanglement between the system and its environment. Our approach separates the issue of finding the canonical state from finding out how close a system is to it, allowing us to go even beyond the usual boltzmannian situation."
848,"the effects of artificial selection on the maize genome",411236,"The Effects of Artificial Selection on the Maize Genome","Domestication promotes rapid phenotypic evolution through artificial selection. We investigated the genetic history by which the wild grass teosinte (Zea mays ssp. parviglumis) was domesticated into modern maize (Z. mays ssp. mays). Analysis of single-nucleotide polymorphisms in 774 genes indicates that 2 to 4% of these genes experienced artificial selection. The remaining genes retain evidence of a population bottleneck associated with domestication. Candidate selected genes with putative function in plant growth are clustered near quantitative trait loci that contribute to phenotypic differences between maize and teosinte. If we assume that our sample of genes is representative, [~]1200 genes throughout the maize genome have been affected by artificial selection. 10.1126/science.1107891"
849,"medical literature as a potential source of new knowledge",411662,"Medical literature as a potential source of new knowledge","Specialized biomedical literatures have been found that are implicitly linked by arguments that they respectively contain, but which nonetheless do not cite or refer to each other. The combined arguments lead to new inferences and conclusions that cannot be drawn from the separate literatures. One such analysis identified one set of articles showing that dietary fish oils lead to certain blood and vascular changes, and a second set containing evidence that similar changes might benefit patients with Raynaud's syndrome. Yet these two literatures had no articles in common and had never before been cited together; neither literature mentioned the other or suggested that dietary fish oil might benefit Raynaud patients. Two years after publication of that analysis, the first clinical trial demonstrating such a beneficial effect was reported independently by others. A second example of literature synthesis, based on eleven indirect connections, led to an inference that magnesium deficiency might be a causal factor in migraine headache. A third example calls attention to implicit connections between arginine intake and blood levels of somatomedins, a potentially fruitful but neglected area of research with implications for the decline with age of thymic function and protein synthesis. A model and an online search strategy to aid in identifying other logically related noninteractive literatures is described. Such structures are probably not rare and may provide the foundation for a literature-based approach to scientific discovery."
850,"artemis sequence visualization and annotation",415431,"Artemis: sequence visualization and annotation","Summary: Artemis is a DNA sequence visualization and annotation tool that allows the results of any analysis or sets of analyses to be viewed in the context of the sequence and its six-frame translation. Artemis is especially useful in analysing the compact genomes of bacteria, archaea and lower eukaryotes, and will cope with sequences of any size from small genes to whole genomes. It is implemented in Java, and can be run on any suitable platform. Sequences and annotation can be read and written directly in EMBL, GenBank and GFF format.Availability: Artemis is available under the GNU General Public License from http://www.sanger.ac.uk/Software/ArtemisContact: kmr@sanger.ac.uk"
851,"the bioperl toolkit perl modules for the life sciences",415502,"The Bioperl toolkit: Perl modules for the life sciences.","The Bioperl project is an international open-source collaboration of biologists, bioinformaticians, and computer scientists that has evolved over the past 7 yr into the most comprehensive library of Perl modules available for managing and manipulating life-science information. Bioperl provides an easy-to-use, stable, and consistent programming interface for bioinformatics application programmers. The Bioperl modules have been successfully and repeatedly used to reduce otherwise complex tasks to only a few lines of code. The Bioperl object model has been proven to be flexible enough to support enterprise-level applications such as EnsEMBL, while maintaining an easy learning curve for novice Perl programmers. Bioperl is capable of executing analyses and processing results from programs such as BLAST, ClustalW, or the EMBOSS suite. Interoperation with modules written in Python and Java is supported through the evolving BioCORBA bridge. Bioperl provides access to data stores such as GenBank and SwissProt via a flexible series of sequence input/output modules, and to the emerging common sequence data storage format of the Open Bioinformatics Database Access project. This study describes the overall architecture of the toolkit, the problem domains that it addresses, and gives specific examples of how the toolkit can be used to solve common life-sciences problems. We conclude with a discussion of how the open-source nature of the project has contributed to the development effort.{\\$}[{\\$}Supplemental material is available online at www.genome.org. Bioperl is available as open-source software free of charge and is licensed under the Perl Artistic License (http://www.perl.com/pub/a/language/misc/Artistic.html). It is available for download at http://www.bioperl.org. Support inquiries should be addressed to bioperl-l\\char64bioperl.org.{\\$}]{\\$}"
852,"least angle regression",416066,"Least angle regression","The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates."
853,"quantum algorithms and the fourier transform",416216,"Quantum Algorithms and the Fourier Transform","The quantum algorithms of Deutsch, Simon and Shor are described in a way which highlights their dependence on the Fourier transform. The general construction of the Fourier transform on an Abelian group is outlined and this provides a unified way of understanding the efficacy of these algorithms. Finally we describe an efficient quantum factoring algorithm based on a general formalism of Kitaev and contrast its structure to the ingredients of Shor's algorithm."
854,"incentives for sharing in peertopeer networks",418523,"Incentives for sharing in peer-to-peer networks","We consider the free-rider problem in peer-to-peer file sharing networks such as Napster: that individual users are provided with no incentive for adding value to the network. We examine the design implications of the assumption that users will selfishly act to maximize their own rewards, by constructing a formal game theoretic model of the system and analyzing equilibria of user strategies under several novel payment mechanisms. We support and extend this work with results from experiments..."
855,"the ins and outs of dna transfer in bacteria",419283,"The ins and outs of DNA transfer in bacteria.","Transformation and conjugation permit the passage of DNA through the bacterial membranes and represent dominant modes for the transfer of genetic information between bacterial cells or between bacterial and eukaryotic cells. As such, they are responsible for the spread of fitness-enhancing traits, including antibiotic resistance. Both processes usually involve the recognition of double-stranded DNA, followed by the transfer of single strands. Elaborate molecular machines are responsible for negotiating the passage of macromolecular DNA through the layers of the cell surface. All or nearly all the machine components involved in transformation and conjugation have been identified, and here we present models for their roles in DNA transport."
856,"data mules modeling a threetier architecture for sparse sensor networks",420474,"Data MULEs: modeling a three-tier architecture for sparse sensor networks","This paper presents and analyzes an architecture to collect sensor data in sparse sensor networks. Our approach exploits the presence of mobile entities (called MULEs) present in the environment. MULEs pick up data from the sensors when in close range, buffer it, and drop off the data to wired access points. This can lead to substantial power savings at the sensors as they only have to transmit over a short range. This paper focuses on a simple analytical model for understanding performance as system parameters are scaled. Our model assumes two-dimensional random walk for mobility and incorporates key system variables such as number of MULEs, sensors and access points. The performance metrics observed are the data success rate (the fraction of generated data that reaches the access points) and the required buffer capacities on the sensors and the MULEs. The modeling along with simulation results can be used for further analysis and provide certain guidelines for deployment of such systems."
857,"activity recognition from userannotated acceleration data",421527,"Activity Recognition from User-Annotated Acceleration Data","Abstract. In this work, algorithms are developed and evaluated to detect physical activities from data acquired using five small biaxial accelerometers worn simultaneously on different parts of the body. Acceleration data was collected from 20 subjects without researcher supervision or observation. Subjects were asked to perform a sequence of everyday tasks but not told specifically where or how to do them. Mean, energy, frequency-domain entropy, and correlation of acceleration data was calculated and several classifiers using these features were tested. Decision tree classifiers showed the best performance recognizing everyday activities with an overall accuracy rate of 84%. The results show that although some activities are recognized well with subject-independent training data, others appear to require subject-specific training data. The results suggest that multiple accelerometers aid in recognition because conjunctions in acceleration feature values can effectively discriminate many activities. With just two biaxial accelerometers – thigh and wrist – the recognition performance dropped only slightly. This is the first work to investigate performance of recognition algorithms with multiple, wire-free accelerometers on 20 activities using datasets annotated by the subjects themselves. 1"
858,"quasispecies diversity determines pathogenesis through cooperative interactions in a viral population",421931,"Quasispecies diversity determines pathogenesis through cooperative interactions in a viral population","An RNA virus population does not consist of a single genotype; rather, it is an ensemble of related sequences, termed quasispecies1, 2, 3, 4. Quasispecies arise from rapid genomic evolution powered by the high mutation rate of RNA viral replication5, 6, 7, 8. Although a high mutation rate is dangerous for a virus because it results in nonviable individuals, it has been hypothesized that high mutation rates create a 'cloud' of potentially beneficial mutations at the population level, which afford the viral quasispecies a greater probability to evolve and adapt to new environments and challenges during infection4, 9, 10, 11. Mathematical models predict that viral quasispecies are not simply a collection of diverse mutants but a group of interactive variants, which together contribute to the characteristics of the population4, 12. According to this view, viral populations, rather than individual variants, are the target of evolutionary selection4, 12. Here we test this hypothesis by examining the consequences of limiting genomic diversity on viral populations. We find that poliovirus carrying a high-fidelity polymerase replicates at wild-type levels but generates less genomic diversity and is unable to adapt to adverse growth conditions. In infected animals, the reduced viral diversity leads to loss of neurotropism and an attenuated pathogenic phenotype. Notably, using chemical mutagenesis to expand quasispecies diversity of the high-fidelity virus before infection restores neurotropism and pathogenesis. Analysis of viruses isolated from brain provides direct evidence for complementation between members in the quasispecies, indicating that selection indeed occurs at the population level rather than on individual variants. Our study provides direct evidence for a fundamental prediction of the quasispecies theory and establishes a link between mutation rate, population dynamics and pathogenesis."
859,"ontology of folksonomy",421963,"Ontology of Folksonomy","Ontologies are enabling technology for the Semantic Web.  They are a means for people to state what they mean by formal terms used in data that they might generate or consume.  Folksonomies are an emergent phenomenon of the social web.  They are created as people associate terms with content that they generate or consume.  Recently the two ideas have been put into opposition, as if they were right and left poles of a political spectrum.  This piece is an attempt to shed some cool light on the subject, and to preview some new work that applies the two ideas together to enable an Internet ecology for folksonomies."
860,"peertopeer caching schemes to address flash crowds",422273,"Peer-to-Peer Caching Schemes to Address Flash Crowds","Flash crowds can cripple a web site's performance. Since they  are infrequent and unpredictable, these floods do not justify the cost of  traditional commercial solutions. We describe Backslash, a collaborative  web mirroring system run by a collective of web sites that wish to protect  themselves from flash crowds. Backslash is built on a distributed hash  table overlay and uses the structure of the overlay to cache aggressively  a resource that experiences an uncharacteristically high request ..."
861,"secure routing for structured peertopeer overlay networks",422282,"{S}ecure routing for structured peer-to-peer overlay networks","Structured peer-to-peer overlay networks provide a sub-strate for the construction of large-scale, decentralized applications, including distributed storage, group com-munication, and content distribution. These overlays are highly resilient; they can route messages correctly even when a large fraction of the nodes crash or the network partitions. But current overlays are not secure; even a small fraction of malicious nodes can prevent correct message delivery throughout the overlay. This prob-lem is particularly serious in open peer-to-peer systems, where many diverse, autonomous parties without pre-existing trust relationships wish to pool their resources. This paper studies attacks aimed at preventing correct message delivery in structured peer-to-peer overlays and presents defenses to these attacks. We describe and eval-uate techniques that allow nodes to join the overlay, to maintain routing state, and to forward messages securely in the presence of malicious nodes. 1"
862,"tissue engineering",422598,"Tissue engineering","The loss or failure of an organ or tissue is one of the most frequent, devastating, and costly problems in human health care. A new field, tissue engineering, applies the principles of biology and engineering to the development of functional substitutes for damaged tissue. This article discusses the foundations and challenges of this interdisciplinary field and its attempts to provide solutions to tissue creation and repair. The loss or failure of an organ or tissue is one of the most frequent, devastating, and costly problems in human health care. A new field, tissue engineering, applies the principles of biology and engineering to the development of functional substitutes for damaged tissue. This article discusses the foundations and challenges of this interdisciplinary field and its attempts to provide solutions to tissue creation and repair."
863,"hierarchical bayesian inference in networks of spiking neurons",423151,"Hierarchical Bayesian Inference in Networks of Spiking Neurons","[Bayesian inference, hidden Markov model, spiking neurons, noisy integrate-and-fire model, single leve, hierarchical inference] There is growing evidence from psychophysical and neurophysiological studies that the brain utilizes Bayesian principles for inference and decision making. An important open question is how Bayesian inference for arbitrary graphical models can be implemented in networks of spiking neurons. In this paper, they show that recurrent networks of noisy integrate-and-fire neurons can perform approximate Bayesian inference for dynamic and hierarchical graphical models. The membrane potential dynamics of neurons is used to implement belief propagation in the log domain. The spiking probability of a neuron is shown to approximate the posterior probability of the preferred state encoded by the neuron, given past inputs. They illustrate the model using two examples: (1) a motion detection network in which the spiking probability of a direction-selective neuron becomes proportional to the posterior probability of motion in a preferred direction, and (2) a two-level hierarchical network that produces attentional effects similar to those observed in visual cortical areas V2 and V4. The hierarchical model offers a new Bayesian interpretation of attentional modulation in V2 and V4."
864,"handbook of computer game studies",423826,"Handbook of computer game studies","{New media students, teachers, and professionals have long needed a comprehensive scholarly treatment of digital games that deals with the history, design, reception, and aesthetics of games along with their social and cultural context. <i>The Handbook of Computer Game Studies</i> fills this need with a definitive look at the subject from a broad range of perspectives. Contributors come from cognitive science and artificial intelligence, developmental, social, and clinical psychology, history, film, theater, and literary studies, cultural studies, and philosophy as well as game design and development. The text includes both scholarly articles and journalism from such well-known voices as Douglas Rushkoff, Sherry Turkle, Henry Jenkins, Katie Salen, Eric Zimmerman, and others.<br /> <br /> Part I considers the ""prehistory"" of computer games (including slot machines and pinball machines), the development of computer games themselves, and the future of mobile gaming. The chapters in part II describe game development from the designer's point of view, including the design of play elements, an analysis of screenwriting, and game-based learning. Part III reviews empirical research on the psychological effects of computer games, and includes a discussion of the use of computer games in clinical and educational settings. Part IV considers the aesthetics of games in comparison to film and literature, and part V discusses the effect of computer games on cultural identity, including gender and ethnicity. Finally, part VI looks at the relation of computer games to social behavior, considering, among other matters, the inadequacy of laboratory experiments linking games and aggression and the different modes of participation in computer game culture.}"
865,"improved prediction of signal peptides signalp",423871,"Improved prediction of signal peptides: SignalP 3.0.","We describe improvements of the currently most popular method for prediction of classically secreted proteins, SignalP. SignalP consists of two different predictors based on neural network and hidden Markov model algorithms, where both components have been updated. Motivated by the idea that the cleavage site position and the amino acid composition of the signal peptide are correlated, new features have been included as input to the neural network. This addition, combined with a thorough error-correction of a new data set, have improved the performance of the predictor significantly over SignalP version 2. In version 3, correctness of the cleavage site predictions has increased notably for all three organism groups, eukaryotes, Gram-negative and Gram-positive bacteria. The accuracy of cleavage site prediction has increased in the range 6–17\\ over the previous version, whereas the signal peptide discrimination improvement is mainly due to the elimination of false-positive predictions, as well as the introduction of a new discrimination score for the neural network. The new method has been benchmarked against other available methods. Predictions can be made at the publicly available web server http://www.cbs.dtu.dk/services/SignalP/"
866,"a theory of timed automata",427434,"A theory of timed automata","We propose timed (finite) automata to model the behavior of real-time systems over time. Our definition provides a simple, and yet powerful, way to annotate state-transition graphs with timing constraints using finitely many real-valued clocks. A timed automaton accepts timed words–infinite sequences in which a real-valued time of occurrence is associated with each symbol. We study timed automata from the perspective of formal language theory: we consider closure properties, decision problems, and subclasses. We consider both nondeterministic and deterministic transition structures, and both Büchi and Muller acceptance conditions. We show that nondeterministic timed automata are closed under union and intersection, but not under complementation, whereas deterministic timed Muller automata are closed under all Boolean operations. The main construction of the paper is an (PSPACE) algorithm for checking the emptiness of the language of a (nondeterministic) timed automaton. We also prove that the universality problem and the language inclusion problem are solvable only for the deterministic automata: both problems are undecidable (Π11-hard) in the nondeterministic case and PSPACE-complete in the deterministic case. Finally, we discuss the application of this theory to automatic verification of real-time requirements of finite-state systems."
867,"navigating towards sustainable development a system dynamics approach",429981,"Navigating towards sustainable development: A system dynamics approach","Traditional fragmented and mechanistic science is unable to cope with issues about sustainability, as these are often related to complex, self-organizing systems. In the paper, sustainable development is seen as an unending process defined neither by fixed goals nor by specific means of achieving them. It is argued that, in order to understand the sources of and the solutions to modern problems, linear and mechanistic thinking must give way to non-linear and organic thinking, more commonly referred to as systems thinking. System Dynamics, which operates in a whole-system fashion, is put forward as a powerful methodology to deal with issues of sustainability. Examples of successful applications are given.Any system in which humans are involved is characterized by the following essential system properties: Bounded rationality, limited certainty, limited predictability, indeterminate causality, and evolutionary change. We need to resort to an adaptive approach, where we go through a learning process and modify our decision rules and our mental models of the real world as we go along. This will enable us to improve system performance by setting dynamic improvement goals (moving targets) for it.Finally, it is demonstrated how causal loop diagrams can be used to find the leverage points of a system."
868,"designing qualitative research",430074,"Designing Qualitative Research","{<img src=""http://www.sagepub.com/repository/binaries/images/ce/ce_9credits.jpg"" border=""0"" /><br />The <strong>Fourth Edition</strong> of this best-selling text, <strong>Designing Qualitative Research</strong>, once again provides pragmatic guidance for developing and successfully defending proposals for qualitative inquiry. With expanded coverage of ethics, analysis processes, and approaches, authors Catherine Marshall and Gretchen B. Rossman, have updated this highly popular text to reflect the advances and challenges presented by provocative developments and new applications since the previous edition. <br /><br /><strong>New to the Fourth Edition: <br /></strong><ul><li>Offers a contemporary perspective: This new edition takes the current political climate into consideration and introduces readers to todays research environment. This edition also includes discussions about distance-based research (such as email interviews and online discussion groups), the implications of postmodern turns, integrating archival material in qualitative research, and creative ways of presenting the research. </li><li>Provides updated references: Updated material enables researchers to find resources that reflect emerging genres and choices guided by the most recent insights, controversies, examples, and research tools. This version of the book addresses recent thinking on ""the researcher in the research setting."" In addition, the end of each chapter features a post script that consists of a dialogue between a student and an advisor which illustrates design dilemmas in real time. </li><li>Emphasizes a step-by-step approach: The text walks readers through the crafting of a research project from start to finish in a step-by-step fashion. Concrete models for the various parts of a proposal are provided to help readers create outlines, entry letters, consent letters, budgets, and timelines. </li></ul><p> </p><br /><strong>Intended Audience: <br /></strong>Perfect for advanced undergraduate and graduate students taking their first or second Qualitative Research Methods or General Research Methods course, as well as for individual researchers across the social science disciplines; in addition it is a must-have resource for the library of those using qualitative approaches}"
869,"implicit interest indicators",430293,"Implicit Interest Indicators","Recommender systems provide personalized suggestions about items that users will find interesting.  Typically, recommender systems require a user interface that can ``intelligently'' determine the interest of a user and use this information to make suggestions.  The common solution, ``explicit ratings'', where users tell the system what they think about a piece of information, is well-understood and fairly precise.  However, having to stop to enter explicit ratings can alter normal patterns of browsing and reading. A more ``intelligent'' method is to use implicit ratings , where a rating is obtained by a method other than obtaining it directly from the user.  These implicit interest indicators have obvious advantages, including removing the cost of the user rating, and that every user interaction with the system can contribute to an implicit rating.   Current recommender systems mostly do not use implicit ratings, nor is the ability of implicit ratings to predict actual user interest well-understood.  This research studies the correlation between various implicit ratings and the explicit rating for a single Web page.  A Web browser was developed to record the user's actions (implicit ratings) and the explicit rating of a page.  Actions included mouse clicks, mouse movement, scrolling and elapsed time. This browser was used by over 80 people that browsed more than 2500 Web pages.   Using the data collected by the browser, the individual implicit ratings and some combinations of implicit ratings were analyzed and compared with the explicit rating.  We found that the time spent on a page, the amount of scrolling on a page and the combination of time and scrolling had a strong correlation with explicit interest, while individual scrolling methods and mouse-clicks were ineffective in predicting explicit interest."
870,"methods and metrics for coldstart recommendations",430299,"Methods and metrics for cold-start recommendations","We have developed a method for recommending items that combines content and collaborative data under a single probabilistic framework. We benchmark our algorithm against a na&iuml;ve Bayes classifier on the   cold-start  problem, where we wish to recommend items that no one in the community has yet rated. We systematically explore three testing methodologies using a publicly available data set, and explain how these methods apply to specific real-world applications. We advocate heuristic recommenders when benchmarking to give competent baseline performance. We introduce a new performance metric, the CROC curve, and demonstrate empirically that the various components of our testing strategy combine to obtain deeper understanding of the performance characteristics of recommender systems. Though the emphasis of our testing is on  cold-start  recommending, our methods for recommending and evaluation are general."
871,"the particle swarm explosion stability and convergence in a multidimensional complex space",431197,"The particle swarm - explosion, stability, and convergence in a multidimensional complex space","The particle swarm is an algorithm for finding optimal regions of complex search spaces through the interaction of individuals in a population of particles. This paper analyzes a particle's trajectory as it moves in discrete time (the algebraic view), then progresses to the view of it in continuous time (the analytical view). A five-dimensional depiction is developed, which describes the system completely. These analyses lead to a generalized model of the algorithm, containing a set of coefficients to control the system's convergence tendencies. Some results of the particle swarm optimizer, implementing modifications derived from the analysis, suggest methods for altering the original algorithm in ways that eliminate problems and increase the ability of the particle swarm to find optima of some well-studied test functions"
872,"data analysis with bayesian networks a bootstrap approach",431383,"Data Analysis with Bayesian Networks: {A} Bootstrap Approach","In recent years there has been significant  progress in algorithms and methods for inducing  Bayesian networks from data. However, in complex  data analysis problems, we need to go beyond  being satisfied with inducing networks with  high scores. We need to provide confidence measures  on features of these networks: Is the existence  of an edge between two nodes warranted?  Is the Markov blanket of a given node robust?  Can we say something about the ordering of the  variables? We should be able to address these  questions, even when the amount of data is not  enough to induce a high scoring network. In this  paper we propose Efron's Bootstrap as a computationally  efficient approach for answering these  questions. In addition, we propose to use these  confidence measures to induce better structures  from the data, and to detect the presence of latent  variables."
873,"dna methylation and human disease",431442,"DNA methylation and human disease"," DNA methylation is a crucial epigenetic modification of the genome that is involved in regulating many cellular processes. These include embryonic development, transcription, chromatin structure, X chromosome inactivation, genomic imprinting and chromosome stability. Consistent with these important roles, a growing number of human diseases have been found to be associated with aberrant DNA methylation. The study of these diseases has provided new and fundamental insights into the roles that DNA methylation and other epigenetic modifications have in development and normal cellular homeostasis."
874,"comparison of discrimination methods for the classification of tumors using gene expression data",431502,"Comparison of Discrimination Methods for the Classification of Tumors Using Gene Expression Data","A reliable and precise classification of tumors is essential for successful treatment of cancer. cDNA microarrays and high-density oligonucleotide chips are novel biotechnologies which are being used increasingly in cancer research. By allowing the monitoring of expression levels for thousands of genes simultaneously, such techniques may lead to a more complete understanding of the molecular variations among tumors and hence to a finer and more informative classification. The ability to successfully distinguish between tumor classes (already known or yet to be discovered) using gene expression data is an important aspect of this novel approach to cancer classification. In this talk, we compare the performance of different discrimination methods for the classification of tumors based on gene expression profiles. These methods include: nearest-neighbor classifiers, linear discriminant analysis, and classification trees. In our comparison, we also consider recent machine learning approaches for aggregating predictors such as bagging and boosting. The methods are applied to three recently published datasets: the leukemia (ALL/AML) dataset of Golub et al. (1999), the lymphoma dataset of Alizadeh et al. (2000), and the 60 cancer cell line (NCI 60) dataset of Ross et al. (2000)."
875,"gene expression correlates of clinical prostate cancer behavior",431521,"Gene expression correlates of clinical prostate cancer behavior","Prostate tumors are among the most heterogeneous of cancers, both histologically and clinically. Microarray expression analysis was used to determine whether global biological differences underlie common pathological features of prostate cancer and to identify genes that might anticipate the clinical behavior of this disease. While no expression correlates of age, serum prostate specific antigen (PSA), and measures of local invasion were found, a set of genes was identified that strongly correlated with the state of tumor differentiation as measured by Gleason score. Moreover, a model using gene expression data alone accurately predicted patient outcome following prostatectomy. These results support the notion that the clinical behavior of prostate cancer is linked to underlying gene expression differences that are detectable at the time of diagnosis."
876,"gene expression profiling predicts clinical outcome of breast cancer",431525,"Gene expression profiling predicts clinical outcome of breast cancer","Breast cancer patients with the same stage of disease can have markedly different treatment responses and overall outcome. The strongest predictors for metastases (for example, lymph node status and histological grade) fail to classify accurately breast tumours according to their clinical behaviour. Chemotherapy or hormonal therapy reduces the risk of distant metastases by approximately one-third; however, 70-80\% of patients receiving this treatment would have survived without it. None of the signatures of breast cancer gene expression reported to date allow for patient-tailored therapy strategies. Here we used DNA microarray analysis on primary breast tumours of 117 young patients, and applied supervised classification to identify a gene expression signature strongly predictive of a short interval to distant metastases ('poor prognosis' signature) in patients without tumour cells in local lymph nodes at diagnosis (lymph node negative). In addition, we established a signature that identifies tumours of BRCA1 carriers. The poor prognosis signature consists of genes regulating cell cycle, invasion, metastasis and angiogenesis. This gene expression profile will outperform all currently used clinical parameters in predicting disease outcome. Our findings provide a strategy to select patients who would benefit from adjuvant therapy."
877,"image metrics in the statistical analysis of dna microarray data",431532,"Image metrics in the statistical analysis of DNA microarray data","DNA microarrays represent an important new method for determining the complete expression profile of a cell. In ``spotted'' microarrays, slides carrying spots of target DNA are hybridized to fluorescently labeled cDNA from experimental and control cells and the arrays are imaged at two or more wavelengths. In this paper, we perform statistical analysis on images of microarrays and show that quantitating the amount of fluorescent DNA bound to microarrays is subject to considerable uncertainty because of large and small-scale intensity fluctuations within spots, nonadditive background, and fabrication artifacts. Pixel-by-pixel analysis of individual spots can be used to estimate these sources of error and establish the precision and accuracy with which gene expression ratios are determined. Simple weighting schemes based on these estimates are effective in improving significantly the quality of microarray data as it accumulates in a multiexperiment database. We propose that error estimates from image-based metrics should be one component in an explicitly probabilistic scheme for the analysis of DNA microarray data."
878,"experimental design for gene expression microarrays",431542,"Experimental Design for gene expression microarrays","We examine experimental design issues arising with gene expression microarray technology. Microarray experiments have multiple sources of variation, and experimental plans should ensure that effects of interest are not confounded with ancillary effects. A commonly used design is shown to violate this principle and to be generally inefficient. We explore the connection between microarray designs and classical block design and use a family of ANOVA models as a guide to choosing a design. We combine principles of good design and A-optimality to give a general set of recommendations for design with microarrays. These recommendations are illustrated in detail for one kind of experimental objective, where we also give the results of a computer search for good designs. 10.1093/biostatistics/2.2.183"
879,"predicting the clinical status of human breast cancer by using gene expression profiles",431561,"Predicting the clinical status of human breast cancer by using gene expression profiles","10.1073/pnas.201162998 Prognostic and predictive factors are indispensable tools in the treatment of patients with neoplastic disease. For the most part, such factors rely on a few specific cell surface, histological, or gross pathologic features. Gene expression assays have the potential to supplement what were previously a few distinct features with many thousands of features. We have developed Bayesian regression models that provide predictive capability based on gene expression data derived from DNA microarray analysis of a series of primary breast cancer samples. These patterns have the capacity to discriminate breast tumors on the basis of estrogen receptor status and also on the categorized lymph node status. Importantly, we assess the utility and validity of such models in predicting the status of tumors in crossvalidation determinations. The practical value of such approaches relies on the ability not only to assess relative probabilities of clinical outcomes for future samples but also to provide an honest assessment of the uncertainties associated with such predictive classifications on the basis of the selection of gene subsets for each validation analysis. This latter point is of critical importance in the ability to apply these methodologies to clinical assessment of tumor phenotype."
880,"assessing gene significance from cdna microarray expression data via mixed models",431562,"Assessing Gene Significance from cDNA Microarray Expression Data via Mixed Models","The determination of a list of differentially expressed genes is a basic objective in many cDNA microarray experiments. We present a statistical approach that allows direct control over the percentage of false positives in such a list and, under certain reasonable assumptions, improves on existing methods with respect to the percentage of false negatives. The method accommodates a wide variety of experimental designs and can simultaneously assess significant differences between multiple types of biological samples. Two interconnected mixed linear models are central to the method and provide a flexible means to properly account for variability both across and within genes. The mixed model also provides a convenient framework for evaluating the statistical power of any particular experimental design and thus enables a researcher to a priori select an appropriate number of replicates. We also suggest some basic graphics for visualizing lists of significant genes. Analyses of published experiments studying human cancer and yeast cells illustrate the results."
881,"validating clustering for gene expression data",431565,"Validating Clustering for Gene Expression Data","MOTIVATION: Many clustering algorithms have been proposed for the analysis of gene expression data, but little guidance is available to help choose among them. We provide a systematic framework for assessing the results of clustering algorithms. Clustering algorithms attempt to partition the genes into groups exhibiting similar patterns of variation in expression level. Our methodology is to apply a clustering algorithm to the data from all but one experimental condition. The remaining condition is used to assess the predictive power of the resulting clusters-meaningful clusters should exhibit less variation in the remaining condition than clusters formed by chance. RESULTS: We successfully applied our methodology to compare six clustering algorithms on four gene expression data sets. We found our quantitative measures of cluster quality to be positively correlated with external standards of cluster quality."
882,"singular value decomposition for genomewide expression data processing and modeling",431569,"Singular value decomposition for genome-wide expression data processing and modeling","We describe the use of singular value decomposition in transforming genome-wide expression data from genes 3 arrays space to reduced diagonalized ''eigengenes'' 3 ''eigenarrays'' space, where the eigengenes (or eigenarrays) are unique orthonormal superpositions of the genes (or arrays). Normalizing the data by filtering out the eigengenes (and eigenarrays) that are inferred to represent noise or experimental artifacts enables meaningful comparison of the expression of different genes across different arrays in different experiments. Sorting the data according to the eigengenes and eigenarrays gives a global picture of the dynamics of gene expression, in which individual genes and arrays appear to be classified into groups of similar regulation and function, or similar cellular state and biological phenotype, respectively. After normalization and sorting, the significant eigengenes and eigenarrays can be associated with observed genome-wide effects of regulators, or with measured samples, in which these regulators are overactive or underactive, respectively."
883,"analysis of variance for gene expression microarray data",431582,"Analysis of variance for gene expression microarray data","Spotted {cDNA} microarrays are emerging as a powerful and cost-effective tool for large-scale analysis of gene expression. Microarrays can be used to measure the relative quantities of specific {mRNAs} in two or more tissue samples for thousands of genes simultaneously. While the power of this technology has been recognized, many open questions remain about appropriate analysis of microarray data. One question is how to make valid estimates of the relative expression for genes that are not biased by ancillary sources of variation. Recognizing that there is inherent ""noise"" in microarray data, how does one estimate the error variation associated with an estimated change in expression, i.e., how does one construct the error bars? We demonstrate that {ANOVA} methods can be used to normalize microarray data and provide estimates of changes in gene expression that are corrected for potential confounding effects. This approach establishes a framework for the general analysis and interpretation of microarray data."
884,"importance of replication in microarray gene expression studies statistical methods and evidence from repetitive cdna hybridizations",431585,"Importance of replication in microarray gene expression studies: Statistical methods and evidence from repetitive cDNA hybridizations","We present statistical methods for analyzing replicated cDNA microarray expression data and report the results of a controlled experiment. The study was conducted to investigate inherent variability in gene expression data and the extent to which replication in an experiment produces more consistent and reliable findings. We introduce a statistical model to describe the probability that mRNA is contained in the target sample tissue, converted to probe, and ultimately detected on the slide. We also introduce a method to analyze the combined data from all replicates. Of the 288 genes considered in this controlled experiment, 32 would be expected to produce strong hybridization signals because of the known presence of repetitive sequences within them. Results based on individual replicates, however, show that there are 55, 36, and 58 highly expressed genes in replicates 1, 2, and 3, respectively. On the other hand, an analysis by using the combined data from all 3 replicates reveals that only 2 of the 288 genes are incorrectly classified as expressed. Our experiment shows that any single microarray output is subject to substantial variability. By pooling data from replicates, we can provide a more reliable analysis of gene expression data. Therefore, we conclude that designing experiments with replications will greatly reduce misclassification rates. We recommend that at least three replicates be used in designing experiments by using cDNA microarrays, particularly when gene expression data from single specimens are being analyzed."
885,"principal components analysis to summarize microarray experiments application to sporulation time series",431587,"Principal components analysis to summarize microarray experiments: application to sporulation time series","A series of microarray experiments produces observations of differential expression for thousands of genes across multiple conditions. It is often not clear whether a set of experiments are measuring fundamentally different gene expression states or are measuring similar states created through different mechanisms. It is useful, therefore, to define a core set of independent features for the expression states that allow them to be compared directly. Principal components analysis (PCA) is a statistical technique for determining the key variables in a multidimensional data set that explain the differences in the observations, and can be used to simplify the analysis and visualization of multidimensional data sets. We show that application of PCA to expression data (where the experimental conditions are the variables, and the gene expression measurements are the observations) allows us to summarize the ways in which gene responses vary under different conditions. Examination of the components also provides insight into the underlying factors that are measured in the experiments. We applied PCA to the publicly released yeast sporulation data set (Chu et al. 1998). In that work, 7 different measurements of gene expression were made over time. PCA on the time-points suggests that much of the observed variability in the experiment can be summarized in just 2 components--i.e. 2 variables capture most of the information. These components appear to represent (1) overall induction level and (2) change in induction level over time. We also examined the clusters proposed in the original paper, and show how they are manifested in principal component space. Our results are available on the internet at http: inverted question markwww.smi.stanford.edu/project/helix/PCArray ."
886,"systematic variation in gene expression patterns in human cancer cell lines",431588,"Systematic variation in gene expression patterns in human cancer cell lines","We used cDNA microarrays to explore the variation in expression of approximately 8,000 unique genes among the 60 cell lines used in the National Cancer Institute's screen for anti-cancer drugs. Classification of the cell lines based solely on the observed patterns of gene expression revealed a correspondence to the ostensible origins of the tumours from which the cell lines were derived. The consistent relationship between the gene expression patterns and the tissue of origin allowed us to recognize outliers whose previous classification appeared incorrect. Specific features of the gene expression patterns appeared to be related to physiological properties of the cell lines, such as their doubling time in culture, drug metabolism or the interferon response. Comparison of gene expression patterns in the cell lines to those observed in normal breast tissue or in breast tumour specimens revealed features of the expression patterns in the tumours that had recognizable counterparts in specific cell lines, reflecting the tumour, stromal and inflammatory components of the tumour tissue. These results provided a novel molecular characterization of this important group of human cell lines and their relationships to tumours in vivo."
887,"a gene expression database for the molecular pharmacology of cancer",431591,"A gene expression database for the molecular pharmacology of cancer","We used cDNA microarrays to assess gene expression profiles in 60 human cancer cell lines used in a drug discovery screen by the National Cancer Institute. Using these data, we linked bioinformatics and chemoinformatics by correlating gene expression and drug activity patterns in the NCI60 lines. Clustering the cell lines on the basis of gene expression yielded relationships very different from those obtained by clustering the cell lines on the basis of their response to drugs. Gene-drug relationships for the clinical agents 5-fluorouracil and L-asparaginase exemplify how variations in the transcript levels of particular genes relate to mechanisms of drug sensitivity and resistance. This is the first study to integrate large databases on gene expression and molecular pharmacology."
888,"estimating the number of clusters in a dataset via the gap statistic",431595,"Estimating the number of clusters in a dataset via the gap statistic","We propose a method (the \Gap statistic&#034;) for estimating the number of clusters (groups) in a set of data. The technique uses the output of any clustering algorithm (e.g. k-means or hierarchical), comparing the change in within cluster dispersion to that expected under an appropriate reference null distribution. Some theory is developed for the proposal and a simulation study that shows that the Gap statistic usually outperforms other methods that have been proposed in the literature. We also briey explore application of the same technique to the problem for estimating the number of linear principal components. 1 Introduction  Cluster analysis is an important tool for \unsupervised&#034; learning  the problem of nding groups in data without the help of a response variable. A major challenge in cluster analysis is estimation of the optimal number of \clusters&#034;. Figure 1 (top right) shows a typical plot of an error measure W k (the within cluster dispersion dened below) for a clustering pr..."
889,"high density synthetic oligonucleotide arrays",431610,"High density synthetic oligonucleotide arrays","Experimental genomics involves taking advantage of sequence information to investigate and understand the workings of genes, cells and organisms. We have developed an approach in which sequence information is used directly to design high- density, two-dimensional arrays of synthetic oligonucleotides. The GeneChip(R) probe arrays are made using spatially patterned, light-directed combinatorial chemical synthesis, and contain up to hundreds of thousands of different oligonucleotides on a small glass surface. The arrays have been designed and used for quantitative and highly parallel measurements of gene expression, to discover polymorphic loci and to detect the presence of thousands of alternative alleles. Here, we describe the fabrication of the arrays, their design and some specific: applications to high-throughput genetic and cellular analysis."
890,"semisupervised support vector machines",431627,"Semi-Supervised Support Vector Machines","We introduce a semi-supervised support vector machine (S  3  VM)  method. Given a training set of labeled data and a working set  of unlabeled data, S  3  VM constructs a support vector machine using  both the training and working sets. We use S  3  VM to solve  the transduction problem using overall risk minimization (ORM)  posed by Vapnik. The transduction problem is to estimate the  value of a classification function at the given points in the working  set. This contrasts with the standard inductive learning problem  of estimating the classification function at all possible values and  then using the fixed function to deduce the classes of the working  set data. We propose a general S  3  VM model that minimizes both  the misclassification error and the function capacity based on all  the available data. We show how the S  3  VM model for 1-norm linear  support vector machines can be converted to a mixed-integer  program and then solved exactly using integer programming. Results  of S  3  VM and the standard 1-norm support vector machine  approach are compared on eleven data sets. Our computational  results support the statistical learning theory results showing that  incorporating working data improves generalization when insu#-  cient training information is available. In every case, S  3  VM either  improved or showed no significant di#erence in generalization compared  to the traditional approach.  # This paper has been accepted for publication in Proceedings of Neural Information Processing Systems, Denver, 1998.  1"
891,"a hierarchical latent variable model for data visualization",431628,"A Hierarchical Latent Variable Model for Data Visualization","Visualization has proven to be a powerful and widely-applicable tool for the analysis and interpretation of multi-variate data. Most visualization algorithms aim to find a projection from the data space down to a two-dimensional visualization space. However, for complex data sets living in a high-dimensional space it is unlikely that a single two-dimensional projection can reveal all of the interesting structure. We therefore introduce a hierarchical visualization algorithm which allows the complete data set to be visualized at the top level, with clusters and sub-clusters of data points visualized at deeper levels. The algorithm is based on a hierarchical mixture of latent variable models, whose parameters are estimated using the expectation-maximization algorithm. We demonstrate the principle of the approach on a toy data set, and we then apply the algorithm to the visualization of a synthetic data set in 12 dimensions obtained from a simulation of multi-phase ,ows in oil pipelines, and to data in 36 dimensions derived from satellite images. A Matlab* software implementation of the algorithm is publicly available from the world-wide web.^L A Hierarchical Latent Variable Model for Data Visualization"
892,"matrix analysis",431650,"Matrix Analysis","{Linear algebra and matrix theory have long been fundamental tools in mathematical disciplines as well as fertile fields for research. In this book the authors present classical and recent results of matrix analysis that have proved to be important to applied mathematics. Facts about matrices, beyond those found in an elementary linear algebra course, are needed to understand virtually any area of mathematical science, but the necessary material has appeared only sporadically in the literature and in university curricula. As interest in applied mathematics has grown, the need for a text and reference offering a broad selection of topics in matrix theory has become apparent, and this book meets that need.    This volume reflects two concurrent views of matrix analysis. First, it encompasses topics in linear algebra that have arisen out of the needs of mathematical analysis. Second, it is an approach to real and complex linear algebraic problems that does not hesitate to use notions from analysis. Both views are reflected in its choice and treatment of topics.}"
893,"yeast microarrays for genome wide parallel genetic and gene expression analysis",431667,"Yeast microarrays for genome wide parallel genetic and gene expression analysis","{{W}e have developed high-density {DNA} microarrays of yeast {ORF}s. {T}hese microarrays can monitor hybridization to {ORF}s for applications such as quantitative differential gene expression analysis and screening for sequence polymorphisms. {A}utomated scripts retrieved sequence information from public databases to locate predicted {ORF}s and select appropriate primers for amplification. {T}he primers were used to amplify yeast {ORF}s in 96-well plates, and the resulting products were arrayed using an automated micro arraying device. {A}rrays containing up to 2,479 yeast {ORF}s were printed on a single slide. {T}he hybridization of fluorescently labeled samples to the array were detected and quantitated with a laser confocal scanning microscope. {A}pplications of the microarrays are shown for genetic and gene expression analysis at the whole genome level.}"
894,"simplified support vector decision rules",431678,"Simplified Support Vector Decision Rules","A Support Vector Machine (SVM) is a universal learning machine whose decision surface is parameterized by a set of support vectors, and by a set of corresponding weights. An SVM is also characterized by a kernel  function. Choice of the kernel determines whether the resulting SVM is a polynomial classifier, a two-layer neural network, a radial basis function machine, or some other learning machine. SVMs are currently considerably slower in test phase than other approaches with similar..."
895,"a comparison of selection schemes used in genetic algorithms",431690,"A Comparison of Selection Schemes used in Genetic Algorithms","Genetic Algorithms are a common probabilistic optimization method based on the model of natural evolution. One important operator in these algorithms is the selection scheme for which in this paper a new description model is introduced. With this a mathematical analysis of tournament selection, ranking selection and truncation selection is carried out that allows an exact prediction of the fitness values after selection. Furthermore several new properties of selection schemes are derived and..."
896,"a study of crossvalidation and bootstrap for accuracy estimation and model selection",431698,"A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection","We review accuracy estimation methods and compare the two most common methods: crossvalidation and bootstrap. Recent experimental results on artificial data and theoretical results in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive leaveone -out cross-validation. We report on a largescale experiment---over half a million runs of C4.5 and a Naive-Bayes algorithm---to estimate the effects of different parameters on these algorithms on real-world datasets. For crossvalidation, we vary the number of folds and whether the folds are stratified or not; for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, the best method to use for model selection is ten-fold stratified cross validation, even if computation power allows using more folds. 1 Introduction It can not be emphasized enough that no claim ..."
897,"bayesian interpolation",431734,"{B}ayesian Interpolation","Although Bayesian analysis has been in use since Laplace, the Bayesian method of model--comparison has only recently been developed in depth.  In this paper, the Bayesian approach to regularisation and model--comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other problems.  Regularising constants are set by examining their posterior probability distribution. Alternative regularisers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. `Occam&#039;s razor&#039; is automatically embodied by this framework.  The way in which Bayes infers the values of regularising constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.  1 Data modelling and Occam&#039;s razor In science, a central task is to develop and compare models to a..."
898,"a review of evolutionary artificial neural networks",431738,"A Review of Evolutionary Artificial Neural Networks","Research on potential interactions between connectionist learning systems, i.e., artificial neural networks (ANNs), and evolutionary search procedures, like genetic algorithms (GAs), has attracted a lot of attention recently. Evolutionary ANNs (EANNs) can be considered as the combination of ANNs and evolutionary search procedures. This paper first distinguishes among three kinds of evolution in EANNs, i.e., the evolution of connection weights, of architectures and of learning rules. Then it reviews each kind of evolution in detail and analyses critical issues related to different evolutions. The review shows that although a lot of work has been done on the evolution of connection weights and of architectures, few attempts have been made to understand the evolution of learning rules. Interactions among different evolutions are seldom mentioned in current research. However, the evolution of learning rules and its interactions with other kinds of evolution play a vital role in EANNs. As the final part, this paper briefly describes a general framework for EANNs, which not only includes the aforementioned three kinds of evolution, but also considers interactions among them."
899,"lightdirected spatially addressable parallel chemical synthesis",431744,"Light-directed, spatially addressable parallel chemical synthesis","Solid-phase chemistry, photolabile protecting groups, and photolithography have been combined to achieve light-directed, spatially addressable parallel chemical synthesis to yield a highly diverse set of chemical products. Binary masking, one of many possible combinatorial synthesis strategies, yields 2n compounds in n chemical steps. An array of 1024 peptides was synthesized in ten steps, and its interaction with a monoclonal antibody was assayed by epifluorescence microscopy. High-density arrays formed by light-directed synthesis are potentially rich sources of chemical diversity for discovering new ligands that bind to biological receptors and for elucidating principles governing molecular interactions. The generality of this approach is illustrated by the light-directed synthesis of a dinucleotide. Spatially directed synthesis of complex compounds could also be used for microfabrication of devices. 10.1126/science.1990438"
900,"a comparative analysis of selection schemes used in genetic algorithms",431745,"A Comparative Analysis of Selection Schemes Used in Genetic Algorithms","This paper considers a number of selection schemes commonly used in modern genetic algorithms. Specifically, proportionate reproduction, ranking selection, tournament selection, and Genitor (or «steady state"") selection are compared on the basis of solutions to deterministic difference or differential equations, which are verified through computer simulations. The analysis provides convenient approximate or exact solutions as well as useful convergence time and growth ratio estimates. The paper recommends practical application of the analyses and suggests a number of paths for more detailed analytical investigation of selection techniques."
901,"rfid security and privacy a research survey",432019,"RFID Security and Privacy: A Research Survey","This article surveys recent technical research on the problems of privacy and security for {RFID} {(Radio} Frequency {IDentification).} {RFID} tags are small, wireless devices that help identify objects and people. Thanks to dropping cost, they are likely to proliferate into the billions in the next several years – and eventually into the trillions. {RFID} tags track objects in supply chains, and are working their way into the pockets, belongings and even the bodies of consumers. This survey examines approaches proposed by scientists for privacy protection and integrity assurance in {RFID} systems, and treats the social and technical context of their work. While geared toward the non-specialist, the survey may also serve as a reference for specialist readers."
902,"an ontologydriven framework for data transformation in scientific workflows",435447,"An ontology-driven framework for data transformation in scientific workflows","Ecologists spend considerable effort integrating heterogeneous data for statistical analyses and simulations, for example, to run and test predictive models. Our research is focused on reducing this effort by providing data integration and transformation tools, allowing researchers to focus on &#8220;real science,&#8221; that is, discovering new knowledge through analysis and modeling. This paper defines a generic framework for transforming heterogeneous data within scientific workflows. Our approach relies on a formalized ontology, which serves as a simple, unstructured global schema. In the framework, inputs and outputs of services within scientific workflows can have structural types and separate semantic types (expressions of the target ontology). In addition, a registration mapping can be defined to relate input and output structural types to their corresponding semantic types. Using registration mappings, appropriate data transformations can then be generated for each desired service composition. Here, we describe our proposed framework and an initial implementation for services that consume and produce XML data."
903,"conformational change of proteins arising from normal mode calculations",436280,"Conformational change of proteins arising from normal mode calculations","A normal mode analysis of 20 proteins in `open' or `closed' forms was performed using simple potential and protein models. The quality of the results was found to depend upon the form of the protein studied, normal modes obtained with the open form of a given protein comparing better with the conformational change than those obtained with the closed form. Moreover, when the motion of the protein is a highly collective one, then, in all cases considered, there is a single low-frequency normal mode whose direction compares well with the conformational change. When it is not, in most cases there is still a single low-frequency normal mode giving a good description of the pattern of the atomic displacements, as they are observed experimentally during the conformational change. Hence a lot of information on the nature of the conformational change of a protein is often found in a single low-frequency normal mode of its open form. Since this information can be obtained through the normal mode analysis of a model as simple as that used in the present study, it is likely that the property captured by such an analysis is for the most part a property of the shape of the protein itself. One of the points that has to be clarified now is whether or not amino acid sequences have been selected in order to allow proteins to follow a single normal mode direction, as least at the very beginning of their conformational change. 10.1093/protein/14.1.1"
904,"why we play games four keys to more emotion without story",436449,"Why We Play Games: Four Keys to More Emotion Without Story","People play games to change or structure their internal experiences. Adults in this study, enjoy filling their heads with thoughts and emotions unrelated to work or school, others enjoy the challenge and chance to test their abilities. Games offer an efficiency and order in playing that they want in life. They value the sensations from doing new things such as dirt-bike racing or flying, that they otherwise lack the skills, resources, or social permission to do. A few like to escape the real world; others enjoy escaping its social norms. Nearly all enjoy the feeling of challenge and complete absorption. The exciting and relaxing effects of games is very appealing and some apply its therapeutic benefits to “get perspective,” calm down after a hard day, or build self-esteem.  Direct observation reveals details about player emotion. We find emotion in player’s visceral, behavioral, cognitive, and social responses to games. Players play to experience these body sensations that result from and drive their actions. Some crave the increased heart rate of excitement from a race, the skin prickling sensation from Wonder, or the tension of Frustration followed by feelings of Fiero. For others it is simply the exchange of worries and thought and feelings for relaxation and contentment or a feeling of achievement knowing they did it right."
905,"evolving genetranscript definitions significantly alter the interpretation of genechip data",436785,"Evolving gene/transcript definitions significantly alter the interpretation of GeneChip data","Genome-wide expression profiling is a powerful tool for implicating novel gene ensembles in cellular mechanisms of health and disease. The most popular platform for genome-wide expression profiling is the Affymetrix GeneChip. However, its selection of probes relied on earlier genome and transcriptome annotation which is significantly different from current knowledge. The resultant informatics problems have a profound impact on analysis and interpretation the data. Here, we address these critical issues and offer a solution. We identified several classes of problems at the individual probe level in the existing annotation, under the assumption that current genome and transcriptome databases are more accurate than those used for GeneChip design. We then reorganized probes on more than a dozen popular GeneChips into gene-, transcript- and exon-specific probe sets in light of up-to-date genome, cDNA/EST clustering and single nucleotide polymorphism information. Comparing analysis results between the original and the redefined probe sets reveals [~]30-50% discrepancy in the genes previously identified as differentially expressed, regardless of analysis method. Our results demonstrate that the original Affymetrix probe set definitions are inaccurate, and many conclusions derived from past GeneChip analyses may be significantly flawed. It will be beneficial to re-analyze existing GeneChip data with updated probe set definitions. 10.1093/nar/gni179"
906,"the hive mind folksonomies and userbased tagging",437260,"The Hive Mind: Folksonomies and User-Based Tagging","There is a revolution happening on the Internet that is alive and building momentum with each passing tag. With the advent of social software and Web 2.0, we usher in a new era of Internet order. One in which the user has the power to effect their own online experience, and contribute to others’. Today, users are adding metadata and using tags to organize their own digital collections, categorize the content of others and build bottom-up classification systems. The wisdom of crowds, the hive mind, and the collective intelligence are doing what heretofore only expert catalogers, information architects and website authors have done. They are categorizing and organizing the Internet and determining the user experience, and it’s working. No longer do the experts have the monopoly on this domain; in this new age users have been empowered to determine their own cataloging needs. Metadata is now in the realm of the Everyman."
907,"rnasoft a suite of rna secondary structure prediction and design software tools",437770,"RNAsoft: A suite of RNA secondary structure prediction and design software tools.","DNA and RNA strands are employed in novel ways in the construction of nanostructures, as molecular tags in libraries of polymers and in therapeutics. New software tools for prediction and design of molecular structure will be needed in these applications. The RNAsoft suite of programs provides tools for predicting the secondary structure of a pair of DNA or RNA molecules, testing that combinatorial tag sets of DNA and RNA molecules have no unwanted secondary structure and designing RNA strands that fold to a given input secondary structure. The tools are based on standard thermodynamic models of RNA secondary structure formation. RNAsoft can be found online at http://www.RNAsoft.ca."
908,"modular setbased analysis from contracts",439126,"Modular Set-Based Analysis from Contracts","In PLT Scheme, programs consist of modules with contracts. The latter describe the inputs and outputs of functions and objects via predicates. A run-time system enforces these predicates; if a predicate fails, the enforcer raises an exception that blames a specific module with an explanation of the fault.In this paper, we show how to use such module contracts to turn set-based analysis into a fully modular parameterized analysis. Using this analysis, a static debugger can indicate for any given contract check whether the corresponding predicate is always satisfied, partially satisfied, or (potentially) completely violated. The static debugger can also predict the source of potential errors, i.e., it is sound with respect to the blame assignment of the contract system."
909,"gromacs fast flexible and free",439131,"GROMACS: Fast, flexible, and free","Abstract 10.1002/jcc.20291.abs This article describes the software suite GROMACS (Groningen MAchine for Chemical Simulation) that was developed at the University of Groningen, The Netherlands, in the early 1990s. The software, written in ANSI C, originates from a parallel hardware project, and is well suited for parallelization on processor clusters. By careful optimization of neighbor searching and of inner loop performance, GROMACS is a very fast program for molecular dynamics simulation. It does not have a force field of its own, but is compatible with GROMOS, OPLS, AMBER, and ENCAD force fields. In addition, it can handle polarizable shell models and flexible constraints. The program is versatile, as force routines can be added by the user, tabulated functions can be specified, and analyses can be easily customized. Nonequilibrium dynamics and free energy determinations are incorporated. Interfaces with popular quantum-chemical packages (MOPAC, GAMES-UK, GAUSSIAN) are provided to perform mixed MM/QM simulations. The package includes about 100 utility and analysis programs. GROMACS is in the public domain and distributed (with source code and documentation) under the GNU General Public License. It is maintained by a group of developers from the Universities of Groningen, Uppsala, and Stockholm, and the Max Planck Institute for Polymer Research in Mainz. Its Web site is http://www.gromacs.org. © 2005 Wiley Periodicals, Inc. J Comput Chem 26: 1701–1718, 2005"
910,"matching as nonparametric preprocessing for reducing model dependence in parametric causal inference",440893,"Matching as Nonparametric Preprocessing for Reducing Model Dependence in Parametric Causal Inference","Although published works rarely include causal estimates from more than a few model specifications, authors usually choose the presented estimates from numerous trial runs readers never see. Given the often large variation in estimates across choices of control variables, functional forms, and other modeling assumptions, how can researchers ensure that the few estimates presented are accurate or representative? How do readers know that publications are not merely demonstrations that it is possible to find a specification that fits the author's favorite hypothesis? And how do we evaluate or even define statistical properties like unbiasedness or mean squared error when no unique model or estimator even exists? Matching methods, which offer the promise of causal inference with fewer assumptions, constitute one possible way forward, but crucial results in this fast-growing methodological literature are often grossly misinterpreted. We explain how to avoid these misinterpretations and propose a unified approach that makes it possible for researchers to preprocess data with matching (such as with the easy-to-use software we offer) and then to apply the best parametric techniques they would have used anyway. This procedure makes parametric models produce more accurate and considerably less model-dependent causal inferences. 10.1093/pan/mpl013"
911,"design and evaluation of a metropolitan area multitier wireless ad hoc network architecture",441049,"Design and evaluation of a metropolitan area multitier wireless ad hoc network architecture","Few real-world applications of mobile ad hoc networks have  been developed or deployed outside the military environment, and  no traces of actual node movement in a real ad hoc network have  been available. In this paper, we propose a novel commercial application  of ad hoc networking, we describe and evaluate a multitier  ad hoc network architecture and routing protocol for this  system, and we document a new source of real mobility traces to  support detailed simulation of ad hoc network applications on a  large scale. The proposed system, which we call Ad Hoc City, is a  multitier wireless ad hoc network routing architecture for general-purpose  wide-area communication. The backbone network in this  architecture is itself also a mobile multihop network, composed of  wireless devices mounted on mobile fleets such as city buses or  delivery vehicles. We evaluate our proposed design through simulation  based on traces of the actual movement of the fleet of city  buses in the Seattle, Washington metropolitan area, on their normal  routes providing passenger bus service throughout the city."
912,"intel virtualization technology",445554,"Intel Virtualization Technology","Once confined to specialized server and mainframe systems, virtualization is now supported in off-the-shelf systems based on Intel architecture hardware. Intel Virtualization Technology provides hardware support for processor virtualization, enabling simplifications of virtual machine monitor software. Resulting VMMs can support a wider range of legacy and future operating systems while maintaining high performance."
913,"what does the photoblog want",446839,"What does the photoblog want?","Theoretical accounts of photography have persistently emphasized, departed from and returned to the issue of the Real, thereby positioning the Real behind or at the   heart of what photography purportedly is and does. But these familiar and familiarizing consistencies in the writing about photography do not make photographs   less of a paradox at the level of being (what they are), or less equivocal at the level of their expressive content (what they mean or know). Digital photography  problematizes the issues yet further even while writing about photography reasserts the familiar pieties. This article presents the results of an ethnographic study of photoblogs as a way of addresssing impasses in the literature on photography and   digital photography. Blogs have become popular in the last three years as an internet-based technology for writing the self. Photoblogs are a type of blog that   adds photographs to text and hyperlinks in the telling of stories. In this article, I argue that photoblogs are (1) entities that identify the repetitions  which paralyse writing about photography and (2) entities that want to position photographs as something more than an outcome, photobloggers as something  more than selves (or authors) and the photoblog as something more than technology. 10.1177/0163443705057675"
914,"fixed point calculus",451503,"Fixed Point Calculus","Fixed point calculus is about the solution of recursive equations defined by a monotonic endofunction on a partially ordered set. This tutorial discusses applications of fixed point calculus in the construction of computer programs, beginning with standard applications and progressing to recent research. The basic properties of least and greatest fixed points are presented. Well-foundedness and inductive properties of relations are expressed in terms of fixed points. A class of fixed point equations, called ``hylo'' equations, is introduced. A methodology of recursive program design based on the use of hylo equations is presented. Current research on generalisations of well-foundedness and inductive properties of relations, making these properties relative to a datatype, is introduced."
915,"monte carlo strategies in scientific computing",454305,"Monte {C}arlo {S}trategies in {S}cientific {C}omputing","{A large number of scientists and engineers employ Monte Carlo simulation and related global optimization techniques (such as simulated annealing) as an essential tool in their work. For such scientists, there is a need to keep up to date with several recent advances in Monte Carlo methodologies such as cluster methods, data- augmentation, simulated tempering and other auxiliary variable methods. There is also a trend in moving towards a population-based approach. All these advances in one way or another were motivated by the need to sample from very complex distribution for which traditional methods would tend to be trapped in local energy minima. It is our aim to provide a self-contained and up to date treatment of the Monte Carlo method to this audience.                                                                              The Monte Carlo method is a computer-based statistical sampling approach for solving numerical problems    concerned with a complex system. The methodology was initially developed in the field of statistical physics during the early days of electronic computing (1945-55) and has now been adopted by researchers in  almost all scientific fields. The fundamental idea for constructing Markov chain based Monte Carlo algorithms was introduced in the 1950s. This idea was later extended to handle more and more complex     physical systems. In the 1980s, statisticians and computer scientists developed Monter Carlo-based algorithms for a wide variety of integration and optimization tasks. In the 1990s, the method began to play an important role in computational biology. Over the past fifty years, reasearchers in diverse scientific fields have studied the Monte Carlo method and contributed to its development. Today, a large number of  scientisits and engineers employ Monte Carlo techniques as an essential tool in their work. For such    scientists, there is a need to keep up-to-date with recent advances in Monte Carlo methodologies.}"
916,"automated de novo identification of repeat sequence families in sequenced genomes",454555,"Automated De Novo Identification of Repeat Sequence Families in Sequenced Genomes","10.1101/gr.88502 Repetitive sequences make up a major part of eukaryotic genomes. We have developed an approach for the de novo identification and classification of repeat sequence families that is based on extensions to the usual approach of single linkage clustering of local pairwise alignments between genomic sequences. Our extensions use multiple alignment information to define the boundaries of individual copies of the repeats and to distinguish homologous but distinct repeat element families. When tested on the human genome, our approach was able to properly identify and group known transposable elements. The program,, should be useful for first-pass automatic classification of repeats in newly sequenced genomes. [The following individuals kindly provided reagents, samples, or unpublished information as indicated in the paper: R. Klein.]"
917,"treefam a curated database of phylogenetic trees of animal gene families",456530,"TreeFam: a curated database of phylogenetic trees of animal gene families.","TreeFam is a database of phylogenetic trees of gene families found in animals. It aims to develop a curated resource that presents the accurate evolutionary history of all animal gene families, as well as reliable ortholog and paralog assignments. Curated families are being added progressively, based on seed alignments and trees in a similar fashion to Pfam. Release 1.1 of TreeFam contains curated trees for 690 families 25 and automatically generated trees for another 11 646 families. These represent over 128 000 genes from nine fully sequenced animal genomes and over 45 000 other animal proteins from UniProt; similar to 40 - 85% of proteins encoded in the fully sequenced animal genomes are included in TreeFam. TreeFam is freely available at http://www.treefam.org and http://treefam.genomics.org.cn."
918,"snpdetector a software tool for sensitive and accurate snp detection",456945,"SNPdetector: A Software Tool for Sensitive and Accurate SNP Detection","Identification of single nucleotide polymorphisms (SNPs) and mutations is important for the discovery of genetic predisposition to complex diseases. PCR resequencing is the method of choice for de novo SNP discovery. However, manual curation of putative SNPs has been a major bottleneck in the application of this method to high-throughput screening. Therefore it is critical to develop a more sensitive and accurate computational method for automated SNP detection. We developed a software tool, SNPdetector, for automated identification of SNPs and mutations in fluorescence-based resequencing reads. SNPdetector was designed to model the process of human visual inspection and has a very low false positive and false negative rate. We demonstrate the superior performance of SNPdetector in SNP and mutation analysis by comparing its results with those derived by human inspection, PolyPhred (a popular SNP detection tool), and independent genotype assays in three large-scale investigations. The first study identified and validated inter- and intra-subspecies variations in 4,650 traces of 25 inbred mouse strains that belong to either the Mus musculus species or the M. spretus species. Unexpected heterozgyosity in CAST/Ei strain was observed in two out of 1,167 mouse SNPs. The second study identified 11,241 candidate SNPs in five ENCODE regions of the human genome covering 2.5 Mb of genomic sequence. Approximately 50&#37; of the candidate SNPs were selected for experimental genotyping; the validation rate exceeded 95&#37;. The third study detected ENU-induced mutations (at 0.04&#37; allele frequency) in 64,896 traces of 1,236 zebra fish. Our analysis of three large and diverse test datasets demonstrated that SNPdetector is an effective tool for genome-scale research and for large-sample clinical studies. SNPdetector runs on Unix/Linux platform and is available publicly (http://lpg.nci.nih.gov)."
919,"coauthorship networks and patterns of scientific collaboration",457047,"Coauthorship networks and patterns of scientific collaboration","10.1073/pnas.0307545100 By using data from three bibliographic databases in biology, physics, and mathematics, respectively, networks are constructed in which the nodes are scientists, and two scientists are connected if they have coauthored a paper. We use these networks to answer a broad variety of questions about collaboration patterns, such as the numbers of papers authors write, how many people they write them with, what the typical distance between scientists is through the network, and how patterns of collaboration vary between subjects and over time. We also summarize a number of recent results by other authors on coauthorship patterns."
920,"an introduction to pounddreverhall laser frequency stabilization",457991,"An introduction to Pound--Drever--Hall laser frequency stabilization","This paper is an introduction to an elegant and powerful technique in modern optics: Pound&#150;Drever&#150;Hall laser frequency stabilization. This introduction is primarily meant to be conceptual, but it includes enough quantitative detail to allow the reader to immediately design a real setup, suitable for research or industrial application. The intended audience is both the researcher learning the technique for the first time and the teacher who wants to cover modern laser locking in an upper-level physics or electrical engineering course. &copy;2001 American Association of Physics Teachers."
921,"against simulation the argument from error",459612,"Against simulation: the argument from error.","According to Simulation Theory, to understand what is going on in another person's mind, the observer uses his or her own mind as a model of the other mind. Recently, philosophers and cognitive neuroscientists have proposed that mirror neurones (which fire in response to both executing and observing a goal directed action) provide a plausible neural substrate for simulation, a mechanism for directly perceiving, or 'resonating' with, the contents of other minds. This article makes the case against Simulation Theory, using evidence from cognitive neuroscience, developmental psychology, and social psychology. In particular, the errors that adults and children make when reasoning about other minds are not consistent with the 'resonance' versions of Simulation Theory."
922,"designing argumentation for conceptual development",460108,"Designing argumentation for conceptual development","If Virtual Learning Environments are to support real learning, they must promote effective teaching-learning processes and interactions. In this paper we describe a collaborative, computer-based framework for argumentation that supports the dialogue process in ways which stimulate belief revision leading to conceptual change and development in science. This pedagogy is specified as a prescriptive &lsquo;dialogue game&rsquo;, which models features of the tutorial process. Within this scheme, the learner adopts the role of an &lsquo;explainer&rsquo; whilst the system plays a facilitating role, and these participants collaborate to develop a shared explanatory model of a qualitative, causal domain. The design framework includes an abstract world model of a qualitative causal system, some &lsquo;commonsense&rsquo; reasoning rules, an interaction language and dialogue strategies and tactics, that are co-ordinated within a facilitating dialogue game. A prototype CoLLeGE (Computer based Lab for Language Games in Education) system implements the framework and operates as a dialogue modelling work-bench for demonstrating, investigating and developing the approach. An empirical study showed that students revised their beliefs and improved their explanatory models, and held to their revised and improved conceptions in a delayed post-test. In using CoLLeGE to simulate these dialogues, we found that the tutor&rsquo;s low-level tactical pedagogies emerged and developed reactively during the dialogues, in response to conceptual difficulties experienced by the students."
923,"ascertainment bias in studies of human genomewide polymorphism",461067,"Ascertainment bias in studies of human genome-wide polymorphism","10.1101/gr.4107905 Large-scale SNP genotyping studies rely on an initial assessment of nucleotide variation to identify sites in the DNA sequence that harbor variation among individuals. This âSNP discoveryâ sample may be quite variable in size and composition, and it has been well established that properties of the SNPs that are found are influenced by the discovery sampling effort. The International HapMap project relied on nearly any piece of information available to identify SNPsâincluding BAC end sequences, shotgun reads, and differences between public and private sequencesâand even made use of chimpanzee data to confirm human sequence differences. In addition, the ascertainment criteria shifted from using only SNPs that had been validated in population samples, to double-hit SNPs, to finally accepting SNPs that were singletons in small discovery samples. In contrast, Perlegen's primary discovery was a resequencing-by-hybridization effort using the 24 people of diverse origin in the Polymorphism Discovery Resource. Here we take these two data sets and contrast two basic summary statistics, heterozygosity and , as well as the site frequency spectra, for 500-kb windows spanning the genome. The magnitude of disparity between these samples in these measures of variability indicates that population genetic analysis on the raw genotype data is ill advised. Given the knowledge of the discovery samples, we perform an ascertainment correction and show how the post-correction data are more consistent across these studies. However, discrepancies persist, suggesting that the heterogeneity in the SNP discovery process of the HapMap project resulted in a data set resistant to complete ascertainment correction. Ascertainment bias will likely erode the power of tests of association between SNPs and complex disorders, but the effect will likely be small, and perhaps more importantly, it is unlikely that the bias will introduce false-positive inferences."
924,"a theory of universal artificial intelligence based on algorithmic complexity",462159,"A Theory of Universal Artificial Intelligence based on Algorithmic Complexity","Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameterless theory of universal Artificial Intelligence. We give strong arguments that the resulting AIXI model is the most intelligent unbiased agent possible. We outline for a number of problem classes, including sequence prediction, strategic games, function minimization, reinforcement and supervised learning, how the AIXI model can formally solve them. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXI-tl, which is still effectively more intelligent than any other time t and space l bounded agent. The computation time of AIXI-tl is of the order tx2^l. Other discussed topics are formal definitions of intelligence order relations, the horizon problem and relations of the AIXI theory to other AI approaches."
925,"handbook of mri pulse sequences",463013,"Handbook of MRI Pulse Sequences","{This indispensable guide gives concise yet comprehensive descriptions of the pulse sequences commonly used on modern MRI scanners. The book consists of a total of 65 self-contained sections, each focused on a single subject. Written primarily for scientists, engineers, radiologists, and graduate students who are interested in an in-depth understanding of various MRI pulse sequences, it serves readers with a diverse set of backgrounds by providing both non-mathematical and mathematical descriptions. <br><br>The book is divided into five parts. Part I of the book describes two mathematical tools, Fourier transforms and the rotating reference frame, that are useful for understanding MRI pulse sequences. The second part is devoted to a wide variety of radiofrequency (RF) pulses, and the third part focuses on gradient waveforms. Data acquisition, image reconstruction, and physiological monitoring related to pulse sequence design form the subject of Part IV of the book. Once this foundation is established, Part V of the book describes the underlying principles, implementation, and selected applications of many pulse sequences commonly in use today. <br><br>The extensive topic coverage and cross-referencing makes this book ideal for beginners learning the building blocks of MRI pulse sequence design, as well as for experienced professionals who are seeking deeper knowledge of a particular technique.<br><br>&#183;Explains pulse sequences, their components, and the associated image reconstruction methods commonly used in MRI<br>&#183;Provides self-contained sections for individual techniques<br>&#183;Can be used as a quick reference guide or as a resource for deeper study<br>&#183;Includes both non-mathematical and mathematical descriptions <br>&#183;Contains numerous figures, tables, references, and worked example problems}"
926,"aramis or the love of technology",463317,"Aramis, or the Love of Technology","{Packet switching works well for moving data -- why not use it for moving humans? In a nutshell, the French Aramis transit project proposed packet switching as a solution to human transport problems (though, so far as I can tell, neither the author nor any reviews I have yet read have made this connection). <P> With all the brouhaha about moving bytes around on the information superhighways, moving people around real cities has become less glamorous -- after all, the current mythology is that telecommuting will render the automobile obsolete, right?  With the prevailing American tendency to think in terms of technological manifest destiny, stories about superior technologies failing miserably are usually glossed over in an obsession with teleology (history is an inevitable march toward greater perfection). <P> In contrast, this book describes an extraordinarily well-designed and highly superior semi-personal robotic transit system developed by the French government -- and then squashed by the French government. It is written in a style that only a Gallic scientist could conceive (for example, in a passage about project complexity, Latour writes: <I>...The monkey is readily identified as a creature of desire...</I>). Because of such stylistic excrescences, I personally I found this book somewhat difficult to read at times, but I recommend it very highly to anyone interested in the history of technology, cross-cultural studies, telecommunications -- or the burgeoning application of packet switching principles to mass transit.} {Bruno Latour has written a unique and wonderful tale of a technological dream gone wrong. As the young engineer and professor follow Aramis' trail--conducting interviews, analyzing documents, assessing the evidence--perspectives     keep shifting: the truth is revealed as multilayered, unascertainable, comprising an array of possibilities worthy of <i>Rashomon</i>. The reader is eventually led to see the project from the point of view of     Aramis, and along the way gains insight into the relationship between human beings and their technological creations. This charming and profound book, part novel and part sociological study, is Bruno Latour at his thought-provoking     best.}"
927,"the endocrine regulation of aging by insulinlike signals",463786,"The endocrine regulation of aging by insulin-like signals.","Reduced signaling of insulin-like peptides increases the life-span of nematodes, flies, and rodents. In the nematode and the fly, secondary hormones downstream of insulin-like signaling appear to regulate aging. In mammals, the order in which the hormones act is unresolved because insulin, insulin-like growth factor-1, growth hormone, and thyroid hormones are interdependent. In all species examined to date, endocrine manipulations can slow aging without concurrent costs in reproduction, but with inevitable increases in stress resistance. Despite the similarities among mammals and invertebrates in insulin-like peptides and their signal cascade, more research is needed to determine whether these signals control aging in the same way in all the species by the same mechanism."
928,"synaptic protein synthesis associated with memory is regulated by the risc pathway in drosophila",464432,"Synaptic Protein Synthesis Associated with Memory Is Regulated by the RISC Pathway in Drosophila"," SummaryLong-lasting forms of memory require protein synthesis, but how the pattern of synthesis is related to the storage of a memory has not been determined. Here we show that neural activity directs the mRNA of the Drosophila Ca2+, Calcium/Calmodulin-dependent Kinase II (CaMKII), to postsynaptic sites, where it is rapidly translated. These features of CaMKII synthesis are recapitulated during the induction of a long-term memory and produce patterns of local protein synthesis specific to the memory. We show that mRNA transport and synaptic protein synthesis are regulated by components of the RISC pathway, including the SDE3 helicase Armitage, which is specifically required for long-lasting memory. Armitage is localized to synapses and lost in a memory-specific pattern that is inversely related to the pattern of synaptic protein synthesis. Therefore, we propose that degradative control of the RISC pathway underlies the pattern of synaptic protein synthesis associated with a stable memory. IntroductionIt has long been known that the establishment of long-lasting forms of memory requires protein synthesis, a feature of memory common to vertebrates and invertebrates (Bailey et al., 2004 and Kelleher et al., 2004). A number of cellular changes accompany protein synthesis; these changes include the modification of synapse and circuit function and, in net effect, behavior. But it is not yet known how protein synthesis is deployed across the nervous system and how it contributes to the formation of a particular memory. Of special interest is protein synthesis localized to the synapse, as this might confer selective synaptic change and the stable modification of a circuit. While regulators of such synthesis are known (reviewed by Richter and Lorenz, 2002), the molecular events underlying synaptic protein synthesis during the establishment of a memory remain unresolved.A well-defined system for the study of memory is the olfactory/electric shock paradigm of Drosophila (Tully and Quinn, 1985). A memory of odor associated with electric shock can be induced in phases that include short-term (STM) and long-term (LTM), phases that are distinguished by their dependence on training protocol, genetic pathway, and protein synthesis. A requirement for protein synthesis in olfactory LTM was demonstrated long ago (Tully et al., 1994) and reinforced by the identification of longterm memory mutants as genes with functions in mRNA transport and translation (Dubnau et al., 2003). This system presents an opportunity to determine how and where protein synthesis is deployed and what mechanisms regulate it during the formation of a memory.To approach these questions, we devised fluorescent reporters of synaptic protein synthesis using sequences of the Drosophila Calcium/Calmodulin-dependent Kinase II (CaMKII), the homolog of the mammalian αCaMKII, which is synthesized at synapses and required for memory (reviewed in Kelleher et al., 2004). We show that the induction of a long-term memory in Drosophila is accompanied by transport of mRNA to synapses and patterns of synaptic protein synthesis that have features of memory specificity. These events are regulated by components of the RNA interference (RISC) pathway. At least one RISC factor, Armitage (Cook et al., 2004), is localized to synapses and degraded by the proteasome in response to neural activity or the induction of an LTM. Therefore, degradative control of RISC underlies the pattern of synaptic protein synthesis associated with the establishment of a stable memory. Results mRNA Determinants of CaMKII Synaptic LocalizationSince the mammalian αCaMKII is found at synapses, where its synthesis is regulated by neural activity, our attention turned to the CaMKII gene of Drosophila. Drosophila CaMKII has a role in neuromuscular synaptic plasticity and memory in the courtship-conditioning paradigm (Griffith et al., 1993 and Koh et al., 1999). CaMKII is localized to both pre- and postsynaptic sites in the adult brain (Figures 1B–1D and 1J). We focused on the olfactory system because of its well-described neural components, circuitry, and paradigms for the establishment of memory (Davis, 2004). This system consists of sensory neurons and interneurons that form an early receptive and processing circuit with synapses organized in bilaterally symmetric centers known as the antennal lobes (Figures 1A and 1B). The first-order interneurons (Projection Neurons; PNs) collect sensory input in a stereotyped array of multisynaptic structures known as glomeruli (Figure 1B), where the PN dendritic synapses collect cholinergic input via nicotinic acetylcholine receptors (nAChRs; Figure 1K). The PNs direct-output to two brain centers via branching axons that project to the “calyx” of the mushroom body and to the lateral horn. These terminals release acetylcholine from choline acyltransferase (ChAT)-positive boutons (Figures 1I and 1J). The PN dendrites also form reciprocal synapses with local interneurons (Ng et al., 2002 and Wilson et al., 2004). On the PN dendrites, CaMKII was localized in postsynaptic puncta with the markers Discs Large (DLG) and ARD (a nAChR β-subunit; Figures 1C and 1K). CaMKII was also concentrated at the PN presynaptic boutons in the calyx and lateral horn (Figure 1J). Thus, within the same neuron, CaMKII is concentrated at both pre- and postsynaptic sites. The mouse αCaMKII mRNA displays dendritic localization and activity-dependent synaptic translation, features conferred by sequences in its 3′UTR (Rook et al., 2000 and Richter and Lorenz, 2002). To determine whether this is the case for Drosophila CaMKII, the 3′UTR was inserted downstream of the EYFP coding sequence in the reporter, UAS-EYFP3′UTR (Figure 1F). An additional pair of constructs was made bearing a translational fusion of EYFP to CaMKII, with the 3′UTRCaMKII present (UAS-CaMKII::EYFP3′UTR; Figure 1H) or absent (UAS-CaMKII::EYFPNUT; Figure 1G).When expressed specifically in PNs using the GAL4, UAS binary system (Brand and Perrimon, 1993), EYFP3′UTR fluorescence was strikingly localized to synapses on the PN dendritic and axonal termini (Figure 1F) and colocalized with ChAT at the presynaptic boutons (Figure 1M) and with the nAChR subunit ARD on dendritic branches in glomeruli (Figure 1K). This distribution roughly matched that of CaMKII protein (Figures 1J and 1K), though EYFP was somewhat more diffuse along the dendritic branches; presumably EYFP does not bind to the postsynaptic apparatus as CaMKII does (Koh et al., 1999). In contrast, a cytoplasmic EYFP reporter lacking CaMKII sequences was distributed poorly to the axons and dendrites (Figure 1E).The CaMKII::EYFP fusion protein synthesized from mRNA harboring the 3′UTR (CaMKII::EYFP3′UTR) displayed synaptic localization in axons and dendrites like that of EYFP3′UTR (Figure 1H) but was notably more concentrated in synaptic puncta (data not shown). The same fusion protein made from mRNA lacking the 3′UTR (CaMKII::EYFPNUT) was strongly localized to axonal presynaptic sites (Figures 1G' and 1N) but found at a very low level on the antennal lobe dendrites (Figure 1G), where it was localized to synaptic puncta (Figure 1L). Thus the CaMKII 3′UTR is necessary and sufficient for the robust localization of CaMKII to dendritic arbors but not required for axonal localization. Activity-Dependent Synaptic Protein SynthesisTo determine how neural activity might affect CaMKII expression, brains were explanted into bath culture with acetylcholine (ACh) or nicotine (an agonist of nAChRs). After 20 min, the tissue was examined by anti-CaMKII immunohistochemistry and quantitative confocal microscopy (Figure 2A). On average (n = 10), when cholinergic synapses were activated, CaMKII immunofluorescence in the antennal lobe increased by 3- to 4-fold (Figures 2A' and 2O; nicotine, p = 0.0014; ACh, p = 0.007). In a time-course experiment, an increase in CaMKII level was detected within 5–10 min of nicotine exposure (data not shown). The increase was widespread in the brain and reflected in a 4-fold increase of CaMKII protein on Western blot analysis (Figure 2D). The CaMKII increase was also specific, as the levels of synaptic proteins DLG and ARD were unchanged (Figures 2B, 2C, and 2O). Consistent with the notion that this regulation occurs via translational control, the effect of cholinergic stimulation was blocked by the ribosomal inhibitor Anisomycin but not by Actinomycin D, an inhibitor of transcription (Figures 2A, 2D, and 2O). These results and the requirement of the CaMKII 3′UTR for dendritic localization suggest that cholinergic activity may induce the translation of CaMKII mRNA at postsynaptic sites. This was examined by monitoring EYFP3′UTR reporter expression in explant culture (Figures 2E and 2F). A 5 min nicotine incubation increased EYFP3′UTR expression by 30% and, after 20 min, by 250% (Figures 2F and 2P). The induced EYFP protein was found in large punctae (Figures 2K–2N). Cholinergic stimulation did not increase EYFP3′UTR expression at the presynaptic terminals in the calyx (data not shown). In contrast, CaMKII::EYFPNUT expression was only slightly increased by nicotine or ACh exposure (Figures 2G, 2H, and 2P). Nicotine incubation did not alter the expression of cytoplasmic EYFP, CD8::GFP, or an EGFP construct harboring an α1-tubulin 3′UTR (Hh::EGFP-3′UTRtub; Figures 2I, 2J, and 2P). These observations indicate that the localization and rapid induction of CaMKII in dendrites is due to 3′UTR-dependent synaptic protein synthesis. Pattern of Synaptic Protein Synthesis Associated with a Long-Term MemoryIn Drosophila, an olfactory LTM is induced by “spaced training,” a protocol where an odor (CS+) and electric shock (US) are presented coincidentally at temporally spaced intervals. A second odor (CS−) follows the CS+ odor in each interval without coincident shock. An LTM appears after several hours and lasts beyond 24 hr, as assayed by tactic behavior in a T-maze (Tully and Quinn, 1985). We followed this protocol and used EYFP3′UTR to report synaptic protein synthesis in animals that developed an olfactory LTM. Our analysis focused on the antennal lobe glomeruli because these structures can be reproducibly identified and display clustered synaptic activity (Ng et al., 2002 and Wang et al., 2003). Furthermore, the first-order antennal lobe synapses might participate in an early stage of memory storage (Yu et al., 2004), including the storage of LTM (Muller, 2000). Our analysis (Figure 3) revealed an odorant-specific pattern of synaptic protein synthesis associated with the induction of a long-term memory. Animals harboring the UAS-EYFP3′UTR reporter driven by the PN-specific GH146-GAL4 were trained and analyzed at times from 4 to 24 hr posttraining. The brains of trained and untrained animals were processed for microscopy in parallel. For each glomerulus, a Z stack of 6–8 confocal microscopic images was recorded and analyzed via a thresholding protocol that isolated pixel groups corresponding to synaptic puncta (see Supplemental Experimental Procedures in the Supplemental Data available with this article online). An average glomerulus intensity change (ΔF/F) was calculated for 5–8 brains in each experiment. Each experiment was repeated five times. LTM was in all cases verified by T-maze performance.The analysis was restricted to a set of glomeruli that included those with a primary response to the odorants octanol (OCT) and methylcyclohexanol (MCH). Only particular glomeruli displayed a training-dependent increase in EYFP3′UTR fluorescence, while others did not; their identities depended on the odorant (CS+) paired with shock. When OCT was the CS+, only glomeruli D and DL3 displayed increased fluorescence, by 115% and 108%, respectively (Figure 3). When MCH was the CS+, fluorescence increased significantly in glomeruli DA1 and VA1 by 95% and 70%, respectively (ANOVA with bonferroni correction, p < 0.05). There were modest but possibly insignificant increases in glomeruli DM6 and VC2. The glomerulus-specific increases were noted as early as 4 hr posttraining and were not observed when odorant and/or electric shock was unpaired or left out or when temporal spacing was not employed (“massed training”). In animals that expressed a cytoplasmic EYFP reporter or CaMKII::EYFPNUT (Figure 3; data not shown), which lack the CaMKII 3′UTR, there were no significant fluorescence changes. This analysis revealed that an odor-specific induction of synaptic protein synthesis occurred when conditioned and unconditioned stimuli were presented coincidentally and with temporal spacing, the experience that establishes an LTM. This plasticity was evidently maintained for at least 24 hr. Activity-Dependent Dendritic mRNA Transport and LocalizationIf CaMKII was synthesized at the synapse, its mRNA would be localized there. To address this question, we utilized an mRNA tracking system based on the bacteriophage coat protein MS2 and its RNA binding site (Rook et al., 2000). The fusion protein MS2::GFP::nls is concentrated in the nucleus by nuclear localization signals (nls) but can be diverted elsewhere by binding to an MS2 binding site (MS2-bs) tagged mRNA. We tagged three mRNAs: the Drosophila CaMKII cDNA, its 3′UTR alone, and the mouse αCaMKII 3′UTR, which mediates dendritic localization and synaptic translation in the mouse (Aakalu et al., 2001 and Rook et al., 2000).GFP fluorescence was examined when MS2::GFP::nls was expressed in projection neurons (PNs) with or without an MS2-bs tagged mRNA. Punctate fluorescence was observed in the dendrites when an MS2-bs-tagged mRNA was coexpressed with MS2::GFP::nls but not with MS2::GFP::nls alone (Figures 4A and 4B). For example, the tagged Drosophila CaMKII mRNA increased the intensity of glomerular fluorescence by 200% (Figure 4I; p < 0.001). In dendrites, particular mRNAs, including the mouse αCaMKII, are localized to particles containing the motor protein Kinesin (Kanai et al., 2004). Consistent with this observation, the GFP-positive dendritic puncta were labeled with an antibody against the major kinesin heavy chain, KHC (Figure 4H; Pearson's coefficient = 0.75; Brendza et al., 2002). We wondered whether the synaptic CaMKII expression induced by cholinergic activity (Figure 2A) might be associated with enhanced dendritic localization of CaMKII mRNA, as was found for the mouse αCaMKII and Arc mRNAs (Rook et al., 2000 and Steward et al., 1998). When explanted into media with nicotine or ACh, brains harboring MS2::GFP::nls and the tagged Drosophila CaMKII mRNA displayed a striking increase in dendritic GFP fluorescence (Figures 4B' and 4I; p < 0.001). The effect with cholinergic stimulation was similar with the tagged mouse αCaMKII 3′UTR: a 70%–73% increase relative to culture without nicotine (Figures 4G and 4I). The activity-enhanced dendritic mRNA transport was blocked by Anisomycin (Figure 4D) but not by Actinomycin D (Figure 4C). We suppose that mainly existing mRNA can be translocated during the short period of culture. Thus, in Drosophila, like mammals (Krichevsky and Kosik, 2001), neural activity increases the rate of mRNA movement to the synapse by a protein synthesis-dependent mechanism.We then asked whether the induction of an LTM might affect mRNA transport to the synapse. When animals expressing MS2::GFP::nls and Drosophila ms2bs-CaMKII were subjected to the spaced training protocol, the number of GFP-labeled puncta in dendrites was substantially increased (80% relative to untrained; n = 10, p < 0.001; Figures 4A, 4F, and 4I). The pattern of dendritic punctae did not display evident glomerular specificity, as observed for synaptic protein synthesis (Figure 3). However, the induced punctae were distributed along the dendritic branches, making a determination of glomerular specificity uncertain. These observations reveal that a coordinated program for synaptic gene expression occurs during the storage of a memory. Regulation of mRNA Transport and Synaptic Protein Synthesis by the RISC PathwayThe RNA interference (RISC) pathway silences gene expression by the targeted degradation of mRNAs or their nondestructive silencing (Sontheimer, 2005). In Drosophila, RISC-mediated translational silencing controls oskar expression in the developing oocyte. An SDE3-class RNA helicase, Armitage (Armi; Cook et al., 2004) acts as part of RISC (Tomari et al., 2004) to control oskar translation and regulate cytoskeletal organization, possibly via control of Kinesin heavy chain (Khc) translation. Both the oskar and Khc 3′UTRs have putative binding sites for the microRNA (miRNA) miR-280. The CaMKII 3′UTR has a remarkably similar miR-280 binding site (Figure 5A). This site and a nearby site for miR-289 satisfy the predictive rule that 7 of 8 nucleotides at the 5′ end of an miRNA are cognate to a target mRNA (Lai et al., 2003 and Stark et al., 2003). Kinesin is also a component of the RNA-containing dendritic particles that bring mRNA to the synapse (Kanai et al., 2004; Figure 4H). Staufen, likewise a mediator of RNA transport (Ferrandon et al., 1994 and Tang et al., 2001), has binding sites for miR-280 and miR-305 in its mRNA 3′UTR (Rajewsky and Socci, 2004). We thus explored the role of RISC in CaMKII, KHC, and Staufen expression. Dicer-2 is one of two Drosophila ribonucleases that produce short RNA components of RISC (Pham et al., 2004). CaMKII synaptic expression was dramatically increased in a dicer-2 mutant, particularly in the antennal lobe and mushroom body (Figures 5E and 5F). In contrast, there was no difference in the expression of the cell adhesion protein Fasciclin II in the same animals (Figures 5E' and 5F'). In Western analysis (Figure 5G, left panel), there was a striking 25-fold increase in CaMKII protein in dicer-2 mutant brains. Synaptic CaMKII expression was also elevated in aubergine and armitage mutant brains (Figures 5G and S1). The aubergine locus encodes an Argonaute protein involved in RISC assembly and function (Cook et al., 2004 and Tomari et al., 2004). The level of Staufen protein was also increased in the armitage mutant brain (Figure 5G, right panel).Whether the miRNA binding sites in the CaMKII 3′UTR might be involved in RISC-mediated regulation was examined with the EYFP3′UTR transgene. When expressed in the PNs, EYFP fluorescence in glomeruli was 80% greater in armi than in the wild-type (Figures 5J and 5O). The EYFP3′UTR fluorescence was localized to large dendritic puncta like those found in brains explanted into nicotine-containing media (Figures 5L and 5M). Indeed, EYFP3′UTR expression in armi brains did not increase further upon explant with nicotine (Figures 5L and 5N), consistent with the notion that cholinergic activation might act via antagonism of RISC. The expression of CaMKII::EYFPNUT, which lacks the 3′UTR, increased slightly (15%) in the armi mutant background (Figure 5I), while other control constructs, such as CD8::GFP, were essentially unchanged (Figure 5H). In addition, RT-PCR analysis of wild-type and armi mutant brains did not reveal a difference in the levels of transgenic mRNAs (data not shown). There was also a substantial increase in EYFP3′UTR and CaMKII::EYFP3′UTR synaptic fluorescence in dicer-2 and aubergine mutants (Figure S1 and data not shown). Therefore we conclude that RISC regulates CaMKII expression by a posttranscriptional mechanism, utilizing sites in the CaMKII 3′UTR.Armitage expression was found in multiple neuronal populations in the brain, including the PNs and mushroom-body Kenyon cells. It was distributed in puncta in cell bodies and dendrites and to axon termini (Figure 5B). A GFP::Armi fusion protein, when expressed in the PNs, displayed a similar punctate distribution (Figures 5C and 6A–6E) that overlapped synaptic puncta containing CaMKII (arrowhead, Figure 5C'). The GFP::Armi fusion protein retains armi+ activity (Cook et al., 2004) such that neurons with high levels of GFP::Armi expression would have increased armi+ activity. Several observations indicate that a posttranscriptional autoregulatory circuit modulates Armi expression (Figure S2). Nonetheless, strong transgenic expression of GFP::Armi reduced the level of CaMKII expression, as revealed by Western blot analysis (Figure 5P) and immunohistochemistry. Neurons that expressed a high level of GFP::Armi displayed reduced expression of both CaMKII and KHC (Figures 5Q, 5R, and S2). A control UAS-CD8-GFP transgene was unaffected by GFP::Armi expression (Figure 5S).Since Armi regulates KHC and Staufen expression, we considered the possibility that it might also regulate the dendritic transport of CaMKII mRNA. When examined with the MS2::GFP system, armi72.1 homozygotes indeed displayed a 78% increase in fluorescence by dendritic GFP-positive puncta, compared to an armi+ control (Figures 4E and 4I; n = 7, p < 0.05). Therefore, Armi regulation of synaptic protein synthesis reflects a coordinated program with multiple miRNA targets, affecting both mRNA transport and translation at the synapse, where Armi protein is found. Neural Activity Induces Rapid Proteasome-Mediated Degradation of ArmitageIf mRNA silencing by RISC plays a role in LTM, we would expect this pathway to be somehow regulated by neural function. Given the inverse relationship between CaMKII expression and armi+ activity, we considered whether Armi might be a regulatory target. As shown in Figure 6, the level of GFP::Armi fluorescence rapidly decreased (by 3.5-fold) in brains explanted into nicotine-containing medium. There was a correlated increase in CaMKII expression (by 4.5-fold) in the PN dendritic arbors of the antennal lobe. A short incubation with nicotine (5 min) resulted in the complete disappearance of Armi protein in Western analysis (Figure 6G, top panel). The GFP::Armi protein was also eliminated upon explant with nicotine (Figure 6G, bottom panel). In contrast, the CaMKII protein level increased (Figure 6H, right panel) and α1-tubulin was unchanged (Figure 6G). Two experiments were performed to determine whether the activity-induced elimination of Armi required the proteasome. First, GFP::Armi was expressed along with a transgenic dominant-negative mutant of the proteasome β subunit (DTS5; Speese et al., 2003). When the DTS5 transgene was present, the level of GFP::Armi fluorescence was elevated by 3.2-fold (Figures 6A and 6B; p < 0.0001, n = 8). In contrast, the DTS5 transgene did not alter the level of CD8::GFP (not shown). Second, incubation with the proteasome inhibitor lactacystin blocked the nicotine-induced loss of GFP::Armi (Figures 6D and 6E) and degradation of endogenous Armi protein (Figure 6H, left panel). Preincubation with lactacystin also blocked nicotine-induced synaptic CaMKII synthesis, as determined by both Western analysis (Figure 6H, right panel) and by immunohistochemistry (Figure 6E). Thus, cholinergic activity evidently acts via the proteasome to induce the degradation of Armitage and synaptic synthesis of CaMKII. A Degradative Pathway for LTMA key question is whether this degradative pathway has a role in synaptic protein synthesis associated with LTM. Animals expressing the GFP::Armi protein in projection neurons were subjected to olfactory spaced training and analyzed by the same microscopic methods used to assess LTM-associated EYFP3′UTR expression (Figure 3). The GFP::Armi protein was found concentrated in synaptic puncta in the glomeruli (Figures 5C and 6J). When examined at either 3 or 24 hr posttraining, GFP::Armi fluorescence was significantly reduced in many glomeruli and most strongly reduced in the glomeruli that had displayed the greatest increase in EYFP3′UTR expression (Figure 3 and Figure 6I, and 6J). Fluorescence in glomeruli DA1 and VA1 decreased by 3.1- and 3.8-fold, respectively, when the odorant MCH was paired with shock. When the odorant OCT was paired with shock, the D and DL3 glomeruli displayed the most significant decreases (2-fold). More modest losses of GFP fluorescence were observed in other glomeruli (Figures 6I and 6J). These observations reveal an inverse relationship between synaptic Armi protein and CaMKII synthesis during the establishment of an LTM. Since these changes were still present at 24 hr posttraining, the change was evidently maintained long-term, perhaps for the term of the memory.Given the role of Armi in the synaptic synthesis of CaMKII, we wondered whether either of these genes might be required to form an olfactory LTM. Several armi hypomorphic alleles display normal adult viability and behavior, including normal odor and shock sensitivity. Given their normal performance in these tests, we examined armi animals for STM and LTM. The animals (armi72.1/armi72.1 or armi72.1/Df(3L)E1) displayed normal memory in the short-term paradigm (Figure 6K; Armi−, PI = 0.55; Armi+, PI = 0.58; p > 0.1) but were profoundly deficient in LTM (Armi−, PI = 0.085 versus Armi+, PI = 0.31; p < 0.05). Expression of the GFP::Armi transgene rescued the armi72.1/armi72.1 LTM deficiency to a normal value (Figure 6K). We achieved a nearly complete and tissue-specific loss of CaMKII by use of a construct that generates a CaMKII hairpin RNA (UAS-CaMKIIhpn; Figure 6L). Animals expressing UAS-CaMKIIhpin in all CaMKII-positive neurons (with the CaMKII-GAL4 driver) retained normal short-term memory (PI = 0.653; CaMKII-GAL4 alone, PI = 0.64), but displayed a near-complete loss of LTM (PI = 0.07 versus CaMKII-GAL4 alone, PI = 0.3; p < 0.001). Thus, both CaMKII and Armitage are required for LTM but not for STM. DiscussionIt has long been known that the establishment of long-lasting forms of memory requires protein synthesis, a feature of memory common to both vertebrates and invertebrates. Of special interest is protein synthesis localized to the synapse, which might result in selective synaptic change and the stable modification of a neural circuit (Bailey et al., 2004 and Kelleher et al., 2004). Yet how and where synaptic protein synthesis occurs in relation to the establishment and maintenance of a memory has not been determined. Here we report that memory-specific patterns of synaptic protein synthesis occur with the induction of a long-term memory in Drosophila. These patterns appear to be controlled by the proteasome-mediated degradation of a RISC pathway component, Armitage, to regulate the transport of mRNA to synapses and its translation once there.To visualize synaptic protein synthesis, we devised fluorescent reporters based on the Drosophila CaMKII gene, which has well-described roles in synaptic plasticity and memory (Griffith et al., 1993 and Koh et al., 1999). The 3′UTR of CaMKII shares regulatory motifs with the mammalian αCaMKII mRNA, which mediate dendritic mRNA localization and neural activity-dependent translation (reviewed in Richter and Lorenz, 2002). We found that the 3′UTR of Drosophila CaMKII was also necessary and sufficient for mRNA localization to dendrites and synaptic translation (Figure 1 and Figure 4). This 3′UTR sufficed for the enhanced dendritic mRNA transport and translation induced by cholinergic stimulation (Figure 2 and Figure 4). Hence we find a simple parallel between the synaptic regulation of CaMKII in Drosophila and mammals.When these fluorescent reporters were utilized in vivo, the induction of synaptic protein synthesis was observed in several Drosophila brain centers following the spaced training paradigm of repetitive odor paired with electric shock that establishes a long-term memory (LTM; Figure 3). There were local patterns of memory specificity identifiable in glomeruli of the antennal lobe where synapses of similar function are clustered. When the odorant OCT was paired with electric shock, protein synthesis was induced selectively in the D and DL3 glomeruli. When the odorant MCH was paired with shock, the DA1 and VA1 glomeruli displayed the most robust enhancement of synaptic protein synthesis. Notably, the animals were exposed to both odorants during training; the pattern of synthesis depended on coincidence with shock. There was no significant induction of protein synthesis when exposure to odor and shock was nonoverlapping, with either stimulus presented alone, or in the absence of temporal spacing (“massed training”). Thus, an odor-specific pattern of synaptic protein synthesis was induced under conditions that produce an LTM.Experiments in the honeybee suggest that the antennal lobe is a “way station” for memory where stimuli are integrated to yield plasticity more labile than a short-term memory (Menzel and Giurfa, 1999). A long-term memory can be formed in the honeybee antennal lobe in a spaced training paradigm (Muller, 2000). The experiments of Yu et al. (2004) revealed plasticity in the Drosophila antennal lobe, where particular glomeruli acquired enhanced synaptic activity after a single episode of paired odor and shock. Remarkably, the enhanced synaptic protein synthesis we observed with spaced training occurred in essentially the same glomeruli that displayed enhanced synaptic activity in the STM protocol (Yu et al., 2004). These glomeruli are distinct from those that display the greatest odor or electric shock-evoked synaptic activity (Ng et al., 2002, Wang et al., 2003 and Wilson et al., 2004). Therefore, we suppose that the mechanism that integrates a single paired odor and shock to produce new synaptic activity might also generate the trigger for synaptic protein synthesis when the paired stimuli are repeated with temporal spacing. We believe this trigger includes the proteasome-mediated degradation of the RISC factor Armitage (Figure 7). Though these “memory traces” have been recorded in the antennal lobe, there is still no evidence for their role in memory. The mushroom body, on the other hand, is required for LTM (Pascual and Preat, 2001). Our current methods cannot resolve patterns of synaptic protein synthesis in the mushroom body because it lacks the stereotyped synaptic architecture of the antennal lobe. When determined, a global brain map of synaptic protein synthesis will provide significant insights into the mechanisms of memory storage.Synaptic protein synthesis and dendritic mRNA transport are well studied for the mammalian αCaMKII gene, which bears recognition motifs in its 3′UTR for CPEB and other proteins with transport and translation control functions (Richter and Lorenz, 2002). The presence of potential recognition motifs for the CPEB, Pumilio, and Staufen proteins in the Drosophila CaMKII 3′UTR (our unpublished observations) suggests that these mechanisms are conserved in Drosophila. Indeed, Staufen, orb (a CPEB family member), and pumilio have been identified as LTM-deficient mutants (Dubnau et al., 2003). The roles of these genes remain to be fully explored.We focused instead on the RISC pathway because of apparent binding motifs for microRNAs miR-280 and miR-289 in the CaMKII 3′UTR (Figure 5A). These sites are similar to those in the 3′UTRs of oskar and Kinesin heavy chain (Khc), which are targets for translational silencing by Armitage and other RISC components in the oocyte (Cook et al., 2004). We found Armitage in synaptic puncta on dendrites, colocalized with CaMKII (Figure 5). When the level of Armitage was decreased or increased by mutation or transgenic expression, CaMKII synaptic expression was modulated in a reciprocal and cell-autonomous fashion (Figure 5 and Figure 6). This regulation could be recapitulated by an EYFP reporter bearing the CaMKII 3′UTR. Mutants for the RISC components Aubergine and Dicer-2 displayed similar phenotypes (Figures 5 and S1). It therefore seems likely that multiple tiers of control regulate CaMKII like oskar, where two systems (Bruno/Cup and RISC) act on distinct sites in its 3′UTR (Cook et al., 2004 and Webster et al., 1997).A second avenue for RISC control of CaMKII synthesis is via mRNA transport. By tagging CaMKII mRNA with a GFP reporter, we observed dendritic punctae whose frequency and intensity increased under the same conditions that induced synaptic protein synthesis: cholinergic activation and olfactory spaced training (Figure 4). The induction of mRNA transport required new protein synthesis but not transcription. Armitage was also found to regulate the frequency and intensity of the GFP-tagged dendritic puncta (Figure 4). Two proteins that play a role in mRNA transport, Kinesin heavy chain (KHC) and Staufen, recapitulate this pattern of regulation by cholinergic stimulation and Armitage (Figure 5). Both of their mRNAs bear targets for miRNA regulation in the 3′UTR (Figure 5; Cook et al., 2004 and Rajewsky and Socci, 2004). Our studies leave open the possibility that the enhanced synaptic localization of CaMKII mRNA underlies the induction of its synaptic translation. However, the presence of miRNA binding sites in the CaMKII 3′UTR, the localization of Armitage with CaMKII in synaptic punctae, and the rapid induction of CaMKII synthesis by cholinergic activity all suggest that RISC acts at the synapse. Furthermore, local translational control may be required to impose the specificity that was not evident in the pattern of mRNA transport associated with the induction of an LTM (Figure 4).A link between the induction of memory and synaptic protein synthesis is the proteasome-mediated degradation of Armitage. In explant culture, cholinergic induction of CaMKII synthesis was accompanied by the rapid degradation of Armi; both events were blocked by inhibition of the proteasome (Figure 6). The relationship between Armi degradation and CaMKII synaptic translation was recapitulated in the brain as animals formed and maintained an LTM. The same glomeruli that displayed the greatest increase in CaMKII synthesis displayed the largest decline in synaptic Armi (Figure 6). This reciprocal relationship between the Armi and CaMKII proteins was detected as early as 3 hr after training and maintained for at least 24 hr posttraining. The training-induced change of synaptic Armi was therefore “locked in,” possibly for the term of the memory, consistent with a role in maintaining an alteration of synaptic function.Therefore we propose a new mechanism for stable memory in which an integrated sensory trigger induces the proteasome-mediated degradation of a RISC factor, releasing synaptic protein synthesis and mRNA transport from microRNA suppression (Figure 7). We suppose that this mechanism is triggered with neuronal specificity in order to produce memory-specific patterns of protein synthesis. Whether this specificity is required for memory or extends to the level of a single synapse are questions that remain to be addressed. Experimental Procedures Drosophila Stocks and GeneticsFly stocks were maintained at 25°C on standard cornmeal agar medium under a constant 24 hr light/dark cycle. aubergine (aubHN and aubQC42) and Df(3L)E1 strains were obtained from the Bloomington Stock Center (Indiana). Transformant lines carrying UAS-GFP::Armi, and armi mutant lines were generously provided by H. Cook and W. Theurkauf (U. Mass., Worcester, MA). The dcr-2L811Ex mutant was a gift of Drs. R. Carthew and H. Ruohola-Baker. The UAS-DTS5 strain (Speese et al., 2003) was crossed to elav-GAL4, UAS-GFP::armi or GH146-GAL4, UAS-GFP::Armi animals and grown at 17°C (permissive temperature), as described in the text. Imunnohistochemical MethodsAdult brains were dissected and processed for immunohistochemistry as described by Kunes et al. (1993). Antibodies were used at the following dilutions: mouse αCaMKII (1:100; Takamatsu et al., 2003), rabbit αCaMKII (1:4000; Koh et al., 1999 and Takamatsu et al., 2003), αARD (1:50), α-Elav (1:100), α-Armi (1:200) and α-KHC (1:50), α-mouse Cy3 (1:100), α-rat Cy5 (1:200), and α-rabbit Cy5 (1:500). Explant ProtocolAdult heads were dissected to remove the brain in AHL medium (Wang et al., 2003) and incubated as described. Nicotine ([−] Nicotine, Sigma) was used at 10 mM, adjusted to pH 7.0–7.4. Acetylcholine (Acetylcholine chloride, 99%, Sigma) was used at 50 μM. The explants were incubated at 22°C as indicated, washed three times in AHL, and fixed in 4% paraformaldehyde. For proteasome inhibition, specimens were incubated for 40 min in lactacystin (100 μM, Sigma) prior to further manipulation. For inhibition of translation or transcription, explants were incubated for 30–60 min in Anisomycin (25 μM) and Actinomycin D (50 μM, A.G. Scientific) prior to nicotine incubation. Behavioral AssaysShort and longterm olfactory memory tests were performed in a T-maze apparatus as described by Tully and Quinn (1985). Most transgenic lines were backcrossed to Canton S (C. Quinn, MIT) for 3–4 generations prior to use. Animals were used at 2–4 days posteclosion. Fluorescence MeasurementsImage quantification and statistical analysis are described in the Supplemental Experimental Procedures. AcknowledgmentsWe thank Drs. S. Ohsako, K. Kosik, E. Gundelfinger, L. Griffith, R. Carthew, H. Ruohola-Baker, H. Cook, and W. Theurkauf for generously sharing reagents. We thank Drs. T. Preat, S. Waddell, and W. Quinn for advice on behavioral assays."
929,"edge cooccurrence in natural images predicts contour grouping performance",465715,"Edge co-occurrence in natural images predicts contour grouping performance.","The human brain manages to correctly interpret almost every visual image it receives from the environment. Underlying this ability are contour grouping mechanisms that appropriately link local edge elements into global contours. Although a general view of how the brain achieves effective contour grouping has emerged, there have been a number of different specific proposals and few successes at quantitatively predicting performance. These previous proposals have been developed largely by intuition and computational trial and error. A more principled approach is to begin with an examination of the statistical properties of contours that exist in natural images, because it is these statistics that drove the evolution of the grouping mechanisms. Here we report measurements of both absolute and Bayesian edge co-occurrence statistics in natural images, as well as human performance for detecting natural-shaped contours in complex backgrounds. We find that contour detection performance is quantitatively predicted by a local grouping rule derived directly from the co-occurrence statistics, in combination with a very simple integration rule (a transitivity rule) that links the locally grouped contour elements into longer contours."
930,"evolutionary changes in cis and trans gene regulation",466050,"Evolutionary changes in cis and trans gene regulation","Differences in gene expression are central to evolution. Such differences can arise from cis-regulatory changes that affect transcription initiation, transcription rate and/or transcript stability in an allele-specific manner, or from trans-regulatory changes that modify the activity or expression of factors that interact with cis-regulatory sequences1, 2. Both cis- and trans-regulatory changes contribute to divergent gene expression, but their respective contributions remain largely unknown3. Here we examine the distribution of cis- and trans-regulatory changes underlying expression differences between closely related Drosophila species, D. melanogaster and D. simulans, and show functional cis-regulatory differences by comparing the relative abundance of species-specific transcripts in F1 hybrids4, 5. Differences in trans-regulatory activity were inferred by comparing the ratio of allelic expression in hybrids with the ratio of gene expression between species. Of 29 genes with interspecific expression differences, 28 had differences in cis-regulation, and these changes were sufficient to explain expression divergence for about half of the genes. Trans-regulatory differences affected 55% (16 of 29) of genes, and were always accompanied by cis-regulatory changes. These data indicate that interspecific expression differences are not caused by select trans-regulatory changes with widespread effects, but rather by many cis-acting changes spread throughout the genome."
931,"drosophila melanogaster a case study of a model genomic sequence and its consequences",466470,"Drosophila melanogaster: A case study of a model genomic sequence and its consequences","The sequencing and annotation of the Drosophila melanogaster genome, first published in 2000 through collaboration between Celera Genomics and the Drosophila Genome Projects, has provided a number of important contributions to genome research. By demonstrating the utility of methods such as whole-genome shotgun sequencing and genome annotation by a community ""jamboree,"" the Drosophila genome established the precedents for the current paradigm used by most genome projects. Subsequent releases of the initial genome sequence have been improved by the Berkeley Drosophila Genome Project and annotated by FlyBase, the Drosophila community database, providing one of the highest-quality genome sequences and annotations for any organism. We discuss the impact of the growing number of genome sequences now available in the genus on current Drosophila research, and some of the biological questions that these resources will enable to be solved in the future."
932,"strangers to ourselves discovering the adaptive unconscious",466558,"Strangers to Ourselves : Discovering the Adaptive Unconscious","(from the jacket) The adaptive unconscious that empirical psychology has revealed is not just a repository of primitive drives and conflict-ridden memories. It is a set of pervasive, sophisticated mental processes with which we evaluate our worlds, set goals and initiated action, all while we are consciously thinking about something else. We are able to narrow our focus of attention on a conversation and block out the surroundings, while unconsciously monitoring them. We are able to learn some things without conscious effort, and out unconscious determines our emotions quickly, giving use intuitions that we dismiss but that turn out to have been correct. Citing evidence that too much introspection can create more confusion rather than less about one's own emotions, the authors makes the case for better ways of discovering our unconscious selves, through paying attention to our actions and others' reactions to them and by adopting a new self-narrative and acting consistently with it. (PsycINFO Database Record (c) 2005 APA, all rights reserved)"
933,"on the emergence of social conventions modeling analysis and simulations",467056,"On the Emergence of Social Conventions: Modeling, Analysis, and Simulations","We define the notion of social conventions in a standard game-theoretic framework, and identify various criteria of consistency of such conventions with the principle of individual rationality. We then investigate the emergence of such conventions in a stochastic setting; we do so within a stylized framework currently popular in economic circles, namely that of stochastic games. This framework comes in several forms; in our setting agents interact with each other through a random process, and accumulate information about the system. As they do so, they continually reevaluate their current choice of strategy in light of the accumulated information. We introduce a simple and natural strategy-selection rule, called highest cumulative reward (HCR). We show a class of games in which HCR guarantees eventual convergence to a rationally acceptable social convention. Most importantly, we investigate the efficiency with which such social conventions are achieved. We give an analytic lower bound on this rate, and then present results about how HCR works out in practice. Specifically, we pick one of the most basic games, namely a basic coordination game (as defined by Lewis), and through extensive computer simulations determine not only the effect of applying HCR, but also the subtle effects of various system parameters, such as the amount of memory and the frequency of update performed by all agents."
934,"the question concerning technology and other essays",467098,"The Question Concerning Technology, and Other Essays","{""To read Heidegger is to set out on an adventure. The essays in this volume--intriguing, challenging, and often baffling to the reader--call him always to abandon all superficial scanning and to enter wholeheartedly into the serious pursuit of thinking....</P><P>""Heidegger is not a 'primitive' or a 'romanitic.' He is not one who seeks escape from the burdens and responsibilities of contemporary life into serenity, either through the re-creating of some idyllic past or through the exalting of some simple experience. Finally, Heidegger is not a foe of technology and science. He neither disdains nor rejects them as though they were only destructive of human life.</P><P>""The roots of Heidegger's hinking lie deep in the Western philosophical tradition. Yet that thinking is unique in many of its aspects, in its language, and in its leterary expression. In the development of this thought Heidegger has been taught chiefly by the Greeks, by German idealism, by phenomenology, and by the scholastic theological tradition. In him these and other elements have been fused by his genius of sensitivity and intellect into a very individual philosophical expression."" --William Lovitt, <I>from the Introduction</I>}"
935,"genomescale evidence of the nematodearthropod clade",467313,"Genome-scale evidence of the nematode-arthropod clade","{BACKGROUND:The} issue of whether coelomates form a single clade, the Coelomata, or whether all animals that moult an exoskeleton (such as the coelomate arthropods and the pseudocoelomate nematodes) form a distinct clade, the Ecdysozoa, is the most puzzling issue in animal systematics and a major open-ended subject in evolutionary biology. Previous single-gene and genome-scale analyses designed to resolve the issue have produced contradictory results. Here we present the first genome-scale phylogenetic evidence that strongly supports the Ecdysozoa {hypothesis.RESULTS:Through} the most extensive phylogenetic analysis carried out to date, the complete genomes of 11 eukaryotic species have been analyzed in order to find homologous sequences derived from 18 human chromosomes. Phylogenetic analysis of datasets showing an increased adjustment to equal evolutionary rates between nematode and arthropod sequences produced a gradual change from support for Coelomata to support for Ecdysozoa. Transition between topologies occurred when fast-evolving sequences of Caenorhabditis elegans were removed. When chordate, nematode and arthropod sequences were constrained to fit equal evolutionary rates, the Ecdysozoa topology was statistically accepted whereas Coelomata was {rejected.CONCLUSIONS:The} reliability of a monophyletic group clustering arthropods and nematodes was unequivocally accepted in datasets where traces of the long-branch attraction effect were removed. This is the first phylogenomic evidence to strongly support the 'moulting clade' hypothesis."
936,"estimating the support of a highdimensional distribution",467844,"Estimating the Support of a High-Dimensional Distribution","Suppose you are given some dataset drawn from an underlying probability  distribution P and you want to estimate a &#034;simple&#034; subset S of input  space such that the probability that a test point drawn from P lies outside of  S is bounded by some a priori specified  between 0 and 1.  We propose a method to approach this problem by trying to estimate a  function f which is positive on S and negative on the complement. The  functional form of f is given by a kernel expansion in terms of a potentially  small subset of the training data; it is regularized by controlling the length of  the weight vector in an associated feature space. The expansion coefficients  are found by solving a quadratic programming problem, which we do by  carrying out sequential optimization over pairs of input patterns. We also  provide a preliminary theoretical analysis of the statistical performance of  our algorithm.  The algorithm is a natural extension of the support vector algorithm to  the case of unlabelled d..."
937,"tilebars visualization of term distribution information in full text information access",468425,"TileBars: visualization of term distribution information in full text information access","The field of information retrieval has traditionally focused on textbases consisting of titles and abstracts. As a consequence, many underlying assumptions must be altered for retrieval from full-length text collections. This paper argues for making use of text structure when retrieving from full text documents, and presents a visualization paradigm, called TileBars, that demonstrates the usefulness of explicit term distribution information in Boolean-type queries. TileBars simultaneously and compactly indicate relative document length, query term frequency, and query term distribution. The patterns in a column of TileBars can be quickly scanned and deciphered, aiding users in making judgments about the potential relevance of the retrieved documents. KEYWORDS: Information retrieval, Full-length text, Visualization. INTRODUCTION Information access systems have traditionally focused on retrieval of documents consisting of titles and abstracts. As a consequence, the underlying assumpt..."
938,"n degrees of separation multidimensional separation of concerns",468934,"{N Degrees} of separation: multi-dimensional separation of concerns","Done well, separation of concerns can provide many software engineering benefits, including reduced complexity, improved reusability, and simpler evolution. The choice of boundaries for separate concerns depends on both requirements on the system and on the kind(s) of decomposition and composition a given formalism supports. The predominant methodologies and formalisms available, however, support only orthogonal separations of concerns, along single dimensions of composition and decomposition. These characteristics lead to a number of well-known and difficult problems.  This paper describes a new paradigm for modeling and implementing software artifacts, one that permits separation of overlapping concerns along multiple dimensions of composition and decomposition. This approach addresses numerous problems throughout the software lifecycle in achieving wellengineered, evolvable, flexible software artifacts and traceability across artifacts.  Keywords  Hypermodules; hyperslices; software..."
939,"automated negotiation prospects methods and challenges",469005,"Automated Negotiation: Prospects, Methods and Challenges","The remainder of this paper is structured as follows. Section 2 presents a generic framework for automated negotiation. This framework is then used to structure the subsequent discussion and analysis of the various negotiation techniques; section 3 deals with game theoretic techniques, section 4 with heuristic techniques, and section 5 with argumentation-based techniques. Finally, section 6 outlines some of the major challenges that need to be addressed before automated negotiation becomes pervasive."
940,"an empirical comparison of seven programming languages",469344,"An Empirical Comparison of Seven Programming Languages","Often heated, debates regarding different programming languages' effectiveness remain inconclusive because of scarce data and a lack of direct comparisons. The author addresses that challenge, comparatively analyzing 80 implementations of the phonecode program in seven different languages (C, C++, Java, Perl, Python, Rexx, and Tcl). Further, for each language, the author analyzes several separate implementations by different programmers. The comparison investigates several aspects of each language, including program length, programming effort, run-time efficiency, memory consumption, and reliability. The author uses comparisons to present insight into program language performance. For example, the study indicates that Java's memory overhead is still huge compared to C or C++, but its runtime efficiency has become quite acceptable. The scripting languages, however, offer reasonable alternatives to C and C++, even for tasks that must handle fair amounts of computation and data."
941,"learning algorithms for keyphrase extraction",469350,"Learning Algorithms for Keyphrase Extraction","Many academic journals ask their authors to provide a list of about five to fifteen keywords, to appear on the first page of each article. Since these key words are often phrases of two or more words, we prefer to call them keyphrases. There is a wide variety of tasks for which keyphrases are useful, as we discuss in this paper. We approach the problem of automatically extracting keyphrases from text as a supervised learning task. We treat a document as a set of phrases, which the learning algorithm must learn to classify as positive or negative examples of keyphrases. Our first set of experiments applies the C4.5 decision tree induction algorithm to this learning task. We evaluate the performance of nine different configurations of C4.5. The second set of experiments applies the GenEx algorithm to the task. We developed the GenEx algorithm specifically for automatically extracting keyphrases from text. The experimental results support the claim that a custom-designed algorithm (GenEx), incorporating specialized procedural domain knowledge, can generate better keyphrases than a generalpurpose algorithm (C4.5). Subjective human evaluation of the keyphrases generated by Extractor suggests that about 80 % of the keyphrases are acceptable to human readers. This level of performance should be satisfactory for a wide variety of applications."
942,"towards multidimensional genome annotation",469427,"Towards multidimensional genome annotation","Our information about the gene content of organisms continues to grow as more genomes are sequenced and gene products are characterized. Sequence-based annotation efforts have led to a list of cellular components, which can be thought of as a one-dimensional annotation. With growing information about component interactions, facilitated by the advancement of various high-throughput technologies, systemic, or two-dimensional, annotations can be generated. Knowledge about the physical arrangement of chromosomes will lead to a three-dimensional spatial annotation of the genome and a fourth dimension of annotation will arise from the study of changes in genome sequences that occur during adaptive evolution. Here we discuss all four levels of genome annotation, with specific emphasis on two-dimensional annotation methods."
943,"literature mining for the biologist from information retrieval to biological discovery",469428,"Literature mining for the biologist: from information retrieval to biological discovery"," For the average biologist, hands-on literature mining currently means a keyword search in PubMed. However, methods for extracting biomedical facts from the scientific literature have improved considerably, and the associated tools will probably soon be used in many laboratories to automatically annotate and analyse the growing number of system-wide experimental data sets. Owing to the increasing body of text and the open-access policies of many journals, literature mining is also becoming useful for both hypothesis generation and biological discovery. However, the latter will require the integration of literature and high-throughput data, which should encourage close collaborations between biologists and computational linguists."
944,"the human condition",469592,"The Human Condition","{<div>A work of striking originality bursting with unexpected insights, <i>The Human Condition</i> is in many respects more relevant now than when it first appeared in 1958. In her study of the state of modern humanity, Hannah Arendt considers humankind from the perspective of the actions of which it is capable. The problems Arendt identified then--diminishing human agency and political freedom, the paradox that as human powers increase through technological and humanistic inquiry, we are less equipped to control the consequences of our actions--continue to confront us today. This new edition, published to coincide with the fortieth anniversary of its original publication, contains an improved and expanded index and a new introduction by noted Arendt scholar Margaret Canovan which incisively analyzes the book's argument and examines its present relevance. A classic in political and social theory, <i>The Human Condition</i> is a work that has proved both timeless and perpetually timely.<br><br>Hannah Arendt (1906-1975) was one of the leading social theorists in the United States. Her <i>Lectures on Kant's Political Philosophy</i> and <i>Love and Saint Augustine</i> are also published by the University of Chicago Press. <br><br></div>}"
945,"computer networks as social networks collaborative work telework and virtual community",471849,"COMPUTER NETWORKS AS SOCIAL NETWORKS: Collaborative Work, Telework, and Virtual Community","When computer networks link people as well as machines, they become social networks. Such computer-supported social networks (CSSNs) are becoming important bases of virtual communities, computer-supported cooperative work, and telework. Computer-mediated communication such as electronic mail and computerized conferencing is usually text-based and asynchronous. It has limited social presence, and on-line communications are often more uninhibited, creative, and blunt than in-person communication. Nevertheless, CSSNs sustain strong, intermediate, and weak ties that provide information and social support in both specialized and broadly based relationships. CSSNs foster virtual communities that are usually partial and narrowly focused, although some do become encompassing and broadly based. CSSNs accomplish a wide variety of cooperative work, connecting workers within and between organizations who are often physically dispersed. CSSNs also link teleworkers from their homes or remote work centers to main organizational offices. Although many relationships function off-line as well as on-line, CSSNs have developed their own norms and structures. The nature of the medium both constrains and facilitates social control. CSSNs have strong societal implications, fostering situations that combine global connectivity, the fragmentation of solidarities, the de-emphasis of local organizations (in the neighborhood and workplace), and the increased importance of home bases."
946,"reducing the computational complexity of protein folding via fragment folding and assembly",472947,"Reducing the computational complexity of protein folding via fragment folding and assembly.","Understanding, and ultimately predicting, how a 1-D protein chain reaches its native 3-D fold has been one of the most challenging problems during the last few decades. Data increasingly indicate that protein folding is a hierarchical process. Hence, the question arises as to whether we can use the hierarchical concept to reduce the practically intractable computational times. For such a scheme to work, the first step is to cut the protein sequence into fragments that form local minima on the polypeptide chain. The conformations of such fragments in solution are likely to be similar to those when the fragments are embedded in the native fold, although alternate conformations may be favored during the mutual stabilization in the combinatorial assembly process. Two elements are needed for such cutting: (1) a library of (clustered) fragments derived from known protein structures and (2) an assignment algorithm that selects optimal combinations to ""cover"" the protein sequence. The next two steps in hierarchical folding schemes, not addressed here, are the combinatorial assembly of the fragments and finally, optimization of the obtained conformations. Here, we address the first step in a hierarchical protein-folding scheme. The input is a target protein sequence and a library of fragments created by clustering building blocks that were generated by cutting all protein structures. The output is a set of cutout fragments. We briefly outline a graph theoretic algorithm that automatically assigns building blocks to the target sequence, and we describe a sample of the results we have obtained."
947,"spread of epidemic disease on networks",475048,"Spread of Epidemic Disease on Networks","The study of social networks, and in particular the spread of disease on networks, has attracted considerable recent attention in the physics community. In this paper, we show that a large class of standard epidemiological models, the so-called susceptible/infective/removed ô°SIRô° models can be solved exactly on a wide variety of networks. In addition to the standard but unrealistic case of fixed infectiveness time and fixed and uncorrelated probability of transmission between all pairs of individuals, we solve cases in which times and probabilities are nonuniform and correlated. We also consider one simple case of an epidemic in a structured population, that of a sexually transmitted disease in a population divided into men and women. We confirm the correctness of our exact solutions with numerical simulations of SIR epidemics on networks."
948,"generalized born models of macromolecular solvation effects",475980,"Generalized born models of macromolecular solvation effects.","▪ Abstract  It would often be useful in computer simulations to use a simple description of solvation effects, instead of explicitly representing the individual solvent molecules. Continuum dielectric models often work well in describing the thermodynamic aspects of aqueous solvation, and approximations to such models that avoid the need to solve the Poisson equation are attractive because of their computational efficiency. Here we give an overview of one such approximation, the generalized Born model, which is simple and fast enough to be used for molecular dynamics simulations of proteins and nucleic acids. We discuss its strengths and weaknesses, both for its fidelity to the underlying continuum model and for its ability to replace explicit consideration of solvent molecules in macromolecular simulations. We focus particularly on versions of the generalized Born model that have a pair-wise analytical form, and therefore fit most naturally into conventional molecular mechanics calculations."
949,"parameter estimation in biochemical pathways a comparison of global optimization methods",476071,"Parameter estimation in biochemical pathways: a comparison of global optimization methods.","10.1101/gr.1262503 Here we address the problem of parameter estimation (inverse problem)of nonlinear dynamic biochemical pathways. This problem is stated as a nonlinear programming (NLP)problem subject to nonlinear differential-algebraic constraints. These problems are known to be frequently ill-conditioned and multimodal. Thus, traditional (gradient-based)local optimization methods fail to arrive at satisfactory solutions. To surmount this limitation, the use of several state-of-the-art deterministic and stochastic global optimization methods is explored. A case study considering the estimation of 36 parameters of a nonlinear biochemical dynamic model is taken as a benchmark. Only a certain type of stochastic algorithm, evolution strategies (ES), is able to solve this problem successfully. Although these stochastic methods cannot guarantee global optimality with certainty, their robustness, plus the fact that in inverse problems they have a known lower bound for the cost function, make them the best available candidates."
950,"social matching a framework and research agenda",477344,"Social matching: A framework and research agenda","Social matching systems bring people together in both physical and online spaces. They have the potential to increase social interaction and foster collaboration. However, social matching systems lack a clear intellectual foundation: the nature of the design space, the key research challenges, and the roster of appropriate methods are all ill-defined. This article begins to remedy the situation. It clarifies the scope of social matching systems by distinguishing them from other recommender systems and related systems and techniques. It identifies a set of issues that characterize the design space of social matching systems and shows how existing systems explore different points within the design space. It also reviews selected social science results that can provide input into system design. Most important, the article presents a research agenda organized around a set of claims. The claims embody our understanding of what issues are most important to investigate, our beliefs about what is most likely to be true, and our suggestions of specific research directions to pursue."
951,"the ecological approach to visual perception",477418,"The Ecological Approach To Visual Perception","The great perception theorist J. J. Gibson (Gibson, 1979) brought about radical changes in the ways we think about perception with his theories of ecological optics, affordances and direct perception.  Gibson assumed that we perceive in order to operate on the environment. Perception is designed for action. Gibson called the perceivable possibilities for action affordances. He claimed that we perceive affordance properties of the environment in a direct and immediate way. This theory is clearly attractive from the perspective of visualization. The goal of most visualization is decision making. In short, Gibson claims that we perceive possibilities for action. i.e. surfaces for walking, handles for pulling, space for navigation, tools for manipulating, etc. In general, our whole evolution has been geared toward perceiving useful possibilities for action.  Affordance example: (Norman, 1988)  You are approaching a door through which you eventually want to pass. The door, and the manner in which it is secured to the wall, permits opening by pushing it from its 'closed' position. We say that the door affords (or allows, or is for) opening by pushing. On approaching that door you observe a flat plate fixed to it at waist height on the 'non-hinge' side, and possibly some sticky finger marks on its otherwise polished surface. You deduce that the door is meant to be pushed open: you therefore push on the plate, whereupon the door opens and you pass through. Here, there is a perceived affordance, triggered by the sight of the plate and the finger marks, that is identical with the actual affordance. Note that the affordance we discuss is neither the door nor the plate: it is a property of the door (the door affords opening by pushing)."
952,"a survey of selfmanagement in dynamic software architecture specifications",477435,"A survey of self-management in dynamic software architecture specifications","As dynamic software architecture use becomes more widespread, a variety of formal specification languages have been developed to gain a better understanding of the foundations of this type of software evolutionary change. In this paper we survey 14 formal specification approaches based on graphs, process algebras, logic, and other formalisms. Our survey will evaluate the ability of each approach to specify self-managing systems as well as the ability to address issues regarding expressiveness and scalability. Based on the results of our survey we will provide recommendations on future directions for improving the specification of dynamic software architectures, specifically self-managed architectures."
953,"citation analysis in research evaluation",477678,"Citation Analysis in Research Evaluation","This book deals with the evaluation of scholarly research performance, and focuses on the contribution of scholarly work to the advancement of scholarly knowledge. Its principal question is: how can citation analysis be used properly as a tool in the assessment of such a contribution? Citation analysis involves the construction and application of a series of indicators of the ‘impact’, ‘influence’ or ‘quality’ of scholarly work, derived from references cited in footnotes or bibliographies of scholarly research publications. It describes primarily the use of data extracted from the Science Citation Index and the Web of Science, published by the Institute for Scientific Information (ISI)/Thomson Scientific. But many aspects to which this book dedicates attention relate to citation analysis in general, It provides a wide range of important facts, and corrects a number of common misunderstandings about citation analysis. It introduces basic notions and distinctions, and deals both with theoretical and technical aspects, and with its applicability in various policy contexts, at the level of individual scholars, research groups, departments, institutions, national scholarly systems, disciplines or subfields, and scholarly journals. Although the major part of the analysis relates to the basic science – a domain in which citation analysis is used most frequently – this book also addresses its uses and limits in the applied and technical sciences, social sciences and humanities. It reveals the enormous potential of quantitative, bibliometric analyses of the scholarly literature for a deeper understanding of scholarly activity and performance, and highlights their policy relevance. But this book is also critical, underlines the limits of citation analysis in research evaluation, and issues warnings for potential misuse. It proposes criteria for proper use of citation analysis as a research evaluation tool. In order to be used properly as a research evaluation tool, it is essential that all participants have insight into the nature of citation analysis, how its indicators are constructed and calculated, what the various theoretical positions state about what they measure, and what are their potentialities and limitations, particularly in relation to peer review. This book aims at providing such insight."
954,"the population genetics of adaptation the distribution of factors fixed during adaptive evolution",478558,"The Population Genetics of Adaptation: The Distribution of Factors Fixed during Adaptive Evolution","We know very little about the genetic basis of adaptation. Indeed, we can make no theoretical predictions, however heuristic, about the distribution of phenotypic effects among factors fixed during adaptation nor about the expected 'size' of the largest factor fixed. Study of this problem requires taking into account that populations gradually approach a phenotypic optimum during adaptation via the stepwise substitution of favorable mutations. Using Fisher's geometric model of adaptation, I analyze this approach to the optimum, and derive an approximate solution to the size distribution of factors fixed during adaptation. I further generalize these results to allow the input of any distribution of mutational effects. The distribution of factors fixed during adaptation assumes a pleasingly simple, exponential form. This result is remarkably insensitive to changes in the fitness function and in the distribution of mutational effects. An exponential trend among factors fixed appears to be a general property of adaptation toward a fixed optimum."
955,"scientific reasoning the bayesian approach",478599,"Scientific Reasoning: The Bayesian Approach","{In this clearly reasoned defense of Bayes's Theorem &#151; that probability can be used to reasonably justify scientific theories &#151; Colin Howson and Peter Urbach examine the way in which scientists appeal to probability arguments, and demonstrate that the classical approach to statistical inference is full of flaws. Arguing the case for the Bayesian method with little more than basic algebra, the authors show that it avoids the difficulties of the classical system. The book also refutes the major criticisms leveled against Bayesian logic, especially that it is too subjective. This newly updated edition of this classic textbook is also suitable for college courses.}"
956,"geneways a system for extracting analyzing visualizing and integrating molecular pathway data",478739,"GeneWays: a system for extracting, analyzing, visualizing, and integrating molecular pathway data.","The immense growth in the volume of research literature and experimental data in the field of molecular biology calls for efficient automatic methods to capture and store information. In recent years, several groups have worked on specific problems in this area, such as automated selection of articles pertinent to molecular biology, or automated extraction of information using natural-language processing, information visualization, and generation of specialized knowledge bases for molecular biology. GeneWays is an integrated system that combines several such subtasks. It analyzes interactions between molecular substances, drawing on multiple sources of information to infer a consensus view of molecular networks. GeneWays is designed as an open platform, allowing researchers to query, review, and critique stored information."
957,"the organization of information second edition library and information science text series",480467,"The Organization of Information : Second Edition (Library and Information Science Text Series)","{The extensively revised and completely updated second edition of this popular textbook provides LIS practitioners and students with a vital guide to the organization of information. After a broad overview of the concept and its role in human endeavors, Taylor proceeds to a detailed and insightful discussion of such basic retrieval tools as bibliographies, catalogs, indexes, finding aids, registers, databases, major bibliographic utilities, and other organizing entities. After tracing the development of the organization of recorded information in Western civilization from 2000 B.C.E. to the present, the author addresses topics that include encoding standards (MARC, SGML, and various DTDs), metadata (description, access, and access control), verbal subject analysis including controlled vocabularies and ontologies, classification theory and methodology, arrangement and display, and system design.}"
958,"activity in prefrontal cortex during dynamic selection of action sequences",482141,"Activity in prefrontal cortex during dynamic selection of action sequences.","Completing everyday tasks often requires the execution of action sequences matched to a particular problem. To study the neural processes underlying these behaviors, we trained monkeys to produce a series of eye movements according to a sequence that changed unpredictably from one block of trials to the next. We then applied a decoding algorithm to estimate which sequence was being represented by the ensemble activity in prefrontal cortex. We found that the sequence predicted by this analysis changed gradually from the sequence that had been correct in the previous block to the sequence that was correct in the current block, closely following the fraction of executed movements that were consistent with the corresponding sequence. Thus, the neural activity dynamically tracked the monkeys' uncertainty about the correct sequence of actions. These results are consistent with prefrontal involvement in representing subjective knowledge of the correct action sequence."
959,"helix to helix packing in proteins",483203,"Helix to helix packing in proteins.","Analysis of the pattern of residue to residue contacts at the interface of 50 helix to helix packings observed in ten proteins of known structure supports a model for helix to helix packing in which the ridges and grooves on the helix surface intercalate. These ridges are formed by rows of residues whose separation in sequence is usually four, occasionally three and rarely one. The model explains the observed predominance of packings whose interhelical angle is ~ -50 [deg]. Of the 50 packings, 38 agree with the model and the general features of another ten packings are described by an extension to the model in which ridges can pack across each other if a small side-chain occurs at the place where they cross."
960,"conformational diversity of ligands bound to proteins",484900,"Conformational Diversity of Ligands Bound to Proteins","The phenomenon of molecular recognition, which underpins almost all biological processes, is dynamic, complex and subtle. Establishing an interaction between a pair of molecules involves mutual structural rearrangements guided by a highly convoluted energy landscape, the accurate mapping of which continues to elude us. Increased understanding of the degree to which the conformational space of a ligand is restricted upon binding may have important implications for docking studies, structure refinement and for function prediction methods based on geometrical comparisons of ligands or their binding sites. Here, we present an analysis of the conformational variability exhibited by three of the most ubiquitous biological ligands in nature, ATP, NAD and FAD. First, we demonstrate qualitatively that these ligands bind to proteins in widely varying conformations, including several cases in which parts of the molecule assume energetically unfavourable orientations. Next, by comparing the distribution of bound ligand shapes with the set of all possible molecular conformations, we provide a quantitative assessment of previous observations that ligands tend to unfold when binding to proteins. We show that, while extended forms of ligands are indeed common in ligand{\^a}protein structures, instances of ligands in almost maximally compact arrangements can also be found. Thirdly, we compare the conformational variation in two sets of ligand molecules, those bound to homologous proteins, and those bound to unrelated proteins. Although most superfamilies bind ligands in a fairly conserved manner, we find several cases in which significant variation in ligand configuration is observed."
961,"computational design of a single amino acid sequence that can switch between two distinct protein folds",485372,"Computational design of a single amino acid sequence that can switch between two distinct protein folds.","The functions of many proteins are mediated by specific conformational changes, and therefore the ability to design primary sequences capable of secondary and tertiary changes is an important step toward the creation of novel functional proteins. To this end, we have developed an algorithm that can optimize a single amino acid sequence for multiple target structures. The algorithm consists of an outer loop, in which sequence space is sampled by a Monte Carlo search with simulated annealing, and an inner loop, in which the effect of a given mutation is evaluated on the various target structures by using the rotamer packing routine and composite energy function of the protein design software, RosettaDesign. We have experimentally tested the method by designing a peptide, Sw2, which can be switched from a 2Cys-2His zinc finger-like fold to a trimeric coiled-coil fold, depending upon the pH or the presence of transition metals. Physical characterization of Sw2 confirms that it is able to reversibly adopt each intended target fold."
962,"globus toolkit version software for serviceoriented systems",487356,"Globus Toolkit Version 4: Software for Service-Oriented Systems","Abstract&nbsp;&nbsp;The Globus Toolkit (GT) has been developed since the late 1990s to support the development of service-oriented distributed computing applications and infrastructures. Core GT components address, within a common framework, fundamental issues relating to security, resource access, resource management, data movement, resource discovery, and so forth. These components enable a broader “Globus ecosystem” of tools and components that build on, or interoperate with, GT functionality to provide a wide range of useful application-level functions. These tools have in turn been used to develop a wide range of both “Grid” infrastructures and distributed applications. I summarize here the principal characteristics of the recent Web Services-based GT4 release, which provides significant improvements over previous releases in terms of robustness, performance, usability, documentation, standards compliance, and functionality. I also introduce the new “dev.globus” community development process, which allows a larger community to contribute to the development of Globus software."
963,"the swissmodel workspace a webbased environment for protein structure homology modelling",489193,"The SWISS-MODEL workspace: a web-based environment for protein structure homology modelling.","Motivation: Homology models of proteins are of great interest for planning and analysing biological experiments when no experimental three-dimensional structures are available. Building homology models requires specialized programs and up-to-date sequence and structural databases. Integrating all required tools, programs and databases into a single web-based workspace facilitates access to homology modelling from a computer with web connection without the need of downloading and installing large program packages and databases. Results: SWISS-MODEL workspace is a web-based integrated service dedicated to protein structure homology modelling. It assists and guides the user in building protein homology models at different levels of complexity. A personal working environment is provided for each user where several modelling projects can be carried out in parallel. Protein sequence and structure databases necessary for modelling are accessible from the workspace and are updated in regular intervals. Tools for template selection, model building and structure quality evaluation can be invoked from within the workspace. Workflow and usage of the workspace are illustrated by modelling human Cyclin A1 and human Transmembrane Protease 3. Availability: The SWISS-MODEL workspace can be accessed freely at http://swissmodel.expasy.org/workspace/ Contact: Torsten.Schwede@unibas.ch Supplementary information: Supplementary data are available at Bioinformatics online."
964,"themeriver visualizing thematic changes in large document collections",491420,"{ThemeRiver}: Visualizing Thematic Changes in Large Document Collections","The ThemeRiver visualization depicts thematic variations over time within a large collection of documents. The thematic changes are shown in the context of a time line and corresponding external events. The focus on temporal thematic change within a context framework allows a user to discern patterns that suggest relationships or trends. For example, the sudden change of thematic strength following an external event may indicate a causal relationship. Such patterns are not readily accessible in other visualizations of the data. We use a river metaphor to convey several key notions. The document collection's time line, selected thematic content, and thematic strength are indicated by the river's directed flow, composition, and changing width, respectively. The directed flow from left to right is interpreted as movement through time and the horizontal distance between two points on the river defines a time interval. At any point in time, the vertical distance, or width, of the river indicates the collective strength of the selected themes. Colored {\\grqq}currents” flowing within the river represent individual themes. A current's vertical width narrows or broadens to indicate decreases or increases in the strength of the individual theme."
965,"retrieval effectiveness of an ontologybased model for information selection",491903,"Retrieval effectiveness of an ontology-based model for information selection","Technology in the field of digital media generates huge amounts of nontextual information, audio, video, and images, along with more familiar textual information. The potential for exchange and retrieval of information is vast and daunting. The key problem in achieving efficient and user-friendly retrieval is the development of a search mechanism to guarantee delivery of minimal irrelevant information (high precision) while insuring relevant information is not overlooked (high recall). The traditional solution employs keyword-based search. The only documents retrieved are those containing user-specified keywords. But many documents convey desired semantic information without containing these keywords. This limitation is frequently addressed through query expansion mechanisms based on the statistical co-occurrence of terms. Recall is increased, but at the expense of deteriorating precision. One can overcome this problem by indexing documents according to context and meaning rather than keywords, although this requires a method of converting words to meanings and the creation of a meaning-based index structure. We have solved the problem of an index structure through the design and implementation of a concept-based model using domain-dependent ontologies. An ontology is a collection of concepts and their interrelationships that provide an abstract view of an application domain. With regard to converting words to meaning, the key issue is to identify appropriate concepts that both describe and identify documents as well as language employed in user requests. This paper describes an automatic mechanism for selecting these concepts. An important novelty is a scalable disambiguation algorithm that prunes irrelevant concepts and allows relevant ones to associate with documents and participate in query generation. We also propose an automatic query expansion mechanism that deals with user requests expressed in natural language. This mechanism generates database queries with appropriate and relevant expansion through knowledge encoded in ontology form. Focusing on audio data, we have constructed a demonstration prototype. We have experimentally and analytically shown that our model, compared to keyword search, achieves a significantly higher degree of precision and recall. The techniques employed can be applied to the problem of information selection in all media types."
966,"secrets and lies digital security in a networked world",493773,"Secrets and Lies: Digital Security in a Networked World","{Whom can you trust? Try Bruce Schneier, whose rare gift for common sense  makes his book <I>Secrets and Lies: Digital Security in a Networked World</I> both  enlightening and practical. He's worked in cryptography and electronic security  for years, and has reached the depressing conclusion that even the loveliest  code and toughest hardware still will yield to attackers who exploit human  weaknesses in the users. The book is neatly divided into three parts, covering  the turn-of-the-century landscape of systems and threats, the technologies used  to protect and intercept data, and strategies for proper implementation of  security systems. Moving away from blind faith in prevention, Schneier advocates  swift detection and response to an attack, while maintaining firewalls and  other gateways to keep out the amateurs.<p>  Newcomers to the world of Schneier will be surprised at how funny he can be,  especially given a subject commonly perceived as quiet and dull. Whether he's  analyzing the security issues of the rebels and the Death Star in <I>Star  Wars</I> or poking fun at the giant software and e-commerce companies that  consistently sacrifice security for sexier features, he's one of the few tech  writers who can provoke laughter consistently. While moderately pessimistic on  the future of systems vulnerability, he goes on to relieve the reader's tension  by comparing our electronic world to the equally insecure paper world we've  endured for centuries--a little smart-card fraud doesn't seem so bad after all.  Despite his unfortunate (but brief) shill for his consulting company in the  book's afterword, you can trust Schneier to dish the dirt in <I>Secrets and  Lies</I>. <I>--Rob Lightner</I>      } {Bestselling author Bruce Schneier offers his expert guidance on achieving  security on a network Internationally recognized computer security expert  Bruce Schneier offers a practical, straightforward guide to achieving  security throughout computer networks. Schneier uses his extensive field  experience with his own clients to dispel the myths that often mislead IT  managers as they try to build secure systems. This practical guide  provides readers with a better understanding of why protecting information is harder in the digital world, what they need to know to protect digital  information, how to assess business and corporate security needs, and much more. * Walks the reader through the real choices they have now for  digital security and how to pick and choose the right one to meet their  business needs * Explains what cryptography can and can't do in achieving  digital security  }"
967,"mining coherent dense subgraphs across massive biological networks for functional discovery",494263,"Mining coherent dense subgraphs across massive biological networks for functional discovery","Motivation: The rapid accumulation of biological network data translates into an urgent need for computational methods for graph pattern mining. One important problem is to identify recurrent patterns across multiple networks to discover biological modules. However, existing algorithms for frequent pattern mining become very costly in time and space as the pattern sizes and network numbers increase. Currently, no efficient algorithm is available for mining recurrent patterns across large collections of genome-wide networks.  Results: We developed a novel algorithm, CODENSE, to efficiently mine frequent coherent dense subgraphs across large numbers of massive graphs. Compared with previous methods, our approach is scalable in the number and size of the input graphs and adjustable in terms of exact or approximate pattern mining. Applying CODENSE to 39 co-expression networks derived from microarray datasets, we discovered a large number of functionally homogeneous clusters and made functional predictions for 169 uncharacterized yeast genes.  Availability: http://zhoulab.usc.edu/CODENSE/  Contact: xjzhou@usc.edu 10.1093/bioinformatics/bti1049"
968,"basic objects in natural categories",494558,"Basic objects in natural categories","Notes that categorizations which humans make of the concrete world are not arbitrary but highly determined; in taxonomies of concrete objects, there is 1 level of abstraction at which the most basic category cuts are made. {B}asic categories are those which carry the most information, possess the highest category cue validity, and are, thus, the most differentiated from one another. {T}he 4 experiments of {P}art 1, with approximately 502 {S}s, define basic objects by demonstrating that in taxonomies of common concrete nouns in {E}nglish based on class inclusion, basic objects are the most inclusive categories whose members (a) possess significant numbers of attributes in common, (b) have motor programs which are similar to one another, (c) have similar shapes, and (d) can be identified from averaged shapes of members of the class. {T}he 8 experiments of {P}art 2, with approximately 502 {S}s, explored implications of the structure of categories. {B}asic objects were shown to be the most inclusive categories for which a concrete image of the category as a whole could be formed, to be the 1st categorizations made during perception of the environment, to be the earliest categories sorted and earliest named by children, and to be the categories most codable, most coded, and most necessary in language."
969,"programming languages application and interpretation",495430,"Programming Languages: Application and Interpretation","Contents Preface iii Acknowledgments vii I Prelude 1 1 Modeling Languages 3 1.1 Modeling Meaning 1.2 Modeling Syntax 1.3 A Primer on Parsers 1.4 Primus Inter Parsers II Rudimentary Interpreters 11 2 Interpreting Arithmetic 13 3 Substitution 15 3.1 Defining Substitution 3.2 Calculating with with 3.3 The Scope of with Expressions 3.4 What Kind of Redundancy do Identifiers Eliminate? 3.5 Are Names Necessary? 4 An Introduction to Functions 27 4.1 Enriching the Language with Functions 4.2 The Scope of Substitution 4.3 The Scope of Function Definitions 5 Deferring Substitution 33 5.1 The Substitution Repository 5.2 Deferring Substitution Correctly ix x CONTENTS 5.3 Fixing the Interpreter 6 First-Class Functions 41 6.1 A Taxonomy of Functions 6.2 Enriching the Language with Functions 6.3 Making with Redundant 6.4 Implementing Functions using Deferred Substitutions 6.5 Some Perspective on Scope 6.5.1 Filtering and Sorting Lists 6.5.2 Differentiation 6.5.3 Callbacks 6.6 Eagerness and Laziness 6.7 Standardizing Terminology III Laziness 57 7 Programming with Laziness 59 7.1 Haskell 7.1.1 Expressions and Definitions 7.1.2 Lists 7.1.3 Polymorphic Type Inference"
970,"turbulence statistics in fully developed channel flow at low reynolds number",498337,"Turbulence statistics in fully developed channel flow at low Reynolds number","A direct numerical simulation of a turbulent channel flow is performed. The unsteady Navier-Stokes equations are solved numerically at a Reynolds number of 3300, based on the mean centreline velocity and channel half-width, with about 4 &times; 106 grid points (192 &times; 129 &times; 160 in x, y, z). All essential turbulence scales are resolved on the computational grid and no subgrid model is used. A large number of turbulence statistics are computed and compared with the existing experimental data at comparable Reynolds numbers. Agreements as well as discrepancies are discussed in detail. Particular attention is given to the behaviour of turbulence correlations near the wall. In addition, a number of statistical correlations which are complementary to the existing experimental data are reported for the first time."
971,"genomewide map of nucleosome acetylation and methylation in yeast",498573,"Genome-wide map of nucleosome acetylation and methylation in yeast.","Eukaryotic genomes are packaged into nucleosomes whose position and chemical modification state can profoundly influence regulation of gene expression. We profiled nucleosome modifications across the yeast genome using chromatin immunoprecipitation coupled with DNA microarrays to produce high-resolution genome-wide maps of histone acetylation and methylation. These maps take into account changes in nucleosome occupancy at actively transcribed genes and, in doing so, revise previous assessments of the modifications associated with gene expression. Both acetylation and methylation of histones are associated with transcriptional activity, but the former occurs predominantly at the beginning of genes, whereas the latter can occur throughout transcribed regions. Most notably, specific methylation events are associated with the beginning, middle, and end of actively transcribed genes. These maps provide the foundation for further understanding the roles of chromatin in gene expression and genome maintenance."
972,"efficient discrimination of temporal patterns by motionsensitive neurons in primate visual cortex",498972,"Efficient discrimination of temporal patterns by motion-sensitive neurons in primate visual cortex.","Although motion-sensitive neurons in macaque middle temporal (MT) area are conventionally characterized using stimuli whose velocity remains constant for 1-3 s, many ecologically relevant stimuli change on a shorter time scale (30-300 ms). We compared neuronal responses to conventional (constant-velocity) and time-varying stimuli in alert primates. The responses to both stimulus ensembles were well described as rate-modulated Poisson processes but with very high precision (approximately 3 ms) modulation functions underlying the time-varying responses. Information-theoretic analysis revealed that the responses encoded only approximately 1 bit/s about constant-velocity stimuli but up to 29 bits/s about the time-varying stimuli. Analysis of local field potentials revealed that part of the residual response variability arose from ""noise"" sources extrinsic to the neuron. Our results demonstrate that extrastriate neurons in alert primates can encode the fine temporal structure of visual stimuli."
973,"deciphering the biology of mycobacterium tuberculosis from the complete genome sequence",499778,"Deciphering the biology of Mycobacterium tuberculosis from the complete genome sequence.","Countless millions of people have died from tuberculosis, a chronic infectious disease caused by the tubercle bacillus. The complete genome sequence of the best-characterized strain of Mycobacterium tuberculosis, H37Rv, has been determined and analysed in order to improve our understanding of the biology of this slow-growing pathogen and to help the conception of new prophylactic and therapeutic interventions. The genome comprises 4,411,529 base pairs, contains around 4,000 genes, and has a very high guanine + cytosine content that is reflected in the biased amino-acid content of the proteins. M. tuberculosis differs radically from other bacteria in that a very large portion of its coding capacity is devoted to the production of enzymes involved in lipogenesis and lipolysis, and to two new families of glycine-rich proteins with a repetitive structure that may represent a source of antigenic variation."
974,"document language models query models and risk minimization for information retrieval",500745,"Document Language Models, Query Models, and Risk Minimization for Information Retrieval","We present a framework for information retrieval that combines document models and query models using a probabilistic ranking function based on Bayesian decision theory. The framework suggests an operational retrieval model that extends recent developments in the language modeling approach to information retrieval.  A language model for each document is estimated, as well as a language model for each query, and the retrieval problem is cast in terms of risk minimization. The query language model can be exploited to model user preferences, the context of a query, synonomy and word senses. While recent work has incorporated word translation models for this purpose, we introduce a new method using Markov chains defined on a set of documents to estimate the query models.  The Markov chain method has connections to algorithms from link analysis and social networks.  The new approach is evaluated on TREC collections and compared to the basic language modeling approach and vector space models together with query expansion using Rocchio.  Significant improvements are obtained over standard query expansion methods for strong baseline TF-IDF systems, with the greatest improvements attained for short queries on Web data."
975,"comparison of bayesian and maximumlikelihood inference of population genetic parameters",501140,"Comparison of Bayesian and maximum-likelihood inference of population genetic parameters.","Comparison of the performance and accuracy of different inference methods, such as maximum likelihood (ML) and Bayesian inference, is difficult because the inference methods are implemented in different programs, often written by different authors. Both methods were implemented in the program MIGRATE, that estimates population genetic parameters, such as population sizes and migration rates, using coalescence theory. Both inference methods use the same Markov chain Monte Carlo algorithm and differ from each other in only two aspects: parameter proposal distribution and maximization of the likelihood function. Using simulated datasets, the Bayesian method generally fares better than the ML approach in accuracy and coverage, although for some values the two approaches are equal in performance. MOTIVATION: The Markov chain Monte Carlo-based ML framework can fail on sparse data and can deliver non-conservative support intervals. A Bayesian framework with appropriate prior distribution is able to remedy some of these problems. RESULTS: The program MIGRATE was extended to allow not only for ML(-) maximum likelihood estimation of population genetics parameters but also for using a Bayesian framework. Comparisons between the Bayesian approach and the ML approach are facilitated because both modes estimate the same parameters under the same population model and assumptions."
976,"markov logic networks",501539,"Markov logic networks","Abstract. We propose a simple approach to combining rst-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a rst-order knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it speci es a ground Markov network containing one feature for each possible grounding of a rst-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are ef ciently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach."
977,"agentorganized networks for dynamic team formation",502008,"Agent-organized networks for dynamic team formation","Many multi-agent systems consist of a complex network of autonomous yet interdependent agents. Examples of such networked multi-agent systems include supply chains and sensor networks. In these systems, agents have a select set of other agents with whom they interact based on environmental knowledge, cognitive capabilities, resource limitations, and communications constraints. Previous findings have demonstrated that the structure of the artificial social network governing the agent interactions is strongly correlated with organizational performance. As multi-agent systems are typically embedded in dynamic environments, we wish to develop distributed, on-line network adaptation mechanisms for discovering effective network structures. Therefore, within the context of dynamic team formation, we propose several strategies for agent-organized networks (AONs) and evaluate their effectiveness for increasing organizational performance."
978,"developing efficient search strategies to identify reports of adverse effects in medline and embase",502645,"Developing efficient search strategies to identify reports of adverse effects in MEDLINE and EMBASE.","OBJECTIVE: This study aimed to assess the performance, in terms of sensitivity and precision, of different approaches to searching MEDLINE and EMBASE to identify studies of adverse effects. METHODS: Five approaches to searching for adverse effects evidence were identified: approach 1, using specified adverse effects; approach 2, using subheadings/qualifiers; approach 3, using text words; approach 4, using indexing terms; approach 5, searching for specific study designs. The sensitivity and precision of these five approaches, and combinations of these approaches, were compared in a case study using a systematic review of the adverse effects of seven anti-epileptic drugs. RESULTS: The most sensitive search strategy in MEDLINE (97.0%) required a combination of terms for specified adverse effects, floating subheadings, and text words for 'adverse effects'. In EMBASE, a combination of terms for specified adverse effects and text words for 'adverse effects' provided the most sensitive search strategy (98.6%). Both these search strategies yielded low precision (2.8%). CONCLUSIONS: A highly sensitive search in either database requires a combination of approaches, and has low precision. This suggests that better reporting and indexing of adverse effects is required and that an effective generic search filter may not yet be feasible."
979,"hints for computer system design",503602,"Hints for computer system design","Studying the design and implementation of a number of computer has led to some general hints for system design. They are described here and illustrated by many examples, ranging from hardware such as the Alto and the Dorado to application programs such as Bravo and Star. 1. Introduction Designing a computer system is very different from designing an algorithm: The external interface (that is, the requirement) is less precisely defined, more complex, and more subject to change. The system has..."
980,"selfmanaging systems a control theory foundation",503636,"Self-Managing Systems: A Control Theory Foundation","Summary form only given. The high cost of ownership of computing systems has resulted in a number of industry initiatives to reduce the burden of operations and management by making systems more self-managing. A major challenge in realizing self-managing systems is understanding how automated actions affect system behavior, especially system stability. Other disciplines such as mechanical, electrical, and aeronautical engineering make use of control theory to design feedback systems. The talk uses control theory as a way to identify a number of requirements for and challenges in building self-managing, or autonomic, systems. In essence, the autonomic computing architecture describes feedback control loops for self-managing systems. The talk has three goals: (1) educating systems oriented computer science researchers and practitioners on the concepts and techniques needed to apply control theory to computing systems; (2) describing how control theory can aid in building self-managing systems and identifying the challenges in doing so; (3) describing a deployable testbed for autonomic computing that is intended to foster research that addresses the challenges identified."
981,"principles of economics",503695,"Principles of Economics","In the beginning, there was Menger. It was this book that reformulated, and really rescued, economic science. It kicked off the Marginalist Revolution, which corrected theoretical errors of the old classical school. These errors concerned value theory, and they had sewn enough confusion to make the dangerous ideology of Marxism seem more plausible than it really was.  Menger set out to elucidate the precise nature of economic value, and root economics firmly in the real-world actions of individual human beings.  For this reason, Carl Menger (1840-1921) was the founder of the Austrian School of economics. It is the book that Mises said turned him into a real economist. What's striking is how nearly a century and a half later, the book still retains its incredible power, both in its prose and its relentless logic.  The Mises Institute's new edition features a new foreword by Peter G. Klein, which summarizes Menger's contribution and places him in the history of ideas. He also explains his continued relevance.  Economics students still say that it is the best introduction to economic logic ever written. The book also deserves the status as a seminal contribution to science in general. Truly, no one can claim to be well read in economics without having mastered Menger's argument.  328 pp. (pb)  ISBN 978-1-933550-12-1 paperback"
982,"theory of games and economic behavior",503699,"Theory of Games and Economic Behavior","This is the classic work upon which modern-day game theory is based. What began more than sixty years ago as a modest proposal that a mathematician and an economist write a short paper together blossomed, in 1944, when Princeton University Press published _Theory of Games and Economic Behavior_. In it, John von Neumann and Oskar Morgenstern conceived a groundbreaking mathematical theory of economic and social organization, based on a theory of games of strategy. Not only would this revolutionize economics, but the entirely new field of scientific inquiry it yielded--game theory--has since been widely used to analyze a host of real-world phenomena from arms races to optimal policy choices of presidential candidates, from vaccination policy to major league baseball salary negotiations. And it is today established throughout both the social sciences and a wide range of other sciences. This sixtieth anniversary edition includes not only the original text but also an introduction by Harold Kuhn, an afterword by Ariel Rubinstein, and reviews and articles on the book that appeared at the time of its original publication in the _New York Times_, tthe _American Economic Review_, and a variety of other publications. Together, these writings provide readers a matchless opportunity to more fully appreciate a work whose influence will yet resound for generations to come."
983,"globalization and its discontents",503703,"Globalization and Its Discontents","{Due to massive media coverage, many people are familiar with the controversy and organized resistance that globalization has generated around the world, yet explaining what globalization actually means in practice is a complicated task. For those wanting to learn more, this book is an excellent place to start. An experienced economist, Joseph Stiglitz had a brilliant career in academia before serving for four years on President Clinton's Council of Economic Advisors and then three years as chief economist and senior vice president of the World Bank. His book clearly explains the functions and powers of the main institutions that govern globalization--the International Monetary Fund, the World Bank, and the World Trade Organization--along with the ramifications, both good and bad, of their policies. He strongly believes that globalization can be a positive force around the world, particularly for the poor, but only if the IMF, World Bank, and WTO dramatically alter the way they operate, beginning with increased transparency and a greater willingness to examine their own actions closely. Of his time at the World Bank, he writes, ""Decisions were made on the basis of what seemed a curious blend of ideology and bad economics, dogma that sometimes seemed to be thinly veiling special interests.... Open, frank discussion was discouraged--there was no room for it."" The book is not entirely critical, however: ""Those who vilify globalization too often overlook its benefits,"" Stiglitz writes, explaining how globalization, along with foreign aid, has improved the living standards of millions around the world. With this clear and balanced book, Stiglitz has contributed significantly to the debate on this important topic. <I>--Shawn Carkonen</I> } {This powerful, unsettling book gives us a rare glimpse behind the closed doors of global financial institutions by the winner of the 2001 Nobel Prize in Economics.  <P>When it was first published, this national bestseller quickly became a touchstone in the globalization debate. Renowned economist and Nobel Prize winner Joseph E. Stiglitz had a ringside seat for most of the major economic events of the last decade, including stints as chairman of the Council of Economic Advisers and chief economist at the World Bank. Particularly concerned with the plight of the developing nations, he became increasingly disillusioned as he saw the International Monetary Fund and other major institutions put the interests of Wall Street and the financial community ahead of the poorer nations.  <P>Those seeking to understand why globalization has engendered the hostility of protesters in Seattle and Genoa will find the reasons here. While this book includes no simple formula on how to make globalization work, Stiglitz provides a reform agenda that will provoke debate for years to come. Rarely do we get such an insider's analysis of the major institutions of globalization as in this penetrating book. With a new foreword for this paperback edition.}"
984,"on the impossibility of informationally efficient markets",503720,"On the Impossibility of Informationally Efficient Markets","If competitive equilibrium is defined as a situation in which prices are such that all arbitrage profits are eliminated, it is not clear whether it is possible that a competitive economy will always be in equilibrium. Clearly not, for then those who arbitrage make no return from their costly activity. Hence the assumptions that all markets, including that for information, are always in equilibrium and always perfectly arbitraged are inconsistent when arbitrage is costly. A model has been proposed in which there is an equilibrium degree of disequilibrium: prices reflect the information of informed individuals but only partially, so that those who expend resources to obtain information do receive compensation. The model is the simplest one in which prices perform a well-articulated role in conveying information from the informed to the uninformed. When informed individuals observe information that the return to a security is going to be high, they bid its price up, and conversely when t"
985,"autoregressive conditional heteroscedasticity with estimates of the variance of united kingdom inflation",503721,"Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation","Traditional econometric models assume a constant one-period forecast variance. To generalize this implausible assumption, a new class of stochastic processes called autoregressive conditional heteroscedastic (ARCH) processes are introduced in this paper. These are mean zero, serially uncorrelated processes with nonconstant variances conditional on the past, but constant unconditional variances. For such processes, the recent past gives information about the one-period forecast variance. A regression model is then introduced with disturbances following an ARCH process. Maximum likelihood estimators are described and a simple scoring iteration formulated. Ordinary least squares maintains its optimality properties in this set-up, but maximum likelihood is more efficient. The relative efficiency is calculated and can be infinite. To test whether the disturbances follow an ARCH process, the Lagrange multiplier procedure is employed. The test is based simply on the autocorrelation of the squared OLS residuals. This model is used to estimate the means and variances of inflation in the U.K. The ARCH effect is found to be significant and the estimated variances increase substantially during the chaotic seventies."
986,"a closedform solution for options with stochastic volatility with applications to bond and currency options",503727,"A closed-form solution for options with stochastic volatility with applications to bond and currency options","I use a new technique to derive a closed-form solution for the price of a European call option on an asset with stochastic volatility. The model allows arbitrary correlation between volatility and spot-asset returns. I introduce stochastic interest rates and show how to apply the model to bond options and foreign currency options. Simulations show that correlation between volatility and the spot asset's price is important for explaining return skewness and strike-price biases in the Black-Scholes (1973) model. The solution technique is based on characteristic functions and can be applied to other problems."
987,"empirical properties of asset returns stylized facts and statistical issues",503737,"Empirical properties of asset returns: stylized facts and statistical issues","We present a set of stylized empirical facts emerging from the statistical analysis of price variations in various types of financial markets. We first discuss some general issues common to all statistical studied of financial time series. Various statistical properties of asset returns are then described: distributional properties, tail properties and extreme fluctuations, pathwise regularity, linear and nonlinear dependence of returns in time and across stocks. Our description emphasizes properties common to a wide variety of markets and instruments. We then show how these statistical properties invalidate many of the common statistical approaches used to study financial data sets and examine some of the statistical problems encountered in each case."
988,"market microstructure theory",503758,"Market Microstructure Theory","{Written by one of the leading authorities in market microstructure research, this book provides a comprehensive guide to the theoretical work in this important area of finance.After an introduction to the general issues and problems in market microstructure, the book examines the main theoretical models developed to address inventory-based issues. There is then an extensive examination and discussion of the information-based models, with particular attention paid to the linkage with rational expectations model and learning models. The concluding chapters are concerned with price dynamics and with applications of the various models to specific microstructure problems including:- Liquidity.- Multi-market trading.- Market structure.- Market Design Market Microstructure Theory includes extensive appendices developing Bayesian learning and the rational expectations framework.}"
989,"allocative efficiency of markets with zerointelligence traders market as a partial substitute for individual rationality",503787,"Allocative Efficiency of Markets with Zero-Intelligence Traders: Market as a Partial Substitute for Individual Rationality","This paper reports market experiments in which human traders are replaced by 'zero-intelligence' programs that submit random bids and offers. Imposing a budget constraint (i.e., n ot permitting traders to sell below their costs or buy above their valu es) is sufficient to raise the allocative efficiency of these auctions close to 100 percent. Allocative efficiency of a double auction deri ves largely from its structure, independent of traders' motivation, intelligence, or learning. Adam Smith's invisible hand may be more powerful than some may have thought; it can generate aggregate rationality not only from individual rationality but also from individual irrationality."
990,"growing artificial societies social science from the bottom up",503791,"Growing Artificial Societies: Social Science from the Bottom Up","How do social structures and group behaviors arise from the interaction of individuals? In this groundbreaking study, Joshua M. Epstein and Robert L. Axtell approach this age-old question with cutting-edge computer simulation techniques. Such fundamental collective behaviors as group formation, cultural transmission, combat, and trade are seen to ""emerge"" from the interaction of individual agents following simple local rules. In their computer model, Epstein and Axtell begin the development of a ""bottom up"" social science. Their program, named Sugarscape, simulates the behavior of artificial people (agents) located on a landscape of a generalized resource (sugar). Agents are born onto the Sugarscape with a vision, a metabolism, a speed, and other genetic attributes. Their movement is governed by a simple local rule: ""look around as far as you can; find the spot with the most sugar; go there and eat the sugar."" Every time an agent moves, it burns sugar at an amount equal to its metabolic rate. Agents die if and when they burn up all their sugar. A remarkable range of social phenomena emerge. For example, when seasons are introduced, migration and hibernation can be observed. Agents are accumulating sugar at all times, so there is always a distribution of wealth. Next, Epstein and Axtell attempt to grow a ""proto-history"" of civilization. It starts with agents scattered about a twin-peaked landscape; over time, there is self-organization into spatially segregated and culturally distinct ""tribes"" centered on the peaks of the Sugarscape. Population growth forces each tribe to disperse into the sugar lowlands between the mountains. There, the two tribes interact, engaging in combat and competing for cultural dominance, to produce complex social histories with violent expansionist phases, peaceful periods, and so on. The proto-history combines a number of ingredients, each of which generates insights of its own. One of these ingredients is sexual reproduction. In some runs, the population becomes thin, birth rates fall, and the population can crash. Alternatively, the agents may over-populate their environment, driving it into ecological collapse. When Epstein and Axtell introduce a second resource (spice) to the Sugarscape and allow the agents to trade, an economic market emerges. The introduction of pollution resulting from resource-mining permits the study of economic markets in the presence of environmental factors. Growing Artificial Societies is also available in CD-ROM format which includes about fifty animations that develop the scenarios described in the text. This study is part of the 2050 Project, a joint venture of the Santa Fe Institute, the World Resources Institute, and the Brookings Institution. The project is an international effort to identify conditions for a sustainable global system in the middle of the next century and to design policy actions to help achieve such a system. Robert L. Axtell is a research associate in the Brookings Foreign Policy Studies program. They are both members of the Santa Fe Institute. Joshua M. Epstein is senior fellow in the Brookings Foreign Policy Studies program and teaches at Princeton University. He is author of Strategy and Force Planning: The Case of the Persian Gulf (1987). The Calculus of Conventional War (1985), and The 1987 Defense Budget and The 1988 Defense Budget."
991,"the complexity of cooperation agentbased models of competition and collaboration",503792,"The Complexity of Cooperation: Agent—Based Models of Competition and Collaboration","Robert Axelrod is widely known for his groundbreaking work in game theory and complexity theory. He is a leader in applying computer modeling to social science problems. His book _The Evolution of Cooperation_ has been hailed as a seminal contribution and has been translated into eight languages since its initial publication. The _Complexity of Cooperation_ is a sequel to that landmark book. It collects seven essays, originally published in a broad range of journals, and adds an extensive new introduction to the collection, along with new prefaces to each essay and a useful new appendix of additional resources. Written in Axelrod's acclaimed, accessible style, this collection serves as an introductory text on complexity theory and computer modeling in the social sciences and as an overview of the current state of the art in the field.  The articles move beyond the basic paradigm of the Prisoner's Dilemma to study a rich set of issues, including how to cope with errors in perception or implementation, how norms emerge, and how new political actors and regions of shared culture can develop. They use the shared methodology of agent-based modeling, a powerful technique that specifies the rules of interaction between individuals and uses computer simulation to discover emergent properties of the social system. _The Complexity of Cooperation_ is essential reading for all social scientists who are interested in issues of cooperation and complexity"
992,"scaling and criticality in a stochastic multiagent model of a financial market",503802,"Scaling and criticality in a stochastic multi-agent model of a financial market","{Financial prices have been found to exhibit some universal characteristics1, 2, 3, 4, 5, 6 that resemble the scaling laws characterizing physical systems in which large numbers of units interact. This raises the question of whether scaling in finance emerges in a similar way — from the interactions of a large ensemble of market participants. However, such an explanation is in contradiction to the prevalent 'efficient market hypothesis'7 in economics, which assumes that the movements of financial prices are an immediate and unbiased reflection of incoming news about future earning prospects. Within this hypothesis, scaling in price changes would simply reflect similar scaling in the 'input' signals that influence them. Here we describe a multi-agent model of financial markets which supports the idea that scaling arises from mutual interactions of participants. Although the 'news arrival process' in our model lacks both power-law scaling and any temporal dependence in volatility, we find that it generates such behaviour as a result of interactions between agents.}"
993,"highresolution haplotype structure in the human genome",504229,"High-resolution haplotype structure in the human genome.","Linkage disequilibrium (LD) analysis is traditionally based on individual genetic markers and often yields an erratic, non-monotonic picture, because the power to detect allelic associations depends on specific properties of each marker, such as frequency and population history. Ideally, LD analysis should be based directly on the underlying haplotype structure of the human genome, but this structure has remained poorly understood. Here we report a high-resolution analysis of the haplotype structure across 500 kilobases on chromosome 5q31 using 103 single-nucleotide polymorphisms (SNPs) in a European-derived population. The results show a picture of discrete haplotype blocks (of tens to hundreds of kilobases), each with limited diversity punctuated by apparent sites of recombination. In addition, we develop an analytical model for LD mapping based on such haplotype blocks. If our observed structure is general (and published data suggest that it may be), it offers a coherent framework for creating a haplotype map of the human genome."
994,"the circuitry of v and v integration of color form and motion",506682,"THE CIRCUITRY OF V1 AND V2: Integration of Color, Form, and Motion","Primary and secondary visual cortex (V1 and V2) form the foundation of the cortical visual system. V1 transforms information received from the lateral geniculate nucleus (LGN) and distributes it to separate domains in V2 for transmission to higher visual areas. During the past 20 years, schemes for the functional organization of V1 and V2 have been based on a tripartite framework developed by Livingstone & Hubel (1988). Since then, new anatomical data have accumulated concerning V1's input, its internal circuitry, and its output to V2. These new data, along with physiological and imaging studies, now make it likely that the visual attributes of color, form, and motion are not neatly segregated by V1 into different stripe compartments in V2. Instead, there are just two main streams, originating from cytochrome oxidase patches and interpatches, that project to V2. Each stream is composed of a mixture of magno, parvo, and konio geniculate signals. Further studies are required to elucidate how the patches and interpatches differ in the output they convey to extrastriate cortex.ABSTRACT FROM AUTHOR"
995,"a survey on the use of pattern recognition methods for abstraction indexing and retrieval of images and video",507239,"A survey on the use of pattern recognition methods for abstraction, indexing and retrieval of images and video","The need for content-based access to image and video information from media archives has captured the attention of researchers in recent years. Research efforts have led to the development of methods that provide access to image and video data. These methods have their roots in pattern recognition. The methods are used to determine the similarity in the visual information content extracted from low level features. These features are then clustered for generation of database indices. This paper presents a comprehensive survey on the use of these pattern recognition methods which enable image and video retrieval by content."
996,"combinatorial optimization",507409,"Combinatorial Optimization","{A complete, highly accessible introduction to one of today's most exciting areas of applied mathematics<br>    <br>    One of the youngest, most vital areas of applied mathematics, combinatorial optimization integrates techniques from combinatorics, linear programming, and the theory of algorithms. Because of its success in solving difficult problems in areas from telecommunications to VLSI, from product distribution to airline crew scheduling, the field has seen a ground swell of activity over the past decade.<br>    <br>    Combinatorial Optimization is an ideal introduction to this mathematical discipline for advanced undergraduates and graduate students of discrete mathematics, computer science, and operations research. Written by a team of recognized experts, the text offers a thorough, highly accessible treatment of both classical concepts and recent results. The topics include:<br>    * Network flow problems<br>    * Optimal matching<br>    * Integrality of polyhedra<br>    * Matroids<br>    * NP-completeness<br>    <br>    Featuring logical and consistent exposition, clear explanations of basic and advanced concepts, many real-world examples, and helpful, skill-building exercises, Combinatorial Optimization is certain to become the standard text in the field for many years to come.}"
997,"generalized gradient approximation made simple",507754,"Generalized gradient approximation made simple","Generalized gradient approximations (GGA’s) for the exchange-correlation energy improve upon the local spin density (LSD) description of atoms, molecules, and solids. We present a simple derivation of a simple GGA, in which all parameters (other than those in LSD) are fundamental constants. Only general features of the detailed construction underlying the Perdew-Wang 1991 (PW91) GGA are invoked. Improvements over PW91 include an accurate description of the linear response of the uniform electron gas, correct behavior under uniform scaling, and a smoother potential."
998,"the ant system optimization by a colony of cooperating agents",511998,"The {A}nt {S}ystem: {O}ptimization by a colony of cooperating agents","An analogy with the way ant colonies function has suggested the definition of a new computational paradigm, which we call ant system (AS). We propose it as a viable new approach to stochastic combinatorial optimization. The main characteristics of this model are positive feedback, distributed computation, and the use of a constructive greedy heuristic. Positive feedback accounts for rapid discovery of good solutions, distributed computation avoids premature convergence, and the greedy heuristic helps find acceptable solutions in the early stages of the search process. We apply the proposed methodology to the classical traveling salesman problem (TSP), and report simulation results. We also discuss parameter selection and the early setups of the model, and compare it with tabu search and simulated annealing using TSP. To demonstrate the robustness of the approach, we show how the ant system (AS) can be applied to other optimization problems like the asymmetric traveling salesman, the quadratic assignment and the job-shop scheduling. Finally we discuss the salient characteristics-global data structure revision, distributed communication and probabilistic transitions of the AS"
999,"sequential minimal optimization a fast algorithm for training support vector machines",512091,"Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines","This paper proposes a new algorithm for training support vector machines: Sequential Minimal Optimization, or SMO. Training a support vector machine requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while the standard chunking SVM algorithm scales somewhere between linear and cubic in the training set size. SMO’s computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. On real- world sparse data sets, SMO can be more than 1000 times faster than the chunking algorithm."
1000,"algebraic quantum field theory",513563,"Algebraic Quantum Field Theory","Algebraic quantum field theory provides a general, mathematically precise description of the structure of quantum field theories, and then draws out consequences of this structure by means of various mathematical tools -- the theory of operator algebras, category theory, etc.. Given the rigor and generality of AQFT, it is a particularly apt tool for studying the foundations of QFT. This paper is a survey of AQFT, with an orientation towards foundational topics. In addition to covering the basics of the theory, we discuss issues related to nonlocality, the particle concept, the field concept, and inequivalent representations. We also provide a detailed account of the analysis of superselection rules by S. Doplicher, R. Haag, and J. E. Roberts (DHR); and we give an alternative proof of Doplicher and Roberts' reconstruction of fields and gauge group from the category of physical representations of the observable algebra. The latter is based on unpublished ideas due to Roberts and the abstract duality theorem for symmetric tensor *-categories, a self-contained proof of which is given in the appendix."
1001,"tapping into argumentation developments in the application of toulmins argument pattern for studying science discourse",513921,"TAPping into argumentation: Developments in the application of Toulmin's Argument Pattern for studying science discourse","Abstract 10.1002/sce.20012.abs This paper reports some methodological approaches to the analysis of argumentation discourse developed as part of the two-and-a-half year project titled “Enhancing the Quality of Argument in School Scienc'' supported by the Economic and Social Research Council in the United Kingdom. In this project researchers collaborated with middle-school science teachers to develop models of instructional activities in an effort to make argumentation a component of instruction. We begin the paper with a brief theoretical justification for why we consider argumentation to be of significance to science education. We then contextualize the use of Toulmin's Argument Pattern in the study of argumentation discourse and provide a justification for the methodological outcomes our approach generates. We illustrate how our work refines and develops research methodologies in argumentation analysis. In particular, we present two methodological approaches to the analysis of argumentation resulting in whole-class as well as small-group student discussions. For each approach, we illustrate our coding scheme and some results as well as how our methodological approach has enabled our inquiry into the quality of argumentation in the classroom. We conclude with some implications for future research in argumentation in science education. © 2004 Wiley Periodicals, Inc. Sci Ed88:915–933, 2004"
1002,"genomic scans for selective sweeps using snp data",515187,"Genomic scans for selective sweeps using SNP data.","Detecting selective sweeps from genomic SNP data is complicated by the intricate ascertainment schemes used to discover SNPs, and by the confounding influence of the underlying complex demographics and varying mutation and recombination rates. Current methods for detecting selective sweeps have little or no robustness to the demographic assumptions and varying recombination rates, and provide no method for correcting for ascertainment biases. Here, we present several new tests aimed at detecting selective sweeps from genomic SNP data. Using extensive simulations, we show that a new parametric test, based on composite likelihood, has a high power to detect selective sweeps and is surprisingly robust to assumptions regarding recombination rates and demography (i.e., has low Type I error). Our new test also provides estimates of the location of the selective sweep(s) and the magnitude of the selection coefficient. To illustrate the method, we apply our approach to data from the Seattle SNP project and to Chromosome 2 data from the HapMap project. In Chromosome 2, the most extreme signal is found in the lactase gene, which previously has been shown to be undergoing positive selection. Evidence for selective sweeps is also found in many other regions, including genes known to be associated with disease risk such as DPP10 and COL4A3."
1003,"micrornas in cell proliferation cell death and tumorigenesis",515943,"MicroRNAs in cell proliferation, cell death, and tumorigenesis.","MicroRNAs (miRNAs) are a recently discovered class of approximately 18-24 nucleotide RNA molecules that negatively regulate target mRNAs. All studied multicellular eukaryotes utilise miRNAs to regulate basic cellular functions including proliferation, differentiation, and death. It is now apparent that abnormal miRNA expression is a common feature of human malignancies. In this review, we will discuss how miRNAs influence tumorigenesis by acting as oncogenes and tumour suppressors."
1004,"designed divergent evolution of enzyme function",516625,"Designed divergent evolution of enzyme function","It is generally believed that proteins with promiscuous functions divergently evolved to acquire higher specificity and activity, and that this process was highly dependent on the ability of proteins to alter their functions with a small number of amino acid substitutions (plasticity). The application of this theory of divergent molecular evolution to promiscuous enzymes may allow us to design enzymes with more specificity and higher activity. Many structural and biochemical analyses have identified the active or binding site residues important for functional plasticity (plasticity residues). To understand how these residues contribute to molecular evolution, and thereby formulate a design methodology, plasticity residues were probed in the active site of the promiscuous sesquiterpene synthase gamma-humulene synthase. Identified plasticity residues were systematically recombined based on a mathematical model in order to construct novel terpene synthases, each catalysing the synthesis of one or a few very different sesquiterpenes. Here we present the construction of seven specific and active synthases that use different reaction pathways to produce the specific and very different products. Creation of these enzymes demonstrates the feasibility of exploiting the underlying evolvability of this scaffold, and provides evidence that rational approaches based on these ideas are useful for enzyme design."
1005,"a visual approach to proteomics",517198,"A visual approach to proteomics.","Cryo-electron tomography is an emerging imaging technique that has unique potential for molecular cell biology. At the present resolution of 4-5 nm, large supramolecular structures can be studied in unperturbed cellular environments and, in the future, it will become possible to map molecular landscapes inside cells in a more comprehensive manner. 'Visual proteomics' aims to complement and extend mass-spectrometry-based inventories, and to provide a quantitative description of the macromolecular interactions that underlie cellular functions."
1006,"structural systems biology modelling protein interactions",517200,"Structural systems biology: modelling protein interactions","Much of systems biology aims to predict the behaviour of biological systems on the basis of the set of molecules involved. Understanding the interactions between these molecules is therefore crucial to such efforts. Although many thousands of interactions are known, precise molecular details are available for only a tiny fraction of them. The difficulties that are involved in experimentally determining atomic structures for interacting proteins make predictive methods essential for progress. Structural details can ultimately turn abstract system representations into models that more accurately reflect biological reality."
1007,"cellsignalling dynamics in time and space",517203,"Cell-signalling dynamics in time and space"," The specificity of cellular responses to receptor stimulation is encoded by the spatial and temporal dynamics of downstream signalling networks. Temporal dynamics are coupled to spatial gradients of signalling activities, which guide pivotal intracellular processes and tightly regulate signal propagation across a cell. Computational models provide insights into the complex relationships between the stimuli and the cellular responses, and reveal the mechanisms that are responsible for signal amplification, noise reduction and generation of discontinuous bistable dynamics or oscillations."
1008,"a survey and comparison of peertopeer overlay network schemes",517896,"A Survey and Comparison of Peer-to-Peer Overlay Network Schemes","Over the Internet today, computing and communications environments are significantly more complex and chaotic than classical distributed systems, lacking any centralized organization or hierarchical control. There has been much interest in emerging Peer-to-Peer (P2P) network overlays because they provide a good substrate for creating large-scale data sharing, content distribution, and application-level multicast applications. These P2P overlay networks attempt to provide a long list of features, such as: selection of nearby peers, redundant storage, efficient search/location of data items, data permanence or guarantees, hierarchical naming, trust and authentication, and anonymity. P2P networks potentially offer an efficient routing architecture that is self-organizing, massively scalable, and robust in the wide-area, combining fault tolerance, load balancing, and explicit notion of locality. In this article we present a survey and comparison of various Structured and Unstructured P2P overlay networks. We categorize the various schemes into these two groups in the design spectrum, and discuss the application-level network performance of each group."
1009,"opendht a public dht service and its uses",517923,"{OpenDHT: A Public DHT Service and Its Uses}","Large-scale distributed systems are hard to deploy, and distributed hash tables (DHTs) are no exception. To lower the barriers facing DHT-based applications, we have created a public DHT service called OpenDHT. Designing a DHT that can be widely shared, both among mutually untrusting clients and among a variety of applications, poses two distinct challenges. First, there must be adequate control over storage allocation so that greedy or malicious clients do not use more than their fair share. Second, the interface to the DHT should make it easy to write simple clients, yet be sufficiently general to meet a broad spectrum of application requirements. In this paper we describe our solutions to these design challenges. We also report our early deployment experience with OpenDHT and describe the variety of applications already using the system."
1010,"cancer statistics",518657,"Cancer statistics, 2005","Each year, the American Cancer Society estimates the number of new cancer cases and deaths expected in the United States in the current year and compiles the most recent data on cancer incidence, mortality, and survival based on incidence data from the National Cancer Institute and mortality data from the National Center for Health Statistics. Incidence and death rates are age-standardized to the 2000 US standard million population. A total of 1,399,790 new cancer cases and 564,830 deaths from cancer are expected in the United States in 2006. When deaths are aggregated by age, cancer has surpassed heart disease as the leading cause of death for those younger than age 85 since 1999. Delay-adjusted cancer incidence rates stabilized in men from 1995 through 2002, but continued to increase by 0.3% per year from 1987 through 2002 in women. Between 2002 and 2003, the actual number of recorded cancer deaths decreased by 778 in men, but increased by 409 in women, resulting in a net decrease of 369, the first decrease in the total number of cancer deaths since national mortality record keeping was instituted in 1930. The death rate from all cancers combined has decreased by 1.5% per year since 1993 among men and by 0.8% per year since 1992 among women. The mortality rate has also continued to decrease for the three most common cancer sites in men (lung and bronchus, colon and rectum, and prostate) and for breast and colon and rectum cancers in women. Lung cancer mortality among women continues to increase slightly. In analyses by race and ethnicity, African American men and women have 40% and 18% higher death rates from all cancers combined than White men and women, respectively. Cancer incidence and death rates are lower in other racial and ethnic groups than in Whites and African Americans for all sites combined and for the four major cancer sites. However, these groups generally have higher rates for stomach, liver, and cervical cancers than Whites. Furthermore, minority populations are more likely to be diagnosed with advanced stage disease than are Whites. Progress in reducing the burden of suffering and death from cancer can be accelerated by applying existing cancer control knowledge across all segments of the population. 10.3322/canjclin.56.2.106"
1011,"a methodology for the structural and functional analysis of signaling and regulatory networks",518807,"A methodology for the structural and functional analysis of signaling and regulatory networks","Background Structural analysis of cellular interaction networks contributes to a deeper understanding of network-wide interdependencies, causal relationships, and basic functional capabilities. While the structural analysis of metabolic networks is a well-established field, similar methodologies have been scarcely developed and applied to signaling and regulatory networks. Results We propose formalisms and methods, relying on adapted and partially newly introduced approaches, which facilitate a structural analysis of signaling and regulatory networks with focus on functional aspects. We use two different formalisms to represent and analyze interaction networks: interaction graphs and (logical) interaction hypergraphs. We show that, in interaction graphs, the determination of feedback cycles and of all the signaling paths between any pair of species is equivalent to the computation of elementary modes known from metabolic networks. Knowledge on the set of signaling paths and feedback loops facilitates the computation of intervention strategies and the classification of compounds into activators, inhibitors, ambivalent factors, and non-affecting factors with respect to a certain species. In some cases, qualitative effects induced by perturbations can be unambiguously predicted from the network scheme. Interaction graphs however, are not able to capture AND relationships which do frequently occur in interaction networks. The consequent logical concatenation of all the arcs pointing into a species leads to Boolean networks. For a Boolean representation of cellular interaction networks we propose a formalism based on logical (or signed) interaction hypergraphs, which facilitates in particular a logical steady state analysis (LSSA). LSSA enables studies on the logical processing of signals and the identification of optimal intervention points (targets) in cellular networks. LSSA also reveals network regions whose parametrization and initial states are crucial for the dynamic behavior. We have implemented these methods in our software tool CellNetAnalyzer (successor of FluxAnalyzer) and illustrate their applicability using a logical model of T-Cell receptor signaling providing non-intuitive results regarding feedback loops, essential elements, and (logical) signal processing upon different stimuli. Conclusion The methods and formalisms we propose herein are another step towards the comprehensive functional analysis of cellular interaction networks. Their potential, shown on a realistic T-cell signaling model, makes them a promising tool."
1012,"integrality gaps of semidefinite programs for vertex cover and relations to ell embeddability of negative type metrics",518808,"Integrality gaps of semidefinite programs for Vertex Cover and relations to $\ell_1$ embeddability of Negative Type metrics","We study various SDP formulations for {\sc Vertex Cover} by adding different constraints to the standard formulation. We show that {\sc Vertex Cover} cannot be approximated better than $2-o(1)$ even when we add the so called pentagonal inequality constraints to the standard SDP formulation, en route answering an open question of Karakostas~\cite{Karakostas}. We further show the surprising fact that by strengthening the SDP with the (intractable) requirement that the metric interpretation of the solution is an $\ell_1$ metric, we get an exact relaxation (integrality gap is 1), and on the other hand if the solution is arbitrarily close to being $\ell_1$ embeddable, the integrality gap may be as big as $2-o(1)$. Finally, inspired by the above findings, we use ideas from the integrality gap construction of Charikar \cite{Char02} to provide a family of simple examples for negative type metrics that cannot be embedded into $\ell_1$ with distortion better than $8/7-\eps$. To this end we prove a new isoperimetric inequality for the hypercube."
1013,"semantic integration and retrieval of multimedia metadata",520148,"Semantic Integration and Retrieval of Multimedia Metadata","The amount of digital media that has to be actually managed has already become unaffordable without fine-grained computerised support. This requires an extensive use of multimedia metadata. MPEG-7 is the greatest metadata framework created to date but it is based on XML Schemas. Therefore, its does not have formal semantics, which makes difficult to manage, extend and integrate it. Consequently, there have been a lot attempts to move MPEG-7 to the Semantic Web. <br /> Our approach contributes a complete and automatic mapping of the whole MPEG-7 standard to OWL. It is based on a generic XML Schema to OWL mapping. The previous mapping is complemented with an XML metadata instances to RDF mapping that completes a transparent transfer of metadata from the XML to the Semantic Web domain. <br /> Once in a semantic space, data integration, which is a crucial factor when several sources of information are available, is facilitated enormously. We have used the generated MPEG-7 OWL ontology as an upper-ontology for multimedia metadata, where three different music schemas have been linked. Thus, it has been possible to retrieve related information from instances of all the metadata sources. Furthermore, detecting and merging instances from different sources has allowed us to enhance the description of audio files, both content-based and editorial data."
1014,"automatic web news extraction using tree edit distance",520460,"Automatic web news extraction using tree edit distance","The Web poses itself as the largest data repository ever available in the history of humankind. Major efforts have been made in order to provide efficient access to relevant information within this huge repository of data. Although several techniques have been developed to the problem of Web data extraction, their use is still not spread, mostly because of the need for high human intervention and the low quality of the extraction results.In this paper, we present a domain-oriented approach to Web data extraction and discuss its application to automatically extracting news from Web sites. Our approach is based on a highly efficient tree structure analysis that produces very effective results. We have tested our approach with several important Brazilian on-line news sites and achieved very precise results, correctly extracting 87.71% of the news in a set of 4088 pages distributed among 35 different sites."
1015,"mixing humans and nonhumans together the sociology of a doorcloser",523209,"Mixing Humans and Nonhumans Together: The Sociology of a Door-Closer","Is sociology the study of social questions, or is it the study of associations? In this paper the author takes the second position and extends the study of our associations to nonhumans. To make the argument clearer, the author chooses one very humble nonhuman, a door-closer, and analyzes how this ""purely"" technical artifact is a highly moral, highly social actor that deserves careful consideration. Then the author proposes a vocabulary to follow human and nonhuman relations without stopping at artificial divides between what is purely technical and what is social. The author builds ""its"" or ""his"" own text in such a way that the text itself is a machine that exemplifies several of the points made by the author. In particular, the author is constructed and deconstructed several times to show how many social actors are inscribed or prescribed by machines and automatisms."
1016,"elucidation of the small rna component of the transcriptome",523878,"Elucidation of the Small RNA Component of the Transcriptome","Small RNAs play important regulatory roles in most eukaryotes, but only a small proportion of these molecules have been identified. We sequenced more than two million small RNAs from seedlings and the inflorescence of the model plant Arabidopsis thaliana. Known and new microRNAs (miRNAs) were among the most abundant of the nonredundant set of more than 75,000 sequences, whereas more than half represented lower abundance small interfering RNAs (siRNAs) that match repetitive sequences, intergenic regions, and genes. Individual or clusters of highly regulated small RNAs were readily observed. Targets of antisense RNA or miRNA did not appear to be preferentially associated with siRNAs. Many genomic regions previously considered featureless were found to be sites of numerous small RNAs."
1017,"greedy function approximation a gradient boosting machine",525214,"Greedy Function Approximation: A gradient boosting machine","Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent “boosting” paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such “TreeBoost” models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed."
1018,"spike count reliability and the poisson hypothesis",525381,"Spike count reliability and the Poisson hypothesis.","The variability of cortical activity in response to repeated presentations of a stimulus has been an area of controversy in the ongoing debate regarding the evidence for fine temporal structure in nervous system activity. We present a new statistical technique for assessing the significance of observed variability in the neural spike counts with respect to a minimal Poisson hypothesis, which avoids the conventional but troubling assumption that the spiking process is identically distributed across trials. We apply the method to recordings of inferotemporal cortical neurons of primates presented with complex visual stimuli. On this data, the minimal Poisson hypothesis is rejected: the neuronal responses are too reliable to be fit by a typical firing-rate model, even allowing for sudden, time-varying, and trial-dependent rate changes after stimulus onset. The statistical evidence favors a tightly regulated stimulus response in these neurons, close to stimulus onset, although not further away."
1019,"preferential attachment in the growth of social networks the case of wikipedia",525472,"Preferential attachment in the growth of social networks: the case of Wikipedia","We present an analysis of the statistical properties and growth of the free on-line encyclopedia Wikipedia. By describing topics by vertices and hyperlinks between them as edges, we can represent this encyclopedia as a directed graph. The topological properties of this graph are in close analogy with that of the World Wide Web, despite the very different growth mechanism. In particular we measure a scale–invariant distribution of the in– and out– degree and we are able to reproduce these features by means of a simple statistical model. As a major consequence, Wikipedia growth can be described by local rules such as the preferential attachment mechanism, though users can act globally on the network."
1020,"the turn to technology in social studies of science",525510,"""The Turn to Technology in Social Studies of Science""","This article examines how the special theoretical significance of the sociology of scientific knowledge (SSK) is affected by attempts to apply relativist-constructivism to technology. The article shows that the failure to confront key analytic ambivalences in the practice of SSK has compromised its original strategic significance. In particular, the construal of SSK as an explanatory formula diminishes its potential for profoundly reconceptualizing epistemic issues. A consideration of critiques of technological determinism, and of some empirical studies, reveals similar analytic ambivalences in the social study of technology (SST). The injunction to consider ""technology as text"" is critically examined. It is concluded that a reflexive interpretation of this slogan is necessary to recover some of the epistemological significance lost in the constructivist move from SSK to SST. 10.1177/016224399101600102"
1021,"a bayesian computer vision system for modeling human interactions",527383,"A Bayesian Computer Vision System for Modeling Human Interactions","We describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task. The system deals in particularly with detecting when interactions between people occur and classifying the type of interaction. Examples of interesting interaction behaviors include following another person, altering one's path to meet another, and so forth. Our system combines top-down with bottom-up information in a closed feedback loop, with both components employing a statistical Bayesian approach. We propose and compare two different state-based learning architectures, namely, HMMs and CHMMs for modeling behaviors and interactions. Finally, a synthetic “Alife-style” training system is used to develop flexible prior models for recognizing human interactions. We demonstrate the ability to use these a priori models to accurately classify real human behaviors and interactions with no additional tuning or training"
1022,"ten simple rules for getting grants",527579,"Ten Simple Rules for Getting Grants","This piece follows an earlier Editorial, {\^a}Ten Simple Rules for Getting Published{\^a}{$[$}1{$]$}, which has generated significant interest, is well read, and continues to generate a variety of positive comments. That Editorial was aimed at students in the early stages of a life of scientific paper writing. This interest has prompted us to try to help scientists in making the next academic career step{\^a}becoming a young principal investigator. Leo Chalupa has joined us in putting together ten simple rules for getting grants, based on our many collective years of writing both successful and unsuccessful grants. While our grant writing efforts have been aimed mainly at United States government funding agencies, we believe the rules presented here are generic, transcending funding institutions and national boundaries."
1023,"correlationbased feature selection for machine learning",530837,"Correlation-based Feature Selection for Machine Learning","A central problem in machine learning is identifying a representative set of features from which to construct a classification model for a particular task. This thesis addresses the problem of feature selection for machine learning through a correlation based approach. The central hypothesis is that good feature sets contain features that are highly correlated with the class, yet uncorrelated with each other. A feature evaluation formula, based on ideas from test theory, provides an operational definition of this hypothesis. CFS (Correlation based Feature Selection) is an algorithm that couples this evaluation formula with an appropriate correlation measure and a heuristic search strategy. CFS was evaluated by experiments on artificial and natural datasets. Three machine learning algorithms were used: C4.5 (a decision tree learner), IB1 (an instance based learner), and naive Bayes. Experiments on artificial datasets showed that CFS quickly identifies and screens irrelevant, redundant,..."
1024,"infinite latent feature models and the indian buffet process",531107,"Infinite latent feature models and the Indian buffet process.","We define a probability distribution over equivalence classes of binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features. We derive the distribution by taking the limit of a distribution over N × K binary matrices as K goes to infinity, a strategy inspired by the derivation of the Chinese restaurant process {(Aldous,} 1985; Pitman, 2002) as the limit of a Dirichlet-multinomial model. This strategy preserves the exchangeability of the rows of matrices. We define several simple generative processes that result in the same distribution over equivalence classes of binary matrices, one of which we call the Indian buffet process. We illustrate the use of this distribution as a prior in an infinite latent feature model, deriving a Markov chain Monte Carlo algorithm for inference in this model and applying this algorithm to an artificial dataset."
1025,"seeing is believing a beginners guide to practical pitfalls in image acquisition",531261,"Seeing is believing? A beginners' guide to practical pitfalls in image acquisition.","Imaging can be thought of as the most direct of experiments. You see something; you report what you see. If only things were truly this simple. Modern imaging technology has brought about a revolution in the kinds of questions we can approach, but this comes at the price of increasingly complex equipment. Moreover, in an attempt to market competing systems, the microscopes have often been inappropriately described as easy to use and suitable for near-beginners. Insuﬀicient understanding of the experimental manipulations and equipment set-up leads to the introduction of errors during image acquisition. In this feature, I review some of the most common practical pitfalls faced by researchers during image acquisition, and how they can aﬀect the interpretation of the experimental data. This article is targeted neither to the microscopy gurus who push forward the frontiers of imaging technology nor to my imaging specialist colleagues who may wince at the overly simplistic comments and lack of detail. Instead, this is for beginners who gulp with alarm when they hear the word ""confocal pinhole"" or sigh as they watch their cells fade and die in front of their very eyes time and time again at the microscope. Take heart, beginners, if microscopes were actually so simple then many people (including myself) would suddenly be out of a job!"
1026,"choral a differential geometry approach to the prediction of the cores of protein structures",532004,"CHORAL: a differential geometry approach to the prediction of the cores of protein structures.","MOTIVATION: Although the cores of homologous proteins are relatively well conserved, amino acid substitutions lead to significant differences in the structures of divergent superfamilies. Thus, the classification of amino acid sequence patterns and the selection of appropriate fragments of the protein cores of homologues of known structure are important for accurate comparative modelling. RESULTS: CHORAL utilizes a knowledge-based method comprising an amalgam of differential geometry and pattern recognition algorithms to identify conserved structural patterns in homologous protein families. Propensity tables are used to classify and to select patterns that most likely represent the structure of the core for a target protein. In our benchmark, CHORAL demonstrates a performance equivalent to that of MODELLER. AVAILABILITY: The algorithm is available via internet on http://www-cryst.bioc.cam.ac.uk/servers.html CONTACT: rinaldo@cryst.bioc.cam.ac.uk."
1027,"toward integrating feature selection algorithms for classification and clustering",539713,"Toward Integrating Feature Selection Algorithms for Classification and Clustering","This paper introduces concepts and algorithms of feature selection, surveys existing feature selection algorithms for classification and clustering, groups and compares different algorithms with a categorizing framework based on search strategies, evaluation criteria, and data mining tasks, reveals unattempted combinations, and provides guidelines in selecting feature selection algorithms. With the categorizing framework, we continue our efforts toward-building an integrated system for intelligent feature selection. A unifying platform is proposed as an intermediate step. An illustrative example is presented to show how existing feature selection algorithms can be integrated into a meta algorithm that can take advantage of individual algorithms. An added advantage of doing so is to help a user employ a suitable algorithm without knowing details of each algorithm. Some real-world applications are included to demonstrate the use of feature selection in data mining. We conclude this work by identifying trends and challenges of feature selection research and development."
1028,"primate motor cortex and free arm movements to visual targets in threedimensional space i relations between single cell discharge and direction of movement",540510,"Primate motor cortex and free arm movements to visual targets in three-dimensional space. I. Relations between single cell discharge and direction of movement.","We describe the relations between the neuronal activity in primate motor cortex and the direction of arm movement in three-dimensional (3-D) space. The electrical signs of discharge of 568 cells were recorded while monkeys made movements of equal amplitude from the same starting position to 8 visual targets in a reaction time task. The layout of the targets in 3-D space was such that the direction of the movement ranged over the whole 3-D directional continuum in approximately equal angular intervals. We found that the discharge rate of 475/568 (83.6%) cells varied in an orderly fashion with the direction of movement: discharge rate was highest with movements in a certain direction (the cell's ""preferred direction"") and decreased progressively with movements in other directions, as a function of the cosine of the angle formed by the direction of the movement and the cell's preferred direction. The preferred directions of different cells were distributed throughout 3-D space. These findings generalize to 3-D space previous results obtained in 2-D space (Georgopoulos et al., 1982) and suggest that the motor cortex is a nodal point in the construction of patterns of output signals specifying the direction of arm movement in extrapersonal space."
1029,"managing update conflicts in bayou a weakly connected replicated storage system",541415,"Managing Update Conflicts in Bayou, a Weakly Connected Replicated Storage System","Bayou is a replicated, weakly consistent storage system designed for a mobile computing environment that includes portable machines with less than ideal network connectivity. To maximize availability, users can read and write any accessible replica. Bayou's design has focused on supporting apphcation-specific mechanisms to detect and resolve the update conflicts that naturally arise in such a system, ensuring that replicas move towards eventual consistency, and defining a protocol by which the..."
1030,"consensus on transaction commit",541533,"{C}onsensus on {Transaction} {Commit}","The distributed transaction commit problem requires reaching agreement on whether a transaction is committed or aborted. The classic Two-Phase Commit protocol blocks if the coordinator fails. Fault-tolerant consensus algorithms also reach agreement, but do not block whenever any majority of the processes are working. The Paxos Commit algorithm runs a Paxos consensus algorithm on the commit/abort decision of each participant to obtain a transaction commit protocol that uses 2 F  &plus; 1 coordinators and makes progress if at least  F  &plus; 1 of them are working properly. Paxos Commit has the same stable-storage write delay, and can be implemented to have the same message delay in the fault-free case as Two-Phase Commit, but it uses more messages. The classic Two-Phase Commit algorithm is obtained as the special  F  &equals; 0 case of the Paxos Commit algorithm."
1031,"how many clusters which clustering method answers via modelbased cluster analysis",543355,"How Many Clusters? Which Clustering Method? Answers Via Model-Based Cluster Analysis","We consider the problem of determining the structure of clustered data, without prior knowledge of the number of clusters or any other information about their composition. Data are represented by a mixture model in which each component corresponds to a different cluster. Models with varying geometric properties are obtained through Gaussian components with different parametrizations and cross-cluster constraints. Noise and outliers can be modelled by adding a Poisson process component. Partitions are determined by the expectation-maximization (EM) algorithm for maximum likelihood, with initial values from agglomerative hierarchical clustering. Models are compared using an approximation to the Bayes factor based on the Bayesian information criterion (BIC); unlike significance tests, this allows comparison of more than two models at the same time, and removes the restriction that the models compared be nested. The problems of determining the number of clusters and the clustering method are solved simultaneously by choosing the best model. Moreover, the EM result provides a measure of uncertainty about the associated classification of each data point. Examples are given, showing that this approach can give performance that is much better than standard procedures, which often fail to identify groups that are either overlapping or of varying sizes and shapes. 10.1093/comjnl/41.8.578"
1032,"a functional imaging study of cooperation in twoperson reciprocal exchange",546174,"A functional imaging study of cooperation in two-person reciprocal exchange.","Cooperation between individuals requires the ability to infer each other's mental states to form shared expectations over mutual gains and make cooperative choices that realize these gains. From evidence that the ability for mental state attribution involves the use of prefrontal cortex, we hypothesize that this area is involved in integrating theory-of-mind processing with cooperative actions. We report data from a functional MRI experiment designed to test this hypothesis. Subjects in a scanner played standard two-person ""trust and reciprocity"" games with both human and computer counterparts for cash rewards. Behavioral data shows that seven subjects consistently attempted cooperation with their human counterpart. Within this group prefrontal regions are more active when subjects are playing a human than when they are playing a computer following a fixed (and known) probabilistic strategy. Within the group of five noncooperators, there are no significant differences in prefrontal activation between computer and human conditions."
1033,"an improved map of conserved regulatory sites for saccharomyces cerevisiae",546717,"An improved map of conserved regulatory sites for Saccharomyces cerevisiae","BACKGROUND: The regulatory map of a genome consists of the binding sites for proteins that determine the transcription of nearby genes. An initial regulatory map for S. cerevisiae was recently published using six motif discovery programs to analyze genome-wide chromatin immunoprecipitation data for 203 transcription factors. The programs were used to identify sequence motifs that were likely to correspond to the DNA-binding specificity of the immunoprecipitated proteins. We report improved versions of two conservation-based motif discovery algorithms, PhyloCon and Converge. Using these programs, we create a refined regulatory map for S. cerevisiae by reanalyzing the same chromatin immunoprecipitation data. RESULTS: Applying the same conservative criteria that were applied in the original study, we find that PhyloCon and Converge each separately discover more known specificities than the combination of all six programs in the previous study. Combining the results of PhyloCon and Converge, we discover significant sequence motifs for 36 transcription factors that were previously missed. The new set of motifs identifies 636 more regulatory interactions than the previous one. The new network contains 28% more regulatory interactions among transcription factors, evidence of greater cross-talk between regulators. CONCLUSION: Combining two complementary computational strategies for conservation-based motif discovery improves the ability to identify the specificity of transcriptional regulators from genome-wide chromatin immunoprecipitation data. The increased sensitivity of these methods significantly expands the map of yeast regulatory sites without the need to alter any of the thresholds for statistical significance. The new map of regulatory sites reveals a more elaborate and complex view of the yeast genetic regulatory network than was observed previously."
1034,"interpretation as abduction",548293,"Interpretation as abduction","Abduction is inference to the best explanation. In the TACITUS project at SRI we have developed an approach to abductive inference, called “weighted abduction”, that has resulted in a significant simplification of how the problem of interpreting texts is conceptualized. The interpretation of a text is the minimal explanation of why the text would be true. More precisely, to interpret a text, one must prove the logical form of the text from what is already mutually known, allowing for coercions, merging redundancies where possible, and making assumptions where necessary. It is shown how such “local pragmatics” problems as reference resolution, the interpretation of compound nominals, the resolution of syntactic ambiguity and metonymy, and schema recognition can be solved in this manner. Moreover, this approach of “interpretation as abduction” can be combined with the older view of “parsing as deduction” to produce an elegant and thorough integration of syntax, semantics, and pragmatics, one that spans the range of linguistic phenomena from phonology to discourse structure. Finally, we discuss means for making the abduction process efficient, possibilities for extending the approach to other pragmatics phenomena, and the semantics of the weights and costs in the abduction scheme."
1035,"in quest of an empirical potential for protein structure prediction",549085,"In quest of an empirical potential for protein structure prediction.","Key to successful protein structure prediction is a potential that recognizes the native state from misfolded structures. Recent advances in empirical potentials based on known protein structures include improved reference states for assessing random interactions, sidechain-orientation-dependent pair potentials, potentials for describing secondary or supersecondary structural preferences and, most importantly, optimization protocols that sculpt the energy landscape to enhance the correlation between native-like features and the energy. Improved clustering algorithms that select native-like structures on the basis of cluster density also resulted in greater prediction accuracy. For template-based modeling, these advances allowed improvement in predicted structures relative to their initial template alignments over a wide range of target-template homology. This represents significant progress and suggests applications to proteome-scale structure prediction."
1036,"simulation for the social scientist",549645,"Simulation for the Social Scientist","What can computer simulation contribute to the social sciences? Which of the many approaches to simulation would be best for my social science project? How do I design, carry out and analyse the results from a computer simulation? Interest in social simulation has been growing rapidly worldwide as a result of increasingly powerful hardware and software and a rising interest in the application of ideas of complexity, evolution, adaptation and chaos in the social sciences. Simulation for the Social Scientist is a practical textbook on the techniques of building computer simulations to assist understanding of social and economic issues and problems. This authoritative book details all the common approaches to social simulation to provide social scientists with an appreciation of the literature and allow those with some programming skills to create their own simulations. New for this edition: A new chapter on designing multi-agent systems to support the fact that multi-agent modelling has become the most common approach to simulation New examples and guides to current software Updated throughout to take new approaches into account The book is an essential tool for social scientists in a wide range of fields, particularly sociology, economics, anthropology, geography, organizational theory, political science, social policy, cognitive psychology and cognitive science. It will also appeal to computer scientists interested in distributed artificial intelligence, multi-agent systems and agent technologies."
1037,"a theory of shape by space carving",551078,"A Theory of Shape by Space Carving","Abstract. In this paper we consider the problem of computing the 3D shape of an unknown, arbitrarily-shaped scene from multiple photographs taken at known but arbitrarily-distributed viewpoints. By studying the equivalence class of all 3D shapes that reproduce the input photographs, we prove the existence of a special member of this class, the photo hull, that (1) can be computed directly from photographs of the scene, and (2) subsumes all other members of this class. We then give a provably-correct algorithm, called Space Carving, for computing this shape and present experimental results on complex real-world scenes. The approach is designed to (1) capture photorealistic shapes that accurately model scene appearance from a wide range of viewpoints, and (2) account for the complex interactions between occlusion, parallax, shading, and their view-dependent effects on scene-appearance."
1038,"the design space of wireless sensor networks",551462,"The Design Space of Wireless Sensor Networks","In the recent past, wireless sensor networks have found their way into a wide variety of applications and systems with vastly varying requirements and characteristics. As a consequence, it is becoming increasingly difficult to discuss typical requirements regarding hardware issues and software support. This is particularly problematic in a multidisciplinary research area such as wireless sensor networks, where close collaboration between users, application domain experts, hardware designers, and software developers is needed to implement efficient systems. In this article we discuss the consequences of this fact with regard to the design space of wireless sensor networks by considering its various dimensions. We justify our view by demonstrating that specific existing applications occupy different points in the design space."
1039,"automated discovery of d motifs for protein function annotation",554015,"Automated discovery of 3D motifs for protein function annotation.","Motivation: Function inference from structure is facilitated by the use of patterns of residues (3D motifs), normally identified by expert knowledge, that correlate with function. As an alternative to often limited expert knowledge, we use machine-learning techniques to identify patterns of 3-10 residues that maximize function prediction. This approach allows us to test the assumption that residues that provide function are the most informative for predicting function. Results: We apply our method, GASPS, to the haloacid dehalogenase, enolase, amidohydrolase and crotonase superfamilies and to the serine proteases. The motifs found by GASPS are as good at function prediction as 3D motifs based on expert knowledge. The GASPS motifs with the greatest ability to predict protein function consist mainly of known functional residues. However, several residues with no known functional role are equally predictive. For four groups, we show that the predictive power of our 3D motifs is comparable with or better than approaches that use the entire fold (Combinatorial-Extension) or sequence profiles (PSI-BLAST). Availability: Source code is freely available for academic use by contacting the authors. Contact: babbitt@cgl.ucsf.edu Supplementary information: Supplementary data are available at Bioinformatics online."
1040,"practical reinforcement learning in continuous spaces",556109,"Practical Reinforcement Learning in Continuous Spaces","Dynamic control tasks are good candidates for the application of reinforcement learning techniques. However, many of these tasks inherently have continuous state or action variables. This can cause problems for traditional reinforcement learning algorithms which assume discrete states and actions. In this paper, we introduce an algorithm that safely approximates the value function for continuous state control tasks, and that learns quickly from a small amount of data. We give experimental..."
1041,"phyme a probabilistic algorithm for finding motifs in sets of orthologous sequences",556147,"PhyME: A probabilistic algorithm for finding motifs in sets of orthologous sequences","BACKGROUND: This paper addresses the problem of discovering transcription factor binding sites in heterogeneous sequence data, which includes regulatory sequences of one or more genes, as well as their orthologs in other species. RESULTS: We propose an algorithm that integrates two important aspects of a motif's significance - overrepresentation and cross-species conservation - into one probabilistic score. The algorithm allows the input orthologous sequences to be related by any user-specified phylogenetic tree. It is based on the Expectation-Maximization technique, and scales well with the number of species and the length of input sequences. We evaluate the algorithm on synthetic data, and also present results for data sets from yeast, fly, and human. CONCLUSIONS: The results demonstrate that the new approach improves motif discovery by exploiting multiple species information."
1042,"opportunistic data structures with applications",556200,"Opportunistic Data Structures with Applications","There is an upsurging interest in designing succinct data structures for basic searching problems (see [Munro99] and references therein). The motivation has to be found in the exponential increase of electronic data nowadays available which is even surpassing the significant increase in memory and disk storage capacities of current computers. Space reduction is an attractive issue because it is also intimately related to performance improvements as noted by several authors (e.g. Knuth [knuth3], Bentley [bentley]). In designing these implicit data structures the goal is to reduce as much as possible the auxiliary information kept together with the input data without introducing a significant slowdown in the final query performance. Yet input data are represented in their entirety thus taking no advantage of possible repetitiveness into them. The importance of those issues is well known to programmers who typically use various tricks to squeeze data as much as possible and still achieve good query performance. Their approaches, thought, boil down to heuristics whose effectiveness is witnessed only by experimentation. In this paper, we address the issue of compressing and indexing data by studying it in a theoretical framework. We devise a novel data structure for indexing and searching whose space occupancy is a function of the entropy of the underlying data set. The novelty resides in the careful combination of a compression algorithm, proposed by Burrows-Wheeler [bw], with the structural properties of a well known indexing tool, the Suffix Array [MM93]. We call the data structure opportunistic since its space occupancy is decreased when the input is compressible at no significant slowdown in the query performance and without any assumption on a particular fixed distribution. More precisely, its space occupancy is optimal in a information-content sense because a text $T[1,u]$ is stored using $O(k(T)) + o(1)$ bits per input symbol, where $k(T)$ is the $k$th order entropy of $T$ (the bound holds for any fixed $k$). Given an arbitrary string $P[1,p]$, the opportunistic data structure allows to search for the $occ$ occurrences of $P$ in $T$ requiring $O(p + occ \log^\epsilon u)$ time complexity (for any fixed $\epsilon &lt;0$). If data are non compressible, then we achieve the best space bound currently known [GV00]; otherwise our solution improves the succinct suffix array in [GV00] and the classical suffix tree and suffix array data structures either in space or in query time complexity or both. It was a belief [Witten:1999:MGC] that some space overhead should be paid to use full-text indices (i.e. suffix trees or suffix arrays) with respect to the word-based indices (i.e. inverted lists). The results in this paper show that no space overhead is needed at all, and as an application we improve space and query time complexity of the well-known Glimpse tool [glimpse]. We finally investigate the modifiability of our opportunistic data structure by studying"
1043,"molecular evolution of foxp a gene involved in speech and language",556513,"Molecular evolution of FOXP2, a gene involved in speech and language.","Language is a uniquely human trait likely to have been a prerequisite for the development of human culture. The ability to develop articulate speech relies on capabilities, such as fine control of the larynx and mouth(1), that are absent in chimpanzees and other great apes. FOXP2 is the first gene relevant to the human ability to develop language(2). A point mutation in FOXP2 co-segregates with a disorder in a family in which half of the members have severe articulation difficulties accompanied by linguistic and grammatical impairment(3). This gene is disrupted by translocation in an unrelated individual who has a similar disorder. Thus, two functional copies of FOXP2 seem to be required for acquisition of normal spoken language. We sequenced the complementary DNAs that encode the FOXP2 protein in the chimpanzee, gorilla, orang-utan, rhesus macaque and mouse, and compared them with the human cDNA. We also investigated intraspecific variation of the human FOXP2 gene. Here we show that human FOXP2 contains changes in amino-acid coding and a pattern of nucleotide polymorphism, which strongly suggest that this gene has been the target of selection during recent human evolution."
1044,"extensive gene traffic on the mammalian x chromosome",556578,"Extensive Gene Traffic on the Mammalian X Chromosome","Mammalian sex chromosomes have undergone profound changes since evolving from ancestral autosomes. By examining retroposed genes in the human and mouse genomes, we demonstrate that, during evolution, the mammalian X chromosome has generated and recruited a disproportionately high number of functional retroposed genes, whereas the autosomes experienced lower gene turnover. Most autosomal copies originating from X-linked genes exhibited testis-biased expression. Such export is incompatible with mutational bias and is likely driven by natural selection to attain male germline function. However, the excess recruitment is consistent with a combination of both natural selection and mutational bias."
1045,"electrostatics calculations latest methodological advances",556694,"Electrostatics calculations: latest methodological advances","Electrostatics plays a major role in the stabilization and function of biomolecules; as such, it remains a major focus of theoretical and computational studies of macromolecules. Electrostatic interactions are long range, and strongly dependent on the solvent and ions surrounding the biomolecule under study. During the past year, progress has been reported in the treatment of electrostatics in explicit and implicit solvent models. Interesting new developments of explicit solvent models include more efficient Ewald summation methods, as well as alternative approaches based on reaction field theory, periodic images and Euler summations. Implicit solvent models remain divided into those that solve the Poisson{\^a}Boltzmann equation numerically and those based on the generalized Born formalism. Both approaches are now included in molecular dynamics simulations and their accuracies may be assessed by direct comparison against experimental data. It is worth mentioning the recent development of web interfaces that facilitate access to and usage of existing tools for computing electrostatic interactions."
1046,"biological imaging by soft xray diffraction microscopy",557078,"Biological imaging by soft x-ray diffraction microscopy","10.1073/pnas.0503305102 We have used the method of x-ray diffraction microscopy to image the complex-valued exit wave of an intact and unstained yeast cell. The images of the freeze-dried cell, obtained by using 750-eV x-rays from different angular orientations, portray several of the cell's major internal components to 30-nm resolution. The good agreement among the independently recovered structures demonstrates the accuracy of the imaging technique. To obtain the best possible reconstructions, we have implemented procedures for handling noisy and incomplete diffraction data, and we propose a method for determining the reconstructed resolution. This work represents a previously uncharacterized application of x-ray diffraction microscopy to a specimen of this complexity and provides confidence in the feasibility of the ultimate goal of imaging biological specimens at 10-nm resolution in three dimensions. ER -  "
1047,"genomic insights that advance the species definition for prokaryotes",557082,"Genomic insights that advance the species definition for prokaryotes","To help advance the species definition for prokaryotes, we have compared the gene content of 70 closely related and fully sequenced bacterial genomes to identify whether species boundaries exist, and to determine the role of the organism's ecology on its shared gene content. We found the average nucleotide identity (ANI) of the shared genes between two strains to be a robust means to compare genetic relatedness among strains, and that ANI values of approximately 94\\ corresponded to the traditional 70% DNA-DNA reassociation standard of the current species definition. At the 94% ANI cutoff, current species includes only moderately homogeneous strains, e.g., most of the >4-Mb genomes share only 65-90% of their genes, apparently as a result of the strains having evolved in different ecological settings. Furthermore, diagnostic genetic signatures (boundaries) are evident between groups of strains of the same species, and the intergroup genetic similarity can be as high as 98-99% ANI, indicating that justifiable species might be found even among organisms that are nearly identical at the nucleotide level. Notably, a large fraction, e.g., up to 65%, of the differences in gene content within species is associated with bacteriophage and transposase elements, revealing an important role of these elements during bacterial speciation. Our findings are consistent with a definition for species that would include a more homogeneous set of strains than provided by the current definition and one that considers the ecology of the strains in addition to their evolutionary distance."
1048,"relevance feedback a power tool for interactive contentbased image retrieval",557230,"Relevance feedback: a power tool for interactive content-based image retrieval","Content-based image retrieval (CBIR) has become one of the most active research areas in the past few years. Many visual feature representations have been explored and many systems built. While these research efforts establish the basis of CBIR, the usefulness of the proposed approaches is limited. Specifically, these efforts have relatively ignored two distinct characteristics of CBIR systems: 1) the gap between high-level concepts and low-level features, and 2) subjectivity of human perception of visual content. This paper proposes a relevance feedback based interactive retrieval approach, which effectively takes into account the above two characteristics in CBIR. During the retrieval process, the user's high-level query and perception subjectivity are captured by dynamically updated weights based on the user's feedback. The experimental results over more than 70 000 images show that the proposed approach greatly reduces the user's effort of composing a query, and captures the user's information need more precisely. © 1998 IEEE."
1049,"low rates of expression profile divergence in highly expressed genes and tissuespecific genes during mammalian evolution",558931,"Low rates of expression profile divergence in highly expressed genes and tissue-specific genes during mammalian evolution.","Evolutionary rates provide important information about the pattern and mechanism of evolution. Although the rate of gene sequence evolution has been well studied, the rate of gene expression evolution is poorly understood. In particular, it is unclear whether the gene expression level and tissue specificity influence the divergence of expression profiles between orthologous genes. Here we address this question using a microarray data set comprising the expression signals of 10,607 pairs of orthologous human and mouse genes from over 60 tissues per species. We show that the level of gene expression and the degree of tissue specificity are generally conserved between the human and mouse orthologs. The rate of gene expression profile change during evolution is negatively correlated with the level of gene expression, measured by either the average or the highest level among all tissues examined. This is analogous to the observation that the rate of gene (or protein) sequence evolution is negatively correlated with the gene expression level. The impacts of the degree of tissue specificity on the evolutionary rate of gene sequence and that of expression profile, however, are opposite. Highly tissue-specific genes tend to evolve rapidly at the gene sequence level but slowly at the expression profile level. Thus, different forces and selective constraints must underlie the evolution of gene sequence and that of gene expression."
1050,"practical network coding",559470,"{Practical Network Coding}","We propose a distributed scheme for practical network coding that obviates  the need for centralized knowledge of the graph topology, the encoding functions,  and the decoding functions, and furthermore obviates the need for information to  be communicated synchronously through the network. The result is a practical  system for network coding that is robust to random packet loss and delay as well  as robust to any changes in the network topology or capacity due to joins, leaves,  node or link failures, congestion, and so on. We simulate such a practical network  coding system using the network topologies of several commercial Internet Service  Providers, and demonstrate that it can achieve close to the theoretically optimal  performance."
1051,"play between worlds exploring online game culture",559842,"Play Between Worlds : Exploring Online Game Culture","{In <i>Play Between Worlds</i>, T. L. Taylor examines multiplayer gaming life as it is lived on the borders, in the gaps--as players slip in and out of complex social networks that cross online and offline space. Taylor questions the common assumption that playing computer games is an isolating and alienating activity indulged in by solitary teenage boys. Massively multiplayer online games (MMOGs), in which thousands of players participate in a virtual game world in real time, are in fact actively designed for sociability. Games like the popular <i>Everquest</i>, she argues, are fundamentally social spaces. 	<br /> <br /> Taylor's detailed look at <i>Everquest</i> offers a snapshot of multiplayer culture. Drawing on her own experience as an <i>Everquest</i> player (as a female Gnome Necromancer)--including her attendance at an <i>Everquest</i> Fan Faire, with its blurring of online-and offline life--and extensive research, Taylor not only shows us something about games but raises broader cultural issues. She considers ""power gamers,"" who play in ways that seem closer to work, and examines our underlying notions of what constitutes play--and why play sometimes feels like work and may even be painful, repetitive, and boring. She looks at the women who play <i>Everquest</i> and finds they don't fit the narrow stereotype of women gamers, which may cast into doubt our standardized and preconceived ideas of femininity. And she explores the questions of who owns game space--what happens when emergent player culture confronts the major corporation behind the game.}"
1052,"ip over pp enabling selfconfiguring virtual ip networks for grid computing",561425,"IP over P2P: Enabling Self-configuring Virtual IP Networks for Grid Computing","Abstract — Peer-to-peer (P2P) networks have mostly focused on task oriented networking, where networks are constructed for single applications, i.e. file-sharing, DNS caching, etc. In this work, we introduce IPOP, a system for creating virtual IP networks on top of a P2P overlay. IPOP enables seamless access to Grid resources spanning multiple domains by aggregating them into a virtual IP network that is completely isolated from the physical network. The virtual IP network provided by IPOP supports deployment of existing IP-based protocols over a robust, self-configuring P2P overlay. We present implementation details as well as experimental measurement results taken from LAN, WAN, and Planet-Lab tests. I."
1053,"what are communities of practice a comparative review of four seminal works",561613,"What are Communities of Practice? A Comparative Review of Four Seminal Works","10.1177/0165551505057016 This paper is a comparative review of four seminal works on communities of practice.                 It is argued that the ambiguities of the terms community and practice are a source                 of the concept's reusability allowing it to be reappropriated for different                 purposes, academic and practical. However, it is potentially confusing that the                 works differ so markedly in their conceptualizations of community, learning, power                 and change, diversity and informality. The three earlier works are underpinned by a                 common epistemological view, but Lave and Wenger's 1991 short monograph is                 often read as primarily about the socialization of newcomers into knowledge by a                 form of apprenticeship, while the focus in Brown and Duguid's article of                 the same year is, in contrast, on improvising new knowledge in an interstitial group                 that forms in resistance to management. Wenger's 1998 book treats                 communities of practice as the informal relations and understandings that develop in                 mutual engagement on an appropriated joint enterprise, but his focus is the impact                 on individual identity. The applicability of the concept to the heavily                 individualized and tightly managed work of the twenty-first century is questionable.                 The most recent work by Wenger â this time with McDermott and Snyder as                 coauthors â marks a distinct shift towards a managerialist stance. The                 proposition that managers should foster informal horizontal groups across                 organizational boundaries is in fact a fundamental redefinition of the concept.                 However it does identify a plausible, if limited, knowledge management (KM) tool.                 This paper discusses different interpretations of the idea of                 'co-ordinating' communities of practice as a management ideology                 of empowerment."
1054,"placental mammal diversification and the cretaceoustertiary boundary",562684,"Placental mammal diversification and the Cretaceous–Tertiary boundary","Competing hypotheses for the timing of the placental mammal radiation focus on whether extant placental orders originated and diversified before or after the Cretaceous-Tertiary (K/T) boundary. Molecular studies that have addressed this issue suffer from single calibration points, unwarranted assumptions about the molecular clock, and/or taxon sampling that lacks representatives of all placental orders. We investigated this problem using the largest available molecular data set for placental mammals, which includes segments of 19 nuclear and three mitochondrial genes for representatives of all extant placental orders. We used the Thorne/Kishino method, which permits simultaneous constraints from the fossil record and allows rates of molecular evolution to vary on different branches of a phylogenetic tree. Analyses that used different sets of fossil constraints, different priors for the base of Placentalia, and different data partitions all support interordinal divergences in the Cretaceous followed by intraordinal diversification mostly after the K/T boundary. Four placental orders show intraordinal diversification that predates the K/T boundary, but only by an average of 10 million years. In contrast to some molecular studies that date the rat–mouse split as old as 46 million years, our results show improved agreement with the fossil record and place this split at 16–23 million years. To test the hypothesis that molecular estimates of Cretaceous divergence times are an artifact of increased body size subsequent to the K/T boundary, we also performed analyses with a “K/T body size” taxon set. In these analyses, interordinal splits remained in the Cretaceous."
1055,"economic analysis of social interactions",562885,"Economic Analysis of Social Interactions","This article discusses the need to broaden empirical analysis of the economics of social interactions in the U.S. in August 2000. Economists have long been ambivalent about what social interactions constitute the proper domain of the discipline. The narrower view has been that economics is primarily the study of markets, a circumscribed class of institutions in which persons interact through an anonymous process of price formation. The broader view has been that economics is defined fundamentally by its concern with the allocation of resources and by its emphasis on the idea that people respond to incentives. In this view, economists may properly study how incentives shape all social interactions that affect the allocation of resources. The weak state of empirical research on social interactions should be a matter of concern both to economists with a policy focus and those with a theoretical focus. Economic theorists need to know what classes of social interactions are prevalent in the real world. Otherwise, theory risks becoming only a self-contained exercise in mathematical logic."
1056,"food webs",563622,"Food Webs","{<div>Food webs are diagrams depicting which species interact or in other words, who eats whom. An understanding of the structure and function of food webs is crucial for any study of how an ecosystem works, including attempts to predict which communities might be more vulnerable to disturbance and therefore in more immediate need of conservation.<br><br>Although it was first published twenty years ago, Stuart Pimm's <i>Food Webs</i> remains the clearest introduction to the study of food webs. Reviewing various hypotheses in the light of theoretical and empirical evidence, Pimm shows that even the most complex food webs follow certain patterns and that those patterns are shaped by a limited number of biological processes, such as population dynamics and energy flow. Pimm provides a variety of mathematical tools for unravelling these patterns and processes, and demonstrates their application through concrete examples. For this edition, he has written a new foreword covering recent developments in the study of food webs and demonstrates their continuing importance to conservation biology.<br><br><br></div>}"
1057,"butterflies and plants a study in coevolution",563914,"Butterflies and Plants: A Study in Coevolution","The reciprocal evolutionary relationships of butterflies and their food plants have been examined on the basis of an extensive survey of patterns of plant utilization and information on factors affecting food plant choice. The evolution of secondary plant substances and the stepwise evolutionary responses to these by phytophagous organisms have clearly been the dominant factors in the evolution of butterflies and other phytophagous groups. Furthermore, these secondary plant substances have probably been critical in the evolution of angiosperm subgroups and perhaps of the angiosperms themselves. The examination of broad patterns of coevolution permits several levels of predictions and shows promise as a route to the understanding of community evolution. Little information useful for the reconstruction of phylogenies is supplied. It is apparent that reciprocal selective responses have been greatly underrated as a factor in the origination of organic diversity. The paramount importance of plant-herbivore interactions in generating terrestrial diversity is suggested. For instance, viewed in this framework the rich diversity of tropical communities may be traced in large part to the hospitality of warm climates toward poikilothermal phytophagous insects."
1058,"the parallel distributed processing approach to semantic cognition",566168,"The parallel distributed processing approach to semantic cognition.","How do we know what properties something has, and which of its properties should be generalized to other objects? How is the knowledge underlying these abilities acquired, and how is it affected by brain disorders? Our approach to these issues is based on the idea that cognitive processes arise from the interactions of neurons through synaptic connections. The knowledge in such interactive and distributed processing systems is stored in the strengths of the connections and is acquired gradually through experience. Degradation of semantic knowledge occurs through degradation of the patterns of neural activity that probe the knowledge stored in the connections. Simulation models based on these ideas capture semantic cognitive processes and their development and disintegration, encompassing domain-specific patterns of generalization in young children, and the restructuring of conceptual knowledge as a function of experience."
1059,"fmri resting state networks define distinct modes of longdistance interactions in the human brain",566988,"fMRI resting state networks define distinct modes of long-distance interactions in the human brain.","Functional magnetic resonance imaging (fMRI) studies of the human brain have suggested that low-frequency fluctuations in resting fMRI data collected using blood oxygen level dependent (BOLD) contrast correspond to functionally relevant resting state networks (RSNs). Whether the fluctuations of resting fMRI signal in RSNs are a direct consequence of neocortical neuronal activity or are low-frequency artifacts due to other physiological processes (e.g., autonomically driven fluctuations in cerebral blood flow) is uncertain. In order to investigate further these fluctuations, we have characterized their spatial and temporal properties using probabilistic independent component analysis (PICA), a robust approach to RSN identification. Here, we provide evidence that: i. RSNs are not caused by signal artifacts due to low sampling rate (aliasing); ii. they are localized primarily to the cerebral cortex; iii. similar RSNs also can be identified in perfusion fMRI data; and iv. at least 5 distinct RSN patterns are reproducible across different subjects. The RSNs appear to reflect ""default"" interactions related to functional networks related to those recruited by specific types of cognitive processes. RSNs are a major source of non-modeled signal in BOLD fMRI data, so a full understanding of their dynamics will improve the interpretation of functional brain imaging studies more generally. Because RSNs reflect interactions in cognitively relevant functional networks, they offer a new approach to the characterization of state changes with pathology and the effects of drugs."
1060,"the art of war",568376,"The art of war","{<I>The Art of War</I> is the Swiss army knife of military theory--pop out a different tool for any situation.  Folded into this small package are compact views on resourcefulness, momentum, cunning, the profit motive, flexibility, integrity, secrecy, speed, positioning, surprise, deception, manipulation, responsibility, and practicality.  Thomas Cleary's translation keeps the package tight, with crisp language and short sections. Commentaries from the Chinese tradition trail Sun-tzu's words, elaborating and picking up on puzzling lines. Take the solitary passage: ""Do not eat food for their soldiers.""  Elsewhere, Sun-tzu has told us to plunder the enemy's stores, but now we're not supposed to eat the food?  The Tang dynasty commentator Du Mu solves the puzzle nicely, ""If the enemy suddenly abandons their food supplies, they should be tested first before eating, lest they be poisoned."" Most passages, however, are the pinnacle of succinct clarity: ""Lure them in with the prospect of gain, take them by confusion"" or ""Invincibility is in oneself, vulnerability is in the opponent."" Sun-tzu's maxims are widely applicable beyond the military because they speak directly to the exigencies of survival. Your new tools will serve you well, but don't flaunt them. Remember Sun-tzu's advice: ""Though effective, appear to be ineffective."" <I>--Brian Bruya</I> } {<div>Widely regarded as ""The Oldest Military Treatise in the World,"" this compact little book, written more than 2,500 years ago, today retains much of its original authoritative merit. American officers during World War II read it closely. The Japanese army studied the work for decades, and many twentieth-century Chinese officers are said to have known the book by heart. Maintaining that ""all warfare is based on deception"" and that ""in war . . . let your great object be victory, not lengthy campaigns,"" the author adds: ""That general is skillful in attack whose opponent does not know what to defend; and he is skillful in defense whose opponent does not know what to attack."" Principles of strategy, tactics, maneuvering, communication, and supplies; the use of terrain, fire, and the seasons of the year; the classification and utilization of spies; the treatment of soldiers, including captives, all have a modern ring to them. The author even provides rules for the ""blitzkrieg,"" prefacing them with the words that ""rapidity is the essence of war."" Still a valuable guide to the conduct of war, this volume will be indispensable to military students and of interest to all those fascinated by military history. Unabridged republication of the edition published by The Military Service Publishing Company, Harrisburg, Pennsylvania, 1944.<br></div>} {Sun Tzu's classic treatise on the art of warfare  }"
1061,"feature selection for unsupervised learning",571029,"Feature Selection for Unsupervised Learning","In this paper, we identify two issues involved in developing an automated feature subset selection algorithm for unlabeled data: the need for finding the number of clusters in conjunction with feature selection, and the need for normalizing the bias of fe ature selection criteria with respect to dimension. We explore the feature selection problem and these issues through FSSEM (Feature Subset Selection using Expectation-Maximization (EM) clustering) and through two different performance criteria for evaluating candidate feature subsets: scatter separability and maximum likelihood. We present proofs on the dimensionality biases of these feature criteria, and present a cross-projection normalization scheme that can be applied to any criterion to ameliorate these biases. Our experiments show the need for feature selection, the need for addressing these two issues, and the effectiveness of our proposed solutions."
1062,"a stochastic parts program and noun phrase parser for unrestricted text",571073,"A stochastic parts program and noun phrase parser for unrestricted text","alice!kwc It is well-known that part of speech depends on context. The word &amp;quot;table, &amp;quot; for example, can be a verb in some contexts (e.g., &amp;quot;He will table the motion&amp;quot;) and a noun in others (e.g., &amp;quot;The table is ready&amp;quot;). A program has been written which tags each word in an input sentence with the most likely part of speech. The program produces the following output for the two &amp;quot;table &amp;quot; sentences just mentioned:. He/PPS will/MD table/VB the/AT motion/NN./.. The/AT table/NN is/BEZ ready/JJ./. (PPS = subject pronoun; MD = modal; VB = verb (no inflection); AT = article; NN = noun; BEZ = present 3rd sg form of &amp;quot;to be&amp;quot;; JJ = adjective; notation is borrowed from [Francis and Kucera,"
1063,"in vivo fiber tractography using dtmri data",572520,"In Vivo Fiber Tractography Using {DT--MRI} Data","Fiber tract trajectories in coherently organized brain white matter pathways were computed from in vivo diffusion tensor magnetic resonance imaging (DT-MRI) data. First, a continuous diffusion tensor field is constructed from this discrete, noisy, measured DT-MRI data. Then a Frenet equation, describing the evolution of a fiber tract, was solved. This approach was validated using synthesized, noisy DT-MRI data. Corpus callosum and pyramidal tract trajectories were constructed and found to be consistent with known anatomy. The method's reliability, however, degrades where the distribution of fiber tract directions is nonuniform. Moreover, background noise in diffusion-weighted MRIs can cause a computed trajectory to hop from tract to tract. Still, this method can provide quantitative information with which to visualize and study connectivity and continuity of neural pathways in the central and peripheral nervous systems in vivo, and holds promise for elucidating architectural features in other fibrous tissues and ordered media. Magn Reson Med 44:625-632, 2000. Published 2000 Wiley-Liss, Inc."
1064,"tracking neuronal fiber pathways in the living human brain",572535,"Tracking Neuronal Fiber Pathways in the Living Human Brain","Functional imaging with positron emission tomography and functional MRI has revolutionized studies of the human brain. Understanding the organization of brain systems, especially those used for cognition, remains limited, however, because no methods currently exist for noninvasive tracking of neuronal connections between functional regions [Crick, F. \\\\& Jones, E. (1993) Nature (London) 361, 109-110]. Detailed connectivities have been studied in animals through invasive tracer techniques, but these invasive studies cannot be done in humans, and animal results cannot always be extrapolated to human systems. We have developed noninvasive neuronal fiber tracking for use in living humans, utilizing the unique ability of MRI to characterize water diffusion. We reconstructed fiber trajectories throughout the brain by tracking the direction of fastest diffusion (the fiber direction) from a grid of seed points, and then selected tracks that join anatomically or functionally (functional MRI) defined regions. We demonstrate diffusion tracking of fiber bundles in a variety of white matter classes with examples in the corpus callosum, geniculo-calcarine, and subcortical association pathways. Tracks covered long distances, navigated through divergences and tight curves, and manifested topological separations in the geniculo-calcarine tract consistent with tracer studies in animals and retinotopy studies in humans. Additionally, previously undescribed topologies were revealed in the other pathways. This approach enhances the power of modern imaging by enabling study of fiber connections among anatomically and functionally defined brain regions in individual human subjects."
1065,"estimation of the effective selfdiffusion tensor from the nmr spin echo",572613,"Estimation of the effective self-diffusion tensor from the NMR spin echo","The diagonal and off-diagonal elements of the effective self-diffusion tensor, Deff, are related to the echo intensity in an NMR spin-echo experiment. This relationship is used to design experiments from which Deff is estimated. This estimate is validated using isotropic and anisotropic media, i.e., water and skeletal muscle. It is shown that significant errors are made in diffusion NMR spectroscopy and imaging of anisotropic skeletal muscle when off-diagonal elements of Deff are ignored, most notably the loss of information needed to determine fiber orientation. Estimation of Deff provides the theoretical basis for a new MRI modality, diffusion tensor imaging, which provides information about tissue microstructure and its physiologic state not contained in scalar quantities such as T1, T2, proton density, or the scalar apparent diffusion constant."
1066,"complex analysis",573506,"Complex Analysis","With this second volume, we enter the intriguing world of complex analysis. From the first theorems on, the elegance and sweep of the results is evident. The starting point is the simple idea of extending a function initially given for real values of the argument to one that is defined when the argument is complex. From there, one proceeds to the main properties of holomorphic functions, whose proofs are generally short and quite illuminating: the Cauchy theorems, residues, analytic continuation, the argument principle. With this background, the reader is ready to learn a wealth of additional material connecting the subject with other areas of mathematics: the Fourier transform treated by contour integration, the zeta function and the prime number theorem, and an introduction to elliptic functions culminating in their application to combinatorics and number theory. Thoroughly developing a subject with many ramifications, while striking a careful balance between conceptual insights and the technical underpinnings of rigorous analysis, _Complex Analysis_ will be welcomed by students of mathematics, physics, engineering and other sciences. The Princeton Lectures in Analysis represents a sustained effort to introduce the core areas of mathematical analysis while also illustrating the organic unity between them. Numerous examples and applications throughout its four planned volumes, of which _Complex Analysis_ is the second, highlight the far- reaching consequences of certain ideas in analysis to other fields of mathematics and a variety of sciences. Stein and Shakarchi move from an introduction addressing Fourier series and integrals to in-depth considerations of complex analysis; measure and integration theory, and Hilbert spaces; and, finally, further topics such as functional analysis, distributions and elements of probability theory."
1067,"gps a general peertopeer simulator and its use for modeling bittorrent",573510,"GPS: a general peer-to-peer simulator and its use for modeling BitTorrent","Peer-to-Peer (P2P) systems have become popular over the past few years. However, their large scale and the open nature of the system makes studying them challenging. This paper presents an extensible framework for simulating P2P networks efficiently and accurately. Efficiency is accomplished by using message level simulation rather than packet level simulation. Moreover, accuracy is maintained by tracking the network infrastructure and using a flow model to accomplish accurate estimate of the message behavior. A second contribution of the paper is to model the BitTorrent (BT) protocol. BT is a widely-used protocol that is significantly more complex than other P2P protocols because file download occurs in chunks from many other peers concurrently. Thus, contrary to models of other P2P systems such as Gnutella or Freenet, which focus on finding the location of a file in the network, BT's complexity occurs in downloading files (locating files in fact occurs out of band using Websites that host the Torrent files). We validate the model against a packet level simulator and also using a real, but small scale, BitTorrent experiment. The simulator is object oriented and extensible for simulating other P2P protocols and applications."
1068,"english verb classes and alternations a preliminary investigation",573991,"{English Verb Classes and Alternations a preliminary investigation}","{<div>In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb's meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. <br><br>The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin's discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. <br><br>Easy to use, <i>English Verb Classes and Alternations</i> sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language. <br><br><br></div>}"
1069,"semantic structures",574003,"Semantic Structures","This vol in the Current Studies in Linguistics series contains an Introduction, III PARTS, & 11 Chpts. The vol refines the theory of conceptual semantics within a broad study of conceptual structure & its lexical & syntactic expression in English, constructed in Jackendoff's earlier works, Semantics and Cognition (Cambridge: MIT Press, 1983) & Consciousness and the Computational Mind (Cambridge: MIT Press, 1987), the major premises of which are summarized herein. PART I - BASIC MACHINERY - contains (1) Overview of Conceptual Semantics - presents an outline of I-conceptual (internal language/concepts) knowledge. Some basic notions relating to lexicon & grammar are discussed & a new approach to the decomposition of lexical concepts that goes beyond simple binary feature treatments is outlined; (2) Argument Structure and Thematic Roles - systematically develops a description of how the lexical conceptual structure of a head is combined with its arguments & modifiers to form a phrasal conceptual structure; (3) Multiple Thematic Roles for a Single NP - explores the theta-criterion & arguments suggesting that it should be weakened. The notion of argument binding is introduced & developed; & (4) Unifying Lexical Entries - outlines abbreviatory conventions intended to facilitate the consolidation of multiple related uses of a single lexical item into one entry. PART II - MOSTLY ON THE PROBLEM OF MEANING - contains (5) Some Further Conceptual Functions - examines basic verbs of manner of motion & configuration; (6) Some Featural Elaborations of Spatial Functions - discusses the properties of distributive location & the restrictions it entails. Properties of verbs of touching, verbs of attachment, & verbs of material composition are also discussed. It is shown that formalization of the conceptual structure of these classes is best accomplished not through the use of simple primitives but through development of primitive semantic fields into coherent feature systems; & (7) The Action Tier and the Analysis of Causation - adapts the phonological tier model to conceptual structure & outlines an analysis of causation. PART III - MOSTLY ON THE PROBLEM OF CORRESPONDENCE - contains (8) Adjuncts That Express an Incorporated Argument - provides a detailed technical discussion of adjuncts using several specific examples. A rule for the passive by-adjunct is also proposed; (9) Adjuncts That Express an Argument of a Modifying Conceptual Clause - examines the structure of three kinds of for-adjuncts: beneficiary, benefit, & exchange; (10) Adjuncts That Express Arguments of a Superordinate Conceptual Clause - examines unusual cases in which the adjunct rather than the verb determines the syntax of the verb phrase (VP). An alternative treatment of resultatives is proposed; & (11) Toward a Theory of Linking - examines how conceptual semantics can improve the formulation of linking theory, & vice versa. The general outlines of a revised theory of linking are presented. Bibliog. B. Annesser Murray"
1070,"automatic retrieval and clustering of similar words",574099,"Automatic Retrieval and Clustering of Similar Words","Bootstrapping semantics from text is one of the greatest challenges in natural language learning. Earlier research showed that it is possible to automatically identify words that are semantically similar to a given word based on the syntactic collocation patterns of the words. We present an approach that goes a step further by obtaining a tree structure among the most similar words so that different senses of a given word can be identified with different subtrees. Submission Type: paper Topic..."
1071,"the measurement of observer agreement for categorical data",574112,"The measurement of observer agreement for categorical data","This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature."
1072,"user modelling for news web sites with word sense based techniques",576263,"User Modelling for News Web Sites with Word Sense Based Techniques","SiteIF is a personal agent for a bilingual news web site that learns user’s interests from the requested pages. In this paper we propose to use a word sense based document representation as a starting point to build a model of the user’s interests. Documents passed over are processed and relevant senses (disambiguated over WordNet) are extracted and then combined to form a semantic network. A filtering procedure dynamically predicts new documents on the basis of the semantic network. There are two main advantages of a sense-based approach: first, the model predictions, being based on senses rather than words, are more accurate; second, the model is language independent, allowing navigation in multilingual sites. We report the results of a comparative experiment that has been carried out to give a quantitative estimation of these improvements."
1073,"spatial transformations of diffusion tensor magnetic resonance images",576317,"Spatial transformations of diffusion tensor magnetic resonance images","The authors address the problem of applying spatial transformations (or ""image warps"") to diffusion tensor magnetic resonance images. The orientational information that these images contain must be handled appropriately when they are transformed spatially during image registration. The authors present solutions for global transformations of three-dimensional images up to 12-parameter affine complexity and indicate how their methods can be extended for higher order transformations. Several approaches are presented and tested using synthetic data. One method, the preservation of principal direction algorithm, which takes into account shearing, stretching and rigid rotation, is shown to be the most effective. Additional registration experiments are performed on human brain data obtained from a single subject, whose head was imaged in three different orientations within the scanner. All of the authors' methods improve the consistency between registered and target images over naive warping algorithms."
1074,"do bilinguals have two personalities a special case of cultural frame switching",576653,"Do bilinguals have two personalities? A special case of cultural frame switching","Four studies examined and empirically documented Cultural Frame Switching (CFS; Hong, Chiu, & Kung, 1997) in the domain of personality. Specifically, we asked whether Spanish-English bilinguals show different personalities when using different languages? If so, are the two personalities consistent with cross-cultural differences in personality? To generate predictions about the specific cultural differences to expect, Study 1 documented personality differences between US and Mexican monolinguals. Studies 2-4 tested CFS in three samples of Spanish-English bilinguals, located in the US and Mexico. Findings replicated across all three studies, suggesting that language activates CFS for Extraversion, Agreeableness, and Conscientiousness. Further analyses suggested the findings were not due to anomalous items or translation effects. Results are discussed in terms of the interplay between culture and self."
1075,"computational geometry an introduction",576758,"Computational Geometry: An Introduction","From the reviews: ""This book offers a coherent treatment, at the graduate textbook level, of the field that has come to be known in the last decade or so as computational geometry. ... ... The book is well organized and lucidly written; a timely contribution by two founders of the field. It clearly demonstrates that computational geometry in the plane is now a fairly well-understood branch of computer science and mathematics. It also points the way to the solution of the more challenging problems in dimensions higher than two."" #Mathematical Reviews#1 ""... This remarkable book is a comprehensive and systematic study on research results obtained especially in the last ten years. The very clear presentation concentrates on basic ideas, fundamental combinatorial structures, and crucial algorithmic techniques. The plenty of results is clever organized following these guidelines and within the framework of some detailed case studies. A large number of figures and examples also aid the understanding of the material. Therefore, it can be highly recommended as an early graduate text but it should prove also to be essential to researchers and professionals in applied fields of computer-aided design, computer graphics, and robotics."" #Biometrical Journal#2"
1076,"the explicit linear quadratic regulator for constrained systems",576807,"The Explicit Linear Quadratic Regulator for Constrained Systems","For discrete-time linear time invariant systems with constraints on inputs and states, we develop an algorithm to determine explicitly, the state feedback control law which minimizes a quadratic performance criterion. We show that the control law is piece-wise linear and continuous for both the finite horizon problem (model predictive control) and the usual infinite time measure (constrained linear quadratic regulation). Thus, the on-line control computation reduces to the simple evaluation of an explicitly defined piecewise linear function. By computing the inherent underlying controller structure, we also solve the equivalent of the Hamilton-Jacobi-Bellman equation for discrete-time linear constrained systems. Control based on on-line optimization has long been recognized as a superior alternative for constrained systems. The technique proposed in this paper is attractive for a wide range of practical problems where the computational complexity of on-line optimization is prohibitive. It also provides an insight into the structure underlying optimization-based controllers."
1077,"challenges in enterprise search",577491,"Challenges in enterprise search","Concerted research effort since the nineteen fifties has lead to effective methods for retrieval of relevant documents from homogeneous collections of text, such as newspaper archives, scientific abstracts and CD-ROM encyclopaedias. However, the triumph of the Web in the nineteen nineties forced a significant paradigm shift in the Information Retrieval field because of the need to address the issues of enormous scale, fluid collection definition, great heterogeneity, unfettered interlinking, democratic publishing, the presence of adversaries and most of all the diversity of purposes for which Web search may be used. Now, the IR field is confronted with a challenge of similarly daunting dimensions -- how to bring highly effective search to the complex information spaces within enterprises. Overcoming the challenge would bring massive economic benefit, but victory is far from assured. The present work characterises enterprise search, hints at its economic magnitude, states some of the unsolved research questions in the domain of enterprise search need, proposes an enterprise search test collection and presents results for a small but interesting sub-problem."
1078,"the persona lifecycle keeping people in mind throughout product design interactive technologies",578311,"The Persona Lifecycle: Keeping People in Mind Throughout Product Design (Interactive Technologies)","{If you design and develop products for people, this book is for you. The Persona Lifecycle addresses the how of creating effective personas and using those personas to design products that people love. It doesnt just describe the value of personas; it offers detailed techniques and tools related to planning, creating, communicating, and using personas to create great product designs. Moreover, it provides rich examples, samples, and illustrations to imitate and model. Perhaps most importantly, it positions personas not as a panacea, but as a method used to complement other user-centered design (UCD) techniques including scenario-based design, cognitive walkthroughs and user testing. <br><br>John Pruitt is the User Research Manager for the Tablet & Mobile PC Division at Microsoft Corporation. Tamara Adlin is a Customer Experience Manager at Amazon.com. For the past six years, John and Tamara have been researching and using personas, leading workshops, and teaching courses at professional conferences and universities. They developed the Persona Lifecycle model to communicate the value and practical application of personas to product design and development professionals.<br><br><b>Features</b><br>* Presentation and discussion of the complete lifecycle of personas, to guide the designer at each stage of product development.<br>* A running case study with rich examples and samples that demonstrate how personas can be used in building a product end-to-end. <br>* Recommended best practices in techniques, tools, and innovative methods.<br>* Hundreds of relevant stories, commentary, opinions, and case studies from user experience professionals across a variety of domains and industries.}"
1079,"a statistical model for user preference",579603,"A Statistical Model for User Preference","Modeling user preference is one of the challenging issues in intelligent information systems. Extensive research has been performed to automatically analyze user preference and to utilize it. One problem still remains: The representation of preference, usually given by measure of vector similarity or probability, does not always correspond to common sense of preference. This problem gets worse in the case of negative preference. To overcome this problem, this paper presents a preference model using mutual information in a statistical framework. This paper also presents a method that combines information of joint features and alleviates problems arising from sparse data. Experimental results, compared with the previous recommendation models, show that the proposed model has the highest accuracy in recommendation tests."
1080,"graph theory and networks in biology",579981,"Graph Theory and Networks in Biology","In this paper, we present a survey of the use of graph theoretical techniques in Biology. In particular, we discuss recent work on identifying and modelling the structure of bio-molecular networks, as well as the application of centrality measures to interaction networks and research on the hierarchical structure of such networks and network motifs. Work on the link between structural network properties and dynamics is also described, with emphasis on synchronization and disease propagation."
1081,"what is a population an empirical evaluation of some genetic methods for identifying the number of gene pools and their degree of connectivity",580617,"What is a population? An empirical evaluation of some genetic methods for identifying the number of gene pools and their degree of connectivity","We review commonly used population definitions under both the ecological paradigm (which emphasizes demographic cohesion) and the evolutionary paradigm (which emphasizes reproductive cohesion) and find that none are truly operational. We suggest several quantitative criteria that might be used to determine when groups of individuals are different enough to be considered 'populations'. Units for these criteria are migration rate (m) for the ecological paradigm and migrants per generation (Nm) for the evolutionary paradigm. These criteria are then evaluated by applying analytical methods to simulated genetic data for a finite island model. Under the standard parameter set that includes L = 20 High mutation (microsatellite-like) loci and samples of S = 50 individuals from each of n = 4 subpopulations, power to detect departures from panmixia was very high (223C100%; P &lt; 0.001) even with high gene flow (Nm = 25). A new method, comparing the number of correct population assignments with the random expectation, performed as well as a multilocus contingency test and warrants further consideration. Use of Low mutation (allozyme-like) markers reduced power more than did halving S or L. Under the standard parameter set, power to detect restricted gene flow below a certain level X (H0: Nm &lt; X) can also be high, provided that true Nm 2264 0.5X. Developing the appropriate test criterion, however, requires assumptions about several key parameters that are difficult to estimate in most natural populations. Methods that cluster individuals without using a priori sampling information detected the true number of populations only under conditions of moderate or low gene flow (Nm 2264 5), and power dropped sharply with smaller samples of loci and individuals. A simple algorithm based on a multilocus contingency test of allele frequencies in pairs of samples has high power to detect the true number of populations even with Nm = 25 but requires more rigorous statistical evaluation. The ecological paradigm remains challenging for evaluations using genetic markers, because the transition from demographic dependence to independence occurs in a region of high migration where genetic methods have relatively little power. Some recent theoretical developments and continued advances in computational power provide hope that this situation may change in the future."
1082,"metabolic stability and epigenesis in randomly constructed genetic nets",581179,"Metabolic stability and epigenesis in randomly constructed genetic nets.","Proto-organisms probably were randomly aggregated nets of chemical reactions. The hypothesis that contemporary organisms are also randomly constructed molecular automata is examined by modeling the gene as a binary (on-off) device and studying the behavior of large, randomly constructed nets of these binary “genes”. The results suggest that, if each “gene” is directly affected by two or three other “genes”, then such random nets: behave with great order and stability; undergo behavior cycles whose length predicts cell replication time as a function of the number of genes per cell; possess different modes of behavior whose number per net predicts roughly the number of cell types in an organism as a function of its number of genes; and under the stimulus of noise are capable of differentiating directly from any mode of behavior to at most a few other modes of behavior. Cellular differentation is modeled as a Markov chain among the modes of behavior of a genetic net. The possibility of a general theory of metabolic behavior is suggested."
1083,"ant colony optimization introduction and recent trends",582065,"Ant colony optimization: Introduction and recent trends","Ant colony optimization is a technique for optimization that was introduced in the early 1990's. The inspiring source of ant colony optimization is the foraging behavior of real ant colonies. This behavior is exploited in artificial ant colonies for the search of approximate solutions to discrete optimization problems, to continuous optimization problems, and to important problems in telecommunications, such as routing and load balancing. First, we deal with the biological inspiration of ant colony optimization algorithms. We show how this biological inspiration can be transfered into an algorithm for discrete optimization. Then, we outline ant colony optimization in more general terms in the context of discrete optimization, and present some of the nowadays best-performing ant colony optimization variants. After summarizing some important theoretical results, we demonstrate how ant colony optimization can be applied to continuous optimization problems. Finally, we provide examples of an interesting recent research direction: The hybridization with more classical techniques from artificial intelligence and operations research."
1084,"products of experts",582131,"Products Of Experts","It is possible to combine multiple probabilistic models of the same data by multiplying the probabilities together and then renormalizing. This is a very efficient way to model high-dimensional data which simultaneously satisfies many different low dimensional constraints. Each individual expert model can focus on giving high probability to data vectors that satisfy just one of the constraints. Data vectors that satisfy this one constraint but violate other constraints will be ruled out by their low probability under the other expert models. Training a product of models appears difficult because, in addition to maximizing the probabilities that the individual models assign to the observed data, it is necessary to make the models disagree on unobserved regions of the data space. However, if the individual models are tractable there is a fairly efficient way to train a product of models. This training algorithm suggests a biologically plausible way of learning neural population codes."
1085,"a maximum entropy approach to adaptive statistical language modeling",582156,"A Maximum Entropy Approach To Adaptive Statistical Language Modeling","An adaptive statistical languagemodel is described, which successfullyintegrates long distancelinguistic information with other knowledge sources. Most existing statistical language models exploit only the immediate history of a text. To extract information from further back in the document's history, we propose and use trigger pairs as the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse. Next, statistical evidence from multiple..."
1086,"the competitive advantage of sanctioning institutions",582356,"The Competitive Advantage of Sanctioning Institutions","Understanding the fundamental patterns and determinants of human cooperation and the maintenance of social order in human societies is a challenge across disciplines. The existing empirical evidence for the higher levels of cooperation when altruistic punishment is present versus when it is absent systematically ignores the institutional competition inherent in human societies. Whether punishment would be deliberately adopted and would similarly enhance cooperation when directly competing with nonpunishment institutions is highly controversial in light of recent findings on the detrimental effects of punishment. We show experimentally that a sanctioning institution is the undisputed winner in a competition with a sanction-free institution. Despite initial aversion, the entire population migrates successively to the sanctioning institution and strongly cooperates, whereas the sanction-free society becomes fully depopulated. The findings demonstrate the competitive advantage of sanctioning institutions and exemplify the emergence and manifestation of social order driven by institutional selection. 10.1126/science.1123633"
1087,"a survey of web metrics",584206,"A survey of Web metrics","The unabated growth and increasing significance of the World Wide Web has resulted in a flurry of research activity to improve its capacity for serving information more effectively. But at the heart of these efforts lie implicit assumptions about ``quality'' and ``usefulness'' of Web resources and services. This observation points towards measurements and models that quantify various attributes of web sites. The science of measuring all aspects of information, especially its storage and retrieval or informetrics has interested information scientists for decades before the existence of the Web. Is Web informetrics any different, or is it just an application of classical informetrics to a new medium? In this article, we examine this issue by classifying and discussing a wide ranging set of Web metrics. We present the origins, measurement functions, formulations and comparisons of well-known Web metrics for quantifying Web graph properties, Web page significance, Web page similarity, search and retrieval, usage characterization and information theoretic properties. We also discuss how these metrics can be applied for improving Web information access and use."
1088,"the free will theorem",584533,"The Free Will Theorem","Abstract&nbsp;&nbsp;On the basis of three physical axioms, we prove that if the choice of a particular type of spin 1 experiment is not a function of the information accessible to the experimenters, then its outcome is equally not a function of the information accessible to the particles. We show that this result is robust, and deduce that neither hidden variable theories nor mechanisms of the GRW type for wave function collapse can be made relativistic and causal. We also establish the consistency of our axioms and discuss the philosophical implications."
1089,"sted microscopy reveals that synaptotagmin remains clustered after synaptic vesicle exocytosis",584547,"STED microscopy reveals that synaptotagmin remains clustered after synaptic vesicle exocytosis","Synaptic transmission is mediated by neurotransmitters that are stored in synaptic vesicles and released by exocytosis upon activation. The vesicle membrane is then retrieved by endocytosis, and synaptic vesicles are regenerated and re-filled with neurotransmitter. Although many aspects of vesicle recycling are understood, the fate of the vesicles after fusion is still unclear. Do their components diffuse on the plasma membrane, or do they remain together? This question has been difficult to answer because synaptic vesicles are too small (approximately 40 nm in diameter) and too densely packed to be resolved by available fluorescence microscopes. Here we use stimulated emission depletion (STED) to reduce the focal spot area by about an order of magnitude below the diffraction limit, thereby resolving individual vesicles in the synapse. We show that synaptotagmin I, a protein resident in the vesicle membrane, remains clustered in isolated patches on the presynaptic membrane regardless of whether the nerve terminals are mildly active or intensely stimulated. This suggests that at least some vesicle constituents remain together during recycling. Our study also demonstrates that questions involving cellular structures with dimensions of a few tens of nanometres can be resolved with conventional far-field optics and visible light."
1090,"creating efficient codebooks for visual recognition",585952,"Creating efficient codebooks for visual recognition","Visual codebook based quantization of robust appearance descriptors extracted from local image patches is an effective means of capturing image statistics for texture analysis and scene classification. Codebooks are usually constructed by using a method such as k-means to cluster the descriptor vectors of patches sampled either densely (‘textons’) or sparsely (‘bags of features’ based on key-points or salience measures) from a set of training images. This works well for texture analysis in homogeneous images, but the images that arise in natural object recognition tasks have far less uniform statistics. We show that for dense sampling, k-means over-adapts to this, clustering centres almost exclusively around the densest few regions in descriptor space and thus failing to code other informative regions. This gives suboptimal codes that are no better than using randomly selected centres. We describe a scalable acceptance-radius based clusterer that generates better codebooks and study its performance on several image classification tasks. We also show that dense representations outperform equivalent keypoint based ones on these tasks and that SVM or Mutual Information based feature selection starting from a dense codebook further improves the performance."
1091,"qosaware middleware for web services composition",587700,"QoS-aware middleware for Web services composition","Abstract--The paradigmatic shift from a Web of manual interactions to a Web of programmatic interactions driven by Web services is creating unprecedented opportunities for the formation of online Business-to-Business (B2B) collaborations. In particular, the creation of value-added services by composition of existing ones is gaining a significant momentum. Since many available Web services provide overlapping or identical functionality, albeit with different Quality of Service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. This paper presents a middleware platform which addresses the issue of selecting Web services for the purpose of their composition in a way that maximizes user satisfaction expressed as utility functions over QoS attributes, while satisfying the constraints set by the user and by the structure of the composite service. Two selection approaches are described and compared: one based on local (task-level) selection of services and the other based on global allocation of tasks to services using integer programming."
1092,"guest editors introduction social networks and social networking",590914,"Guest Editors' Introduction: Social Networks and Social Networking","This issue's theme includes three articles on research activities that have drawn on ideas from social networking to drive innovative designs. The focus covers the design, development, and study of social technologies at the level of individuals, groups, and organizations. Although the tools described here are all intended for individuals, each article highlights how new technologies and technical competencies will further push our understanding of human social-networking drives and desires."
1093,"estimation of probabilities from sparse data for the language model component of a speech recognizer",593144,"Estimation of Probabilities from Sparse Data for the Language model Component of a Speech Recognizer","The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises."
1094,"introduction to wordnet an online lexical database",593175,"Introduction to WordNet: An On-line Lexical Database","WordNet is an on-line lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. English nouns, verbs, and adjectives are organized into synonym sets, each representing one underlying lexical concept. Different relations link the synonym sets. Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list. Unfortunately, there is no obvious alternative, no other simple way for lexicographers to keep track of what has been done or for readers to find the word they are looking for. But a frequent objection to this solution is that finding things on an alphabetical list can be tedious and time-consuming. Many people who would like to refer to a dictionary decide not to bother with it because finding the information would interrupt their work and break their train of thought. In this age of computers, however, there is an answer to that complaint. One obvious reason to resort to on-line dictionaries - lexical databases that can be read by computers - is that computers can search such alphabetical lists much faster than people can. A dictionary entry can be available as soon as the target word is selected or typed into the keyboard. Moreover, since dictionaries are printed from tapes that are read by computers, it is a relatively simple matter to convert those tapes into the appropriate kind of lexical database. Putting conventional dictionaries on line seems a simple and natural marriage of the old and the new. Once computers are enlisted in the service of dictionary users, however, it quickly becomes apparent that it is grossly inefficient to use these powerful machines as little more than rapid page-turners. The challenge is to think what further use to make of them. WordNet is a proposal for a more effective combination of traditional lexicographic information and modern high-speed computation. This, and the accompanying four papers, is a detailed report of the state of WordNet as of 1990. In order to reduce the unnecessary repetition, the papers are written to be read consecutively."
1095,"the danger model a renewed sense of self",594772,"The Danger Model: A Renewed Sense of Self","For over 50 years immunologists have based their thoughts, experiments, and clinical treatments on the idea that the immune system functions by making a distinction between self and nonself. Although this paradigm has often served us well, years of detailed examination have revealed a number of inherent problems. This Viewpoint outlines a model of immunity based on the idea that the immune system is more concerned with entities that do damage than with those that are foreign."
1096,"overview of biocreative critical assessment of information extraction for biology",594821,"Overview of BioCreAtIvE: critical assessment of information extraction for biology.","BACKGROUND: The goal of the first BioCreAtIvE challenge (Critical Assessment of Information Extraction in Biology) was to provide a set of common evaluation tasks to assess the state of the art for text mining applied to biological problems. The results were presented in a workshop held in Granada, Spain March 28-31, 2004. The articles collected in this BMC Bioinformatics supplement entitled ""A critical assessment of text mining methods in molecular biology"" describe the BioCreAtIvE tasks, systems, results and their independent evaluation. RESULTS: BioCreAtIvE focused on two tasks. The first dealt with extraction of gene or protein names from text, and their mapping into standardized gene identifiers for three model organism databases (fly, mouse, yeast). The second task addressed issues of functional annotation, requiring systems to identify specific text passages that supported Gene Ontology annotations for specific proteins, given full text articles. CONCLUSION: The first BioCreAtIvE assessment achieved a high level of international participation (27 groups from 10 countries). The assessment provided state-of-the-art performance results for a basic task (gene name finding and normalization), where the best systems achieved a balanced 80% precision / recall or better, which potentially makes them suitable for real applications in biology. The results for the advanced task (functional annotation from free text) were significantly lower, demonstrating the current limitations of text-mining approaches where knowledge extrapolation and interpretation are required. In addition, an important contribution of BioCreAtIvE has been the creation and release of training and test data sets for both tasks. There are 22 articles in this special issue, including six that provide analyses of results or data quality for the data sets, including a novel inter-annotator consistency assessment for the test set used in task 2."
1097,"biocreative task a gene mention finding evaluation",594822,"BioCreAtIvE Task 1A: gene mention finding evaluation","BACKGROUND: The biological research literature is a major repository of knowledge. As the amount of literature increases, it will get harder to find the information of interest on a particular topic. There has been an increasing amount of work on text mining this literature, but comparing this work is hard because of a lack of standards for making comparisons. To address this, we worked with colleagues at the Protein Design Group, CNB-CSIC, Madrid to develop BioCreAtIvE (Critical Assessment for Information Extraction in Biology), an open common evaluation of systems on a number of biological text mining tasks. We report here on task 1A, which deals with finding mentions of genes and related entities in text. ""Finding mentions"" is a basic task, which can be used as a building block for other text mining tasks. The task makes use of data and evaluation software provided by the (US) National Center for Biotechnology Information (NCBI). RESULTS: 15 teams took part in task 1A. A number of teams achieved scores over 80% F-measure (balanced precision and recall). The teams that tried to use their task 1A systems to help on other BioCreAtIvE tasks reported mixed results. CONCLUSION: The 80% plus F-measure results are good, but still somewhat lag the best scores achieved in some other domains such as newswire, due in part to the complexity and length of gene names, compared to person or organization names in newswire."
1098,"genomewide computational prediction of transcriptional regulatory modules reveals new insights into human gene expression",595408,"Genome-wide computational prediction of transcriptional regulatory modules reveals new insights into human gene expression","The identification of regulatory regions is one of the most important and challenging problems toward the functional annotation of the human genome. In higher eukaryotes, transcription-factor (TF) binding sites are often organized in clusters called cis-regulatory modules (CRM). While the prediction of individual TF-binding sites is a notoriously difficult problem, CRM prediction has proven to be somewhat more reliable. Starting from a set of predicted binding sites for more than 200 TF families documented in Transfac, we describe an algorithm relying on the principle that CRMs generally contain several phylogenetically conserved binding sites for a few different TFs. The method allows the prediction of more than 118,000 CRMs within the human genome. A subset of these is shown to be bound in vivo by TFs using ChIP-chip. Their analysis reveals, among other things, that CRM density varies widely across the genome, with CRM-rich regions often being located near genes encoding transcription factors involved in development. Predicted CRMs show a surprising enrichment near the 3′ end of genes and in regions far from genes. We document the tendency for certain TFs to bind modules located in specific regions with respect to their target genes and identify TFs likely to be involved in tissue-specific regulation. The set of predicted CRMs, which is made available as a public database called PReMod (http://genomequebec.mcgill.ca/PReMod), will help analyze regulatory mechanisms in specific biological systems."
1099,"duplicationdivergence model of protein interaction network",595633,"Duplication-divergence model of protein interaction network","We investigate a very simple model describing the evolution of protein-protein interaction networks via duplication and divergence. The model exhibits a remarkably rich behavior depending on a single parameter, the probability to retain a duplicated link during divergence. When this parameter is large, the network growth is not self-averaging and an average node degree increases algebraically. The lack of self-averaging results in a great diversity of networks grown out of the same initial condition. When less than a half of links are (on average) preserved after divergence, the growth is self-averaging, the average degree increases very slowly or tends to a constant, and a degree distribution has a power-law tail. The predicted degree distributions are in a very good agreement with the distributions observed in real protein networks."
1100,"complexity and philosophy",598208,"Complexity and Philosophy","The science of complexity is based on a new way of thinking that stands in sharp contrast to the philosophy underlying Newtonian science, which is based on reductionism, determinism, and objective knowledge. This paper reviews the historical development of this new world view, focusing on its philosophical foundations. Determinism was challenged by quantum mechanics and chaos theory. Systems theory replaced reductionism by a scientifically based holism. Cybernetics and postmodern social science showed that knowledge is intrinsically subjective. These developments are being integrated under the header of ""complexity science"". Its central paradigm is the multi-agent system. Agents are intrinsically subjective and uncertain about their environment and future, but out of their local interactions, a global organization emerges. Although different philosophers, and in particular the postmodernists, have voiced similar ideas, the paradigm of complexity still needs to be fully assimilated by philosophy. This will throw a new light on old philosophical issues such as relativism, ethics and the role of the subject."
1101,"classifying spatial patterns of brain activity with machine learning methods application to lie detection",598868,"Classifying spatial patterns of brain activity with machine learning methods: Application to lie detection","Patterns of brain activity during deception have recently been characterized with fMRI on the multi-subject average group level. The clinical value of fMRI in lie detection will be determined by the ability to detect deception in individual subjects, rather than group averages. High-dimensional non-linear pattern classification methods applied to functional magnetic resonance (fMRI) images were used to discriminate between the spatial patterns of brain activity associated with lie and truth. In 22 participants performing a forced-choice deception task, 99% of the true and false responses were discriminated correctly. Predictive accuracy, assessed by cross-validation in participants not included in training, was 88%. The results demonstrate the potential of non-linear machine learning techniques in lie detection and other possible clinical applications of fMRI in individual subjects, and indicate that accurate clinical tests could be based on measurements of brain function with fMRI."
1102,"rapid and sensitive protein similarity searches",599904,"{Rapid and Sensitive protein Similarity Searches}","An algorithm was developed which facilitates the search for similarities between newly determined amino acid sequences and sequences already available in databases. Because of the algorithm's efficiency on many microcomputers, sensitive protein database searches may now become a routine procedure for molecular biologists. The method efficiently identifies regions of similar sequence and then scores the aligned identical and differing residues in those regions by means of an amino acid replacability matrix. This matrix increases sensitivity by giving high scores to those amino acid replacements which occur frequently in evolution. The algorithm has been implemented in a computer program designed to search protein databases very rapidly. For example, comparison of a 200-amino-acid sequence to the 500,000 residues in the National Biomedical Research Foundation library would take less than 2 minutes on a minicomputer, and less than 10 minutes on a microcomputer (IBM PC)."
1103,"deciphering principles of transcription regulation in eukaryotic genomes",600388,"Deciphering principles of transcription regulation in eukaryotic genomes","Transcription regulation has been responsible for organismal complexity and diversity in the course of biological evolution and adaptation, and it is determined largely by the context-dependent behavior of cis-regulatory elements (CREs). Therefore, understanding principles underlying CRE behavior in regulating transcription constitutes a fundamental objective of quantitative biology, yet these remain poorly understood. Here we present a deterministic mathematical strategy, the motif expression decomposition (MED) method, for deriving principles of transcription regulation at the single-gene resolution level. MED operates on all genes in a genome without requiring any a priori knowledge of gene cluster membership, or manual tuning of parameters. Applying MED to Saccharomyces cerevisiae transcriptional networks, we identified four functions describing four different ways that CREs can quantitatively affect gene expression levels. These functions, three of which have extrema in different positions in the gene promoter (short-, mid-, and long-range) whereas the other depends on the motif orientation, are validated by expression data. We illustrate how nature could use these principles as an additional dimension to amplify the combinatorial power of a small set of CREs in regulating transcription."
1104,"term weighting approaches in automatic text retrieval",600541,"Term Weighting Approaches in Automatic Text Retrieval","The experimental evidence accumulated over the past 20 years indicates that textindexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective term weighting systems. This paper summarizes the insights gained in automatic term weighting, and provides baseline single term indexing models with which other more elaborate content analysis procedures can be compared."
1105,"the measurement of selection on correlated characters",603893,"The Measurement of Selection on Correlated Characters","Multivariate statistical methods are derived for measuring selection solely from observed changes in the distribution of phenotypic characters in a population within a generation. Selective effects are readily detectable in characters that do not change with age, such as meristic traits or adult characters in species with determinate growth. Ontogenetic characters, including allometric growth rates, can be analyzed in longitudinal studies where individuals are followed through time. Following an approach pioneered by Pearson (1903), this analysis helps to reveal the target(s) of selection, and to quantify its intensity, without identifying the selective agent(s). By accounting for indirect selection through correlated characters, separate forces of directional and stabilizing (or disruptive) selection acting directly on each character can be measured. These directional and stabilizing selection coefficients are respectively the parameters that describe the best linear and quadratic approximations to the selective surface of individual fitness as a function of the phenotypic characters. The theory is illustrated by estimating selective forces on morphological characters influencing survival in pentatomid bugs [Euschistus variolarius] and in house sparrows [Passer domesticus] during severe weather conditions."
1106,"ontologies come of age",604304,"Ontologies Come of Age","Ontologies have moved beyond the domains of library science, philosophy, and knowledge representation.  They are now the concerns of marketing departments, CEOs, and mainstream business.  Research analyst companies such as Forrester Research report on the critical roles of ontologies in support of browsing and search for e-commerce and in support of interoperability for facilitation of knowledge management and configuration.  One now sees ontologies used as central controlled vocabularies that are integrated into catalogues, databases, web publications, knowledge management applications, etc.  Large ontologies are essential components in many online applications including search (such as Yahoo and Lycos),  e-commerce (such as Amazon and eBay), configuration (such as Dell and PC-Order), etc.   One also sees ontologies that have long life spans, sometimes in multiple projects (such as UMLS, SIC codes, etc.).  Such diverse usage generates many implications for ontology environments.  In this paper, we will discuss ontologies and requirements in their current instantiations on the web today.  We will describe some desirable properties of ontologies.  We will also discuss how both simple and complex ontologies are being and may be used to support varied applications.  We will conclude with a discussion of emerging trends in ontologies and their environments and briefly mention our evolving ontology evolution environment."
1107,"the frame of the game blurring the boundary between fiction and reality in mobile experiences",608465,"The Frame of the Game: Blurring the Boundary between Fiction and Reality in Mobile Experiences","Mobile experiences that take place in public settings such as on city streets create new opportunities for interweaving the fictional world of a performance or game with the everyday physical world. A study of a touring performance reveals how designers generated excitement and dramatic tension by implicating bystanders and encouraging the (apparent) crossing of normal boundaries of behaviour. The study also shows how designers dealt with associated risks through a process of careful orchestration. Consequently, we extend an existing framework for designing spectator interfaces with the concept of performance frames, enabling us to distinguish audience from bystanders. We conclude that using ambiguity to blur the frame can be a powerful design tactic, empowering players to willingly suspend disbelief, so long as a safety-net of orchestration ensures that they do not stray into genuine difficulty."
1108,"physically realistic homology models built with rosetta can be more accurate than their templates",608690,"Physically realistic homology models built with ROSETTA can be more accurate than their templates.","We have developed a method that combines the rosetta de novo protein folding and refinement protocol with distance constraints derived from homologous structures to build homology models that are frequently more accurate than their templates. We test this method by building complete-chain models for a benchmark set of 22 proteins, each with 1 or 2 candidate templates, for a total of 39 test cases. We use structure-based and sequence-based alignments for each of the test cases. All atoms, including hydrogens, are represented explicitly. The resulting models contain approximately the same number of atomic overlaps as experimentally determined crystal structures and maintain good stereochemistry. The most accurate models can be identified by their energies, and in 22 of 39 cases a model that is more accurate than the template over aligned regions is one of the 10 lowest-energy models."
1109,"software reflexion models bridging the gap between design and implementation",608904,"Software Reflexion Models: Bridging the Gap between Design and Implementation","The artifacts constituting a software system often drift apart over time. We have developed the software reflexion model technique to help engineers perform various software engineering tasks by exploiting, rather than removing, the drift between design and implementation. More specifically, the technique helps an engineer compare artifacts by summarizing where one artifact (such as a design) is consistent with and inconsistent with another artifact (such as source). The technique can be applied to help a software engineer evolve a structural mental model of a system to the point that it is &ldquo;good enough&rdquo; to be used for reasoning about a task at hand. The software reflexion model technique has been applied to support a variety of tasks, including design conformance, change assessment, and an experimental reengineering of the million-lines-of-code Microsoft Excel product. We provide a formal characterization of the reflexion model technique, discuss practical aspects of the approach, relate experiences of applying the approach and tools, and place the technique into the context of related work"
1110,"thousands of samples are needed to generate a robust gene list for predicting outcome in cancer",612089,"Thousands of samples are needed to generate a robust gene list for predicting outcome in cancer","10.1073/pnas.0601231103 Predicting at the time of discovery the prognosis and metastatic potential of cancer is a major challenge in current clinical research. Numerous recent studies searched for gene expression signatures that outperform traditionally used clinical parameters in outcome prediction. Finding such a signature will free many patients of the suffering and toxicity associated with adjuvant chemotherapy given to them under current protocols, even though they do not need such treatment. A reliable set of predictive genes also will contribute to a better understanding of the biological mechanism of metastasis. Several groups have published lists of predictive genes and reported good predictive performance based on them. However, the gene lists obtained for the same clinical types of patients by different groups differed widely and had only very few genes in common. This lack of agreement raised doubts about the reliability and robustness of the reported predictive gene lists, and the main source of the problem was shown to be the small number of samples that were used to generate the gene lists. Here, we introduce a previously undescribed mathematical method, probably approximately correct (PAC) sorting, for evaluating the robustness of such lists. We calculate for several published data sets the number of samples that are needed to achieve any desired level of reproducibility. For example, to achieve a typical overlap of 50% between two predictive lists of genes, breast cancer studies would need the expression profiles of several thousand early discovery patients."
1111,"noncoding rna",612596,"Non-coding RNA","The term non-coding RNA (ncRNA) is commonly employed for RNA that does not encode a protein, but this does not mean that such RNAs do not contain information nor have function. Although it has been generally assumed that most genetic information is transacted by proteins, recent evidence suggests that the majority of the genomes of mammals and other complex organisms is in fact transcribed into ncRNAs, many of which are alternatively spliced and/or processed into smaller products. These ncRNAs include microRNAs and snoRNAs (many if not most of which remain to be identified), as well as likely other classes of yet-to-be-discovered small regulatory RNAs, and tens of thousands of longer transcripts (including complex patterns of interlacing and overlapping sense and antisense transcripts), most of whose functions are unknown. These RNAs (including those derived from introns) appear to comprise a hidden layer of internal signals that control various levels of gene expression in physiology and development, including chromatin architecture/epigenetic memory, transcription, RNA splicing, editing, translation and turnover. RNA regulatory networks may determine most of our complex characteristics, play a significant role in disease and constitute an unexplored world of genetic variation both within and between species. 10.1093/hmg/ddl046"
1112,"opinion dynamics and bounded confidence models analysis and simulation",613092,"Opinion dynamics and bounded confidence models, analysis and simulation","Abstract When does opinion formation within an interacting group lead to consensus, polarization or fragmentation? The article investigates various models for the dynamics of continuous opinions by analytical methods as well as by computer simulations. Section 2 develops within a unified framework the classical model of consensus formation, the variant of this model due to Friedkin and Johnsen, a time-dependent version and a nonlinear version with bounded confidence of the agents. Section 3 presents for all these models major analytical results. Section 4 gives an extensive exploration of the nonlinear model with bounded confidence by a series of computer simulations. An appendix supplies needed mathematical definitions, tools, and theorems."
1113,"staged configuration through specialization and multilevel configuration of feature models",613130,"Staged configuration through specialization and multilevel configuration of feature models","Feature modeling is a key technique for capturing commonalities and variabilities in system families and product lines. In this article, we propose a cardinality-based notation for feature modeling, which integrates a number of existing extensions of previous approaches. We then introduce and motivate the novel concept of staged configuration. Staged configuration can be achieved by the stepwise specialization of feature models or by multilevel configuration, where the configuration choices available in each stage are defined by separate feature models. Staged configuration is important because, in a realistic development process, different groups and different people make product configuration choices in different stages. Finally, we also discuss how multilevel configuration avoids a breakdown between the different abstraction levels of individual features. This problem, sometimes referred to as 'analysis paralysis', easily occurs in feature modeling because features can denote entities at arbitrary levels of abstraction within a system family. Copyright © 2005 John Wiley & Sons, Ltd."
1114,"improved boosting algorithms using confidencerated predictions",619738,"Improved boosting algorithms using confidence-rated predictions","We describe several improvements to Freund and Schapire‘s AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper."
1115,"dating of the humanape splitting by a molecular clock of mitochondrial dna",619753,"Dating of the human-ape splitting by a molecular clock of mitochondrial {DNA}","A new statistical method for estimating divergence dates of species from DNA sequence data by a molecular clock approach is developed. This method takes into account effectively the information contained in a set of DNA sequence data. The molecular clock of mitochondrial DNA (mtDNA) was calibrated by setting the date of divergence between primates and ungulates at the Cretaceous-Tertiary boundary (65 million years ago), when the extinction of dinosaurs occurred. A generalized least-squares method was applied in fitting a model to mtDNA sequence data, and the clock gave dates of 92.3 +/- 11.7, 13.3 +/- 1.5, 10.9 +/- 1.2, 3.7 +/- 0.6, and 2.7 +/- 0.6 million years ago (where the second of each pair of numbers is the standard deviation) for the separation of mouse, gibbon, orangutan, gorilla, and chimpanzee, respectively, from the line leading to humans. Although there is some uncertainty in the clock, this dating may pose a problem for the widely believed hypothesis that the pipedal creature Australopithecus afarensis, which lived some 3.7 million years ago at Laetoli in Tanzania and at Hadar in Ethiopia, was ancestral to man and evolved after the human-ape splitting. Another likelier possibility is that mtDNA was transferred through hybridization between a proto-human and a proto-chimpanzee after the former had developed bipedalism."
1116,"fluid concepts and creative analogies computer models of the fundamental mechanisms of thought",622020,"Fluid Concepts And Creative Analogies: Computer Models Of The Fundamental Mechanisms Of Thought","Douglas Hofstadter, best known for his masterpiece <i>Godel, Escher, Bach: An Eternal Golden Braid,</i> tackles the subject of artificial intelligence and machine learning in his thought-provoking work <i>Fluid Concepts and Creative Analogies,</i> written in conjunction with the Fluid Analogies Research Group at the University of Michigan. Driven to discover whether computers can be made to ""think"" like humans, Hofstadter and his colleagues created a variety of computer programs that extrapolate sequences, apply pattern-matching strategies, make analogies, and even act ""creative."" As always, Hofstadter's work requires devotion on the part of the reader, but rewards him with fascinating insights into the nature of both human and machine intelligence. ""Will change your idea of what it is to be creative and even what it is to be human.""--(William Poundstone, <I>New York Times Book Review</I>)"
1117,"three ways to grow designs a comparison of embryogenies for an evolutionary design problem",624835,"Three Ways to Grow Designs: A Comparison of Embryogenies for an Evolutionary Design Problem","This paper explores the use of growth processes, or embryogenies, to map genotypes to phenotypes within evolutionary systems. Following a summary of the significant features of embryogenies, the three main types of embryogenies in Evolutionary Computation are then identified and explained: external, explicit and implicit. An experimental comparison between these three different embryogenies and an evolutionary algorithm with no embryogeny is performed. The problem set to the four evolutionary systems is to evolve tessellating tiles. In order to assess the scalability of the embryogenies, the problem is increased in difficulty by enlarging the size of tiles to be evolved. The results are surprising, with the implicit embryogeny outperforming all other techniques by showing no significant increase in the size of the genotypes or decrease in accuracy of evolution as the scale of the problem is increased."
1118,"an experimental study of the skype peertopeer voip system",624990,"An Experimental Study of the Skype Peer-to-Peer VoIP System","Despite its popularity, relatively little is known about the trafc characteristics of the Skype VoIP system and how they differ from other P2P systems. We describe an experimental study of Skype VoIP trafc conducted over a ve month period, where over 82 mil­ lion datapoints were collected regarding the population of online clients, the number of supernodes, and their trafc characteristics. This data was collected from September 1, 2005 to January 14, 2006. Experiments on this data were done in a black­box manner, i.e., without knowing the internals or specics of the Skype system or messages, as Skype encrypts all user trafc and signaling trafc payloads. The results indicate that although the structure of the Skype system appears to be similar to other P2P systems, particu­ larly KaZaA, there are several signicant differences in trafc. The number of active clients shows diurnal and work­week behavior, correlating with normal working hours regardless of geography. The population of supernodes in the system tends to be relatively stable; thus node churn, a signicant concern in other systems, seems less problematic in Skype. The typical bandwidth load on a supernode is relatively low, even if the supernode is relaying VoIP trafc. The paper aims to aid further understanding of a signicant, successful P2P VoIP system, as well as provide experimental data that may be useful for future design and modeling of such sys­ tems. These results also imply that the nature of a VoIP P2P system like Skype differs fundamentally from earlier P2P systems that are oriented toward le­sharing, and music and video download appli­ cations, and deserves more attention from the research community."
1119,"evolutionary turnover of mammalian transcription start sites",625347,"Evolutionary turnover of mammalian transcription start sites.","Alignments of homologous genomic sequences are widely used to identify functional genetic elements and study their evolution. Most studies tacitly equate homology of functional elements with sequence homology. This assumption is violated by the phenomenon of turnover, in which functionally equivalent elements reside at locations that are nonorthologous at the sequence level. Turnover has been demonstrated previously for transcription-factor-binding sites. Here, we show that transcription start sites of equivalent genes do not always reside at equivalent locations in the human and mouse genomes. We also identify two types of partial turnover, illustrating evolutionary pathways that could lead to complete turnover. These findings suggest that the signals encoding transcription start sites are highly flexible and evolvable, and have cautionary implications for the use of sequence-level conservation to detect gene regulatory elements."
1120,"a neural mass model for megeeg coupling and neuronal dynamics",626055,"A neural mass model for MEG/EEG: coupling and neuronal dynamics.","Although MEG/EEG signals are highly variable, systematic changes in distinct frequency bands are commonly encountered. These frequency-specific changes represent robust neural correlates of cognitive or perceptual processes (for example, alpha rhythms emerge on closing the eyes). However, their functional significance remains a matter of debate. Some of the mechanisms that generate these signals are known at the cellular level and rest on a balance of excitatory and inhibitory interactions within and between populations of neurons. The kinetics of the ensuing population dynamics determine the frequency of oscillations. In this work we extended the classical nonlinear lumped-parameter model of alpha rhythms, initially developed by Lopes da Silva and colleagues [Kybernetik 15 (1974) 27], to generate more complex dynamics. We show that the whole spectrum of MEG/EEG signals can be reproduced within the oscillatory regime of this model by simply changing the population kinetics. We used the model to examine the influence of coupling strength and propagation delay on the rhythms generated by coupled cortical areas. The main findings were that (1) coupling induces phase-locked activity, with a phase shift of 0 or [pi] when the coupling is bidirectional, and (2) both coupling and propagation delay are critical determinants of the MEG/EEG spectrum. In forthcoming articles, we will use this model to (1) estimate how neuronal interactions are expressed in MEG/EEG oscillations and establish the construct validity of various indices of nonlinear coupling, and (2) generate event-related transients to derive physiologically informed basis functions for statistical modelling of average evoked responses."
1121,"globalization and its discontents essays on the new mobility of people and money",631023,"Globalization and Its Discontents: Essays on the New Mobility of People and Money","{Groundbreaking essays on the new global economy from an ""expert observer"" (Forecast). Saskia Sassen is an internationally recognized expert on globalization whose writings have appeared in journals and magazines worldwide. Now available in paperback, Globalization and Its Discontents is a collection of Sassen's essays dealing with topics such as the ""global city,"" gender and migration (reconceived as the globalization of labor), information technology, and the new dynamics of inequality.  Sassen brings together cultural and literary studies, feminist theory, political economics, sociology, and political science, showing how vast the chasm between metropolitan business centers and low-income inner cities has become. Incisive and original, she takes on common political, cultural, and economic misconceptions of globalization and offers a thoughtful, provocative new look at our increasingly global society. }"
1122,"connectionism and cognitive architecture a critical analysis",634084,"Connectionism and Cognitive Architecture: A Critical Analysis","(from the chapter) When taken as a way of modeling cognitive architecture, connectionism really does represent an approach that is quite different from that of the classical cognitive science that it seeks to replace. /// First, we discuss some methodological questions about levels of explanation that have become enmeshed in the substantive controversy over connectionism. Second, we try to say what it is that makes connectionist and classical theories of mental structure incompatible. Third, we review and extend some of the traditional arguments for the classical architecture. We hope to make it clear how various aspects of the classical doctrine cohere and why rejecting the classical picture of reasoning leads connectionists to say the very implausible things they do about logic and semantics. Finally, we return to the question what makes the connectionist approach appear attractive to so many people. In doing so we'll consider some arguments that have been offered in favor of connectionist networks as general models of cognitive processing. /// Topics discussed include: the nature of the dispute (complex mental representations, structure-sensitive operations)  the need for symbol systems: productivity, systematicity, and inferential coherence (productivity of thought, systematicity of cognitive representation and inference)  and concluding comments: connectionism as a theory of implementation. ((c) 1997 APA/PsycINFO, all rights reserved)"
1123,"usability engineering interactive technologies",634897,"Usability Engineering (Interactive Technologies)","{An authoritative text by one of the premier researchers in usability engineering in the 1990s, Jakob Nielsen's <I>Usability Engineering</I> provides a landmark guide to software design that has helped bring this area of research into the mainstream of computing. ""Usability"" is the measurement of how easy or difficult it is to be productive with a piece of software. It often looks at the user interface--what elements appear onscreen and how efficient, confusing, and/or intuitive they are for beginning, intermediate, and advanced users. ""Usability engineering"" is the formal study of usability. It grew out of research on human factors, which looked at the way people interact with their environment.<p> The best thing about this book is its concise, cut-to-the-chase approach when defining usability and ways to measure and improve it. As the author notes, in the old days of computing, documents that attempted to define usability might have over 1,000 rules. The author offers just a handful of guiding principles for creating better software that apply even today. (Published just before the Internet revolution, this book's principles still hold true for Web designers, as well as those who create more traditional applications.)<p> Throughout this text, the author argues for the benefits of improved software usability. With software use as with all things, time is money and making more efficient interfaces translates into lower personnel costs and more productivity. The book also does a fine job of integrating usability design into the software development process, with guides for planning, working with end users, and running tests with users (whether on videotape or in person). The 50-page bibliography attests to the author's previous research on usability.<p> For anyone who needs to create better, more efficient software, <I>Usability Engineering</I> can help. This clear and intelligent guide to the science of usability engineering has helped enhance the potential of computers to work with end users more efficiently. In the new century, software developers will undoubtedly seek new advances in usability, in part because of the groundwork laid by books like this one. <I>--Richard Dragan</I><p> <B>Topics covered</B>: Usability basics, measuring usability, types of users, history of user interfaces, the usability engineering lifecycle, design techniques, heuristics and hints for improving usability, testing, managing user tests, assessing usability, interface standards, internationalization, and Computer-Aided Usability Engineering (CAUSE) tools.} {Written by the author of the best-selling <B>HyperText & HyperMedia,</B> this book is an excellent guide to the methods of usability engineering. The book provides the tools needed to avoid usability surprises and improve product quality. Step-by-step information on which method to use at various stages during the development lifecycle are included, along with detailed information on how to run a usability test and the unique issues relating to international usability.<br><br>* Emphasizes cost-effective methods that developers can implement immediately<br>* Instructs readers about which methods to use when, throughout the development lifecycle, which ultimately helps in cost-benefit analysis. <br>* Shows readers how to avoid the four most frequently listed reasons for delay in software projects.<br>* Includes detailed information on how to run a usability test.<br>* Covers unique issues of international usability.<br>* Features an extensive bibliography allowing readers to find additional information.<br>* Written by an internationally renowned expert in the field and the author of the best-selling HyperText & HyperMedia.}"
1124,"consequences of changing biodiversity",635513,"Consequences of changing biodiversity","Human alteration of the global environment has triggered the sixth major extinction event in the history of life and caused widespread changes in the global distribution of organisms. These changes in biodiversity alter ecosystem processes and change the resilience of ecosystems to environmental change. This has profound consequences for services that humans derive from ecosystems. The large ecological and societal consequences of changing biodiversity should be minimized to preserve options for future solutions to global environmental problems."
1125,"reinforcement learning i an introduction",643915,"Reinforcement Learning I: An Introduction","Introduction Richard S. Sutton and Andrew G. Barto c fl All rights reserved [In which we try to give a basic intuitive sense of what reinforcement learning is and how it differs and relates to other fields, e.g., supervised learning and neural networks, genetic algorithms and artificial life, control theory. Intuitively, RL is trial and error (variation and selection, search) plus learning (association, memory). We argue that RL is the only field that seriously addresses the special features of ..."
1126,"when is a functional program not a functional program",653744,"When is a Functional Program Not a Functional Program?","In an impure functional language, there are programs whose behaviour is completely functional (in that they behave extensionally on inputs), but the functions they compute cannot be written in the purely functional fragment of the language. That is, the class of programs with functional behaviour is more expressive than the usual class of pure functional programs. In this paper we introduce this extended class of functional programs by means of examples in Standard ML, and explore what they..."
1127,"the structure of the information visualization design space",654294,"The structure of the information visualization design space","Research on information visualization has reached the point where a number of successful point designs have been proposed and a variety of techniques have been discovered. It is now appropriate to describe and analyze portions of the design space so as to understand the differences among designs and to suggest new possibilities. This paper proposes an organization of the information visualization literature and illustrates it with a series of examples. The result is a framework for designing new visualizations and augmenting existing designs."
1128,"model selection and model averaging in phylogenetics advantages of akaike information criterion and bayesian approaches over likelihood ratio tests",658235,"Model selection and model averaging in phylogenetics: advantages of akaike information criterion and bayesian approaches over likelihood ratio tests.","Model selection is a topic of special relevance in molecular phylogenetics that affects many, if not all, stages of phylogenetic inference. Here we discuss some fundamental concepts and techniques of model selection in the context of phylogenetics. We start by reviewing different aspects of the selection of substitution models in phylogenetics from a theoretical, philosophical and practical point of view, and summarize this comparison in table format. We argue that the most commonly implemented model selection approach, the hierarchical likelihood ratio test, is not the optimal strategy for model selection in phylogenetics, and that approaches like the Akaike Information Criterion (AIC) and Bayesian methods offer important advantages. In particular, the latter two methods are able to simultaneously compare multiple nested or nonnested models, assess model selection uncertainty, and allow for the estimation of phylogenies and model parameters using all available models (model-averaged inference or multimodel inference). We also describe how the relative importance of the different parameters included in substitution models can be depicted. To illustrate some of these points, we have applied AIC-based model averaging to 37 mitochondrial DNA sequences from the subgenus Ohomopterus (genus Carabus) ground beetles described by Sota and Vogler (2001)."
1129,"the role of the basal ganglia in habit formation",658841,"The role of the basal ganglia in habit formation","Many organisms, especially humans, are characterized by their capacity for intentional, goal-directed actions. However, similar behaviours often proceed automatically, as habitual responses to antecedent stimuli. How are goal-directed actions transformed into habitual responses? Recent work combining modern behavioural assays and neurobiological analysis of the basal ganglia has begun to yield insights into the neural basis of habit formation."
1130,"a piece of place modeling the digital on the real in second life",663686,"A piece of place: Modeling the digital on the real in Second Life","Digital worlds exist as synthetic models and have no need for the constraints of the real world.  This freedom allows digital worlds a vast design space of representational choices, ranging from near correspondence to the real world to complete abstraction.  The digital world Second Life was designed to allow its residents enormous creative freedom and to be as broadly appealing as possible.  Second Life chose to mirror the real world in many important aspects in order to provide a place that felt familiar and comfortable, while granting freedoms not possible in the real world.  This Article will cover the environment of Second Life, the reasons for the choice and the challenges that arose."
1131,"massively multiplayer online roleplaying games the people the addiction and the playing experience",663718,"Massively Multiplayer Online Role-Playing Games: The People, the Addiction and the Playing Experience","{This book is about the fastest growing form of electronic game in the world&#151;the Massively Multiplayer Online Role Playing Game (MMORPG). It introduces these self-contained three-dimensional virtual worlds, often inhabited by thousands of players, and describes their evolution and sometimes become addicted to it. It also delves into the psychology of the people who inhabit the game universe and explores the development of the unique cultures, economies, moral codes, and slang in these virtual communities. It explains how the games are built, the spin-offs that players create to enhance their game lives, and peeks at the future of MMORPGs as they evolve from a form of amusement to an educational, scientific, and business tool.  <P>   Based on hundreds of interviews over a three-year period, the work explores reasons people are attracted to and addicted to these games. It also surveys many existing and upcoming games, identifying their unique features and attractions. Two appendices list online addiction organizations and MMORPG information sites.}"
1132,"the colonial origins of comparative development an empirical investigation",663920,"The Colonial Origins of Comparative Development: An Empirical Investigation","Author contact info: Daron Acemoglu Department of Economics MIT, E52-380B 50 Memorial Drive Cambridge, MA 02142-1347 Tel: 617/253-1927 Fax: 617/253-1330 E-Mail: daron@mit.edu Simon Johnson Sloan School of Management Massachusetts Institute of Technology 50 Memorial Drive, E52-562 Cambridge, MA 02142-1347 Tel: 202/623-8720 Fax: 202/623-8291 E-Mail: sjohnson@mit.edu James A. Robinson Harvard University Department of Government N309, 1737 Cambridge Street Cambridge, MA 02138 Tel: 617/496-2839 Fax: 617/495-0438 E-Mail: jrobinson@gov.harvard.edu We exploit differences in the mortality rates faced by European colonialists to estimate the effect of institutions on economic performance. Our argument is that Europeans adopted very different colonization policies in different colonies, with different associated institutions. The choice of colonization strategy was, at least in part, determined by whether Europeans could settle in the colony. In places where Europeans faced high mortality rates, they could not settle and they were more likely to set up worse (extractive) institutions. These early institutions persisted to the present. We document evidence supporting these hypotheses. Exploiting differences in mortality rates faced by soldiers, bishops and sailors in the colonies in the 17th, 18th and 19th centuries as an instrument for current institutions, we estimate large effects of institutions on income per capita. Our estimates imply that differences in institutions explain approximately three-quarters of the income per capita differences across former colonies. Once we control for the effect of institutions, we find that countries in Africa or those farther away from the equator do not have lower incomes."
1133,"domains motifs and scaffolds the role of modular interactions in the evolution and wiring of cell signaling circuits",665925,"Domains, Motifs, and Scaffolds: The Role of Modular Interactions in the Evolution and Wiring of Cell Signaling Circuits","Abstract Living cells display complex signal processing behaviors, many of which are mediated by networks of proteins specialized for signal transduction. Here we focus on the question of how the remarkably diverse array of eukaryotic signaling circuits may have evolved. Many of the mechanisms that connect signaling proteins into networks are highly modular: The core catalytic activity of a signaling protein is physically and functionally separable from molecular domains or motifs that determine its linkage to both inputs and outputs. This high degree of modularity may make these systems more evolvable—in principle, novel circuits, and therefore highly innovative regulatory behaviors, can arise from relatively simple genetic events such as recombination, deletion, or insertion. In support of this hypothesis, recent studies show that such modular systems can be exploited to engineer nonnatural signaling proteins and pathways with novel behavior."
1134,"mobile interaction design",665989,"Mobile Interaction Design","{<i>Mobile Interaction Design</i> shifts the design perspective away from the technology and concentrates on usability; in other words the book concentrates on developing interfaces and devices with a great deal of sensitivity to human needs, desires and capabilities.     <ul>     <li>Presents key interaction design ideas and successes in an accessible, relevant way     <li>Exercises, case studies and study questions make this book ideal for students.     <li>Provides ideals and techniques which will enable designers to create the next generation of effective mobile applications.     <li>Critiques current mobile interaction design (bloopers) to help designers avoid pitfalls.     <li>Design challenges and worked examples are given to reinforce ideas.     <li>Discusses the new applications and gadgets requiring knowledgeable and inspired thinking about usability and design.     <li>Authors have extensive experience in mobile interaction design, research, industry and teaching     </ul>} {Mobile Interaction Design shifts the design perspective away from the technology and concentrates on usability; in other words the book concentrates on developing interfaces and devices with a great deal of sensitivity to human needs, desires and capabilities. Presents key interaction design ideas and successes in an accessible, relevant way  Exercises, case studies and study questions make this book ideal for students.  Provides ideals and techniques which will enable designers to create the next generation of effective mobile applications.  Critiques current mobile interaction design (bloopers) to help designers avoid pitfalls.  Design challenges and worked examples are given to reinforce ideas.  Discusses the new applications and gadgets requiring knowledgeable and inspired thinking about usability and design.  Authors have extensive experience in mobile interaction design, research, industry and teaching}"
1135,"the problem with threads",666883,"The Problem with Threads","Threads are a seemingly straightforward adaptation of the dominant sequential model of computation to concurrent systems. Languages require little or no syntactic changes to support threads, and operating systems and architectures have evolved to efficiently support them. Many technologists are pushing for increased use of multithreading in software in order to take advantage of the predicted increases in parallelism in computer architectures. In this paper, I argue that this is not a good idea. Although threads seem to be a small step from sequential computation, in fact, they represent a huge step. They discard the most essential and appealing properties of sequential computation: understandability, predictability, and determinism. Threads, as a model of computation, are wildly nondeterministic, and the job of the programmer becomes one of pruning that nondeterminism. Although many research techniques improve the model by offering more effective pruning, I argue that this is approaching the problem backwards. Rather than pruning nondeterminism, we should build from essentially deterministic, composable components. Nondeterminism should be explicitly and judiciously introduced where needed, rather than removed where not needed. The consequences of this principle are profound. I argue for the development of concurrent coordination languages based on sound, composable formalisms. I believe that such languages will yield much more reliable, and more concurrent programs."
1136,"hotspots for copy number variation in chimpanzees and humans",667832,"Hotspots for copy number variation in chimpanzees and humans","Copy number variation is surprisingly common among humans and can be involved in phenotypic diversity and variable susceptibility to complex diseases, but little is known of the extent of copy number variation in nonhuman primates. We have used two array-based comparative genomic hybridization platforms to identify a total of 355 copy number variants (CNVs) in the genomes of 20 wild-born chimpanzees (Pan troglodytes) and have compared the identified chimpanzee CNVs to known human CNVs from previous studies. Many CNVs were observed in the corresponding regions in both chimpanzees and humans; especially those CNVs of higher frequency. Strikingly, these loci are enriched 20-fold for ancestral segmental duplications, which may facilitate CNV formation through nonallelic homologous recombination mechanisms. Therefore, some of these regions may be unstable ""hotspots"" for the genesis of copy number variation, with recurrent duplications and deletions occurring across and within species. 10.1073/pnas.0602318103"
1137,"distancescaled finite idealgas reference state improves structurederived potentials of mean force for structure selection and stability prediction",668238,"Distance-scaled, finite ideal-gas reference state improves structure-derived potentials of mean force for structure selection and stability prediction.","The distance-dependent structure-derived potentials developed so far all employed a reference state that can be characterized as a residue (atom)-averaged state. Here, we establish a new reference state called the distance-scaled, finite ideal-gas reference (DFIRE) state. The reference state is used to construct a residue-specific all-atom potential of mean force from a database of 1011 nonhomologous (less than 30% homology) protein structures with resolution less than 2 A. The new all-atom potential recognizes more native proteins from 32 multiple decoy sets, and raises an average Z-score by 1.4 units more than two previously developed, residue-specific, all-atom knowledge-based potentials. When only backbone and C(beta) atoms are used in scoring, the performance of the DFIRE-based potential, although is worse than that of the all-atom version, is comparable to those of the previously developed potentials on the all-atom level. In addition, the DFIRE-based all-atom potential provides the most accurate prediction of the stabilities of 895 mutants among three knowledge-based all-atom potentials. Comparison with several physical-based potentials is made."
1138,"identifying hierarchical structure in sequences a lineartime algorithm",673554,"Identifying Hierarchical Structure in Sequences: A linear-time algorithm","SEQUITUR is an algorithm that infers a hierarchical structure from a sequence of discrete symbols by replacing repeated phrases with a grammatical rule that generates the phrase, and continuing this process recursively. The result is a hierarchical representation of the original sequence, which offers insights into its lexical structure. The algorithm is driven by two constraints that reduce the size of the grammar, and produce structure as a by-product. SEQUITUR breaks new ground by operating incrementally. Moreover, the method's simple structure permits a proof that it operates in space and time that is linear in the size of the input. Our implementation can process 50,000 symbols per second and has been applied to an extensive range of real world sequences. 1. Introduction Many sequences of discrete symbols exhibit natural hierarchical structure. Text is made up of paragraphs, sentences, phrases, and words. Music is composed from major sections, motifs, bars, and notes. Records of ..."
1139,"a map of human genome sequence variation containing million single nucleotide polymorphisms",680937,"A map of human genome sequence variation containing 1.42 million single nucleotide polymorphisms","We describe a map of 1.42 million single nucleotide polymorphisms (SNPs) distributed throughout the human genome, providing an average density on available sequence of one SNP every 1.9 kilobases. These SNPs were primarily discovered by two projects: The SNP Consortium and the analysis of clone overlaps by the International Human Genome Sequencing Consortium. The map integrates all publicly available SNPs with described genes and other genomic features. We estimate that 60,000 SNPs fall within exon (coding and untranslated regions), and 85% of exons are within 5 kb of the nearest SNP. Nucleotide diversity varies greatly across the genome, in a manner broadly consistent with a standard population genetic model of human history. This high-density SNP map provides a public resource for defining haplotype variation across the genome, and should help to identify biomedically important genes for diagnosis and therapy."
1140,"evaluating similarity measures a largescale study in the orkut social network",681467,"Evaluating similarity measures: a large-scale study in the orkut social network","Online information services have grown too large for users to navigate without the help of automated tools such as col- laborative filtering, which makes recommendations to users based on their collective past behavior. While many similar- ity measures have been proposed and individually evaluated, they have not been evaluated relative to each other in a large real-world environment. We present an extensive empirical comparison of six distinct measures of similarity for recom- mending online communities to members of the Orkut social network. We determine the usefulness of the different rec- ommendations by actually measuring users' propensity to visit and join recommended communities. We also exam- ine how the ordering of recommendations influenced user selection, as well as interesting social issues that arise in recommending communities within a real social network."
1141,"cognitive radio brainempowered wireless communications",681886,"Cognitive radio: brain-empowered wireless communications","Cognitive radio is viewed as a novel approach for improving the utilization of a precious natural resource: the radio electromagnetic spectrum. The cognitive radio, built on a software-defined radio, is defined as an intelligent wireless communication system that is aware of its environment and uses the methodology of understanding-by-building to learn from the environment and adapt to statistical variations in the input stimuli, with two primary objectives in mind: /spl middot/ highly reliable communication whenever and wherever needed; /spl middot/ efficient utilization of the radio spectrum. Following the discussion of interference temperature as a new metric for the quantification and management of interference, the paper addresses three fundamental cognitive tasks. 1) Radio-scene analysis. 2) Channel-state estimation and predictive modeling. 3) Transmit-power control and dynamic spectrum management. This work also discusses the emergent behavior of cognitive radio."
1142,"pgcresponsive genes involved in oxidative phosphorylation are coordinately downregulated in human diabetes",683079,"PGC-1α-responsive genes involved in oxidative phosphorylation are coordinately downregulated in human diabetes","DNA microarrays can be used to identify gene expression changes characteristic of human disease. This is challenging, however, when relevant differences are subtle at the level of individual genes. We introduce an analytical strategy, Gene Set Enrichment Analysis, designed to detect modest but coordinate changes in the expression of groups of functionally related genes. Using this approach, we identify a set of genes involved in oxidative phosphorylation whose expression is coordinately decreased in human diabetic muscle. Expression of these genes is high at sites of insulin-mediated glucose disposal, activated by PGC-1 and correlated with total-body aerobic capacity. Our results associate this gene set with clinically important variation in human metabolism and illustrate the value of pathway relationships in the analysis of genomic profiling experiments."
1143,"scopus database a review",685683,"Scopus database: a review.","The Scopus database provides access to STM journal articles and the references included in those articles, allowing the searcher to search both forward and backward in time. The database can be used for collection development as well as for research. This review provides information on the key points of the database and compares it to Web of Science. Neither database is inclusive, but complements each other. If a library can only afford one, choice must be based in institutional needs."
1144,"pattern languages in hci a critical review",691332,"Pattern Languages in HCI: A Critical Review","This article presents a critical review of patterns and pattern languages in human-computer interaction (HCI). In recent years, patterns and pattern languages have received considerable attention in HCI for their potential as a means for developing and communicating information and knowledge to support good design. This review examines the background to patterns and pattern languages in HCI, and seeks to locate pattern languages in relation to other approaches to interaction design. The review explores four key issues: What is a pattern? What is a pattern language? How are patterns and pattern languages used? and How are values reflected in the pattern-based approach to design? Following on from the review, a future research agenda is proposed for patterns and pattern languages in HCI."
1145,"an adaptive system for the personalized access to news",691971,"An Adaptive System for the Personalized Access to News","Personalization is one of the keys for the success of web services. In this paper we present SeAN (Server for Adaptive News), an adaptive system for the personalized access to news servers on the WWW. The aims of the system are (i) to select the sections (topics) and news in the server that are most relevant for each user, (ii) to customize the detail level of each news item to the user's characteristics and (iii) to select the advertisements that are most appropriate for each page and user. In the paper we discuss the functionalities of the system and we present the choices we made in its design. In particular, we focus on the techniques we adopted for structuring the news archive, for creating and maintaining the user model and for generating the personalized hypertext for browsing the news server."
1146,"the problem with awareness introductory remarks on awareness in cscw",694515,"The problem with ``awareness'': Introductory remarks on ``awareness in CSCW''","At a very early stage in the course of CSCW, it became evident that categories such as ‘conversation’ or ‘workflow’ were quite insufficient for characterizing and understanding the ways in which cooperative work is coordinated and integrated. It quickly became obvious that cooperating actors somehow, while doing their individual bits, take heed of the context of their joint effort. More specifically, the early harvest of ethnographic field studies in CSCW (e.g., Harper et al., 1989b; Harper et al., 1989a; Heath and Luff, 1991) indicated that cooperating actors align and integrate their activities with those of their colleagues in a seemingly ‘seamless’ manner, that is, without interrupting each other, for instance by asking, suggesting, requesting, ordering, reminding, etc. others of this or that. As a placeholder for these elusive practices of taking heed of what is going on in the setting which seem to play a key role in cooperative work, the term ‘awareness’ was soon adopted."
1147,"access and mobility of wireless pda users",694702,"Access and mobility of wireless PDA users","In this paper, we analyze the mobility patterns of users of wireless hand-held PDAs in a campus wireless network using an eleven week trace of wireless network activity. Our study has two goals. First, we characterize the high-level mobility and access patterns of hand-held PDA users and compare these characteristics to previous workload mobility studies focused on laptop users. Second, we develop two wireless network topology models for use in wireless mobility studies: an  evolutionary topology model  based on user proximity and a  campus waypoint model  that serves as a trace-based complement to the random waypoint model. We use our evolutionary topology model as a case study for preliminary evaluation of three ad hoc routing algorithms on the network topologies created by the access and mobility patterns of users of modern wireless PDAs. Based upon the mobility characteristics of our trace-based campus waypoint model, we find that commonly parameterized synthetic mobility models have overly aggressive mobility characteristics for scenarios where user movement is limited to walking. Mobility characteristics based on realistic models can have significant implications for evaluating systems designed for mobility. When evaluated using our evolutionary topology model, for example, popular ad hoc routing protocols were very successful at adapting to user mobility, and user mobility was not a key factor in their performance."
1148,"image completion with structure propagation",695505,"Image completion with structure propagation","In this paper, we introduce a novel approach to image completion, which we call structure propagation. In our system, the user manually specifies important missing structure information by extending a few curves or line segments from the known to the unknown regions. Our approach synthesizes image patches along these user-specified curves in the unknown region using patches selected around the curves in the known region. Structure propagation is formulated as a global optimization problem by enforcing structure and consistency constraints. If only a single curve is specified, structure propagation is solved using Dynamic Programming. When multiple intersecting curves are specified, we adopt the Belief Propagation algorithm to find the optimal patches. After completing structure propagation, we fill in the remaining unknown regions using patch-based texture synthesis. We show that our approach works well on a number of examples that are challenging to state-of-the-art techniques."
1149,"physiological timeseries analysis using approximate entropy and sample entropy",697330,"Physiological time-series analysis using approximate entropy and sample entropy","Entropy, as it relates to dynamical systems, is the rate of information production. Methods for estimation of the entropy of a system represented by a time series are not, however, well suited to analysis of the short and noisy data sets encountered in cardiovascular and other biological studies. Pincus introduced approximate entropy (ApEn), a set of measures of system complexity closely related to entropy, which is easily applied to clinical cardiovascular and other time series. ApEn statistics, however, lead to inconsistent results. We have developed a new and related complexity measure, sample entropy (SampEn), and have compared ApEn and SampEn by using them to analyze sets of random numbers with known probabilistic character. We have also evaluated cross-ApEn and cross-SampEn, which use cardiovascular data sets to measure the similarity of two distinct time series. SampEn agreed with theory much more closely than ApEn over a broad range of conditions. The improved accuracy of SampEn statistics should make them useful in the study of experimental clinical cardiovascular and other biological time series."
1150,"supporting ethnographic studies of ubiquitous computing in the wild",697912,"Supporting Ethnographic Studies of Ubiquitous Computing in the Wild","Ethnography has become a staple feature of IT research over the last twenty years, shaping our understanding of the social character of computing systems and informing their design in a wide variety of settings. The emergence of ubiquitous computing raises new challenges for ethnography however, distributing interaction across a burgeoning array of small, mobile devices and online environments which exploit invisible sensing systems. Understanding interaction requires ethnographers to reconcile interactions that are, for example, distributed across devices on the street with online interactions in order to assemble coherent understandings of the social character and purchase of ubiquitous computing systems. We draw upon four recent studies to show how ethnographers are replaying system recordings of interaction alongside existing resources such as video recordings to do this and identify key challenges that need to be met to support ethnographic study of ubiquitous computing in the wild."
1151,"what properties characterize the hub proteins of the proteinprotein interaction network of saccharomyces cerevisiae",699834,"What properties characterize the hub proteins of the protein-protein interaction network of Saccharomyces cerevisiae?","BACKGROUND: Most proteins interact with only a few other proteins while a small number of proteins (hubs) have many interaction partners. Hub proteins and non-hub proteins differ in several respects; however, understanding is not complete about what properties characterize the hubs and set them apart from proteins of low connectivity. Therefore, we have investigated what differentiates hubs from non-hubs and static hubs (party hubs) from dynamic hubs (date hubs) in the protein-protein interaction network of Saccharomyces cerevisiae. RESULTS: The many interactions of hub proteins can only partly be explained by bindings to similar proteins or domains. It is evident that domain repeats, which are associated with binding, are enriched in hubs. Moreover, there is an over representation of multi-domain proteins and long proteins among the hubs. In addition, there are clear differences between party hubs and date hubs. Fewer of the party hubs contain long disordered regions compared to date hubs, indicating that these regions are important for flexible binding but less so for static interactions. Furthermore, party hubs interact to a large extent with each other, supporting the idea of party hubs as the cores of highly clustered functional modules. In addition, hub proteins, and in particular party hubs, are more often ancient. Finally, the more recent paralogs of party hubs are underrepresented. CONCLUSION: Our results indicate that multiple and repeated domains are enriched in hub proteins and, further, that long disordered regions, which are common in date hubs, are particularly important for flexible binding."
1152,"the sources of innovation",707182,"The Sources of Innovation","{It has long been assumed that new product innovations are typically developed by product manufacturers, an assumption that has inevitably had a major impact on innovation-related research and activities ranging from how firms organize their research and development to how governments measure innovation.  In this synthesis of his seminal research, von Hippel challenges that basic assumption and demonstrates that innovation occurs in different places in different industries. Presenting a series of studies showing that end-users, material suppliers, and others are the typical sources of innovation in some fields, von Hippel explores why this variation in the ""functional"" sources of innovation occurs and how it might be predicted.  He also proposes and tests some implications of replacing a manufacturer-as-innovator assumption with a view of the innovation process as predictably distributed across users, manufacturers, and suppliers.  Innovation, he argues, will take place where there is greatest economic benefit to the innovator.}"
1153,"talk to me foundations for successful individualgroup interactions in online communities",707725,"Talk to me: foundations for successful individual-group interactions in online communities","""People come to online communities seeking information, encouragement, and conversation. When a community re- sponds, participants benefit and become more committed. Yet interactions often fail. In a longitudinal sample of 6,172 messages from 8 Usenet newsgroups, 27% of posts re- ceived no response. The information context, posters’ prior engagement in the community, and the content of their posts all influenced the likelihood that they received a re- ply, and, as a result, their willingness to continue active participation. Posters were less likely to get a reply if they were newcomers. Posting on-topic, introducing oneself via autobiographical testimonials, asking questions, using less complex language and other features of the messages, in- creased replies. Results suggest ways that developers might increase the ability of online communities to support successful individual-group interactions"""
1154,"optimization of mutual information for multiresolution image registration",708641,"Optimization of mutual information for multiresolution image registration","{Abstract{\\textemdash}We} propose a new method for the intermodal registration of images using a criterion known as mutual information. Our main contribution is an optimizer that we specifically designed for this criterion. We show that this new optimizer is well adapted to a multiresolution approach because it typically converges in fewer criterion evaluations than other optimizers. We have built a multiresolution image pyramid, along with an interpolation process, an optimizer, and the criterion itself, around the unifying concept of spline-processing. This ensures coherence in the way we model data and yields good performance. We have tested our approach in a variety of experimental conditions and report excellent results. We claim an accuracy of about a hundredth of a pixel under ideal conditions. We are also robust since the accuracy is still about a tenth of a pixel under very noisy conditions. In addition, a blind evaluation of our results compares very favorably to the work of several other researchers. Index {Terms{\\textemdash}B-spline,} intermodal volume alignment, {Marquardt{\\textendash}Levenberg,} Parzen window, pyramid."
1155,"gatewatching collaborative online news production digital formations",712068,"Gatewatching: Collaborative Online News Production (Digital Formations)","{<I>Gatewatching: Collaborative Online News Production</I> is the first comprehensive study of the latest wave of online news publications. The book investigates the collaborative publishing models of key news Websites, ranging from the worldwide <I>Indymedia</I> network to the massively successful technology news site <I>Slashdot,</I> and further to the multitude of Weblogs that have emerged in recent years. Building on collaborative approaches borrowed from the open source software development community, this book illustrates how gatewatching provides an alternative to gatekeeping and other traditional journalistic models of reporting, and has enabled millions of users around the world to participate in the online news publishing process.}"
1156,"bipartite structure of all complex networks",713345,"Bipartite structure of all complex networks","The analysis and modelling of various complex networks has received much attention in the last few years. Some such networks display a natural bipartite structure: two kinds of nodes coexist with links only between nodes of different kinds. This bipartite structure has not been deeply studied until now, mainly because it appeared to be specific to only a few complex networks. However, we show here that all complex networks can be viewed as bipartite structures sharing some important statistics, like degree distributions. The basic properties of complex networks can be viewed as consequences of this underlying bipartite structure. This leads us to propose the first simple and intuitive model for complex networks which captures the main properties met in practice."
1157,"optimal decision making and the anterior cingulate cortex",714224,"Optimal decision making and the anterior cingulate cortex","Learning the value of options in an uncertain environment is central to optimal decision making. The anterior cingulate cortex (ACC) has been implicated in using reinforcement information to control behavior. Here we demonstrate that the ACC's critical role in reinforcement-guided behavior is neither in detecting nor in correcting errors, but in guiding voluntary choices based on the history of actions and outcomes. ACC lesions did not impair the performance of monkeys (Macaca mulatta) immediately after errors, but made them unable to sustain rewarded responses in a reinforcement-guided choice task and to integrate risk and payoff in a dynamic foraging task. These data suggest that the ACC is essential for learning the value of actions."
1158,"visualizationbased information retrieval on the web",732865,"Visualization-based information retrieval on the Web","The application of visualization techniques to information retrieval (IR) has resulted in the development of innovative systems and interfaces that are now available for public use. Visualization tools have emerged in research environments and more recently on the Web to retrieve information. Questions arise in regard to the utility of Web-based IR visualization tools for assisting users not only in manipulating search output, but also in managing the information retrieval process. To understand how Web-based visualization tools enable visual information retrieval, this article reviews some of the human perceptual theory behind the graphical interface of information visualization systems, analyzes iconic representations and information density on visualization displays, and examines information retrieval tasks that have been used in visualization system user research. This article is timely since it addresses new technologies for Web information retrieval and discusses future information visualization user research directions."
1159,"a structural approach to assessing innovation construct development of innovation locus type and characteristics",732884,"{A Structural Approach to Assessing Innovation: Construct Development of Innovation Locus, Type, and Characteristics}","We take a structural approach to assessing innovation. We develop a comprehensive set of measures to assess an innovation's locus, type, and characteristics. We find that the concepts of competence destroying and competence enhancing are composed of two distinct constructs that, although correlated, separately characterize an innovation: new competence acquisition and competence enhancement/destruction. We develop scales to measure these constructs and show that new competence acquisition and competence enhancing/destroying are different from other innovation characteristics including core/peripheral and incremental/radical, as well as architectural and generational innovation types. We show that innovations can be evaluated distinctively on these various dimensions with generally small correlations between them. We estimate the impact these different innovation characteristics and types have on time to introduction and perceived commercial success. Our results indicate the importance of taking a structural approach to describing innovations and to the differential importance of innovation locus, type, and characteristics on innovation outcomes. Our results also raise intriguing questions regarding the locus of competence acquisition (internal vs. external) and both innovation outcomes. 10.1287/mnsc.48.9.1103.174"
1160,"monte carlo methods in statistical physics",735298,"{Monte Carlo Methods in Statistical Physics}","{This book provides an introduction to Monte Carlo simulations in classical statistical physics and is aimed both at students beginning work in the field and at more experienced researchers who wish to learn more about Monte Carlo methods. The material covered includes methods for both equilibrium and out of equilibrium systems, and common algorithms like the Metropolis and heat-bath algorithms are discussed in detail, as well as more sophisticated ones such as continuous time Monte Carlo, cluster algorithms, multigrid methods, entropic sampling and simulated tempering. Data analysis techniques are also explained starting with straightforward measurement and error-estimation techniques and progressing to topics such as the single and multiple histogram methods and finite size scaling. The last few chapters of the book are devoted to implementation issues, including discussions of such topics as lattice representations, efficient implementation of data structures, multispin coding, parallelization of Monte Carlo algorithms, and random number generation. At the end of the book the authors give a number of example programs demonstrating the applications of these techniques to a variety of well-known models.}"
1161,"pdbpqr an automated pipeline for the setup of poissonboltzmann electrostatics calculations",735744,"PDB2PQR: an automated pipeline for the setup of Poisson-Boltzmann electrostatics calculations.","Continuum solvation models, such as Poisson-Boltzmann and Generalized Born methods, have become increasingly popular tools for investigating the influence of electrostatics on biomolecular structure, energetics and dynamics. However, the use of such methods requires accurate and complete structural data as well as force field parameters such as atomic charges and radii. Unfortunately, the limiting step in continuum electrostatics calculations is often the addition of missing atomic coordinates to molecular structures from the Protein Data Bank and the assignment of parameters to biomolecular structures. To address this problem, we have developed the PDB2PQR web service (http://agave.wustl.edu/pdb2pqr/). This server automates many of the common tasks of preparing structures for continuum electrostatics calculations, including adding a limited number of missing heavy atoms to biomolecular structures, estimating titration states and protonating biomolecules in a manner consistent with favorable hydrogen bonding, assigning charge and radius parameters from a variety of force fields, and finally generating 'PQR' output compatible with several popular computational biology packages. This service is intended to facilitate the setup and execution of electrostatics calculations for both experts and non-experts and thereby broaden the accessibility to the biological community of continuum electrostatics analyses of biomolecular systems."
1162,"microrna genes are transcribed by rna polymerase ii",737763,"MicroRNA genes are transcribed by RNA polymerase II","MicroRNAs (miRNAs) constitute a large family of noncoding RNAs that function as guide molecules in diverse gene silencing pathways. Current efforts are focused on the regulatory function of miRNAs, while little is known about how these unusual genes themselves are regulated. Here we present the first direct evidence that miRNA genes are transcribed by RNA polymerase II (pol II). The primary miRNA transcripts (pri-miRNAs) contain cap structures as well as poly(A) tails, which are the unique properties of class II gene transcripts. The treatment of human cells with alpha-amanitin decreased the level of pri-miRNAs at a concentration that selectively inhibits pol II activity. Furthermore, chromatin immunoprecipitation analyses show that pol II is physically associated with a miRNA promoter. We also describe, for the first time, the detailed structure of a miRNA gene by determining the promoter and the terminator of mir-23a approximately 27a approximately 24-2. These data indicate that pol II is the main, if not the only, RNA polymerase for miRNA gene transcription. Our study offers a basis for understanding the structure and regulation of miRNA genes."
1163,"a distinct small rna pathway silences selfish genetic elements in the germline",739398,"A Distinct Small RNA Pathway Silences Selfish Genetic Elements in the Germline.","In the Drosophila germline, repeat-associated small interfering RNAs (rasiRNAs) ensure genomic stability by silencing endogenous selfish genetic elements such as retrotransposons and repetitive sequences. Whereas small interfering RNAs (siRNAs) derive from both the sense and antisense strands of their double-stranded RNA precursors, rasiRNAs arise mainly from the antisense strand. rasiRNA production appears not to require Dicer-1, which makes microRNAs (miRNAs), or Dicer-2, which makes siRNAs, and rasiRNAs lack the 2',3' hydroxy termini characteristic of animal siRNA and miRNA. Unlike siRNAs and miRNAs, rasiRNAs function through the Piwi, rather than the Ago, Argonaute protein subfamily. Our data suggest that rasiRNAs protect the fly germline through a silencing mechanism distinct from both the miRNA and RNA interference pathways."
1164,"a collaborative web browsing system for multiple mobile users",739792,"A Collaborative Web Browsing System for Multiple Mobile Users","In mobile computing environments, handheld devices with low functionality restrict the services provided for mobile users. We propose a new concept of collaborative browsing, where mobile users collaboratively browse web pages designed for desktop PC. In collaborative browsing, a web page is divided into multiple components, and each is distributed to a different device. In mobile computing environments, the number of handheld devices, their capabilities, and other conditions can vary widely amongst mobile users who want to browse content. Therefore, we developed a page partitioning method for collaborative browsing, which divides a web page into multiple components. Moreover, we designed and implemented a collaborative web browsing system in which users can search and browse their target information by discussing and watching partial pages displayed on multiple devices."
1165,"functional connectivity in the resting brain a network analysis of the default mode hypothesis",745563,"Functional connectivity in the resting brain: a network analysis of the default mode hypothesis","10.1073/pnas.0135058100 Functional imaging studies have shown that certain brain regions, including posterior cingulate cortex (PCC) and ventral anterior cingulate cortex (vACC), consistently show greater activity during resting states than during cognitive tasks. This finding led to the hypothesis that these regions constitute a network supporting a default mode of brain function. In this study, we investigate three questions pertaining to this hypothesis: Does such a resting-state network exist in the human brain? Is it modulated during simple sensory processing? How is it modulated during cognitive processing? To address these questions, we defined PCC and vACC regions that showed decreased activity during a cognitive (working memory) task, then examined their functional connectivity during rest. PCC was strongly coupled with vACC and several other brain regions implicated in the default mode network. Next, we examined the functional connectivity of PCC and vACC during a visual processing task and show that the resultant connectivity maps are virtually identical to those obtained during rest. Last, we defined three lateral prefrontal regions showing increased activity during the cognitive task and examined their resting-state connectivity. We report significant inverse correlations among all three lateral prefrontal regions and PCC, suggesting a mechanism for attenuation of default mode network activity during cognitive processing. This study constitutes, to our knowledge, the first resting-state connectivity analysis of the default mode and provides the most compelling evidence to date for the existence of a cohesive default mode network. Our findings also provide insight into how this network is modulated by task demands and what functions it might subserve."
1166,"identification of genomescale metabolic network models using experimentally measured flux profiles",748961,"Identification of Genome-Scale Metabolic Network Models Using Experimentally Measured Flux Profiles","Genome-scale metabolic network models can be reconstructed for well-characterized organisms using genomic annotation and literature information. However, there are many instances in which model predictions of metabolic fluxes are not entirely consistent with experimental data, indicating that the reactions in the model do not match the active reactions in the in vivo system. We introduce a method for determining the active reactions in a genome-scale metabolic network based on a limited number of experimentally measured fluxes. This method, called optimal metabolic network identification (OMNI), allows efficient identification of the set of reactions that results in the best agreement between in silico predicted and experimentally measured flux distributions. We applied the method to intracellular flux data for evolved Escherichia coli mutant strains with lower than predicted growth rates in order to identify reactions that act as flux bottlenecks in these strains. The expression of the genes corresponding to these bottleneck reactions was often found to be downregulated in the evolved strains relative to the wild-type strain. We also demonstrate the ability of the OMNI method to diagnose problems in E. coli strains engineered for metabolite overproduction that have not reached their predicted production potential. The OMNI method applied to flux data for evolved strains can be used to provide insights into mechanisms that limit the ability of microbial strains to evolve towards their predicted optimal growth phenotypes. When applied to industrial production strains, the OMNI method can also be used to suggest metabolic engineering strategies to improve byproduct secretion. In addition to these applications, the method should prove to be useful in general for reconstructing metabolic networks of ill-characterized microbial organisms based on limited amounts of experimental data."
1167,"preservation of duplicate genes by complementary degenerative mutations",750011,"Preservation of duplicate genes by complementary, degenerative mutations.","The origin of organismal complexity is generally thought to be tightly coupled to the evolution of new gene functions arising subsequent to gene duplication. Under the classical model for the evolution of duplicate genes, one member of the duplicated pair usually degenerates within a few million years by accumulating deleterious mutations, while the other duplicate retains the original function. This model further predicts that on rare occasions, one duplicate may acquire a new adaptive function, resulting in the preservation of both members of the pair, one with the new function and the other retaining the old. However, empirical data suggest that a much greater proportion of gene duplicates is preserved than predicted by the classical model. Here we present a new conceptual framework for understanding the evolution of duplicate genes that may help explain this conundrum. Focusing on the regulatory complexity of eukaryotic genes, we show how complementary degenerative mutations in different regulatory elements of duplicated genes can facilitate the preservation of both duplicates, thereby increasing long-term opportunities for the evolution of new gene functions. The duplication-degeneration-complementation (DDC) model predicts that (1) degenerative mutations in regulatory elements can increase rather than reduce the probability of duplicate gene preservation and (2) the usual mechanism of duplicate gene preservation is the partitioning of ancestral functions rather than the evolution of new functions. We present several examples (including analysis of a new engrailed gene in zebrafish) that appear to be consistent with the DDC model, and we suggest several analytical and experimental approaches for determining whether the complementary loss of gene subfunctions or the acquisition of novel functions are likely to be the primary mechanisms for the preservation of gene duplicates. For a newly duplicated paralog, survival depends on the outcome of the race between entropic decay and chance acquisition of an advantageous regulatory mutation. Sidow 1996(p. 717) On one hand, it may fix an advantageous allele giving it a slightly different, and selectable, function from its original copy. This initial fixation provides substantial protection against future fixation of null mutations, allowing additional mutations to accumulate that refine functional differentiation. Alternatively, a duplicate locus can instead first fix a null allele, becoming a pseudogene. Walsh 1995 (p. 426) Duplicated genes persist only if mutations create new and essential protein functions, an event that is predicted to occur rarely. Nadeau and Sankoff 1997 (p. 1259) Thus overall, with complex metazoans, the major mechanism for retention of ancient gene duplicates would appear to have been the acquisition of novel expression sites for developmental genes, with its accompanying opportunity for new gene roles underlying the progressive extension of development itself. Cooke et al. 1997 (p. 362)"
1168,"web metasearch rank vs score based rank aggregation methods",753025,"Web Metasearch: Rank vs. Score Based Rank Aggregation Methods","Given a set of rankings, the task of ranking fusion is the problem of combining these lists in such a way to optimize the performance of the combination. The ranking fusion problem is encountered in many situations and, e.g., metasearch is a prominent one. It deals with the problem of combining the result lists returned by multiple search engines in response to a given query, where each item in a result list is ordered with respect to a search engine and a relevance score. Several ranking..."
1169,"identifying strategies to improve access to credible and relevant information for public health professionals a qualitative study",753171,"Identifying strategies to improve access to credible and relevant information for public health professionals: a qualitative study.","BACKGROUND: Movement towards evidence-based practices in many fields suggests that public health (PH) challenges may be better addressed if credible information about health risks and effective PH practices is readily available. However, research has shown that many PH information needs are unmet. In addition to reviewing relevant literature, this study performed a comprehensive review of existing information resources and collected data from two representative PH groups, focusing on identifying current practices, expressed information needs, and ideal systems for information access. METHODS: Nineteen individual interviews were conducted among employees of two domains in a state health department--communicable disease control and community health promotion. Subsequent focus groups gathered additional data on preferences for methods of information access and delivery as well as information format and content. Qualitative methods were used to identify themes in the interview and focus group transcripts. RESULTS: Informants expressed similar needs for improved information access including single portal access with a good search engine; automatic notification regarding newly available information; access to best practice information in many areas of interest that extend beyond biomedical subject matter; improved access to grey literature as well as to more systematic reviews, summaries, and full-text articles; better methods for indexing, filtering, and searching for information; and effective ways to archive information accessed. Informants expressed a preference for improving systems with which they were already familiar such as PubMed and listservs rather than introducing new systems of information organization and delivery. A hypothetical ideal model for information organization and delivery was developed based on informants' stated information needs and preferred means of delivery. Features of the model were endorsed by the subjects who reviewed it. CONCLUSION: Many critical information needs of PH practitioners are not being met efficiently or at all. We propose a dual strategy of: 1) promoting incremental improvements in existing information delivery systems based on the expressed preferences of the PH users of the systems and 2) the concurrent development and rigorous evaluation of new models of information organization and delivery that draw on successful resources already operating to deliver information to clinical medical practitioners."
1170,"microrna expression in zebrafish embryonic development",754255,"MicroRNA Expression in Zebrafish Embryonic Development","MicroRNAs (miRNAs) are small noncoding RNAs, about 21 nucleotides in length, that can regulate gene expression by base-pairing to partially complementary mRNAs. Regulation by miRNAs can play essential roles in embryonic development. We determined the temporal and spatial expression patterns of 115 conserved vertebrate miRNAs in zebrafish embryos by microarrays and by in situ hybridizations, using locked-nucleic acid-modified oligonucleotide probes. Most miRNAs were expressed in a highly tissue-specific manner during segmentation and later stages, but not early in development, which suggests that their role is not in tissue fate establishment but in differentiation or maintenance of tissue identity. 10.1126/science.1114519"
1171,"microeconometrics methods and applications",754945,"Microeconometrics: Methods and Applications","{This book provides the most comprehensive treatment to date of microeconometrics, the analysis of individual-level data on the economic behavior of individuals or firms using regression methods for cross section and panel data. The book is oriented to the practitioner. A basic understanding of the linear regression model with matrix algebra is assumed. The text can be used for a microeconometrics course, typically a second-year economics PhD course; for data-oriented applied microeconometrics field courses; and as a reference work for graduate students and applied researchers who wish to fill in gaps in their toolkit. Distinguishing features of the book include emphasis on nonlinear models and robust inference, simulation-based estimation, and problems of complex survey data. The book makes frequent use of numerical examples based on generated data to illustrate the key models and methods. More substantially, it systematically integrates into the text empirical illustrations based on seven large and exceptionally rich data sets.}"
1172,"link prediction approach to collaborative filtering",756195,"Link prediction approach to collaborative filtering","Recommender systems can provide valuable services in a digital library environment, as demonstrated by its commercial success in book, movie, and music industries. One of the most commonly-used and successful recommendation algorithms is collaborative filtering, which explores the correlations within user-item interactions to infer user interests and preferences. However, the recommendation quality of collaborative filtering approaches is greatly limited by the data sparsity problem. To alleviate this problem we have previously proposed graph-based algorithms to explore transitive user-item associations. In this paper, we extend the idea of analyzing user-item interactions as graphs and employ link prediction approaches proposed in the recent network modeling literature for making collaborative filtering recommendations. We have adapted a wide range of linkage measures for making recommendations. Our preliminary experimental results based on a book recommendation dataset show that some of these measures achieved significantly better performance than standard collaborative filtering algorithms."
1173,"multiframe correspondence estimation using subspace constraints",759594,"Multi-Frame Correspondence Estimation Using Subspace Constraints","When a rigid scene is imaged by a moving camera, the set of all displacements of all points across multiple frames often resides in a low-dimensional linear subspace. Linear subspace constraints have been used successfully in the past for recovering 3D structure and 3D motion information from multiple frames (e.g., by using the factorization method of Tomasi and Kanade (1992, International Journal of Computer Vision, 9:137–154)). These methods assume that the 2D correspondences have been precomputed. However, correspondence estimation is a fundamental problem in motion analysis. In this paper we show how the multi-frame subspace constraints can be used for constraining the 2D correspondence estimation process itself. We show that the multi-frame subspace constraints are valid not only for affine cameras, but also for a variety of imaging models, scene models, and motion models. The multi-frame subspace constraints are first translated from constraints on correspondences to constraints directly on image measurements (e.g., image brightness quantities). These brightness-based subspace constraints are then used for estimating the correspondences, by requiring that all corresponding points across all video frames reside in the appropriate low-dimensional linear subspace. The multi-frame subspace constraints are geometrically meaningful, and are {not} violated at depth discontinuities, nor when the camera-motion changes abruptly. These constraints can therefore replace {heuristic} constraints commonly used in optical-flow estimation, such as spatial or temporal smoothness."
1174,"the long tail why the future of business is selling less of more",761824,"The Long Tail: Why the Future of Business Is Selling Less of More","{<b>What happens when the bottlenecks that stand between supply and demand in our culture go away and everything becomes available to everyone?</b>  <P>""The Long Tail"" is a powerful new force in our economy: the rise of the niche. As the cost of reaching consumers drops dramatically, our markets are shifting from a one-size-fits-all model of mass appeal to one of unlimited variety for unique tastes. From supermarket shelves to advertising agencies, the ability to offer vast choice is changing everything, and causing us to rethink where our markets lie and how to get to them. Unlimited selection is revealing truths about what consumers want and how they want to get it, from DVDs at Netflix to songs on iTunes to advertising on Google.  <P>However, this is not just a virtue of online marketplaces; it is an example of an entirely new economic model for business, one that is just beginning to show its power. After a century of obsessing over the few products at the head of the demand curve, the new economics of distribution allow us to turn our focus to the many more products in the tail, which collectively can create a new market as big as the one we already know.  <P><i>The Long Tail</i> is really about the economics of abundance. New efficiencies in distribution, manufacturing, and marketing are essentially resetting the definition of what&#146;s commercially viable across the board. If the 20th century was about hits, the 21st will be equally about niches.}"
1175,"shuffling of cisregulatory elements is a pervasive feature of the vertebrate lineage",765744,"Shuffling of cis-regulatory elements is a pervasive feature of the vertebrate lineage","BACKGROUND: All vertebrates share a remarkable degree of similarity in their development as well as in the basic functions of their cells. Despite this, attempts at unearthing genome-wide regulatory elements conserved throughout the vertebrate lineage using BLAST-like approaches have thus far detected noncoding conservation in only a few hundred genes, mostly associated with regulation of transcription and development. RESULTS: We used a unique combination of tools to obtain regional global-local alignments of orthologous loci. This approach takes into account shuffling of regulatory regions that are likely to occur over evolutionary distances greater than those separating mammalian genomes. This approach revealed one order of magnitude more vertebrate conserved elements than was previously reported in over 2,000 genes, including a high number of genes found in the membrane and extracellular regions. Our analysis revealed that 72% of the elements identified have undergone shuffling. We tested the ability of the elements identified to enhance transcription in zebrafish embryos and compared their activity with a set of control fragments. We found that more than 80% of the elements tested were able to enhance transcription significantly, prevalently in a tissue-restricted manner corresponding to the expression domain of the neighboring gene. CONCLUSION: Our work elucidates the importance of shuffling in the detection of cis-regulatory elements. It also elucidates how similarities across the vertebrate lineage, which go well beyond development, can be explained not only within the realm of coding genes but also in that of the sequences that ultimately govern their expression."
1176,"genomic clocks and evolutionary timescales",768502,"{Genomic clocks and evolutionary timescales}","For decades, molecular clocks have helped to illuminate the evolutionary timescale of life, but now genomic data pose a challenge for time estimation methods. It is unclear how to integrate data from many genes, each potentially evolving under a different model of substitution and at a different rate. Current methods can be grouped by the way the data are handled (genes considered separately or combined into a ‘supergene’) and the way gene-specific rate models are applied (global versus local clock). There are advantages and disadvantages to each of these approaches, and the optimal method has not yet emerged. Fortunately, time estimates inferred using many genes or proteins have greater precision and appear to be robust to different approaches."
1177,"an energyefficient mac protocol for wireless sensor networks",768516,"An energy-efficient MAC protocol for wireless sensor networks","This paper proposes S-MAC, a medium-access control (MAC) protocol designed for wireless sensor networks. Wireless sensor networks use battery-operated computing and sensing devices. A network of these devices will collaborate for a common application such as environmental monitoring. We expect sensor networks to be deployed in an ad hoc fashion, with individual nodes remaining largely inactive for long periods of time, but then becoming suddenly active when something is detected. These characteristics of sensor networks and applications motivate a MAC that is different from traditional wireless MACs such as IEEE 802.11 in almost every way: energy conservation and self-configuration are primary goals, while per-node fairness and latency are less important. S-MAC uses three novel techniques to reduce energy consumption and support self-configuration. To reduce energy consumption in listening to an idle channel, nodes periodically sleep. Neighboring nodes form virtual clusters to auto-synchronize on sleep schedules. Inspired by PAMAS, S-MAC also sets the radio to sleep during transmissions of other nodes. Unlike PAMAS, it only uses in-channel signaling. Finally, S-MAC applies message passing to reduce contention latency for sensor-network applications that require store-and-forward processing as data move through the network. We evaluate our implementation of S-MAC over a sample sensor node, the Mote, developed at University of California, Berkeley. The experiment results show that, on a source node, an 802.11-like MAC consumes 2-6 times more energy than S-MAC for traffic load with messages sent every 1-10 s."
1178,"a fast and flexible statistical model for largescale population genotype data applications to inferring missing genotypes and haplotypic phase",768767,"A fast and flexible statistical model for large-scale population genotype data: applications to inferring missing genotypes and haplotypic phase.","We present a statistical model for patterns of genetic variation in samples of unrelated individuals from natural populations. This model is based on the idea that, over short regions, haplotypes in a population tend to cluster into groups of similar haplotypes. To capture the fact that, because of recombination, this clustering tends to be local in nature, our model allows cluster memberships to change continuously along the chromosome according to a hidden Markov model. This approach is flexible, allowing for both “block-like” patterns of linkage disequilibrium (LD) and gradual decline in LD with distance. The resulting model is also fast and, as a result, is practicable for large data sets (e.g., thousands of individuals typed at hundreds of thousands of markers). We illustrate the utility of the model by applying it to dense single-nucleotide–polymorphism genotype data for the tasks of imputing missing genotypes and estimating haplotypic phase. For imputing missing genotypes, methods based on this model are as accurate or more accurate than existing methods. For haplotype estimation, the point estimates are slightly less accurate than those from the best existing methods (e.g., for unrelated Centre d’Etude du Polymorphisme Humain individuals from the HapMap project, switch error was 0.055 for our method vs. 0.051 for PHASE) but require a small fraction of the computational cost. In addition, we demonstrate that the model accurately reflects uncertainty in its estimates, in that probabilities computed using the model are approximately well calibrated. The methods described in this article are implemented in a software package, fastPHASE, which is available from the Stephens Lab Web site."
1179,"deterministic coupling of single quantum dots to single nanocavity modes",769669,"Deterministic Coupling of Single Quantum Dots to Single Nanocavity Modes","We demonstrate a deterministic approach to the implementation of solid-state cavity quantum electrodynamics (QED) systems based on a precise spatial and spectral overlap between a single self-assembled quantum dot and a photonic crystal membrane nanocavity. By fine-tuning nanocavity modes with a high quality factor into resonance with any given quantum dot exciton, we observed clear signatures of cavity QED (such as the Purcell effect) in all fabricated structures. This approach removes the major hindrances that had limited the application of solid-state cavity QED and enables the realization of experiments previously proposed in the context of quantum information processing. 10.1126/science.1109815"
1180,"quorum sensing in bacteria",772702,"Quorum Sensing in Bacteria","▪ Abstract  Quorum sensing is the regulation of gene expression in response to fluctuations in cell-population density. Quorum sensing bacteria produce and release chemical signal molecules called autoinducers that increase in concentration as a function of cell density. The detection of a minimal threshold stimulatory concentration of an autoinducer leads to an alteration in gene expression. Gram-positive and Gram-negative bacteria use quorum sensing communication circuits to regulate a diverse array of physiological activities. These processes include symbiosis, virulence, competence, conjugation, antibiotic production, motility, sporulation, and biofilm formation. In general, Gram-negative bacteria use acylated homoserine lactones as autoinducers, and Gram-positive bacteria use processed oligo-peptides to communicate. Recent advances in the field indicate that cell-cell communication via autoinducers occurs both within and between bacterial species. Furthermore, there is mounting data suggesting that bacterial autoinducers elicit specific responses from host organisms. Although the nature of the chemical signals, the signal relay mechanisms, and the target genes controlled by bacterial quorum sensing systems differ, in every case the ability to communicate with one another allows bacteria to coordinate the gene expression, and therefore the behavior, of the entire community. Presumably, this process bestows upon bacteria some of the qualities of higher organisms. The evolution of quorum sensing systems in bacteria could, therefore, have been one of the early steps in the development of multicellularity."
1181,"midbrain dopamine neurons encode decisions for future action",774482,"Midbrain dopamine neurons encode decisions for future action","Current models of the basal ganglia and dopamine neurons emphasize their role in reinforcement learning. However, the role of dopamine neurons in decision making is still unclear. We recorded from dopamine neurons in monkeys engaged in two types of trial: reference trials in an instructed-choice task and decision trials in a two-armed bandit decision task. We show that the activity of dopamine neurons in the decision setting is modulated according to the value of the upcoming action. Moreover, analysis of the probability matching strategy in the decision trials revealed that the dopamine population activity and not the reward during reference trials determines choice behavior. Because dopamine neurons do not have spatial or motor properties, we conclude that immediate decisions are likely to be generated elsewhere and conveyed to the dopamine neurons, which play a role in shaping long-term decision policy through dynamic modulation of the efficacy of basal ganglia synapses."
1182,"on counting position weight matrix matches in a sequence with application to discriminative motif finding",776652,"{On counting position weight matrix matches in a sequence, with application to discriminative motif finding}","Motivation and Results: The position weight matrix (PWM) is a popular method to model transcription factor binding sites. A fundamental problem in cis-regulatory analysis is to ""count"" the occurrences of a PWM in a DNA sequence. We propose a novel probabilistic score to solve this problem of counting PWM occurrences. The proposed score has two important properties: (1) It gives appropriate weights to both strong and weak occurrences of the PWM, without using thresholds. (2) For any given PWM, this score can be computed while allowing for occurrences of other, a priori known PWMs, in a statistically sound framework. Additionally, the score is efficiently differentiable with respect to the PWM parameters, which has important consequences for designing search algorithms. The second problem we address is to find, ab initio, PWMs that have high counts in one set of sequences, and low counts in another. We develop a novel algorithm to solve this ""discriminative motif-finding problem"", using the proposed score for counting a PWM in the sequences. The algorithm is a local search technique that exploits derivative information on an objective function to enhance speed and performance. It is extensively tested on synthetic data, and shown to perform better than other discriminative as well as non-discriminative PWM finding algorithms. It is then applied to cis-regulatory modules involved in development of the fruitfly embryo, to elicit known and novel motifs. We finally use the algorithm on genes predictive of social behavior in the honey bee, and find interesting motifs. Availability: The program is available upon request from the author. Contact: sinhas@cs.uiuc.edu"
1183,"reducing the dimensionality of data with neural networks",778023,"Reducing the Dimensionality of Data with Neural Networks","High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such ""autoencoder"" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
1184,"knowledge flows within multinational corporations",781534,"Knowledge flows within multinational corporations","Pursuing a nodal (i.e., subsidiary) level of analysis, this paper advances and tests an overarching theoretical framework pertaining to intracorporate knowledge transfers within multinational corporations (MNCs). We predicted that (i) knowledge outflows from a subsidiary would be positively associated with value of the subsidiary?s knowledge stock, its motivational disposition to share knowledge, and the richness of transmission channels; and (ii) knowledge inflows into a subsidiary would be positively associated with richness of transmission channels, motivational disposition to acquire knowledge, and the capacity to absorb the incoming knowledge. These predictions were tested empirically with data from 374 subsidiaries within 75 MNCs headquartered in the U.S., Europe, and Japan. Except for our predictions regarding the impact of source unit's motivational disposition on knowledge outflows, the data provide either full or partial support to all of the other elements of our theoretical framework. Copyright © 2000 John Wiley & Sons, Ltd."
1185,"decoherence the measurement problem and interpretations of quantum mechanics",781872,"Decoherence, the measurement problem, and interpretations of quantum mechanics","Environment-induced decoherence and superselection have been a subject of intensive research over the past two decades, yet their implications for the foundational problems of quantum mechanics, most notably the quantum measurement problem, have remained a matter of great controversy. This paper is intended to clarify key features of the decoherence program, including its more recent results, and to investigate their application and consequences in the context of the main interpretive approaches of quantum mechanics."
1186,"scaling universality and renormalization three pillars of modern critical phenomena",781890,"Scaling, universality, and renormalization: Three pillars of modern critical phenomena","This brief overview is designed to introduce some of the advances that have occurred in our understanding of phase transitions and critical phenomena. The presentation is organized around three simple questions: (i) What are the basic phenomena under consideration? (ii) Why do we care? (iii) What do we actually do? To answer the third question, the author shall briefly review scaling, universality, and renormalization, three of the many important themes which have served to provide the framework of much of our current understanding of critical phenomena. The style is that of a colloquium, not that of a mini-review article."
1187,"integrating quantitative and qualitative research how is it done",787025,"{Integrating quantitative and qualitative research: how is it done?}","This article seeks to move beyond typologies of the ways in which quantitative and qualitative research are integrated to an examination of the ways that they are   combined in practice. The article is based on a content analysis of 232 social science articles in which the two were combined. An examination of the research  methods and research designs employed suggests that on the quantitative side structured interview and questionnaire research within a cross-sectional design   tends to predominate, while on the qualitative side the semi-structured interview within a cross-sectional design tends to predominate. An examination of the rationales that are given for employing a mixed-methods research approach and the ways it is used in practice indicates that the two do not always correspond. The implications of this finding for how we think about mixed-methods research are outlined. 10.1177/1468794106058877"
1188,"automatic document classification of biological literature",789655,"Automatic document classification of biological literature","BACKGROUND: Document classification is a wide-spread problem with many applications, from organizing search engine snippets to spam filtering. We previously described Textpresso, a text-mining system for biological literature, which marks up full text according to a shallow ontology that includes terms of biological interest. This project investigates document classification in the context of biological literature, making use of the Textpresso markup of a corpus of Caenorhabditis elegans literature. RESULTS: We present a two-step text categorization algorithm to classify a corpus of C. elegans papers. Our classification method first uses a support vector machine-trained classifier, followed by a novel, phrase-based clustering algorithm. This clustering step autonomously creates cluster labels that are descriptive and understandable by humans. This clustering engine performed better on a standard test-set (Reuters 21578) compared to previously published results (F-value of 0.55 vs. 0.49), while producing cluster descriptions that appear more useful. A web interface allows researchers to quickly navigate through the hierarchy and look for documents that belong to a specific concept. CONCLUSION: We have demonstrated a simple method to classify biological documents that embodies an improvement over current methods. While the classification results are currently optimized for Caenorhabditis elegans papers by human-created rules, the classification engine can be adapted to different types of documents. We have demonstrated this by presenting a web interface that allows researchers to quickly navigate through the hierarchy and look for documents that belong to a specific concept."
1189,"microbial diversity in the deep sea and the underexplored rare biosphere",791438,"Microbial diversity in the deep sea and the underexplored ""rare biosphere"".","10.1073/pnas.0605127103 The evolution of marine microbes over billions of years predicts that the composition of microbial communities should be much greater than the published estimates of a few thousand distinct kinds of microbes per liter of seawater. By adopting a massively parallel tag sequencing strategy, we show that bacterial communities of deep water masses of the North Atlantic and diffuse flow hydrothermal vents are one to two orders of magnitude more complex than previously reported for any microbial environment. A relatively small number of different populations dominate all samples, but thousands of low-abundance populations account for most of the observed phylogenetic diversity. This “rare biosphere” is very ancient and may represent a nearly inexhaustible source of genomic innovation. Members of the rare biosphere are highly divergent from each other and, at different times in earth's history, may have had a profound impact on shaping planetary processes."
1190,"protein structure prediction",796286,"Protein structure prediction.","The prediction of protein structure, based primarily on sequence and structure homology, has become an increasingly important activity. Homology models have become more accurate and their range of applicability has increased. Progress has come, in part, from the flood of sequence and structure information that has appeared over the past few years, and also from improvements in analysis tools. These include profile methods for sequence searches, the use of three-dimensional structure information in sequence alignment and new homology modeling tools, specifically in the prediction of loop and side-chain conformations. There have also been important advances in understanding the physical chemical basis of protein stability and the corresponding use of physical chemical potential functions to identify correctly folded from incorrectly folded protein conformations."
1191,"optical conformal mapping",797020,"Optical Conformal Mapping","An invisibility device should guide light around an object as if nothing were there, regardless of where the light comes from. Ideal invisibility devices are impossible, owing to the wave nature of light. This study develops a general recipe for the design of media that create perfect invisibility within the accuracy of geometrical optics. The imperfections of invisibility can be made arbitrarily small to hide objects that are much larger than the wavelength. With the use of modern metamaterials, practical demonstrations of such devices may be possible. The method developed here can also be applied to escape detection by other electromagnetic waves or sound. 10.1126/science.1126493"
1192,"how reliable are empirical genomic scans for selective sweeps",797379,"How reliable are empirical genomic scans for selective sweeps?","10.1101/gr.5105206 The beneficial substitution of an allele shapes patterns of genetic variation at linked sites. Thus, in principle, adaptations can be mapped by looking for the signature of directional selection in polymorphism data. In practice, such efforts are hampered by the need for an accurate characterization of the demographic history of the species and of the effects of positive selection. In an attempt to circumvent these difficulties, researchers are increasingly taking a purely empirical approach, in which a large number of genomic regions are ordered by summaries of the polymorphism data, and loci with extreme values are considered to be likely targets of positive selection. We evaluated the reliability of the “empirical” approach, focusing on applications to human data and to maize. To do so, we considered a coalescent model of directional selection in a sensible demographic setting, allowing for selection on standing variation as well as on a new mutation. Our simulations suggest that while empirical approaches will identify several interesting candidates, they will also miss many—in some cases, most—loci of interest. The extent of the trade-off depends on the mode of positive selection and the demographic history of the population. Specifically, the false-discovery rate is higher when directional selection involves a recessive rather than a co-dominant allele, when it acts on a previously neutral rather than a new allele, and when the population has experienced a population bottleneck rather than maintained a constant size. One implication of these results is that, insofar as attributes of the beneficial mutation (e.g., the dominance coefficient) affect the power to detect targets of selection, genomic scans will yield an unrepresentative subset of loci that contribute to adaptations."
1193,"a comparison and evaluation of multiview stereo reconstruction algorithms",800904,"A Comparison and Evaluation of Multi-View Stereo Reconstruction Algorithms","This paper presents a quantitative comparison of several multi-view stereo reconstruction algorithms. Until now, the lack of suitable calibrated multi-view image datasets with known ground truth (3D shape models) has prevented such direct comparisons. In this paper, we first survey multi-view stereo algorithms and compare them qualitatively using a taxonomy that differentiates their key properties. We then describe our process for acquiring and calibrating multiview image datasets with high-accuracy ground truth and introduce our evaluation methodology. Finally, we present the results of our quantitative comparison of state-of-the-art multi-view stereo reconstruction algorithms on six benchmark datasets. The datasets, evaluation details, and instructions for submitting new models are available online at http://vision.middlebury.edu/mview."
1194,"constrained kmeans clustering with background knowledge",801335,"Constrained K-means Clustering with Background Knowledge","Clustering is traditionally viewed as an unsupervised method for data analysis. However, in some cases information about the problem domain is available in addition to the data instances themselves. In this paper, we demonstrate how the popular k-means clustering algorithm can be protably modi-ed to make use of this information. In experiments with articial constraints on six data sets, we observe improvements in clustering accuracy. We also apply this method to the real-world problem of automatically detecting road lanes from GPS data and observe dramatic increases in performance. 1."
1195,"revealing the world of rna interference",801860,"Revealing the world of RNA interference.","The recent discoveries of RNA interference and related RNA silencing pathways have revolutionized our understanding of gene regulation. RNA interference has been used as a research tool to control the expression of specific genes in numerous experimental organisms and has potential as a therapeutic strategy to reduce the expression of problem genes. At the heart of RNA interference lies a remarkable RNA processing mechanism that is now known to underlie many distinct biological phenomena."
1196,"autonomic live adaptation of virtual computational environments in a multidomain infrastructure",802360,"Autonomic Live Adaptation of Virtual Computational Environments in a Multi-Domain Infrastructure","A shared distributed infrastructure is formed by federating computation resources from multiple domains. Such shared infrastructures are increasing in popularity and are providing massive amounts of aggregated computation resources to large numbers of users. Meanwhile, virtualization technologies, at machine and network levels, are maturing and enabling mutually isolated virtual computation environments for executing arbitrary parallel/distributed applications on top of such a shared physical infrastructure. In this paper, we go one step further by supporting autonomic adaptation of virtual computation environments as active, integrated entities. More specifically, driven by both dynamic availability of infrastructure resources and dynamic application resource demand, a virtual computation environment is able to automatically relocate itself across the infrastructure and scale its share of infrastructural resources. Such autonomic adaptation is transparent to both users of virtual environments and administrators of infrastructures, maintaining the look and feel of a stable, dedicated environment for the user. As our proofof-concept, we present the design, implementation, and evaluation of a system called VIOLIN, which is composed of a virtual network of virtual machines capable of live migration across a multi-domain physical infrastructure. 1"
1197,"accelerating collaboration with social tools",804369,"Accelerating Collaboration With Social Tools","As more and more corporate ethnographic work is crossing international borders, we are increasingly collaborating with teams that are spread across the globe. As a result, we need tools that enable us to work across boundaries. Since early 2004, the authors have been collaborating on a research project developed by an American company seeking to develop solutions specific to the Indian market. One of us, an Indian sociologist, led a team of ethnographers in India, while the other, an American anthropologist, managed research and analysis for concept development in the US. While all of the US -based team members spent time in the field in India during the project, integrating the teams into the same ""brainspace"" was a challenge. This paper describes how we used social tools to enable each set of team members to understand the work being done on the other side of the world."
1198,"the evolution of adaptive immune systems",808558,"The Evolution of Adaptive Immune Systems"," A clonally diverse anticipatory repertoire in which each lymphocyte bears a unique antigen receptor is the central feature of the adaptive immune system that evolved in our vertebrate ancestors. The survival advantage gained through adding this type of adaptive immune system to a pre-existing innate immune system led to the evolution of alternative ways for lymphocytes to generate diverse antigen receptors for use in recognizing and repelling pathogen invaders. All jawed vertebrates assemble their antigen-receptor genes through recombinatorial rearrangement of different immunoglobulin or T cell receptor gene segments. The surviving jawless vertebrates, lampreys and hagfish, instead solved the receptor diversification problem by the recombinatorial assembly of leucine-rich-repeat genetic modules to encode variable lymphocyte receptors. The convergent evolution of these remarkably different adaptive immune systems involved innovative genetic modification of innate-immune-system components."
1199,"discrete mathematics methods and challenges",808574,"Discrete mathematics: methods and challenges","Combinatorics is a fundamental mathematical discipline as well as an essential component of many mathematical areas, and its study has experienced an impressive growth in recent years. One of the main reasons for this growth is the tight connection between Discrete Mathematics and Theoretical Computer Science, and the rapid development of the latter. While in the past many of the basic combinatorial results were obtained mainly by ingenuity and detailed reasoning, the modern theory has grown out of this early stage, and often relies on deep, well developed tools. This is a survey of two of the main general techniques that played a crucial role in the development of modern combinatorics; algebraic methods and probabilistic methods. Both will be illustrated by examples, focusing on the basic ideas and the connection to other areas."
1200,"microrna promoter element discovery in arabidopsis",812862,"MicroRNA promoter element discovery in Arabidopsis","In this study we present a method of identifying Arabidopsis miRNA promoter elements using known transcription factor binding motifs. We provide a comparative analysis of the representation of these elements in miRNA promoters, protein-coding gene promoters, and random genomic sequences. We report five transcription factor (TF) binding motifs that show evidence of overrepresentation in miRNA promoter regions relative to the promoter regions of protein-coding genes. This investigation is based on the analysis of 800-nucleotide regions upstream of 63 experimentally verified Transcription Start Sites (TSS) for miRNA primary transcripts in Arabidopsis. While the TATA-box binding motif was also previously reported by Xie and colleagues, the transcription factors AtMYC2, ARF, SORLREP3, and LFY are identified for the first time as overrepresented binding motifs in miRNA promoters."
1201,"artificial neural networks a tutorial",814224,"Artificial neural networks: A tutorial","Numerous efforts have been made in developing &amp;quot;intelligent &amp;quot; programs based on the Von Neumann&#039;s centralized architecture. However, these efforts have not been very successful in building general-purpose intelligent systems. Inspired by biological neural networks, researchers in a number of scientific disciplines are designing artificial neural networks (ANNs) to solve a variety of problems in decision making, optimization, prediction, and control. Artificial neural networks can be viewed as parallel and distributed processing systems which consist of a huge number of simple and massively connected processors. There has been a resurgence of interest in the field of ANNs for several years. This article intends to serve as a tutorial for those readers with little or no knowledge about ANNs to enable them to understand the remaining articles of this special issue. We discuss the motivations behind developing ANNs, basic network models, and two main issues in designing ANNs: network architecture and learning process. We also present one of the most successful application of ANNs, namely automatic character recognition."
1202,"a review of cardiac image registration methods",816093,"A review of cardiac image registration methods.","In this paper, the current status of cardiac image registration methods is reviewed. The combination of information from multiple cardiac image modalities, such as magnetic resonance imaging, computed tomography, positron emission tomography, single-photon emission computed tomography, and ultrasound, is of increasing interest in the medical community for physiologic understanding and diagnostic purposes. Registration of cardiac images is a more complex problem than brain image registration because the heart is a nonrigid moving organ inside a moving body. Moreover, as compared to the registration of brain images, the heart exhibits much fewer accurate anatomical landmarks. In a clinical context, physicians often mentally integrate image information from different modalities. Automatic registration, based on computer programs, might, however, offer better accuracy and repeatability and save time."
1203,"from wikipedia to the classroom exploring online publication and learning",816513,"From Wikipedia to the classroom: exploring online publication and learning","Wikipedia represents an intriguing new publishing paradigm---can it be used to engage students in authentic collaborative writing activities? How can we design wiki publishing tools and curricula to support learning among student authors? We suggest that wiki publishing environments can create learning opportunities that address four dimensions of authenticity: personal, real world, disciplinary, and assessment. We have begun a series of design studies to investigate links between wiki publishing experiences and writing-to-learn. The results of an initial study in an undergraduate government course indicate that perceived audience plays an important role in helping students monitor the quality of writing; however, students' perception of audience on the Internet is not straightforward. This preliminary iteration resulted in several guidelines that are shaping efforts to design and implement new wiki publishing tools and curricula for students and teachers."
1204,"computational detection of genomic cisregulatory modules applied to body patterning in the early drosophila embryo",816965,"Computational detection of genomic cis-regulatory modules applied to body patterning in the early Drosophila embryo.","BACKGROUND: Regulation of gene transcription is crucial for the function and development of all organisms. While gene prediction programs that identify protein coding sequence are used with remarkable success in the annotation of genomes, the development of computational methods to analyze noncoding regions and to delineate transcriptional control elements is still in its infancy. RESULTS: Here we present novel algorithms to detect cis-regulatory modules through genome wide scans for clusters of transcription factor binding sites using three levels of prior information. When binding sites for the factors are known, our statistical segmentation algorithm, Ahab, yields about 150 putative gap gene regulated modules, with no adjustable parameters other than a window size. If one or more related modules are known, but no binding sites, repeated motifs can be found by a customized Gibbs sampler and input to Ahab, to predict genes with similar regulation. Finally using only the genome, we developed a third algorithm, Argos, that counts and scores clusters of overrepresented motifs in a window of sequence. Argos recovers many of the known modules, upstream of the segmentation genes, with no training data. CONCLUSIONS: We have demonstrated, in the case of body patterning in the Drosophila embryo, that our algorithms allow the genome-wide identification of regulatory modules. We believe that Ahab overcomes many problems of recent approaches and we estimated the false positive rate to be about 50\%. Argos is the first successful attempt to predict regulatory modules using only the genome without training data. Complete results and module predictions across the Drosophila genome are available at http://uqbar.rockefeller.edu/~siggia/."
1205,"mining newsgroups using networks arising from social behavior",820132,"Mining newsgroups using networks arising from social behavior","Recent advances in information retrieval over hyperlinked corpora have convincingly demonstrated that links carry less noisy information than text. We investigate the feasibility of applying link-based methods in new applications domains. The specific application we consider is to partition authors into opposite camps within a given topic in the context of newsgroups. A typical newsgroup posting consists of one or more quoted lines from another posting followed by the opinion of the author. This social behavior gives rise to a network in which the vertices are individuals and the links represent ""responded-to"" relationships. An interesting characteristic of many newsgroups is that people more frequently respond to a message when they disagree than when they agree. This behavior is in sharp contrast to the WWW link graph, where linkage is an indicator of agreement or common interest. By analyzing the graph structure of the responses, we are able to effectively classify people into opposite camps. In contrast, methods based on statistical analysis of text yield low accuracy on such datasets because the vocabulary used by the two sides tends to be largely identical, and many newsgroup postings consist of relatively few words of text."
1206,"an introduction to roc analysis",820297,"An introduction to ROC analysis","Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research. (c) 2005 Elsevier B.V. All rights reserved."
1207,"granger causality basic theory and application to neuroscience",820473,"Granger Causality: Basic Theory and Application to Neuroscience","Multi-electrode neurophysiological recordings produce massive quantities of data. Multivariate time series analysis provides the basic framework for analyzing the patterns of neural interactions in these data. It has long been recognized that neural interactions are directional. Being able to assess the directionality of neuronal interactions is thus a highly desired capability for understanding the cooperative nature of neural computation. Research over the last few years has shown that Granger causality is a key technique to furnish this capability. The main goal of this article is to provide an expository introduction to the concept of Granger causality. Mathematical frameworks for both bivariate Granger causality and conditional Granger causality are developed in detail with particular emphasis on their spectral representations. The technique is demonstrated in numerical examples where the exact answers of causal influences are known. It is then applied to analyze multichannel local field potentials recorded from monkeys performing a visuomotor task. Our results are shown to be physiologically interpretable and yield new insights into the dynamical organization of large-scale oscillatory cortical networks."
1208,"identification of genetic networks from a small number of gene expression patterns under the boolean network model",820512,"{{I}dentification of genetic networks from a small number of gene expression patterns under the {B}oolean network model}","{{L}iang, {F}uhrman and {S}omogyi ({PSB}98, 18-29, 1998) have described an algorithm for inferring genetic network architectures from state transition tables which correspond to time series of gene expression patterns, using the {B}oolean network model. {T}heir results of computational experiments suggested that a small number of state transition ({INPUT}/{OUTPUT}) pairs are sufficient in order to infer the original {B}oolean network correctly. {T}his paper gives a mathematical proof for their observation. {P}recisely, this paper devises a much simpler algorithm for the same problem and proves that, if the indegree of each node (i.e., the number of input nodes to each node) is bounded by a constant, only {O}(log n) state transition pairs (from 2n pairs) are necessary and sufficient to identify the original {B}oolean network of n nodes correctly with high probability. {W}e made computational experiments in order to expose the constant factor involved in {O}(log n) notation. {T}he computational results show that the {B}oolean network of size 100,000 can be identified by our algorithm from about 100 {INPUT}/{OUTPUT} pairs if the maximum indegree is bounded by 2. {I}t is also a merit of our algorithm that the algorithm is conceptually so simple that it is extensible for more realistic network models.}"
1209,"conservation of dna regulatory motifs and discovery of new motifs in microbial genomes",820838,"Conservation of {DNA} regulatory motifs and discovery of new motifs in microbial genomes","Regulatory motifs can be found by local multiple alignment of upstream regions from coregulated sets of genes, or regulons. {W}e searched for regulatory motifs using the program {A}lign{ACE} together with a set of filters that helped us choose the motifs most likely to be biologically relevant in 17 complete microbial genomes. {W}e searched the upstream regions of potentially coregulated genes grouped by three methods: (1) genes that make up functional pathways; (2) genes homologous to regulons from a well-studied species ({E}scherichia coli); and (3) groups of genes derived from conserved operons. {T}his last group is based on the observation that genes making up homologous regulons in different species are often assorted into coregulated operons in different combinations. {T}his allows partial reconstruction of regulons by looking at operon structure across several species. {U}nlike other methods for predicting regulons, this method does not depend on the availability of experimental data other than the genome sequence and the locations of genes. {N}ew, statistically significant motifs were found in the genome sequence of each organism using each grouping method. {T}he most significant new motif was found upstream of genes in the methane-metabolism functional group in {M}ethanobacterium thermoautotrophicum. {W}e found that at least 27% of the known {E}. coli {DNA}-regulatory motifs are conserved in one or more distantly related eubacteria. {W}e also observed significant motifs that differed from the {E}. coli motif in other organisms upstream of sets of genes homologous to known {E}. coli regulons, including {C}rp, {L}ex{A}, and {A}rc{A} in {B}acillus subtilis; four anaerobic regulons in {A}rchaeoglobus fulgidus ({N}ar{L}, {N}ar{P}, {F}nr, and {M}od{E}); and the {P}ho{B}, {P}ur{R}, {R}po{H}, and {F}hl{A} regulons in other archaebacterial species. {W}e also used motif conservation to aid in finding new motifs by grouping upstream regions from closely related bacteria, thus increasing the number of instances of the motif in the sequence to be aligned. {F}or example, by grouping upstream sequences from three archaebacterial species, we found a conserved motif that may regulate ferrous ion transport that was not found in individual genomes. {D}iscovery of conserved motifs becomes easier as the number of closely related genome sequences increases."
1210,"pairwise alignment of protein interaction networks",821236,"Pairwise alignment of protein interaction networks.","With an ever-increasing amount of available data on protein-protein interaction (PPI) networks and research revealing that these networks evolve at a modular level, discovery of conserved patterns in these networks becomes an important problem. Although available data on protein-protein interactions is currently limited, recently developed algorithms have been shown to convey novel biological insights through employment of elegant mathematical models. The main challenge in aligning PPI networks is to define a graph theoretical measure of similarity between graph structures that captures underlying biological phenomena accurately. In this respect, modeling of conservation and divergence of interactions, as well as the interpretation of resulting alignments, are important design parameters. In this paper, we develop a framework for comprehensive alignment of PPI networks, which is inspired by duplication/divergence models that focus on understanding the evolution of protein interactions. We propose a mathematical model that extends the concepts of match, mismatch, and gap in sequence alignment to that of match, mismatch, and duplication in network alignment and evaluates similarity between graph structures through a scoring function that accounts for evolutionary events. By relying on evolutionary models, the proposed framework facilitates interpretation of resulting alignments in terms of not only conservation but also divergence of modularity in PPI networks. Furthermore, as in the case of sequence alignment, our model allows flexibility in adjusting parameters to quantify underlying evolutionary relationships. Based on the proposed model, we formulate PPI network alignment as an optimization problem and present fast algorithms to solve this problem. Detailed experimental results from an implementation of the proposed framework show that our algorithm is able to discover conserved interaction patterns very effectively, in terms of both accuracies and computational cost."
1211,"conservation of expression and sequence of metabolic genes is reflected by activity across metabolic states",821916,"Conservation of Expression and Sequence of Metabolic Genes Is Reflected by Activity Across Metabolic States.","Variation in gene expression levels on a genomic scale has been detected among different strains, among closely related species, and within populations of genetically identical cells. What are the driving forces that lead to expression divergence in some genes and conserved expression in others? Here we employ flux balance analysis to address this question for metabolic genes. We consider the genome-scale metabolic model of Saccharomyces cerevisiae, and its entire space of optimal and near-optimal flux distributions. We show that this space reveals underlying evolutionary constraints on expression regulation, as well as on the conservation of the underlying gene sequences. Genes that have a high range of optimal flux levels tend to display divergent expression levels among different yeast strains and species. This suggests that gene regulation has diverged in those parts of the metabolic network that are less constrained. In addition, we show that genes that are active in a large fraction of the space of optimal solutions tend to have conserved sequences. This supports the possibility that there is less selective pressure to maintain genes that are relevant for only a small number of metabolic states."
1212,"on numbers and games",822067,"On Numbers and Games","{ONAG, as the book is known, is one of those rare publications that sprang to life in a  moment of creative energy and has remained influential for over a quarter of a century. Still in high  demand, it is being republished with some adjustments and corrections. The original motivation for  writing the book was an attempt to understand the relation between the theories of transfinite numbers and  mathematical games. By defining numbers as the strengths of positions in certain games, the author  arrives at a new class, the surreal numbers (so named by Donald Knuth) that includes at the same time the  real numbers and the ordinal numbers.   <P>This new edition ends with an epilogue that sets the stage for further research on surreal numbers. The  book is a must-have for all readers with a serious interest in the mathematical foundations of game  strategies.}"
1213,"nast a multiple sequence alignment server for comparative analysis of s rrna genes",822218,"NAST: a multiple sequence alignment server for comparative analysis of 16S rRNA genes.","Microbiologists conducting surveys of bacterial and archaeal diversity often require comparative alignments of thousands of 16S rRNA genes collected from a sample. The computational resources and bioinformatics expertise required to construct such an alignment has inhibited high-throughput analysis. It was hypothesized that an online tool could be developed to efficiently align thousands of 16S rRNA genes via the NAST (Nearest Alignment Space Termination) algorithm for creating multiple sequence alignments (MSA). The tool was implemented with a web-interface at http://greengenes.lbl.gov/NAST. Each user-submitted sequence is compared with Greengenes' 'Core Set', comprising approximately 10,000 aligned non-chimeric sequences representative of the currently recognized diversity among bacteria and archaea. User sequences are oriented and paired with their closest match in the Core Set to serve as a template for inserting gap characters. Non-16S data (sequence from vector or surrounding genomic regions) are conveniently removed in the returned alignment. From the resulting MSA, distance matrices can be calculated for diversity estimates and organisms can be classified by taxonomy. The ability to align and categorize large sequence sets using a simple interface has enabled researchers with various experience levels to obtain bacterial and archaeal community profiles."
1214,"efficient pairwise rna structure prediction and alignment using sequence alignment constraints",828031,"Efficient pairwise RNA structure prediction and alignment using sequence alignment constraints","BACKGROUND:We are interested in the problem of predicting secondary structure for small sets of homologous RNAs, by incorporating limited comparative sequence information into an RNA folding model. The Sankoff algorithm for simultaneous RNA folding and alignment is a basis for approaches to this problem. There are two open problems in applying a Sankoff algorithm: development of a good unified scoring system for alignment and folding and development of practical heuristics for dealing with the computational complexity of the algorithm.RESULTS:We use probabilistic models (pair stochastic context-free grammars, pairSCFGs) as a unifying framework for scoring pairwise alignment and folding. A constrained version of the pairSCFG structural alignment algorithm was developed which assumes knowledge of a few confidently aligned positions (pins). These pins are selected based on the posterior probabilities of a probabilistic pairwise sequence alignment.CONCLUSION:Pairwise RNA structural alignment improves on structure prediction accuracy relative to single sequence folding. Constraining on alignment is a straightforward method of reducing the runtime and memory requirements of the algorithm. Five practical implementations of the pairwise Sankoff algorithm - this work (Consan), David Mathews' Dynalign, Ian Holmes' Stemloc, Ivo Hofacker's PMcomp, and Jan Gorodkin's FOLDALIGN - have comparable overall performance with different strengths and weaknesses."
1215,"evaluation of itembased topn recommendation algorithms",832827,"Evaluation of Item-Based Top-N Recommendation Algorithms","The explosive growth of the world-wide-web and the emergence of e-commerce has led to the development of recommender systems---a personalized information filtering technology used to identify a set of N items that will be of interest to a certain user. User-based Collaborative filtering is the most successful technology for building recommender systems to date, and is extensively used in many commercial recommender systems. Unfortunately, the computational complexity of these methods grows..."
1216,"public library towards a new mission for public libraries as a network of community knowledge",834982,"Public Library 2.0: Towards a new mission for public libraries as a ?network of community knowledge?","This article seeks to propose a new vision for public libraries in the digital age. This conceptual paper is based on an understanding of the recent developments in ICT, internet and digital libraries; and also on the authors' personal experience in research and development in library and information science - especially in relation to public libraries - and digital libraries. The study argues that currently there are no proper mechanisms for capturing, preserving and disseminating community knowledge, and proposes that public libraries in the digital age should take a new role whereby they should act not only as a gateway to knowledge, but also as a platform facilitating the creation of, and access to, local community knowledge. The paper proposes a model for PL2.0 where public libraries can take on this new role to build a network of community knowledge. It also proposes a conceptual model for the second generation of public libraries, and further studies are required to test and implement the model. The paper proposes that the new role of public libraries will be to shift from solely providing access to knowledge to acting as a platform for the storage and dissemination of local community knowledge within the global context created by twenty-first century digital technologies."
1217,"the twodimensional spatial structure of simple receptive fields in cat striate cortex",837815,"The Two-Dimensional Spatial Structure of Simple Receptive Fields in Cat Striate Cortex","1. A reverse correlation (6, 8, 25, 35) method is developed that allows quantitative determination of visual receptive-field structure in two spatial dimensions. This method is applied to simple cells in the cat striate cortex. 2. It is demonstrated that the reverse correlation method yields results with several desirable properties, including convergence and reproducibility independent of modest changes in stimulus parameters. 3. In contrast to results obtained with moving stimuli, we find that the bright and dark excitatory subregions in simple receptive fields do not overlap to any great extent. This difference in results may be attributed to confounding the independent variables space and time when using moving stimuli. 4. All simple receptive fields have subregions that vary smoothly in all directions in space. There are no sharp transitions either between excitatory subregions or between subregions and the area surrounding the receptive field. 5. Simple receptive fields vary both in the number of subregions observed, in the elongation of each subregion, and in the overall elongation of the field. In contrast with results obtained using moving stimuli, we find that subregions within a given receptive field need not be the same length. 6. The hypothesis that simple receptive fields can be modeled as either even symmetric or odd symmetric about a central axis is evaluated. This hypothesis is found to be false in general. Most simple receptive fields are neither even symmetric nor odd symmetric. 7. The hypothesis that simple receptive fields can be modeled as the product of a width response profile and an orthogonal length response profile (Cartesian separability) is evaluated. This hypothesis is found to be true for only approximately 50% of the cells in our sample."
1218,"recovering high dynamic range radiance maps from photographs",841692,"Recovering high dynamic range radiance maps from photographs","We present a method of recovering high dynamic range radiance maps from photographs taken with conventional imaging equipment. In our method, multiple photographs of the scene are taken with different amounts of exposure. Our algorithm uses these differently exposed photographs to recover the response function of the imaging process, up to factor of scale, using the assumption of reciprocity. With the known response function, the algorithm can fuse the multiple photographs into a single, high dynamic range radiance map whose pixel values are proportional to the true radiance values in the scene. We demonstrate our method on images acquired with both photochemical and digital imaging processes. We discuss how this work is applicable in many areas of computer graphics involving digitized photographs, including image-based modeling, image compositing, and image processing. Lastly, we demonstrate a few applications of having high dynamic range radiance maps, such as synthesizing realistic motion blur and simulating the response of the human visual system."
1219,"jamming phase diagram for attractive particles",841946,"Jamming phase diagram for attractive particles","A wide variety of systems, including granular media, colloidal suspensions and molecular systems, exhibit non-equilibrium transitions from a fluid-like to a solid-like state, characterized solely by the sudden arrest of their dynamics. Crowding or jamming of the constituent particles traps them kinetically, precluding further exploration of the phase space(1). The disordered fluid-like structure remains essentially unchanged at the transition. The jammed solid can be refluidized by thermalization, through temperature or vibration, or by an applied stress. The generality of the jamming transition led to the proposal(2) of a unifying description, based on a jamming phase diagram. It was further postulated that attractive interactions might have the same effect in jamming the system as a confining pressure, and thus could be incorporated into the generalized description. Here we study experimentally the fluid-to-solid transition of weakly attractive colloidal particles, which undergo markedly similar gelation behaviour with increasing concentration and decreasing thermalization or stress. Our results support the concept of a jamming phase diagram for attractive colloidal particles, providing a unifying link between the glass transition(3), gelation(4,5) and aggregation(6-8)."
1220,"ecosystems and the biosphere as complex adaptive systems",848440,"{Ecosystems and the Biosphere as Complex Adaptive Systems}","ABSTRACT&nbsp;&nbsp; Ecosystems are prototypical examples of complex adaptive systems, in which patterns at higher levels emerge from localized interactions and selection processes acting at lower levels. An essential aspect of such systems is nonlinearity, leading to historical dependency and multiple possible outcomes of dynamics. Given this, it is essential to determine the degree to which system features are determined by environmental conditions, and the degree to which they are the result of self-organization. Furthermore, given the multiple levels at which dynamics become apparent and at which selection can act, central issues relate to how evolution shapes ecosystems properties, and whether ecosystems become buffered to changes (more resilient) over their ecological and evolutionary development or proceed to critical states and the edge of chaos."
1221,"cpg island methylation in human lymphocytes is highly correlated with dna sequence repeats and predicted dna structure",848639,"CpG island methylation in human lymphocytes is highly correlated with DNA sequence, repeats, and predicted DNA structure.","CpG island methylation plays an important role in epigenetic gene control during mammalian development and is frequently altered in disease situations such as cancer. The majority of CpG islands is normally unmethylated, but a sizeable fraction is prone to become methylated in various cell types and pathological situations. The goal of this study is to show that a computational epigenetics approach can discriminate between CpG islands that are prone to methylation from those that remain unmethylated. We develop a bioinformatics scoring and prediction method on the basis of a set of 1,184 DNA attributes, which refer to sequence, repeats, predicted structure, CpG islands, genes, predicted binding sites, conservation, and single nucleotide polymorphisms. These attributes are scored on 132 CpG islands across the entire human Chromosome 21, whose methylation status was previously established for normal human lymphocytes. Our results show that three groups of DNA attributes, namely certain sequence patterns, specific DNA repeats, and a particular DNA structure, are each highly correlated with CpG island methylation (correlation coefficients of 0.64, 0.66, and 0.49, respectively). We predicted, and subsequently experimentally examined 12 CpG islands from human Chromosome 21 with unknown methylation patterns and found more than 90&#37; of our predictions to be correct. In addition, we applied our prediction method to analyzing Human Epigenome Project methylation data on human Chromosome 6 and again observed high prediction accuracy. In summary, our results suggest that DNA composition of CpG islands (sequence, repeats, and structure) plays a significant role in predisposing CpG islands for DNA methylation. This finding may have a strong impact on our understanding of changes in CpG island methylation in development and disease."
1222,"symbol grounding and meaning a comparison of highdimensional and embodied theories of meaning",850491,"Symbol grounding and meaning: {A} comparison of high-dimensional and embodied theories of meaning.","Latent Semantic Analysis (Landauer & Dumais, 1997) and Hyperspace Analogue to Language (Burgess & Lund, 1997) model meaning as the relations among abstract symbols that are arbitrarily related to what they signify. These symbols are ungrounded in that they are not tied to perceptual experience or action. Because the symbols are ungrounded, they cannot, in principle, capture the meaning of novel situations. In contrast, participants in three experiments found it trivially easy to discriminate between descriptions of sensible novel situations (e.g., using a newspaper to protect one's face from the wind) and nonsense novel situations (e.g., using a matchbook to protect one's face from the wind). These results support the Indexical Hypothesis that the meaning of a sentence is constructed by (a) indexing words and phrases to real objects or perceptual, analog symbols; (b) deriving affordances from the objects and symbols; and (c) meshing the affordances under the guidance of syntax. Copyright 2000 Academic Press."
1223,"finding the flow in web site search",867069,"Finding the flow in web site search","this article we focus on the middle part of the answer type spectrum by posing the question of how to design a search system and interface that provide a browsing the shelves sensation for large collections of information items. We first summarize what is known from usability results about how to design good search user interfaces. We then illustrate these principles with a browse-and-search interface framework we have developed that has been successful in preliminary usability studies"
1224,"comparisons of dnds are time dependent for closely related bacterial genomes",867481,"Comparisons of dN/dS are time dependent for closely related bacterial genomes","The ratio of non-synonymous (dN) to synonymous (dS) changes between taxa is frequently computed to assay the strength and direction of selection. Here we note that for comparisons between closely related strains and/or species a second parameter needs to be considered, namely the time since divergence of the two sequences under scrutiny. We demonstrate that a simple time lag model provides a general, parsimonious explanation of the extensive variation in the dN/dS ratio seen when comparing closely related bacterial genomes. We explore this model through simulation and comparative genomics, and suggest a role for hitch-hiking in the accumulation of non-synonymous mutations. We also note taxon-specific differences in the change of dN/dS over time, which may indicate variation in selection, or in population genetics parameters such as population size or the rate of recombination. The effect of comparing intra-species polymorphism and inter-species substitution, and the problems associated with these concepts for asexual prokaryotes, are also discussed. We conclude that, because of the critical effect of time since divergence, inter-taxa comparisons are only possible by comparing trajectories of dN/dS over time and it is not valid to compare taxa on the basis of single time points."
1225,"language within our grasp",874803,"Language within our grasp","In monkeys, the rostral part of ventral premotor cortex (area F5) contains neurons that discharge, both when the monkey grasps or manipulates objects and when it observes the experimenter making similar actions. These neurons (mirror neurons) appear to represent a system that matches observed events to similar, internally generated actions, and in this way forms a link between the observer and the actor. Transcranial magnetic stimulation and positron emission tomography (PET) experiments suggest that a mirror system for gesture recognition also exists in humans and includes Broca's area. We propose here that such an observation/execution matching system provides a necessary bridge from'doing' to'communicating',as the link between actor and observer becomes a link between the sender and the receiver of each message Type: JOURNAL ARTICLE Type: REVIEW Type: REVIEW, TUTORIAL Language: Eng"
1226,"the semantic grid past present and future",875656,"The Semantic Grid: Past, Present and Future.","Grid computing offers significant enhancements to our capabilities for computation, information processing, and collaboration, and has exciting ambitions in many fields of endeavor. We argue that the full richness of the Grid vision, with its application in e-Science, e-Research, or e-Business, requires the ""Semantic Grid."" The Semantic Grid is an extension of the current Grid in which information and services are given well-defined meaning, better enabling computers and people to work in cooperation. To this end, we outline the requirements of the Semantic Grid, discuss the state of the art in achieving them, and identify the key research challenges in realizing this vision."
1227,"genetic and developmental basis of evolutionary pelvic reduction in threespine sticklebacks",876030,"Genetic and developmental basis of evolutionary pelvic reduction in threespine sticklebacks","Hindlimb loss has evolved repeatedly in many different animals by means of molecular mechanisms that are still unknown. To determine the number and type of genetic changes underlying pelvic reduction in natural populations, we carried out genetic crosses between threespine stickleback fish with complete or missing pelvic structures. Genome-wide linkage mapping shows that pelvic reduction is controlled by one major and four minor chromosome regions. Pitx1 maps to the major chromosome region controlling most of the variation in pelvic size. Pelvic-reduced fish show the same left–right asymmetry seen in Pitx1 knockout mice, but do not show changes in Pitx1 protein sequence. Instead, pelvic-reduced sticklebacks show site-specific regulatory changes in Pitx1 expression, with reduced or absent expression in pelvic and caudal fin precursors. Regulatory mutations in major developmental control genes may provide a mechanism for generating rapid skeletal changes in natural populations, while preserving the essential roles of these genes in other processes."
1228,"meme discovering and analyzing dna and protein sequence motifs",876469,"MEME: discovering and analyzing DNA and protein sequence motifs.","{MEME} {(Multiple} {EM} for Motif Elicitation) is one of the most widely used tools for searching for novel 'signals' in sets of biological sequences. Applications include the discovery of new transcription factor binding sites and protein domains. {MEME} works by searching for repeated, ungapped sequence patterns that occur in the {DNA} or protein sequences provided by the user. Users can perform {MEME} searches via the web server hosted by the National Biomedical Computation Resource (http://meme.nbcr.net) and several mirror sites. Through the same web server, users can also access the Motif Alignment and Search Tool to search sequence databases for matches to motifs encoded in several popular formats. By clicking on buttons in the {MEME} output, users can compare the motifs discovered in their input sequences with databases of known motifs, search sequence databases for matches to the motifs and display the motifs in various formats. This article describes the freely accessible web server and its architecture, and discusses ways to use {MEME} effectively to find new sequence patterns in biological sequences and analyze their significance."
1229,"bioprospector discovering conserved dna motifs in upstream regulatory regions of coexpressed genes",876610,"Bioprospector: Discovering conserved DNA motifs in upstream regulatory regions of co-expressed genes.","The development of genome sequencing and DNA microarray analysis of gene expression gives rise to the demand for data-mining tools. BioProspector, a C program using a Gibbs sampling strategy, examines the upstream region of genes in the same gene expression pattern group and looks for regulatory sequence motifs. BioProspector uses zero to third-order Markov background models whose parameters are either given by the user or estimated from a specified sequence file. The significance of each motif found is judged based on a motif score distribution estimated by a Monte Carlo method. In addition, BioProspector modifies the motif model used in the earlier Gibbs samplers to allow for the modeling of gapped motifs and motifs with palindromic patterns. All these modifications greatly improve the performance of the program. Although testing and development are still in progress, the program has shown preliminary success in finding the binding motifs for Saccharomyces cerevisiae RAP1, Bacillus subtilis RNA polymerase, and Escherichia coli CRP. We are currently working on combining BioProspector with a clustering program to explore gene expression networks and regulatory mechanisms. For a copy of the program and documentation for UNIX systems, please contact xliu@smi.stanford.edu."
1230,"possible ancestral structure in human populations",876801,"Possible Ancestral Structure in Human Populations","Determining the evolutionary relationships between fossil hominid groups such as Neanderthals and modern humans has been a question of enduring interest in human evolutionary genetics. Here we present a new method for addressing whether archaic human groups contributed to the modern gene pool (called ancient admixture), using the patterns of variation in contemporary human populations. Our method improves on previous work by explicitly accounting for recent population history before performing the analyses. Using sequence data from the Environmental Genome Project, we find strong evidence for ancient admixture in both a European and a West African population (p &#8776; 10&#8722;7), with contributions to the modern gene pool of at least 5&#37;. While Neanderthals form an obvious archaic source population candidate in Europe, there is not yet a clear source population candidate in West Africa."
1231,"large datasets at a glance combining textures and colors in scientific visualization",878218,"Large datasets at a glance: combining textures and colors in scientific visualization","We present a new method for using texture and color to visualize multivariate data elements arranged on an underlying height field. We combine simple texture patterns with perceptually uniform colors to increase the number of attribute values we can display simultaneously. Our technique builds multicolored perceptual texture elements (or pexels) to represent each data element. Attribute values encoded in an element are used to vary the appearance of its pexel. Texture and color patterns that form when the pexels are displayed can be used to rapidly and accurately explore the dataset. Our pexels are built by varying three separate texture dimensions: height, density, and regularity. Results from computer graphics, computer vision, and human visual psychophysics have identified these dimensions as important for the formation of perceptual texture patterns. The pexels are colored using a selection technique that controls color distance, linear separation, and color category. Proper use of these criteria guarantees colors that are equally distinguishable from one another. We describe a set of controlled experiments that demonstrate the effectiveness of our texture dimensions and color selection criteria. We then discuss new work that studies how texture and color can be used simultaneously in a single display"
1232,"d complex a structural classification of protein complexes",886600,"3D Complex: a Structural Classification of Protein Complexes","Most of the proteins in a cell assemble into complexes to carry out their function. It is therefore crucial to understand the physico-chemical properties as well as the evolution of interactions between proteins. The Protein Data Bank represents an important source of information for such studies, because more than half of the structures are homo- or heteromeric protein complexes. Here we propose the first hierarchical classification of whole protein complexes of known three-dimensional structure, based on representing their fundamental structural features as a graph. This classification provides the first overview of all the complexes in the Protein Data Bank and allows non-redundant sets to be derived at different levels of detail. This reveals that between one half and two thirds of known structures are multimeric, depending on the level of redundancy accepted. We also analyse the structures in terms of the topological arrangement of their subunits, and find that they form a small number of arrangements compared to all theoretically possible ones. This is because most complexes contain four subunits or less, and the large majority are homomeric. In addition, there is a strong tendency for symmetry in complexes, even for heteromeric complexes. Finally, through comparison of Biological Units in the Protein Data Bank with the Protein Quaternary Structure database, we identified many possible errors in quaternary structure assignments. Our classification, available as a database and web server at www.3Dcomplex.org, will be a starting point for future work aimed at understanding the structure and evolution of protein complexes."
1233,"from plant traits to plant communities a statistical mechanistic approach to biodiversity",888790,"From Plant Traits to Plant Communities: A Statistical Mechanistic Approach to Biodiversity","We developed a quantitative method, analogous to those used in statistical mechanics, to predict how biodiversity will vary across environments, which plant species from a species pool will be found in which relative abundances in a given environment, and which plant traits determine community assembly. This provides a scaling from plant traits to ecological communities while bypassing the complications of population dynamics. Our method treats community development as a sorting process involving species that are ecologically equivalent except with respect to particular functional traits, which leads to a constrained random assembly of species; the relative abundance of each species adheres to a general exponential distribution as a function of its traits. Using data for eight functional traits of 30 herbaceous species and community-aggregated values of these traits in 12 sites along a 42-year chronosequence of secondary succession, we predicted 94% of the variance in the relative abundances. 10.1126/science.1131344"
1234,"scaling personalized web search",891480,"Scaling personalized web search","Recent web search techniques augment traditional text matching with a global notion of ""importance"" based on the linkage structure of the web, such as in Google's PageRank algorithm. For more refined searches, this global notion of importance can be specialized to create personalized views of importance--for example, importance scores can be biased according to a user-specified set of initially-interesting pages. Computing and storing all possible personalized views in advance is impractical, as is computing personalized views at query time, since the computation of each view requires an iterative computation over the web graph. We present new graph-theoretical results, and a new technique based on these results, that encode personalized views as partial vectors. Partial vectors are shared across multiple personalized views, and their computation and storage costs scale well with the number of views. Our approach enables incremental computation, so that the construction of personalized views from partial vectors is practical at query time. We present efficient dynamic programming algorithms for computing partial vectors, an algorithm for constructing personalized views from partial vectors, and experimental results demonstrating the effectiveness and scalability of our techniques."
1235,"biod an r package for the comparative analysis of protein structures",892193,"Bio3d: an R package for the comparative analysis of protein structures.","Summary: An automated procedure for the analysis of homologous protein structures has been developed. The method facilitates the characterization of internal conformational differences and inter-conformer relationships and provides a framework for the analysis of protein structural evolution. The method is implemented in bio3d, an R package for the exploratory analysis of structure and sequence data. Availability: The bio3d package is distributed with full source code as a platform-independent R package under a GPL2 license from: http://mccammon.ucsd.edu/~bgrant/bio3d/ Contact: bgrant@mccammon.ucsd.edu"
1236,"panarchy understanding transformations in human and natural systems",898525,"Panarchy: Understanding Transformations in Human and Natural Systems","{<div>Creating institutions to meet the challenge of sustainability is arguably the most important task confronting society; it is also dauntingly complex. Ecological, economic, and social elements all play a role, but despite ongoing efforts, researchers have yet to succeed in integrating the various disciplines in a way that gives adequate representation to the insights of each.<i>Panarchy</i>, a term devised to describe evolving hierarchical systems with multiple interrelated elements, offers an important new framework for understanding and resolving this dilemma. Panarchy is the structure in which systems, including those of nature (e.g., forests) and of humans (e.g., capitalism), as well as combined human-natural systems (e.g., institutions that govern natural resource use such as the Forest Service), are interlinked in continual adaptive cycles of growth, accumulation, restructuring, and renewal. These transformational cycles take place at scales ranging from a drop of water to the biosphere, over periods from days to geologic epochs. By understanding these cycles and their scales, researchers can identify the points at which a system is capable of accepting positive change, and can use those leverage points to foster resilience and sustainability within the system.This volume brings together leading thinkers on the subject -- including Fikret Berkes, Buz Brock, Steve Carpenter, Carl Folke, Lance Gunderson, C.S. Holling, Don Ludwig, Karl-Goran Maler, Charles Perrings, Marten Scheffer, Brian Walker, and Frances Westley -- to develop and examine the concept of panarchy and to consider how it can be applied to human, natural, and human-natural systems. Throughout, contributors seek to identify adaptive approaches to management that recognize uncertainty and encourage innovation while fostering resilience.The book is a fundamental new development in a widely acclaimed line of inquiry. It represents the first step in integrating disciplinary knowledge for the adaptive management of human-natural systems across widely divergent scales, and offers an important base of knowledge from which institutions for adaptive management can be developed. It will be an invaluable source of ideas and understanding for students, researchers, and professionals involved with ecology, conservation biology, ecological economics, environmental policy, or related fields.</div>}"
1237,"text mining and its potential applications in systems biology",907397,"Text mining and its potential applications in systems biology"," With biomedical literature increasing at a rate of several thousand papers per week, it is impossible to keep abreast of all developments; therefore, automated means to manage the information overload are required. Text mining techniques, which involve the processes of information retrieval, information extraction and data mining, provide a means of solving this. By adding meaning to text, these techniques produce a more structured analysis of textual knowledge than simple word searches, and can provide powerful tools for the production and analysis of systems biology models."
1238,"caregivers of stroke patient family members behavioral and attitudinal indicators of overprotective care",909969,"Caregivers of Stroke Patient Family Members: Behavioral and Attitudinal Indicators of Overprotective Care","Three models of the sources of overprotection in stroke patients were tested in a study of the behavioral and attitudinal concomitants of overprotective caregiving. Stroke patients and their family member caregivers were interviewed to assess feelings of overprotection, physical and mental functioning, and caregiving-related attitudes. The couples were then videotaped as they interacted on four tasks and the tapes were coded for specific behaviors and affect directed toward the patient. Strong support was found for the Resentment model that overprotection will be associated with an overcontrolling caregiving style, negative affect, and resentment toward the stroke patient. No support was found for the model that overprotection is related to overhelping. In addition, feeling overprotected was associated with patient dependency. Two conclusions are discussed: the variety of potential sources of overprotection found in the study and the importance of the emotional tone with which caregivers provide assistance."
1239,"locating mammalian transcription factor binding sites a survey of computational and experimental techniques",910675,"Locating mammalian transcription factor binding sites: A survey of computational and experimental techniques","10.1101/gr.4140006 Fields such as genomics and systems biology are built on the synergism between computational and experimental techniques. This type of synergism is especially important in accomplishing goals like identifying all functional transcription factor binding sites in vertebrate genomes. Precise detection of these elements is a prerequisite to deciphering the complex regulatory networks that direct tissue specific and lineage specific patterns of gene expression. This review summarizes approaches for in silico, in vitro, and in vivo identification of transcription factor binding sites. A variety of techniques useful for localized- and high-throughput analyses are discussed here, with emphasis on aspects of data generation and verification."
1240,"bayesian inference with probabilistic population codes",913189,"Bayesian inference with probabilistic population codes.","Recent psychophysical experiments indicate that humans perform near-optimal Bayesian inference in a wide variety of tasks, ranging from cue integration to decision making to motor control. This implies that neurons both represent probability distributions and combine those distributions according to a close approximation to Bayes' rule. At first sight, it would seem that the high variability in the responses of cortical neurons would make it difficult to implement such optimal statistical inference in cortical circuits. We argue that, in fact, this variability implies that populations of neurons automatically represent probability distributions over the stimulus, a type of code we call probabilistic population codes. Moreover, we demonstrate that the Poisson-like variability observed in cortex reduces a broad class of Bayesian inference to simple linear combinations of populations of neural activity. These results hold for arbitrary probability distributions over the stimulus, for tuning curves of arbitrary shape and for realistic neuronal variability."
1241,"the god delusion",913652,"The God Delusion","{Discover magazine recently called Richard Dawkins ""Darwin's Rottweiler"" for his fierce and effective defense of evolution. Prospect magazine voted him among the top three public intellectuals in the world (along with Umberto Eco and Noam Chomsky). Now Dawkins turns his considerable intellect on religion, denouncing its faulty logic and the suffering it causes.  He critiques God in all his forms, from the sex-obsessed tyrant of the Old Testament to the more benign (but still illogical) Celestial Watchmaker favored by some Enlightenment thinkers. He eviscerates the major arguments for religion and demonstrates the supreme improbability of a supreme being. He shows how religion fuels war, foments bigotry, and abuses children, buttressing his points with historical and contemporary evidence. In so doing, he makes a compelling case that belief in God is not just irrational, but potentially deadly.  Dawkins has fashioned an impassioned, rigorous rebuttal to religion, to be embraced by anyone who sputters at the inconsistencies and cruelties that riddle the Bible, bristles at the inanity of ""intelligent design,"" or agonizes over fundamentalism in the Middle East&#8212;or Middle America.}"
1242,"benchmarking sets for molecular docking",913782,"Benchmarking Sets for Molecular Docking","Ligand enrichment among top-ranking hits is a key metric of molecular docking. To avoid bias, decoys should resemble ligands physically, so that enrichment is not simply a separation of gross features, yet be chemically distinct from them, so that they are unlikely to be binders. We have assembled a directory of useful decoys (DUD), with 2950 ligands for 40 different targets. Every ligand has 36 decoy molecules that are physically similar but topologically distinct, leading to a database of 98 266 compounds. For most targets, enrichment was at least half a log better with uncorrected databases such as the MDDR than with DUD, evidence of bias in the former. These calculations also allowed 40 × 40 cross-docking, where the enrichments of each ligand set could be compared for all 40 targets, enabling a specificity metric for the docking screens. DUD is freely available online as a benchmarking set for docking at http://blaster.docking.org/dud/."
1243,"the emergence of linguistic structure an overview of the iterated learning model",916031,"The emergence of linguistic structure: An overview of the iterated learning model","Introduction  As language users humans possess a culturally transmitted system of unparalleled complexity in the natural world. Linguistics has revealed over the past 40 years the degree to which the syntactic structure of language in particular is strikingly complex. Furthermore, as Pinker and Bloom point out in their agenda-setting paper Natural Language and Natural Selection \grammar is a complex mechanism tailored to the transmission of propositional structures through a serial interface..."
1244,"multiagent simulations and ecosystem management a review",916577,"Multi-agent simulations and ecosystem management: a review","This paper proposes a review of the development and use of multi-agent simulations (MAS) for ecosystem management. The use of this methodology and the associated tools accompanies the shifts in various paradigms on the study of ecological complexity. Behavior and interactions are now key issues for understanding and modeling ecosystem organization, and models are used in a constructivist way. MAS are introduced conceptually and are compared with individual-based modeling approaches. Various architectures of agents are presented, the role of the environment is emphasized and some computer tools are presented. A discussion follows on the use of MAS for ecosystem management. The strength of MAS has been discussed for social sciences and for spatial issues such as land-use change. We argue here that MAS are useful for problems integrating social and spatial aspects. Then we discuss how MAS can be used for several purposes, from theorization to collective decision-making support. We propose some research perspectives on individual decision making processes, institutions, scales, the credibility of models and the use of MAS. In conclusion we argue that researchers in the field of ecosystem management can use multi-agent systems to go beyond the role of the individual and to study more deeply and more effectively the different forms of organization (spatial, networks, hierarchies) and interactions among different organizational levels. For that objective there is considerably more fruit to be had on the tree of collaboration between social, ecological, and computer scientists than has so far been harvested. (C) 2004 Elsevier B.V. All rights reserved."
1245,"diversity of micrornas in human and chimpanzee brain",921343,"Diversity of microRNAs in human and chimpanzee brain","We used massively parallel sequencing to compare the microRNA (miRNA) content of human and chimpanzee brains, and we identified 447 new miRNA genes. Many of the new miRNAs are not conserved beyond primates, indicating their recent origin, and some miRNAs seem species specific, whereas others are expanded in one species through duplication events. These data suggest that evolution of miRNAs is an ongoing process and that along with ancient, highly conserved miRNAs, there are a number of emerging miRNAs."
1246,"selfcooling of a micromirror by radiation pressure",921768,"Self-cooling of a micromirror by radiation pressure","Cooling of mechanical resonators is currently a popular topic in many fields of physics including ultra-high precision measurements, detection of gravitational waves and the study of the transition between classical and quantum behaviour of a mechanical system. Here we report the observation of self-cooling of a micromirror by radiation pressure inside a high-finesse optical cavity. In essence, changes in intensity in a detuned cavity, as caused by the thermal vibration of the mirror, provide the mechanism for entropy flow from the mirror's oscillatory motion to the low-entropy cavity field. The crucial coupling between radiation and mechanical motion was made possible by producing free-standing micromirrors of low mass (m approximately 400 ng), high reflectance (more than 99.6%) and high mechanical quality (Q approximately 10,000). We observe cooling of the mechanical oscillator by a factor of more than 30; that is, from room temperature to below 10 K. In addition to purely photothermal effects we identify radiation pressure as a relevant mechanism responsible for the cooling. In contrast with earlier experiments, our technique does not need any active feedback. We expect that improvements of our method will permit cooling ratios beyond 1,000 and will thus possibly enable cooling all the way down to the quantum mechanical ground state of the micromirror."
1247,"gene prediction with a hidden markov model and a new intron submodel",922550,"Gene prediction with a hidden Markov model and a new intron submodel","Motivation: The problem of finding the genes in eukaryotic DNA sequences by computational methods is still not satisfactorily solved. Gene finding programs have achieved relatively high accuracy on short genomic sequences but do not perform well on longer sequences with an unknown number of genes in them. Here existing programs tend to predict many false exons. Results: We have developed a new program, AUGUSTUS, for the ab initio prediction of protein coding genes in eukaryotic genomes. The program is based on a Hidden Markov Model and integrates a number of known methods and submodels. It employs a new way of modeling intron lengths. We use a new donor splice site model, a new model for a short region directly upstream of the donor splice site model that takes the reading frame into account and apply a method that allows better GC-content dependent parameter estimation. AUGUSTUS predicts on longer sequences far more human and drosophila genes accurately than the ab initio gene prediction programs we compared it with, while at the same time being more specific. Availability: A web interface for AUGUSTUS and the executable program are located at http://augustus.gobics.de. Supplementary Information: The datasets used for testing and training are available at http://augustus.gobics.de/datasets/ Contact: mstanke@gwdg.de"
1248,"genemarks a selftraining method for prediction of gene starts in microbial genomes implications for finding sequence motifs in regulatory regions",922563,"GeneMarkS: a self-training method for prediction of gene starts in microbial genomes. Implications for finding sequence motifs in regulatory regions","Improving the accuracy of prediction of gene starts is one of a few remaining open problems in computer prediction of prokaryotic genes. Its difficulty is caused by the absence of relatively strong sequence patterns identifying true translation initiation sites. In the current paper we show that the accuracy of gene start prediction can be improved by combining models of protein-coding and non-coding regions and models of regulatory sites near gene start within an iterative Hidden Markov model based algorithm. The new gene prediction method, called GeneMarkS, utilizes a non-supervised training procedure and can be used for a newly sequenced prokaryotic genome with no prior knowledge of any protein or rRNA genes. The GeneMarkS implementation uses an improved version of the gene finding program GeneMark.hmm, heuristic Markov models of coding and non-coding regions and the Gibbs sampling multiple alignment program. GeneMarkS predicted precisely 83.2\% of the translation starts of GenBank annotated Bacillus subtilis genes and 94.4\% of translation starts in an experimentally validated set of Escherichia coli genes. We have also observed that GeneMarkS detects prokaryotic genes, in terms of identifying open reading frames containing real genes, with an accuracy matching the level of the best currently used gene detection methods. Accurate translation start prediction, in addition to the refinement of protein sequence N-terminal data, provides the benefit of precise positioning of the sequence region situated upstream to a gene start. Therefore, sequence motifs related to transcription and translation regulatory sites can be revealed and analyzed with higher precision. These motifs were shown to possess a significant variability, the functional and evolutionary connections of which are discussed."
1249,"tigrscan and glimmerhmm two open source ab initio eukaryotic genefinders",922585,"TigrScan and GlimmerHMM: two open source ab initio eukaryotic gene-finders","Summary: We describe two new Generalized Hidden Markov Model implementations for ab initio eukaryotic gene prediction. The C/C++ source code for both is available as open source and is highly reusable due to their modular and extensible architectures. Unlike most of the currently available gene-finders, the programs are re-trainable by the end user. They are also re-configurable and include several types of probabilistic submodels which can be independently combined, such as Maximal Dependence Decomposition trees and interpolated Markov models. Both programs have been used at TIGR for the annotation of the Aspergillus fumigatus and Toxoplasma gondii genomes.  Availability: Source code and documentation are available under the open source Artistic License from http://www.tigr.org/software/pirate. 10.1093/bioinformatics/bth315"
1250,"comparative ab initio prediction of gene structures using pair hmms",922588,"Comparative ab initio prediction of gene structures using pair HMMs","We present a novel comparative method for the ab initio prediction of protein coding genes in eukaryotic genomes. The method simultaneously predicts the gene structures of two un-annotated input DNA sequences which are homologous to each other and retrieves the subsequences which are conserved between the two DNA sequences. It is capable of predicting partial, complete and multiple genes and can align pairs of genes which differ by events of exon-fusion or exon-splitting. The method employs a probabilistic pair hidden Markov model. We generate annotations using our model with two different algorithms: the Viterbi algorithm in its linear memory implementation and a new heuristic algorithm, called the stepping stone, for which both memory and time requirements scale linearly with the sequence length. We have implemented the model in a computer program called DOUBLESCAN. In this article, we introduce the method and confirm the validity of the approach on a test set of 80 pairs of orthologous DNA sequences from mouse and human. More information can be found at: http://www.sanger.ac.uk/Software/analysis/doublescan/"
1251,"a taskoriented noninteractive evaluation methodology for information retrieval systems",925514,"A Task-Oriented Non-Interactive Evaluation Methodology for Information Retrieval Systems","Past research has identified many different types of relevance in information retrieval (IR). So far, however, most evaluation of IR systems has been through batch experiments conducted with test collections containing only expert, topical relevance judgements. Recently, there has been some movement away from this traditional approach towards interactive, more user-centred methods of evaluation. However, these are expensive for evaluators in terms both of time and of resources. This paper describes a new evaluation methodology, using a task-oriented test collection, which combines the advantages of traditional non-interactive testing with a more user-centred emphasis. The main features of a task-oriented test collection are the adoption of the task, rather than the query, as the primary unit of evaluation and the naturalistic character of the relevance judgements."
1252,"a simple rulebased partofspeech tagger",930113,"A simple rule-based part-of-speech tagger","Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease..."
1253,"a statistical approach to language translation",930125,"A statistical approach to language translation","An approach to automatic translation is outlined that utilizes techniques of statistical information extraction from large data bases. The method is based on the availability of pairs of large corresponding texts that are translations of each other. In our case, the texts are in English and French.Fundamental to the technique is a complex glossary of correspondence of fixed locutions. The steps of the proposed translation process are: (1) Partition the source text into a set of fixed locutions. (2) Use the glossary plus contextual information to select the corresponding set of fixed locutions into a sequence forming the target sentence. (3) Arrange the words of the target fixed locutions into a sequence forming the target sentence.We have developed statistical techniques facilitating both the automatic creation of the glossary, and the performance of the three translation steps, all on the basis of an alignment of corresponding sentences in the two texts.While we are not yet able to provide examples of French / English translation, we present some encouraging intermediate results concerning glossary creation and the arrangement of target word sequences."
1254,"fast effective rule induction",930236,"Fast effective rule induction","Many existing rule learning systems are computationally expensive on large noisy datasets. In this paper we evaluate the recently-proposed rule learning algorithm IREP on a large and diverse collection of benchmark problems. We show that while IREP is extremely efficient, it frequently gives error rates higher than those of C4.5 and C4.5rules. We then propose a number of modifications resulting in an algorithm RIPPERk that is very competitive with C4.5rules with respect to error rates, but much more efficient on large samples. RIPPERk obtains error rates lower than or equivalent to C4.5rules on 22 of 37 benchmark problems, scales nearly linearly with the number of training examples, and can efficiently process noisy datasets containing hundreds of thousands of examples. 1 INTRODUCTION Systems that learn sets of rules have a number of desirable properties. Rule sets are relatively easy for people to understand [ Catlett, 1991 ] , and rule learning systems outperform decision tree lear..."
1255,"the oxford handbook of computational linguistics",931018,"The {O}xford {H}andbook of {C}omputational {L}inguistics","Thirty-seven chapters, commissioned from experts all over the world, describe major concepts, methods, and applications in computational linguistics. Part I, Linguistic Fundamentals, provides an overview of the field suitable for senior undergraduates and non-specialists from other fields of linguistics and related disciplines. Part II describes current tasks, techniques, and tools in Natural Language Processing and aims to meet the needs of post-doctoral workers and others embarking on computational language research. Part III surveys current Applications. The book is a state-of-the-art reference to one of the most active and productive fields in linguistics. It will be of interest and practical use to a wide range of linguists, as well as to researchers in such fields as informatics, artificial intelligence, language engineering, and cognitive science."
1256,"shallow parsing using noisy and nonstationary training material",931189,"Shallow Parsing using Noisy and Non-Stationary Training Material","Shallow parsers are usually assumed to be trained on noise-free material, drawn from the same distribution as the testing material. However, when either the training set is noisy or else drawn from a different distributions, performance may be degraded. Using the parsed Wall Street Journal, we investigate the performance of four shallow parsers (maximum entropy, memory-based learning, N-grams and ensemble learning) trained using various types of artificially noisy material. Our first set of results show that shallow parsers are surprisingly robust to synthetic noise, with performance gradually decreasing as the rate of noise increases. Further results show that no single shallow parser performs best in all noise situations. Final results show that simple, parser-specific extensions can improve noise-tolerance. Our second set of results addresses the question of whether naturally occurring disfluencies undermines performance more than does a change in distribution. Results using the parsed Switchboard corpus suggest that, although naturally occurring disfluencies might harm performance, differences in distribution between the training set and the testing set are more significant."
1257,"parsing english with a link grammar",931732,"Parsing English with a Link Grammar","We define a new type of formal grammatical system called a _link grammar_. A sequence of words is in the language of a link grammar if there is a way to draw _links_ between words in such a way that (1) the local requirements of each word are satisfied, (2) the links do not cross, and (3) the words form a connected graph. We have encoded English grammar into such a system, and written a program (based on new algorithms) for efficiently parsing with a link grammar. The breadth of English phenomena that our system handles is quite large -- it handles: noun-verb agreement, questions, imperatives, complex and irregular verbs, many types of nouns, past- or present-participles in noun phrases, commas, a variety of adjective types, prepositions, adverbs, relative clauses, possessives, coordinating conjunctions, and many other things. A number of sophisticated and new techniques were used to allow efficient parsing of this very complex grammar. Our program is written in C, and the entire system may be obtained via anonymous ftp."
1258,"toward machine emotional intelligence analysis of affective physiological state",933268,"Toward Machine Emotional Intelligence: Analysis of Affective Physiological State","—The ability to recognize emotion is one of the hallmarks of emotional intelligence, an aspect of human intelligence that has been argued to be even more important than mathematical and verbal intelligences. This paper proposes that machine intelligence needs to include emotional intelligence and demonstrates results toward this goal: developing a machine's ability to recognize human affective state given four physiological signals. We describe difficult issues unique to obtaining reliable affective data and collect a large set of data from a subject trying to elicit and experience each of eight emotional states, daily, over multiple weeks. This paper presents and compares multiple algorithms for feature-based recognition of emotional state from this data. We analyze four physiological signals that exhibit problematic day-to-day variations: The features of different emotions on the same day tend to cluster more tightly than do the features of the same emotion on different days. To handle the daily variations, we propose new features and algorithms and compare their performance. We find that the technique of seeding a Fisher Projection with the results of Sequential Floating Forward Search improves the performance of the Fisher Projection and provides the highest recognition rates reported to date for classification of affect from physiology: 81 percent recognition accuracy on eight classes of emotion, including neutral."
1259,"beyond personal webpublishing an exploratory study of conversational blogging practices",933410,"Beyond Personal Webpublishing: An Exploratory Study of Conversational Blogging Practices","Although initially developed as low-threshold tools to publish on-line, weblogs increasingly appear to facilitate conversations. The objective of this study is to identify practices of conversational blogging. This paper presents results of an exploratory qualitative analysis of a weblog-mediated conversation case, focusing on participation rhythm, media choices and specific linking practices. Based on our findings we propose attributes of conversational blogging: linking as conversational glue, tangential conversations and interplays between conversation with self and conversations with others. Finally, future research directions are discussed."
1260,"compact universal dna microarrays to comprehensively determine transcriptionfactor binding site specificities",934384,"Compact, universal DNA microarrays to comprehensively determine transcription-factor binding site specificities","Transcription factors ( TFs) interact with specific DNA regulatory sequences to control gene expression throughout myriad cellular processes. However, the DNA binding specificities of only a small fraction of TFs are sufficiently characterized to predict the sequences that they can and cannot bind. We present a maximally compact, synthetic DNA sequence design for protein binding microarray ( PBM) experiments(1) that represents all possible DNA sequence variants of a given length k ( that is, all 'k- mers') on a single, universal microarray. We constructed such all k- mer microarrays covering all 10 - base pair ( bp) binding sites by converting high- density single- stranded oligonucleotide arrays to double- stranded ( ds) DNA arrays. Using these microarrays we comprehensively determined the binding specificities over a full range of affinities for five TFs of different structural classes from yeast, worm, mouse and human. The unbiased coverage of all k-mers permits high- throughput interrogation of binding site preferences, including nucleotide interdependencies, at unprecedented resolution."
1261,"supporting collaborative process knowledge management in new product development teams",942734,"Supporting Collaborative Process Knowledge Management in New Product Development Teams","Knowledge centric activities of developing new products and services are becoming the primary source of sustainable competitive advantage in an era characterized by short product life cycles, dynamic markets and complex processes. We view new product development (NPD) as a knowledge-intensive activity. Based on a case study in the consumer electronics industry, we identify problems associated with knowledge management (KM) in the context of NPD by cross-functional collaborative teams. We map these problems to broad Information Technology enabled solutions and subsequently translate these into specific system characteristics and requirements. A prototype system that meets these requirements developed to capture and manage tacit and explicit process knowledge is further discussed. The functionalities of the system include functions for representing context with informal components, easy access to process knowledge, assumption surfacing, review of past knowledge, and management of dependencies. We demonstrate the validity our proposed solutions using scenarios drawn from our case study."
1262,"fmradaptation a tool for studying the functional properties of human cortical neurons",943475,"fMR-adaptation: a tool for studying the functional properties of human cortical neurons","The invariant properties of human cortical neurons cannot be studied directly by fMRI due to its limited spatial resolution. One voxel obtained from a fMRI scan contains several hundred thousands neurons. Therefore, the fMRI signal may average out a heterogeneous group of highly selective neurons. Here, we present a novel experimental paradigm for fMRI, functional magnetic resonance-adaptation (fMR-A), that enables to tag specific neuronal populations within an area and investigate their functional properties. This approach contrasts with conventional mapping methods that measure the averaged activity of a region. The application of fMR-A to study the functional properties of cortical neurons proceeds in two stages: First, the neuronal population is adapted by repeated presentation of a single stimulus. Second, some property of the stimulus is varied and the recovery from adaptation is assessed. If the signal remains adapted, it will indicate that the neurons are invariant to that attribute. However, if the fMRI signal will recover from the adapted state it would imply that the neurons are sensitive to the property that was varied. Here, an application of fMR-A for studying the invariant properties of high-order object areas (lateral occipital complex - LOC) to changes in object size, position, illumination and rotation is presented. The results show that LOC is less sensitive to changes in object size and position compared to changes of illumination and viewpoint. fMR-A can be extended to other neuronal systems in which adaptation is manifested and can be used with event-related paradigms as well. By manipulating experimental parameters and testing recovery from adaptation it should be possible to gain insight into the functional properties of cortical neurons which are beyond the spatial resolution limits imposed by conventional fMRI."
1263,"principles for the buffering of genetic variation",943570,"Principles for the Buffering of Genetic Variation","Most genetic research has used inbred organisms and has not explored the complexity of natural genetic variation present in outbred populations. The translation of genotype to phenotype is complicated by gene interactions observed as epistasis, canalization, robustness, or buffering. Analysis of double mutations in inbred experimental organisms suggests some principles for gene interaction that may apply to natural variation as well. The buffering of variation in one gene is most often due to a small number of other genes that function in the same biochemical process. However, buffering can also result from genes functioning in processes extrinsic to that of the primary gene."
1264,"a free energy principle for the brain",944934,"A free energy principle for the brain","By formulating Helmholtz’s ideas about perception, in terms of modern-day theories, one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts: using constructs from statistical physics, the problems of inferring the causes of sensory input and learning the causal structure of their generation can be resolved using exactly the same principles. Furthermore, inference and learning can proceed in a biologically plausible fashion. The ensuing scheme rests on Empirical Bayes and hierarchical models of how sensory input is caused. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of cortical organisation and responses. In this paper, we show these perceptual processes are just one aspect of emergent behaviours of systems that conform to a free energy principle. The free energy considered here measures the difference between the probability distribution of environmental quantities that act on the system and an arbitrary distribution encoded by its configuration. The system can minimise free energy by changing its configuration to affect the way it samples the environment or change the distribution it encodes. These changes correspond to action and perception respectively and lead to an adaptive exchange with the environment that is characteristic of biological systems. This treatment assumes that the system’s state and structure encode an implicit and probabilistic model of the environment. We will look at the models entailed by the brain and how minimisation of its free energy can explain its dynamics and structure."
1265,"bilateral filtering for gray and color images",948687,"{Bilateral Filtering for Gray and Color Images}","Bilateral filtering smooths images while preserving edges, by means of a nonlinear combination of nearby image values. The method is noniterative, local, and simple. It combines gray levels or colors based on both their geometric closeness and their photometric similarity, and prefers near values to distant values in both domain and range. In contrast with filters that operate on the three bands of a color image separately, a bilateral filter can enforce the perceptual metric underlying the CIE-Lab color space, and smooth colors and preserve edges in a way that is tuned to human perception. Also, in contrast with standard filtering, bilateral filtering produces no phantom colors along edges in color images, and reduces phantom colors where they appear in the original image."
1266,"a statistical model for general contextual object recognition",948882,"A Statistical Model for General Contextual Object Recognition","We consider object recognition as the process of attaching meaningful labels to specific regions of an image, and propose a model that learns spatial relationships between objects. Given a set of images and their associated text (e.g. keywords, captions, descriptions), the objective is to segment an image, in either a crude or sophisticated fashion, then to find the proper associations between words and regions. Previous models are limited by the scope of the representation. In particular, they fail to exploit spatial context in the images and words. We develop a more expressive model that takes this into account. We formulate a spatially consistent probabilistic mapping between continuous image feature vectors and the supplied word tokens. By learning both word-to-region associations and object relations, the proposed model augments scene segmentations due to smoothing implicit in spatial consistency. Context introduces cycles to the undirected graph, so we cannot rely on a straightforward implementation of the EM algorithm for estimating the model parameters and densities of the unknown alignment variables. Instead, we develop an approximate EM algorithm that uses loopy belief propagation in the inference step and iterative scaling on the pseudo-likelihood approximation in the parameter update step. The experiments indicate that our approximate inference and learning algorithm converges to good local solutions. Experiments on a diverse array of images show that spatial context considerably improves the accuracy of object recognition. Most significantly, spatial context combined with a nonlinear discrete object representation allows our models to cope well with over-segmented scenes."
1267,"using multiple segmentations to discover objects and their extent in image collections",948905,"Using Multiple Segmentations to Discover Objects and their Extent in Image Collections","Given a large dataset of images, we seek to automatically determine the visually similar object and scene classes together with their image segmentation. To achieve this we combine two ideas: (i) that a set of segmented objects can be partitioned into visual object classes using topic discovery models from statistical text analysis; and (ii) that visual object classes can be used to assess the accuracy of a segmentation. To tie these ideas together we compute multiple segmentations of each image and then: (i) learn the object classes; and (ii) choose the correct segmentations. We demonstrate that such an algorithm succeeds in automatically discovering many familiar objects in a variety of image datasets, including those from Caltech, MSRC and LabelMe."
1268,"textonboost joint appearance shape and context modeling for multiclass object recognition and segmentation",948907,"TextonBoost: Joint Appearance, Shape and Context Modeling for Multi-Class Object Recognition and Segmentation","This paper proposes a new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently. The learned model is used for automatic visual recognition and semantic segmentation of photographs. Our discriminative model exploits novel features, based on textons, which jointly model shape and texture. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating these classifiers in a conditional random field. Efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods. High classification and segmentation accuracy are demonstrated on three different databases: i) our own 21-object class database of photographs of real objects viewed under general lighting conditions, poses and viewpoints, ii) the 7-class Corel subset and iii) the 7-class Sowerby database used in [1]. The proposed algorithm gives competitive results both for highly textured (e.g. grass, trees), highly structured (e.g. cars, faces, bikes, aeroplanes) and articulated objects (e.g. body, cow). © Springer-Verlag Berlin Heidelberg 2006."
1269,"contextual priming for object detection",948910,"Contextual Priming for Object Detection","There is general consensus that context can be a rich source of information about an object&#039;s identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes."
1270,"cortical surfacebased analysis ii inflation flattening and a surfacebased coordinate system",950849,"Cortical Surface-Based Analysis {II}: Inflation, Flattening, and a Surface-Based Coordinate System","The surface of the human cerebral cortex is a highly folded sheet with the majority of its surface area buried within folds. As such, it is a difficult domain for computational as well as visualization purposes. We have therefore designed a set of procedures for modifying the representation of the cortical surface to (i) inflate it so that activity buried inside sulci may be visualized, (ii) cut and flatten an entire hemisphere, and (iii) transform a hemisphere into a simple parameterizable surface such as a sphere for the purpose of establishing a surface-based coordinate system."
1271,"rate coding versus temporal order coding what the retinal ganglion cells tell the visual cortex",952015,"Rate coding versus temporal order coding: what the retinal ganglion cells tell the visual cortex.","It is often supposed that the messages sent to the visual cortex by the retinal ganglion cells are encoded by the mean firing rates observed on spike trains generated with a Poisson process. Using an information transmission approach, we evaluate the performances of two such codes, one based on the spike count and the other on the mean interspike interval, and compare the results with a rank order code, where the first ganglion cells to emit a spike are given a maximal weight. Our results show that the rate codes are far from optimal for fast information transmission and that the temporal structure of the spike train can be efficiently used to maximize the information transfer rate under conditions where each cell needs to fire only one spike."
1272,"navigating socialecological systems building resilience for complexity and change",952543,"{Navigating Social-Ecological Systems: Building Resilience for Complexity and Change}","{Drawing on complex systems theory, this book investigates how human societies deal with change in linked social-ecological systems, and build capacity to adapt to change. The concept of resilience is central in this context. Resilient social-ecological systems have the potential to sustain development in a manner that does not lead to loss of future options. Resilient systems provide capacity for renewal and innovation in the face of rapid transformation and crisis. Case studies and examples from several geographic areas, cultures and resource types are included; merging forefront research from natural sciences, social sciences and the humanities into an innovative framework for sustainable systems.}"
1273,"wikis und blogs planen einrichten verwalten",954839,"Wikis und Blogs. Planen. Einrichten. Verwalten","Im Internet sind sie schon als unkompliziertes Dokumentations- und Marketinginstrument etabliert: Wikis und Blogs. In Firmen befinden sie sich auf dem Vormarsch. Beide Medien eignen sich besonders dann, wenn die Anwender keine Computerprofis sind. Was auf Anwenderseite sehr einfach ist, verlangt vom Administrator besondere Kenntnisse. Dieses Buch gibt Entscheidern und Administratoren einen Überblick über die bestehenden Implementierungen im Internet und liefert Ideen für den Einsatz im eigenen Netzwerk, wobei sogar die den gesetzlichen Anforderungen entsprechende Revisionssicherheit von Wikis als Dokumentationsmittel in der Arbeitsgruppe herausgearbeitet wird. Es werden ausführlich die verschiedenen Techniken vorgestellt und es wird detailliert beschrieben, wie die Engines eingerichtet, konfiguriert und administriert werden. Die Erweiterung der jeweiligen Systeme mit C-sharp, Java, Perl, PHP und Basic wird ausführlich erklärt. Selbstverständlich kommt auch das Backoffice nicht zu kurz: der Einrichtung einer Webserver- und Datenbank-Umgebung ist ein Extrakapitel gewidmet. Die Themen: - Die Wiki- und Blogoshäre:Ideen und Anregungen - Arbeiten mit Wikis und Blogs: die Techniken - Blogs im Detail: Wordpress, BLOG:CMS, LifeType, Pivot, Loudblog und SnipSnap - Wikis installieren, konfigurieren und verwalten: MediaWiki, PmWiki, FlexWiki, TWiki, JSPWiki, UseModWiki und viele andere - Zukunftsperspektive Semantic Wiki - Das Backoffice: Server und Datenbanken"
1274,"reflective design",955090,"Reflective design","sengers at cs.cornell.edu, kab18   sd256   jofish at cornell.edu As computing moves into every aspect of our daily lives, the values and assumptions that underlie our technical practices may unwittingly be propagated throughout our culture. Drawing on existing critical approaches in computing, we argue that reflection on unconscious values embedded in computing and the practices that it supports can and should be a core principle of technology design. Building on a growing body of work in critical computing, reflective design combines analysis of the ways in which technologies reflect and perpetuate unconscious cultural assumptions, with design, building, and evaluation of new computing devices that reflect alternative possibilities. We illustrate this approach through two design case studies."
1275,"strategic information transmission",955467,"Strategic Information Transmission","This paper develops a model of strategic communication, in which a better-informed Sender (S) sends a possibly noisy signal to a Receiver (R), who then takes an action that determines the welfare of both. We characterize the set of Bayesian Nash equilibria under standard assumptions, and show that equilibrium signaling always takes a strikingly simple form, in which S partitions the support of the (scalar) variable that represents his private information and introduces noise into his signal by reporting, in effect, only which element of the partition his observation actually lies in. We show under further assumptions that before S observes his private information, the equilibrium whose partition has the greatest number of elements is Pareto-superior to all other equilibria, and that if agents coordinate on this equilibrium, R's equilibrium expected utility rises when agents' preferences become more similar. Since R bases his choice of action on rational expectations, this establishes a sense in which equilibrium signaling is more informative when agents' preferences are more similar."
1276,"extracting semantic predications from medline citations for pharmacogenomics",957758,"Extracting semantic predications from Medline citations for pharmacogenomics","We describe a natural language processing system (Enhanced SemRep) to identify core assertions on pharmacogenomics in Medline citations. Extracted information is represented as semantic predications covering a range of relations relevant to this domain. The specific relations addressed by the system provide greater precision than that achievable with methods that rely on entity co-occurrence. The development of Enhanced SemRep is based on the adaptation of an existing system and crucially depends on domain knowledge in the Unified Medical Language System. We provide a preliminary evaluation (55\\ recall and 73% precision) and discuss the potential of this system in assisting both clinical practice and scientific investigation."
1277,"a shrinkage approach to largescale covariance matrix estimation and implications for functional genomics",963455,"A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and Implications for Functional Genomics","Inferring large-scale covariance matrices from sparse genomic data is an ubiquitous problem in bioinformatics. Clearly, the widely used standard covariance and correlation estimators are ill-suited for this purpose. As statistically efficient and computationally fast alternative we propose a novel shrinkage covariance estimator that exploits the Ledoit-Wolf (2003) lemma for analytic calculation of the optimal shrinkage intensity. Subsequently, we apply this improved covariance estimator (which has guaranteed minimum mean squared error, is well-conditioned, and is always positive definite even for small sample sizes) to the problem of inferring large-scale gene association networks. We show that it performs very favorably compared to competing approaches both in simulations as well as in application to real expression data."
1278,"local features for object class recognition",964843,"Local Features for Object Class Recognition","In this paper, we compare the performance of local detectors and descriptors in the context of object class recognition. Recently, many detectors/descriptors have been evaluated in the context of matching as well as invariance to viewpoint changes (Mikolajczyk and Schmid, 2004). However, it is unclear if these results can be generalized to categorization problems, which require different properties of features. We evaluate 5 state-of-the-art scale invariant region detectors and 5 descriptors. Local features are computed for 20 object classes and clustered using hierarchical agglomerative clustering. We measure the quality of appearance clusters and location distributions using entropy as well as precision. We also measure how the clusters generalize from training set to novel test data. Our results indicate that attended SIFT descriptors (Mikolajczyk and Schmid, 2005) computed on Hessian-Laplace regions perform best. Second score is obtained by salient regions (Kadir and Brady, 2001). The results also show that these two detectors provide complementary features. The new detectors/descriptors significantly improve the performance of a state-of-the art recognition approach (Leibe, et al., 2005) in pedestrian detection task"
1279,"multiclass object recognition with sparse localized features",965068,"Multiclass Object Recognition with Sparse, Localized Features","We apply a biologically inspired model of visual object recognition to the multiclass object categorization problem. Our model modifies that of Serre, Wolf, and Poggio. As in that work, we first apply Gabor filters at all positions and scales; feature complexity and position/scale invariance are then built up by alternating template matching and max pooling operations. We refine the approach in several biologically plausible ways, using simple versions of sparsification and lateral inhibition. We demonstrate the value of retaining some position and scale information above the intermediate feature level. Using feature selection we arrive at a model that performs better with fewer features. Our final model is tested on the Caltech 101 object categories and the UIUC car localization task, in both cases achieving state-of-the-art performance. The results strengthen the case for using this class of model in computer vision."
1280,"semisupervised learning",968552,"Semi-Supervised Learning","{In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research. <br /> <br /> <i>Semi-Supervised Learning</i> first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction.<br /> <br /> Olivier Chapelle and Alexander Zien are Research Scientists and Bernhard Schölkopf is Professor and Director at the Max Planck Institute for Biological Cybernetics in Tübingen. Schölkopf is coauthor of <i>Learning with Kernels</i> (MIT Press, 2002) and is a coeditor of <i>Advances in Kernel Methods: Support Vector Learning</i> (1998), <i>Advances in Large-Margin Classifiers</i> (2000), and <i>Kernel Methods in Computational Biology</i> (2004), all published by The MIT Press.}"
1281,"pathogen recognition and innate immunity",970694,"Pathogen Recognition and Innate Immunity"," Microorganisms that invade a vertebrate host are initially recognized by the innate immune system through germline-encoded pattern-recognition receptors (PRRs). Several classes of PRRs, including Toll-like receptors and cytoplasmic receptors, recognize distinct microbial components and directly activate immune cells. Exposure of immune cells to the ligands of these receptors activates intracellular signaling cascades that rapidly induce the expression of a variety of overlapping and unique genes involved in the inflammatory and immune responses. New insights into innate immunity are changing the way we think about pathogenesis and the treatment of infectious diseases, allergy, and autoimmunity."
1282,"reconciling carboncycle concepts terminology and methods",975240,"Reconciling Carbon-cycle Concepts, Terminology, and Methods","Abstract&nbsp;&nbsp;Recent projections of climatic change have focused a great deal of scientific and public attention on patterns of carbon (C) cycling as well as its controls, particularly the factors that determine whether an ecosystem is a net source or sink of atmospheric carbon dioxide (CO2). Net ecosystem production (NEP), a central concept in C-cycling research, has been used by scientists to represent two different concepts. We propose that NEP be restricted to just one of its two original definitions—the imbalance between gross primary production (GPP) and ecosystem respiration (ER). We further propose that a new term—net ecosystem carbon balance (NECB)—be applied to the net rate of C accumulation in (or loss from [negative sign]) ecosystems. Net ecosystem carbon balance differs from NEP when C fluxes other than C fixation and respiration occur, or when inorganic C enters or leaves in dissolved form. These fluxes include the leaching loss or lateral transfer of C from the ecosystem; the emission of volatile organic C, methane, and carbon monoxide; and the release of soot and CO2 from fire. Carbon fluxes in addition to NEP are particularly important determinants of NECB over long time scales. However, even over short time scales, they are important in ecosystems such as streams, estuaries, wetlands, and cities. Recent technological advances have led to a diversity of approaches to the measurement of C fluxes at different temporal and spatial scales. These approaches frequently capture different components of NEP or NECB and can therefore be compared across scales only by carefully specifying the fluxes included in the measurements. By explicitly identifying the fluxes that comprise NECB and other components of the C cycle, such as net ecosystem exchange (NEE) and net biome production (NBP), we can provide a less ambiguous framework for understanding and communicating recent changes in the global C cycle."
1283,"social networks incentives and search",988558,"Social networks, incentives, and search","The role of network structure has grown in significance over the past ten years in the field of information retrieval, stimulated to a great extent by the importance of link analysis in the development of Web search techniques [4]. This body of work has focused primarily on the network that is most clearly visible on the Web: the network of hyperlinks connecting documents to documents. But the Web has always contained a second network, less explicit but equally important, and this is the social network on its users, with latent person-to-person links encoding a variety of relationships including friendship, information exchange, and influence. Developments over the past few years --- including the emergence of social networking systems and rich social media, as well as the availability of large-scale e-mail and instant messenging datasets --- have highlighted the crucial role played by on-line social networks, and at the same time have made them much easier to uncover and analyze. There is now a considerable opportunity to exploit the information content inherent in these networks, and this prospect raises a number of interesting research challenge.Within this context, we focus on some recent efforts to formalize the problem of searching a social network. The goal is to capture the issues underlying a variety of related scenarios: a member of a social networking system such as MySpace seeks a piece of information that may be held by a friend of a friend [27, 28]; an employee in a large company searches his or her network of colleagues for expertise in a particular subject [9]; a node in a decentralized peer-to-peer file-sharing system queries for a file that is likely to be a small number of hops away [2, 6, 16, 17]; or a user in a distributed IR or federated search setting traverses a network of distributed resources connected by links that may not just be informational but also economic or contractual [3, 5, 7, 8, 13, 18, 21]. In their most basic forms, these scenarios have some essential features in common: a node in a network, without global knowledge, must find a short path to a desired ""target"" node (or to one of several possible target nodes).To frame the underlying problem, we go back to one of the most well-known pieces of empirical social network analysis --- Stanley Milgram's research into the small-world phenomenon, also known as the ""six degrees of separation"" [19, 24, 25]. The form of Milgram's experiments, in which randomly chosen starters had to forward a letter to a designated target individual, established not just that short chains connecting far-flung pairs of people are abundant in large social networks, but also that the individuals in these networks, operating with purely local information about their own friends and acquaintances, are able to actually find these chains [10]. The Milgram experiments thus constituted perhaps the earliest indication that large-scale social networks are structured to support this type of decentralized search. Within a family of random-graph models proposed by Watts and Strogatz [26], we have shown that the ability of a network to support this type of decentralized search depends in subtle ways on how its ""long-range"" connections are correlated with the underlying spatial or organizational structure in which it is embedded [10, 11]. Recent studies using data on communication within organizations [1] and the friendships within large on-line communities [15] have established the striking fact that real social networks closely match some of the structural features predicted by these mathematical models.If one looks further at the on-line settings that provide the initial motivation for these issues, there is clearly interest from many directions in their long-term economic implications --- essentially, the consequences that follow from viewing distributed information retrieval applications, peer-to-peer systems, or social-networking sites as providing marketplaces for information and services. How does the problem of decentralized search in a network change when the participants are not simply agents following a fixed algorithm, but strategic actors who make decisions in their own self-interest, and may demand compensation for taking part in a protocol? Such considerations bring us into the realm of algorithmic game theory, an active area of current research that uses game-theoretic notions to quantify the performance of systems in which the participants follow their own self-interest [20, 23] In a simple model for decentralized search in the presence of incentives, we find that performance depends crucially on both the rarity of the information and the richness of the network topology [12] --- if the network is too structurally impoverished, an enormous investment may be required to produce a path from a query to an answer."
1284,"population coding of shape in area v",990151,"Population coding of shape in area V4.","Shape is represented in the visual system by patterns of activity across populations of neurons. We studied the population code for shape in area V4 of macaque monkeys, which is part of the ventral (object-related) pathway in primate visual cortex. We have previously found that many macaque V4 neurons are tuned for the curvature and object-centered position of boundary fragments (such as 'concavity on the right'). Here we tested the hypothesis that populations of such cells represent complete shapes as aggregates of boundary fragments. To estimate the population representation of a given shape, we scaled each cell's tuning peak by its response to that shape, summed across cells and smoothed. The resulting population response surface contained 3-8 peaks that represented major boundary features and could be used to reconstruct (approximately) the original shape. This exemplifies how a multi-peaked neural population response can represent a complex stimulus in terms of its constituent elements."
1285,"dynamic causal modeling a generative model of slice timing in fmri",997130,"Dynamic causal modeling: A generative model of slice timing in fMRI","{Dynamic causal modeling (DCM) of functional magnetic resonance imaging (fMRI) data allows one to make inferences about the architecture of distributed networks in the brain, in terms of effective connectivity. fMRI data are usually acquired using echo planar imaging (EPI). EPI sequences typically acquire slices at different times over a few seconds. DCM, in its original inception, was not informed about these slice timings and assumed that all slices were acquired simultaneously. It has been shown that DCM can cope with slice timing differences of up to 1 s. However, many fMRI studies employ a repetition time (TR) of 3 to 5[no-break space]s, which precludes a straightforward DCM of these data.We show that this limitation can be overcome easily by including slice timing in the DCM. Using synthetic data we show that the extended DCM furnishes veridical posterior means, even if there are large slice-timing differences. Model comparisons show that, in general, the extended DCM out-performs the original model. We contrast the modeling of slice timing, in the context of DCM, with the less effective approach of ‘slice-timing correction’, prior to modeling. We apply our procedure to real data and show that slice timings are important parameters. We conclude that, generally, one should use DCM with slice timing.}"
1286,"capturing chromosome conformation",997264,"Capturing Chromosome Conformation","We describe an approach to detect the frequency of interaction between any two genomic loci. Generation of a matrix of interaction frequencies between sites on the same or different chromosomes reveals their relative spatial disposition and provides information about the physical properties of the chromatin fiber. This methodology can be applied to the spatial organization of entire genomes in organisms from bacteria to human. Using the yeast Saccharomyces cerevisiae, we could confirm known qualitative features of chromosome organization within the nucleus and dynamic changes in that organization during meiosis. We also analyzed yeast chromosome III at the G1 stage of the cell cycle. We found that chromatin is highly flexible throughout. Furthermore, functionally distinct AT- and GC-rich domains were found to exhibit different conformations, and a population-average 3D model of chromosome III could be determined. Chromosome III emerges as a contorted ring."
1287,"statistical analysis of realtime pcr data",997319,"Statistical analysis of real-time PCR data","BACKGROUND:Even though real-time PCR has been broadly applied in biomedical sciences, data processing procedures for the analysis of quantitative real-time PCR are still lacking; specifically in the realm of appropriate statistical treatment. Confidence interval and statistical significance considerations are not explicit in many of the current data analysis approaches. Based on the standard curve method and other useful data analysis methods, we present and compare four statistical approaches and models for the analysis of real-time PCR data.RESULTS:In the first approach, a multiple regression analysis model was developed to derive DeltaDeltaCt from estimation of interaction of gene and treatment effects. In the second approach, an ANCOVA (analysis of covariance) model was proposed, and the DeltaDeltaCt can be derived from analysis of effects of variables. The other two models involve calculation DeltaCt followed by a two group t-test and non-parametric analogous Wilcoxon test. SAS programs were developed for all four models and data output for analysis of a sample set are presented. In addition, a data quality control model was developed and implemented using SAS.CONCLUSION:Practical statistical solutions with SAS programs were developed for real-time PCR data and a sample dataset was analyzed with the SAS programs. The analysis using the various models and programs yielded similar results. Data quality control and analysis procedures presented here provide statistical elements for the estimation of the relative expression of genes using real-time PCR."
1288,"transcriptional regulation by competing transcription factor modules",997773,"Transcriptional Regulation by Competing Transcription Factor Modules","Gene regulatory networks lie at the heart of cellular computation. In these networks, intracellular and extracellular signals are integrated by transcription factors, which control the expression of transcription units by binding to cis -regulatory regions on the DNA. The designs of both eukaryotic and prokaryotic cis -regulatory regions are usually highly complex. They frequently consist of both repetitive and overlapping transcription factor binding sites. To unravel the design principles of these promoter architectures, we have designed in silico prokaryotic transcriptional logic gates with predefined inputâ€“output relations using an evolutionary algorithm. The resulting cis -regulatory designs are often composed of modules that consist of tandem arrays of binding sites to which the transcription factors bind cooperatively. Moreover, these modules often overlap with each other, leading to competition between them. Our analysis thus identifies a new signal integration motif that is based upon the interplay between intramodular cooperativity and intermodular competition. We show that this signal integration mechanism drastically enhances the capacity of cis -regulatory domains to integrate signals. Our results provide a possible explanation for the complexity of promoter architectures and could be used for the rational design of synthetic gene circuits."
1289,"kcore organization of complex networks",1007150,"k-core organization of complex networks","We analytically describe the architecture of randomly damaged uncorrelated networks as a set of successively enclosed substructures— k -cores. The  k -core is the largest subgraph where vertices have at least  k  interconnections. We find the structure of  k -cores, their sizes, and their birthpoints—the bootstrap percolation thresholds. We show that in networks with a finite mean number  z 2  of the second-nearest neighbors, the emergence of a  k -core is a hybrid phase transition. In contrast, if  z 2  diverges, the networks contain an infinite sequence of  k -cores which are ultrarobust against random damage."
1290,"the interaction networks of structured rnas",1008038,"The interaction networks of structured RNAs","All pairwise interactions occurring between bases which could be detected in three-dimensional structures of crystallized RNA molecules are annotated on new planar diagrams. The diagrams attempt to map the underlying complex networks of base-base interactions and, especially, they aim at conveying key relationships between helical domains: co-axial stacking, bending and all Watson-Crick as well as non-Watson-Crick base pairs. Although such wiring diagrams cannot replace full stereographic images for correct spatial understanding and representation, they reveal structural similarities as well as the conserved patterns and distances between motifs which are present within the interaction networks of folded RNAs of similar or unrelated functions. Finally, the diagrams could help devising methods for meaningfully transforming RNA structures into graphs amenable to network analysis. 10.1093/nar/gkl963"
1291,"direct manipulation interfaces",1009267,"Direct Manipulation Interfaces","Direct manipulation has been lauded as a good form of interface design, and some interfaces that have this property have been well received by users. In this article we seek a cognitive account of both the advantages and disadvantages of direct manipulation interfaces. We identify two underlying phenomena that give rise to the feeling of directness. One deals with the information processing distance between the user's intentions and the facilities provided by the machine. Reduction of this distance makes the interface feel direct by reducing the effort required of the user to accomplish goals. The second phenomenon concerns the relation between the input and output vocabularies of the interface language. In particular, direct manipulation requires that the system provide representations of objects that behave as if they are the objects themselves. This provides the feeling of directness of manipulation."
1292,"biocapital the constitution of postgenomic life",1014999,"Biocapital: The Constitution of Postgenomic Life","{<I>Biocapital</I> is a major theoretical contribution to science studies and political economy. Grounding his analysis in a multi-sited ethnography of genomic research and drug development marketplaces in the United States and India, Kaushik Sunder Rajan argues that contemporary biotechnologies such as genomics can only be understood in relation to the economic markets within which they emerge. Sunder Rajan conducted fieldwork in biotechnology labs and in small start-up companies in the United States (mostly in the San Francisco Bay area) and India (mainly in New Delhi, Hyderabad, and Bombay) over a five-year period spanning 1999 to 2004. He draws on his research with scientists, entrepreneurs, venture capitalists, and policymakers to compare drug development in the two countries, examining the practices and goals of research, the financing mechanisms, the relevant government regulations, and the hype and marketing surrounding promising new technologies. In the process, he illuminates the global flow of ideas, information, capital, and people connected to biotech initiatives.<BR><BR>Sunder Rajan&rsquo;s ethnography informs his theoretically sophisticated inquiry into how the contemporary world is shaped by the marriage of biotechnology and market forces, by what he calls technoscientific capitalism. Bringing Marxian theories of value into conversation with Foucaultian notions of biopolitics, he traces how the life sciences came to be significant producers of both economic and epistemic value in the late twentieth century and early twenty-first.}"
1293,"a new era in citation and bibliometric analyses web of science scopus and google scholar",1018763,"A New Era in Citation and Bibliometric Analyses: Web of Science, Scopus, and Google Scholar","Academic institutions, federal agencies, publishers, editors, authors, and librarians increasingly rely on citation analysis for making hiring, promotion, tenure, funding, and/or reviewer and journal evaluation and selection decisions. The Institute for Scientific Information's (ISI) citation databases have been used for decades as a starting point and often as the only tools for locating citations and/or conducting citation analyses. ISI databases (or Web of Science), however, may no longer be adequate as the only or even the main sources of citations because new databases and tools that allow citation searching are now available. Whether these new databases and tools complement or represent alternatives to Web of Science (WoS) is important to explore. Using a group of 15 library and information science faculty members as a case study, this paper examines the effects of using Scopus and Google Scholar (GS) on the citation counts and rankings of scholars as measured by WoS. The paper discusses the strengths and weaknesses of WoS, Scopus, and GS, their overlap and uniqueness, quality and language of the citations, and the implications of the findings for citation analysis. The project involved citation searching for approximately 1,100 scholarly works published by the study group and over 200 works by a test group (an additional 10 faculty members). Overall, more than 10,000 citing and purportedly citing documents were examined. WoS data took about 100 hours of collecting and processing time, Scopus consumed 200 hours, and GS a grueling 3,000 hours."
1294,"wikipedias collaborative webbased encyclopedias as complex networks",1018856,"Wikipedias: Collaborative web-based encyclopedias as complex networks","Wikipedia is a popular web-based encyclopedia edited freely and collaboratively by its users. In this paper we present an analysis of Wikipedias in several languages as complex networks. The hyperlinks pointing from one Wikipedia article to another are treated as directed links while the articles represent the nodes of the network. We show that many network characteristics are common to different language versions of Wikipedia, such as their degree distributions, growth, topology, reciprocity, clustering, assortativity, path lengths, and triad significance profiles. These regularities, found in the ensemble of Wikipedias in different languages and of different sizes, point to the existence of a unique growth process. We also compare Wikipedias to other previously studied networks."
1295,"automatically categorizing written texts by author gender",1019645,"Automatically Categorizing Written Texts by Author Gender","The problem of automatically determining the gender of a document's author would appear to be a more subtle problem than those of categorization by topic or authorship attribution. Nevertheless, it is shown that automated text categorization techniques can exploit combinations of simple lexical and syntactic features to infer the gender of the author of an unseen formal written document with approximately 80 per cent accuracy. The same techniques can be used to determine if a document is fiction or non-fiction with approximately 98 per cent accuracy. 10.1093/llc/17.4.401"
1296,"the topology of the regulatory interactions predicts the expression pattern of the segment polarity genes in drosophila melanogaster",1021904,"The topology of the regulatory interactions predicts the expression pattern of the segment polarity genes in Drosophila melanogaster.","Expression of the Drosophila segment polarity genes is initiated by a prepattern of pair-rule gene products and maintained by a network of regulatory interactions throughout several stages of embryonic development. Analysis of a model of gene interactions based on differential equations showed that wild-type expression patterns of these genes can be obtained for a wide range of kinetic parameters, which suggests that the steady states are determined by the topology of the network and the type of regulatory interactions between components, not the detailed form of the rate laws. To investigate this, we propose and analyze a Boolean model of this network which is based on a binary ON/OFF representation of transcription and protein levels, and in which the interactions are formulated as logical functions. In this model the spatial and temporal patterns of gene expression are determined by the topology of the network and whether components are present or absent, rather than the absolute levels of the mRNAs and proteins and the functional details of their interactions. The model is able to reproduce the wild type gene expression patterns, as well as the ectopic expression patterns observed in over-expression experiments and various mutants. Furthermore, we compute explicitly all steady states of the network and identify the basin of attraction of each steady state. The model gives important insights into the functioning of the segment polarity gene network, such as the crucial role of the wingless and sloppy paired genes, and the network's ability to correct errors in the prepattern."
1297,"polymirts database linking polymorphisms in microrna target sites with complex traits",1024536,"PolymiRTS Database: linking polymorphisms in microRNA target sites with complex traits.","Polymorphism in microRNA Target Site (PolymiRTS) database is a collection of naturally occurring DNA variations in putative microRNA target sites. PolymiRTSs may affect gene expression and cause variations in complex phenotypes. The database integrates sequence polymorphism, phenotype and expression microarray data, and characterizes PolymiRTSs as potential candidates responsible for the quantitative trait locus (QTL) effects. It is a resource for studying PolymiRTSs and their implications in phenotypic variations. PolymiRTS database can be accessed at http://compbio.utmem.edu/miRSNP/."
1298,"statistical properties of sampled networks",1026394,"Statistical properties of sampled networks","We study the statistical properties of the sampled scale-free networks, deeply related to the proper identification of various real-world networks. We exploit three methods of sampling and investigate the topological properties such as degree and betweenness centrality distribution, average path length, assortativity, and clustering coefficient of sampled networks compared with those of original networks. It is found that the quantities related to those properties in sampled networks appear to be estimated quite differently for each sampling method. We explain why such a biased estimation of quantities would emerge from the sampling procedure and give appropriate criteria for each sampling method to prevent the quantities from being overestimated or underestimated."
1299,"absolute protein expression profiling estimates the relative contributions of transcriptional and translational regulation",1030375,"Absolute protein expression profiling estimates the relative contributions of transcriptional and translational regulation","We report a method for large-scale absolute protein expression measurements (APEX) and apply it to estimate the relative contributions of transcriptional- and translational-level gene regulation in the yeast and Escherichia coli proteomes. APEX relies upon correcting each protein's mass spectrometry sampling depth (observed peptide count) by learned probabilities for identifying the peptides. APEX abundances agree with measurements from controls, western blotting, flow cytometry and two-dimensional gels, as well as known correlations with mRNA abundances and codon bias, providing absolute protein concentrations across approximately three to four orders of magnitude. Using APEX, we demonstrate that 73\% of the variance in yeast protein abundance (47\% in E. coli) is explained by mRNA abundance, with the number of proteins per mRNA log-normally distributed about approximately 5,600 ( approximately 540 in E. coli) protein molecules/mRNA. Therefore, levels of both eukaryotic and prokaryotic proteins are set per mRNA molecule and independently of overall protein concentration, with >70\% of yeast gene expression regulation occurring through mRNA-directed mechanisms."
1300,"micrornas small rnas with a big role in gene regulation",1031386,"MicroRNAs: small RNAs with a big role in gene regulation"," MicroRNAs are a family of small, non-coding RNAs that regulate gene expression in a sequence-specific manner. The two founding members of the microRNA family were originally identified in Caenorhabditis elegans as genes that were required for the timed regulation of developmental events. Since then, hundreds of microRNAs have been identified in almost all metazoan genomes, including worms, flies, plants and mammals. MicroRNAs have diverse expression patterns and might regulate various developmental and physiological processes. Their discovery adds a new dimension to our understanding of complex gene regulatory networks."
1301,"empirical bayes analysis of a microarray experiment",1031537,"Empirical Bayes Analysis of a Microarray Experiment","Microarrays are a novel technology that facilitates the simultaneous measurement of thousands of gene expression levels. A typical microarray experiment can produce millions of data points, raising serious problems of data reduction, and simultaneous inference. We consider one such experiment in which oligonucleotide arrays were employed to assess the genetic effects of ionizing radiation on seven thousand human genes. A simple nonparametric empirical Bayes model is introduced, which is used to guide the efficient reduction of the data to a single summary statistic per gene, and also to make simultaneous inferences concerning which genes were affected by the radiation. Although our focus is on one specific experiment, the proposed methods can be applied quite generally. The empirical Bayes inferences are closely related to the frequentist false discovery rate (FDR) criterion."
1302,"games with a purpose",1031598,"Games with a Purpose","Through online games, people can collectively solve large-scale computational problems. Such games constitute a general mechanism for using brain power to solve open problems. In fact, designing such a game is much like designing an algorithm - it must be proven correct, its efficiency can be analyzed, a more efficient version can supersede a less efficient one, and so on. ""Games with a purpose"" have a vast range of applications in areas as diverse as security, computer vision, Internet accessibility, adult content filtering, and Internet search. Any game designed to address these and other problems must ensure that game play results in a correct solution and, at the same time, is enjoyable. People will play such games to be entertained, not to solve a problem - no matter how laudable the objective."
1303,"largescale mapping and validation of escherichia coli transcriptional regulation from a compendium of expression profiles",1032933,"Large-Scale Mapping and Validation of Escherichia coli Transcriptional Regulation from a Compendium of Expression Profiles","Machine learning approaches offer the potential to systematically identify transcriptional regulatory interactions from a compendium of microarray expression profiles. However, experimental validation of the performance of these methods at the genome scale has remained elusive. Here we assess the global performance of four existing classes of inference algorithms using 445 Escherichia coli Affymetrix arrays and 3,216 known E. coli regulatory interactions from RegulonDB. We also developed and applied the context likelihood of relatedness (CLR) algorithm, a novel extension of the relevance networks class of algorithms. CLR demonstrates an average precision gain of 36&#037; relative to the next-best performing algorithm. At a 60&#037; true positive rate, CLR identifies 1,079 regulatory interactions, of which 338 were in the previously known network and 741 were novel predictions. We tested the predicted interactions for three transcription factors with chromatin immunoprecipitation, confirming 21 novel interactions and verifying our RegulonDB-based performance estimates. CLR also identified a regulatory link providing central metabolic control of iron transport, which we confirmed with real-time quantitative PCR. The compendium of expression data compiled in this study, coupled with RegulonDB, provides a valuable model system for further improvement of network inference algorithms using experimental data"
1304,"intactopen source resource for molecular interaction data",1036116,"IntAct--open source resource for molecular interaction data.","IntAct is an open source database and software suite for modeling, storing and analyzing molecular interaction data. The data available in the database originates entirely from published literature and is manually annotated by expert biologists to a high level of detail, including experimental methods, conditions and interacting domains. The database features over 126,000 binary interactions extracted from over 2100 scientific publications and makes extensive use of controlled vocabularies. The web site provides tools allowing users to search, visualize and download data from the repository. IntAct supports and encourages local installations as well as direct data submission and curation collaborations. IntAct source code and data are freely available from http://www.ebi.ac.uk/intact."
1305,"classes of complex networks defined by roletorole connectivity profiles",1036807,"Classes of complex networks defined by role-to-role connectivity profiles","In physical, biological, technological and social systems, interactions between units give rise to intricate networks. These-typically non-trivial-structures, in turn, critically affect the dynamics and properties of the system. The focus of most current research on complex networks is, still, on global network properties. A caveat of this approach is that the relevance of global properties hinges on the premise that networks are homogeneous, whereas most real-world networks have a markedly modular structure. Here, we report that networks with different functions, including the Internet, metabolic, air transportation and protein interaction networks, have distinct patterns of connections among nodes with different roles, and that, as a consequence, complex networks can be classified into two distinct functional classes on the basis of their link type frequency. Importantly, we demonstrate that these structural features cannot be captured by means of often studied global properties."
1306,"on how to perform a gold standard based evaluation of ontology learning",1037601,"On How to Perform a Gold Standard based Evaluation of Ontology Learning","In recent years several measures for the gold standard based evaluation of ontology learning were proposed. They can be distinguished by the layers of an ontology (e.g. lexical term layer and concept hierarchy) they evaluate. Judging those measures with a list of criteria we show that there exist some measures sufficient for evaluating the lexical term layer. However, existing measures for the evaluation of concept hierarchies fail to meet basic criteria. This paper presents a new taxonomic measure which overcomes the problems of current approaches."
1307,"fathumb a facetbased interface for mobile search",1041298,"FaThumb: a facet-based interface for mobile search","In this paper we describe a novel approach for searching large data sets from a mobile phone. Existing interfaces for mobile search require keyword text entry and are not suited for browsing. Our alternative uses a hybrid model to de-emphasize tedious keyword entry in favor of iterative data filtering. We propose navigation and selection of hierarchical metadata (facet navigation), with incremental text entry to further narrow the results. We conducted a formative evaluation to understand the relative advantages of keyword entry versus facet navigation for both browse and search tasks on the phone. We found keyword entry to be more powerful when the name of the search target is known, while facet navigation is otherwise more effective and strongly preferred."
1308,"exploration normalization and genotype calls of highdensity oligonucleotide snp array data",1044715,"Exploration, normalization, and genotype calls of high-density oligonucleotide SNP array data.","In most microarray technologies, a number of critical steps are required to convert raw intensity measurements into the data relied upon by data analysts, biologists, and clinicians. These data manipulations, referred to as preprocessing, can influence the quality of the ultimate measurements. In the last few years, the high-throughput measurement of gene expression is the most popular application of microarray technology. For this application, various groups have demonstrated that the use of modern statistical methodology can substantially improve accuracy and precision of the gene expression measurements, relative to ad hoc procedures introduced by designers and manufacturers of the technology. Currently, other applications of microarrays are becoming more and more popular. In this paper, we describe a preprocessing methodology for a technology designed for the identification of DNA sequence variants in specific genes or regions of the human genome that are associated with phenotypes of interest such as disease. In particular, we describe a methodology useful for preprocessing Affymetrix single-nucleotide polymorphism chips and obtaining genotype calls with the preprocessed data. We demonstrate how our procedure improves existing approaches using data from 3 relatively large studies including the one in which large numbers of independent calls are available. The proposed methods are implemented in the package oligo available from Bioconductor. 10.1093/biostatistics/kxl042"
1309,"a climatechange risk analysis for world ecosystems",1049337,"A climate-change risk analysis for world ecosystems.","We quantify the risks of climate-induced changes in key ecosystem processes during the 21st century by forcing a dynamic global vegetation model with multiple scenarios from 16 climate models and mapping the proportions of model runs showing forest/nonforest shifts or exceedance of natural variability in wildfire frequency and freshwater supply. Our analysis does not assign probabilities to scenarios or weights to models. Instead, we consider distribution of outcomes within three sets of model runs grouped by the amount of global warming they simulate: <2 degrees C (including simulations in which atmospheric composition is held constant, i.e., in which the only climate change is due to greenhouse gases already emitted), 2-3 degrees C, and >3 degrees C. High risk of forest loss is shown for Eurasia, eastern China, Canada, Central America, and Amazonia, with forest extensions into the Arctic and semiarid savannas; more frequent wildfire in Amazonia, the far north, and many semiarid regions; more runoff north of 50 degrees N and in tropical Africa and northwestern South America; and less runoff in West Africa, Central America, southern Europe, and the eastern U.S. Substantially larger areas are affected for global warming >3 degrees C than for <2 degrees C; some features appear only at higher warming levels. A land carbon sink of approximately 1 Pg of C per yr is simulated for the late 20th century, but for >3 degrees C this sink converts to a carbon source during the 21st century (implying a positive climate feedback) in 44% of cases. The risks continue increasing over the following 200 years, even with atmospheric composition held constant."
1310,"identification of tightly regulated groups of genes during drosophila melanogaster embryogenesis",1050114,"Identification of tightly regulated groups of genes during Drosophila melanogaster embryogenesis","Time-series analysis of whole-genome expression data during Drosophila melanogaster development indicates that up to 86% of its genes change their relative transcript level during embryogenesis. By applying conservative filtering criteria and requiring 'sharp' transcript changes, we identified 1534 maternal genes, 792 transient zygotic genes, and 1053 genes whose transcript levels increase during embryogenesis. Each of these three categories is dominated by groups of genes where all transcript levels increase and/or decrease at similar times, suggesting a common mode of regulation. For example, 34% of the transiently expressed genes fall into three groups, with increased transcript levels between 2.5–12, 11–20, and 15–20 h of development, respectively. We highlight common and distinctive functional features of these expression groups and identify a coupling between downregulation of transcript levels and targeted protein degradation. By mapping the groups to the protein network, we also predict and experimentally confirm new functional associations."
1311,"a mitochondrial paradigm of metabolic and degenerative diseases aging and cancer a dawn for evolutionary medicine",1051429,"A MITOCHONDRIAL PARADIGM OF METABOLIC AND DEGENERATIVE DISEASES, AGING, AND CANCER: A Dawn for Evolutionary Medicine","Abstract Life is the interplay between structure and energy, yet the role of energy deficiency in human disease has been poorly explored by modern medicine. Since the mitochondria use oxidative phosphorylation (OXPHOS) to convert dietary calories into usable energy, generating reactive oxygen species (ROS) as a toxic by-product, I hypothesize that mitochondrial dysfunction plays a central role in a wide range of age-related disorders and various forms of cancer. Because mitochondrial DNA (mtDNA) is present in thousands of copies per cell and encodes essential genes for energy production, I propose that the delayed-onset and progressive course of the age-related diseases results from the accumulation of somatic mutations in the mtDNAs of post-mitotic tissues. The tissue-specific manifestations of these diseases may result from the varying energetic roles and needs of the different tissues. The variation in the individual and regional predisposition to degenerative diseases and cancer may result from the interaction of modern dietary caloric intake and ancient mitochondrial genetic polymorphisms. Therefore the mitochondria provide a direct link between our environment and our genes and the mtDNA variants that permitted our forbears to energetically adapt to their ancestral homes are influencing our health today."
1312,"enrichment or depletion of a go category within a class of genes which test",1056288,"Enrichment or depletion of a GO category within a class of genes: which test?","Motivation: A number of available program packages determine the significant enrichments and/or depletions of GO categories among a class of genes of interest. Whereas a correct formulation of the problem leads to a single exact null distribution, these GO tools use a large variety of statistical tests whose denominations often do not clarify the underlying P-value computations.  Summary: We review the different formulations of the problem and the tests they lead to: the binomial, {chi}2, equality of two probabilities, Fisher's exact and hypergeometric tests. We clarify the relationships existing between these tests, in particular the equivalence between the hypergeometric test and Fisher's exact test. We recall that the other tests are valid only for large samples, the test of equality of two probabilities and the {chi}2-test being equivalent. We discuss the appropriateness of one- and two-sided P-values, as well as some discreteness and conservatism issues.  Contact: isabelle.rivals@espci.fr  Supplementary information: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btl633"
1313,"extending faceted navigation for rdf data",1059893,"Extending faceted navigation for RDF data","Abstract. Data on the Semantic Web is semi-structured and does not follow one fixed schema. Faceted browsing [23] is a natural technique for navigating such data, partitioning the information space into orthogonal conceptual dimensions. Current faceted interfaces are manually constructed and have limited query expressiveness. We develop an expressive faceted interface for semi-structured data and formally show the improvement over existing interfaces. Secondly, we develop metrics for automatic ranking of facet quality, bypassing the need for manual construction of the interface. We develop a prototype for faceted navigation of arbitrary RDF data. Experimental evaluation shows improved usability over current interfaces."
1314,"repression of the human dihydrofolate reductase gene by a noncoding interfering transcript",1060049,"Repression of the human dihydrofolate reductase gene by a non-coding interfering transcript","Alternative promoters within the same gene are a general phenomenon in gene expression. Mechanisms of their selective regulation vary from one gene to another and are still poorly understood. Here we show that in quiescent cells the mechanism of transcriptional repression of the major promoter of the gene encoding dihydrofolate reductase depends on a non-coding transcript initiated from the upstream minor promoter and involves both the direct interaction of the RNA and promoter-specific interference. The specificity and efficiency of repression is ensured by the formation of a stable complex between non-coding RNA and the major promoter, direct interaction of the non-coding RNA with the general transcription factor IIB and dissociation of the preinitiation complex from the major promoter. By using in vivo and in vitro assays such as inducible and reconstituted transcription, RNA bandshifts, RNA interference, chromatin immunoprecipitation and RNA immunoprecipitation, we show that the regulatory transcript produced from the minor promoter has a critical function in an epigenetic mechanism of promoter-specific transcriptional repression."
1315,"largescale discovery of promoter motifs in drosophila melanogaster",1062025,"Large-Scale Discovery of Promoter Motifs in Drosophila melanogaster","A key step in understanding gene regulation is to identify the repertoire of transcription factor binding motifs (TFBMs) that form the building blocks of promoters and other regulatory elements. Identifying these experimentally is very laborious, and the number of TFBMs discovered remains relatively small, especially when compared with the hundreds of transcription factor genes predicted in metazoan genomes. We have used a recently developed statistical motif discovery approach, NestedMICA, to detect candidate TFBMs from a large set of Drosophila melanogaster promoter regions. Of the 120 motifs inferred in our initial analysis, 25 were statistically significant matches to previously reported motifs, while 87 appeared to be novel. Analysis of sequence conservation and motif positioning suggested that the great majority of these discovered motifs are predictive of functional elements in the genome. Many motifs showed associations with specific patterns of gene expression in the D. melanogaster embryo, and we were able to obtain confident annotation of expression patterns for 25 of our motifs, including eight of the novel motifs. The motifs are available through Tiffin, a new database of DNA sequence motifs. We have discovered many new motifs that are overrepresented in D. melanogaster promoter regions, and offer several independent lines of evidence that these are novel TFBMs. Our motif dictionary provides a solid foundation for further investigation of regulatory elements in Drosophila, and demonstrates techniques that should be applicable in other species. We suggest that further improvements in computational motif discovery should narrow the gap between the set of known motifs and the total number of transcription factors in metazoan genomes."
1316,"automated cell lineage tracing in caenorhabditis elegans",1067406,"Automated cell lineage tracing in Caenorhabditis elegans.","The invariant cell lineage and cell fate of Caenorhabditis elegans provide a unique opportunity to decode the molecular mechanisms of animal development. To exploit this opportunity, we have developed a system for automated cell lineage tracing during C. elegans embryogenesis, based on 3D, time-lapse imaging and automated image analysis. Using ubiquitously expressed histone-GFP fusion protein to label cells/nuclei and a confocal microscope, the imaging protocol captures embryogenesis at high spatial (31 planes at 1 microm apart) and temporal (every minute) resolution without apparent effects on development. A set of image analysis algorithms then automatically recognizes cells at each time point, tracks cell movements, divisions and deaths over time and assigns cell identities based on the canonical naming scheme. Starting from the four-cell stage (or earlier), our software, named starrynite, can trace the lineage up to the 350-cell stage in 25 min on a desktop computer. The few errors of automated lineaging can then be corrected in a few hours with a graphic interface that allows easy navigation of the images and the reported lineage tree. The system can be used to characterize lineage phenotypes of genes and/or extended to determine gene expression patterns in a living embryo at the single-cell level. We envision that this automation will make it practical to systematically decipher the developmental genes and pathways encoded in the genome of C. elegans."
1317,"data analysis using regression and multilevelhierarchical models",1068847,"Data Analysis Using Regression and Multilevel/Hierarchical Models","{Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors&#8217; own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.}"
1318,"a virus in a fungus in a plant threeway symbiosis required for thermal tolerance",1069345,"A Virus in a Fungus in a Plant: Three-Way Symbiosis Required for Thermal Tolerance","A mutualistic association between a fungal endophyte and a tropical panic grass allows both organisms to grow at high soil temperatures. We characterized a virus from this fungus that is involved in the mutualistic interaction. Fungal isolates cured of the virus are unable to confer heat tolerance, but heat tolerance is restored after the virus is reintroduced. The virus-infected fungus confers heat tolerance not only to its native monocot host but also to a eudicot host, which suggests that the underlying mechanism involves pathways conserved between these two groups of plants. 10.1126/science.1136237"
1319,"unsupervised and supervised machine learning in user modeling for intelligent learning environments",1069750,"Unsupervised and Supervised Machine Learning in User Modeling for Intelligent Learning Environments.","In this research, we outline a user modeling framework that uses both unsupervised and supervised machine learning in order to reduce development costs of building user models, and facilitate transferability. We apply the framework to model student learning during interaction with the Adaptive Coach for Exploration (ACE) learning environment (using both interface and eye-tracking data). In addition to demonstrating framework effectiveness, we also compare results from previous research on applying the framework to a different learning environment and data type. Our results also confirm previous research on the value of using eye-tracking data to assess student learning."
1320,"looking for information second edition a survey of research on information seeking needs and behavior library and information science",1082136,"Looking for Information, Second Edition: A Survey of Research on Information Seeking, Needs, and Behavior (Library and Information Science)","{<I>Looking for Information</I> explores human information seeking and use. It provides examples of methods, models and theories used in information behavior research, and reviews more than four decades of research on the topic. The book should prove useful for scholars in related fields, but also for students at the graduate and advanced undergraduate levels. It is intended for use not only in information studies and communication, but also in the disciplines of education, management, business, medicine, nursing, public health, and social work. <br><br>This second editon of <I>Looking for Information</I> reflects a vastly increased literature on the topic of information behavior. Among the additions are over 400 new citations to relevant works, most of which appeared between March, 2002, and January, 2006. Many new studies are described in the section reviewing research findings (Chapters Eleven and Twelve), Chapter Nines examples of methods, and a widely expanded discussion of theories applied in information behavior research (Chapter Seven).<br><br>*Reviews over 1,100 works -- 60% more than the first edition<br>*Adds many new studies conducted from 2002 to 2006<br>*Expanded coverage of models and theories of information behavior<br>*Many new examples of occupations and roles -- the contexts of information seeking}"
1321,"a method for using blocked and eventrelated fmri data to study resting state functional connectivity",1086538,"A method for using blocked and event-related fMRI data to study ""resting state"" functional connectivity.","Resting state functional connectivity MRI (fcMRI) has become a particularly useful tool for studying regional relationships in typical and atypical populations. Because many investigators have already obtained large data sets of task-related fMRI, the ability to use this existing task data for resting state fcMRI is of considerable interest. Two classes of data sets could potentially be modified to emulate resting state data. These data sets include: (1) ""interleaved"" resting blocks from blocked or mixed blocked/event-related sets, and (2) residual timecourses from event-related sets that lack rest blocks. Using correlation analysis, we compared the functional connectivity of resting epochs taken from a mixed blocked/event-related design fMRI data set and the residuals derived from event-related data with standard continuous resting state data to determine which class of data can best emulate resting state data. We show that, despite some differences, the functional connectivity for the interleaved resting periods taken from blocked designs is both qualitatively and quantitatively very similar to that of ""continuous"" resting state data. In contrast, despite being qualitatively similar to ""continuous"" resting state data, residuals derived from event-related design data had several distinct quantitative differences. These results suggest that the interleaved resting state data such as those taken from blocked or mixed blocked/event-related fMRI designs are well-suited for resting state functional connectivity analyses. Although using event-related data residuals for resting state functional connectivity may still be useful, results should be interpreted with care."
1322,"a method for comparing group fmri data using independent component analysis application to visual motor and visuomotor tasks",1088311,"A method for comparing group fMRI data using independent component analysis: application to visual, motor and visuomotor tasks.","Independent component analysis (ICA) is an approach for decomposing fMRI data into spatially independent maps and time courses. We have recently proposed a method for ICA of multisubject data; in the current paper, an extension is proposed for allowing ICA group comparisons. This method is applied to data from experiments designed to stimulate visual cortex, motor cortex or both visual and motor cortices. Several intergroup and intragroup metrics are proposed for assessing the utility of the components for comparisons of group ICA data. The proposed method may prove to be useful in answering questions requiring multigroup comparisons when a flexible modeling approach is desired."
1323,"toward a comprehensive atlas of the physical interactome of saccharomyces cerevisiae",1088398,"Toward a comprehensive atlas of the physical interactome of Saccharomyces cerevisiae.","Defining protein complexes is critical to virtually all aspects of cell biology. Two recent affinity purification/mass spectrometry studies in Saccharomyces cerevisiae have vastly increased the available protein interaction data. The practical utility of such high throughput interaction sets, however, is substantially decreased by the presence of false positives. Here we created a novel probabilistic metric that takes advantage of the high density of these data, including both the presence and absence of individual associations, to provide a measure of the relative confidence of each potential protein-protein interaction. This analysis largely overcomes the noise inherent in high throughput immunoprecipitation experiments. For example, of the 12,122 binary interactions in the general repository of interaction data (BioGRID) derived from these two studies, we marked 7504 as being of substantially lower confidence. Additionally, applying our metric and a stringent cutoff we identified a set of 9074 interactions (including 4456 that were not among the 12,122 interactions) with accuracy comparable to that of conventional small scale methodologies. Finally we organized proteins into coherent multisubunit complexes using hierarchical clustering. This work thus provides a highly accurate physical interaction map of yeast in a format that is readily accessible to the biological community."
1324,"modeling singleneuron dynamics and computations a balance of detail and abstraction",1090474,"Modeling single-neuron dynamics and computations: a balance of detail and abstraction.","The fundamental building block of every nervous system is the single neuron. Understanding how these exquisitely structured elements operate is an integral part of the quest to solve the mysteries of the brain. Quantitative mathematical models have proved to be an indispensable tool in pursuing this goal. We review recent advances and examine how single-cell models on five levels of complexity, from black-box approaches to detailed compartmental simulations, address key questions about neural dynamics and signal processing."
1325,"the concept of ba building a foundation for knowledge creation",1091142,"The concept of ""Ba"": Building a Foundation for Knowledge Creation","A paper introduces the Japanese concept of ""Ba"" to organizational theory. Ba (equivalent to ""place"" in English) is a shared space for emerging relationships. It can be a physical, virtual, or mental space. Knowledge, in contrast to information, cannot be separated from the context - it is embedded in ba. To support the process of knowledge creation, a foundation in ba is required. The paper develops and explains 4 specific platforms and their relationships to knowledge creation. Each of the knowledge conversion modes is promoted by a specific ba. A self-transcending process of knowledge creation can be supported by providing ba on different organizational levels. This article presents case studies of 3 companies that employ ba on the team, division, and corporate level to enhance knowledge creation."
1326,"games and simulations in online learning research and development frameworks",1094357,"Games And Simulations in Online Learning: Research and Development Frameworks","{Nearly all early learning happens during play, and new technology has added video games to the list of ways children learn interaction and new concepts. Although video games are everywhere, on Web sites, in stores, streamed to the desktop, on television, they are absent from the classroom. Computer-based simulations, a form of computer games, have begun to appear, but they are not as wide-spread as email, discussion threads, and blogs.    Games and Simulations in Online Learning: Research and Development Frameworks examines the potential of games and simulations in online learning, and how the future could look as developers learn to use the emerging capabilities of the Semantic Web. It presents a general understanding of how the Semantic Web will impact education and how games and simulations can evolve to become robust teaching resources.}"
1327,"improved precision and accuracy for microarrays using updated probe set definitions",1096254,"Improved precision and accuracy for microarrays using updated probe set definitions","BACKGROUND: Microarrays enable high throughput detection of transcript expression levels. Different investigators have recently introduced updated probe set definitions to more accurately map probes to our current knowledge of genes and transcripts. RESULTS: We demonstrate that updated probe set definitions provide both better precision and accuracy in probe set estimates compared to the original Affymetrix definitions. We show that the improved precision mainly depends on the increased number of probes that are integrated into each probe set, but we also demonstrate an improvement when the same number of probes is used. CONCLUSION: Updated probe set definitions does not only offer expression levels that are more accurately associated to genes and transcripts but also improvements in the estimated transcript expression levels. These results give support for the use of updated probe set definitions for analysis and meta-analysis of microarray data."
1328,"bioinfer a corpus for information extraction in the biomedical domain",1097083,"BioInfer: a corpus for information extraction in the biomedical domain","BACKGROUND:Lately, there has been a great interest in the application of information extraction methods to the biomedical domain, in particular, to the extraction of relationships of genes, proteins, and RNA from scientific publications. The development and evaluation of such methods requires annotated domain corpora.RESULTS:We present BioInfer (Bio Information Extraction Resource), a new public resource providing an annotated corpus of biomedical English. We describe an annotation scheme capturing named entities and their relationships along with a dependency analysis of sentence syntax. We further present ontologies defining the types of entities and relationships annotated in the corpus. Currently, the corpus contains 1100 sentences from abstracts of biomedical research articles annotated for relationships, named entities, as well as syntactic dependencies. Supporting software is provided with the corpus. The corpus is unique in the domain in combining these annotation types for a single set of sentences, and in the level of detail of the relationship annotation.CONCLUSION:We introduce a corpus targeted at protein, gene, and RNA relationships which serves as a resource for the development of information extraction systems and their components such as parsers and domain analyzers. The corpus will be maintained and further developed with a current version being available at http://www.it.utu.fi/BioInfer."
1329,"identification of the proliferationdifferentiation switch in the cellular network of multicellular organisms",1103162,"Identification of the proliferation/differentiation switch in the cellular network of multicellular organisms.","The protein-protein interaction networks, or interactome networks, have been shown to have dynamic modular structures, yet the functional connections between and among the modules are less well understood. Here, using a new pipeline to integrate the interactome and the transcriptome, we identified a pair of transcriptionally anticorrelated modules, each consisting of hundreds of genes in multicellular interactome networks across different individuals and populations. The two modules are associated with cellular proliferation and differentiation, respectively. The proliferation module is conserved among eukaryotic organisms, whereas the differentiation module is specific to multicellular organisms. Upon differentiation of various tissues and cell lines from different organisms, the expression of the proliferation module is more uniformly suppressed, while the differentiation module is upregulated in a tissue- and species-specific manner. Our results indicate that even at the tissue and organism levels, proliferation and differentiation modules may correspond to two alternative states of the molecular network and may reflect a universal symbiotic relationship in a multicellular organism. Our analyses further predict that the proteins mediating the interactions between these modules may serve as modulators at the proliferation/differentiation switch."
1330,"why people punish defectors weak conformist transmission can stabilize costly enforcement of norms in cooperative dilemmas",1105908,"Why People Punish Defectors Weak Conformist Transmission can Stabilize Costly Enforcement of Norms in Cooperative Dilemmas","In this paper, we present a cultural evolutionary model in which norms for cooperation and punishment are acquired via two cognitive mechanisms: (1) payoff-biased transmission—a tendency to copy the most successful individual; and (2) conformist transmission—a tendency to copy the most frequent behavior in the population. We first show that if a finite number of punishment stages is permitted (e.g. two stages of punishment occur if some individuals punish people who fail to punish non-cooperators), then an arbitrarily small amount of conformist transmission will stabilize cooperative behavior by stabilizing punishment at some n -th stage. We then explain how, once cooperation is stabilized in one group, it may spread through a multi-group population via cultural group selection. Finally, once cooperation is prevalent, we show how prosocial genes favoring cooperation and punishment may invade in the wake of cultural group selection."
1331,"open information extraction from the web",1106067,"Open Information Extraction from the Web","Traditionally, Information Extraction (IE) has fo- cused on satisfying precise, narrow, pre-speciﬁed requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). Shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. This manual labor scales linearly with the number of target relations. This paper introduces Open IE (OIE), a new ex- traction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. The paper also introduces T EXT RUNNER, a fully implemented, highly scalable OIE system where the tuples are assigned a probability and indexed to support efﬁcient extraction and explo- ration via user queries. We report on experiments over a 9,000,000 Web page corpus that compare T EXT RUNNER with K NOW I TA LL, a state-of-the-art Web IE system. T EXT RUNNER achieves an error reduction of 33% on a comparable set of extractions. Furthermore, in the amount of time it takes K NOW I TA LL to per- form extraction for a handful of pre-speciﬁed re- lations, T EXT RUNNER extracts a far broader set of facts reﬂecting orders of magnitude more rela- tions, discovered on the ﬂy. We report statistics on T EXT RUNNER’s 11,000,000 highest probability tuples, and show that they contain over 1,000,000 concrete facts and over 6,500,000 more abstract as- sertions."
1332,"models and languages for parallel computation",1106569,"Models and languages for parallel computation","We survey parallel programming models and languages using six criteria to assess their suitability for realistic portable parallel programming. We argue that an ideal model should by easy to program, should have a software development methodology, should be architecture-independent, should be easy to understand, should guarantee performance, and should provide accurate information about the cost of programs. These criteria reflect our belief that developments in parallelism must be driven by a parallel software industry based on portability and efficiency. We consider programming models in six categories, depending on the level of abstraction they provide. Those that are very abstract conceal even the presence of parallelism at the software level. Such models make software easy to  build and port, but efficient and predictable performance is usually hard to achieve. At the other end of the spectrum, low-level models make all of the messy issues of parallel programming explicit (how many threads, how to place them, how to express communication, and how to schedule communication), so that software is hard to build and not very portable, but is usually efficient. Most recent models are near the center of this spectrum, exploring the best tradeoffs between expressiveness and performance. A few models have achieved both abstractness and efficiency. Both kinds of models raise the possibility of parallelism as part of the mainstream of computing."
1333,"supercontinuum generation in photonic crystal fiber",1107595,"Supercontinuum generation in photonic crystal fiber","A topical review of numerical and experimental studies of supercontinuum generation in photonic crystal fiber is presented over the full range of experimentally reported parameters, from the femtosecond to the continuous-wave regime. Results from numerical simulations are used to discuss the temporal and spectral characteristics of the supercontinuum, and to interpret the physics of the underlying spectral broadening processes. Particular attention is given to the case of supercontinuum generation seeded by femtosecond pulses in the anomalous group velocity dispersion regime of photonic crystal fiber, where the processes of soliton fission, stimulated Raman scattering, and dispersive wave generation are reviewed in detail. The corresponding intensity and phase stability properties of the supercontinuum spectra generated under different conditions are also discussed."
1334,"personalized search based on user search histories",1109408,"Personalized search based on user search histories","User profiles, descriptions of user interests, can be used by search engines to provide personalized search results. Many approaches to creating user profiles collect user information through proxy servers (to capture browsing histories) or desktop bots (to capture activities on a personal computer). Both these techniques require participation of the user to install the proxy server or the bot. In this study, we explore the use of a less-invasive means of gathering user information for personalized search. In particular, we build user profiles based on activity at the search site itself and study the use of these profiles to provide personalized search results. By implementing a wrapper around the Google search engine, we were able to collect information about individual user search activities. In particular, we collected the queries for which at least one search result was examined, and the snippets (titles and summaries) for each examined result. User profiles were created by classifying the collected information (queries or snippets) into concepts in a reference concept hierarchy. These profiles were then used to re-rank the search results and the rank-order of the user-examined results before and after re-ranking were compared. Our study found that user profiles based on queries were as effective as those based on snippets. We also found that our personalized re-ranking resulted in a 34% improvement in the rankorder of the user-selected results."
1335,"the evolutionary significance of cisregulatory mutations",1110638,"The evolutionary significance of cis-regulatory mutations"," For decades, evolutionary biologists have argued that changes in cis-regulatory sequences constitute an important part of the genetic basis for adaptation. Although originally based on first principles, this claim is now empirically well supported: numerous studies have identified cis-regulatory mutations with functionally significant consequences for morphology, physiology and behaviour. The focus has now shifted to considering whether cis-regulatory and coding mutations make qualitatively different contributions to phenotypic evolution. Cases in which parallel mutations have produced parallel trait modifications in particular suggest that some phenotypic changes are more likely to result from cis-regulatory mutations than from coding mutations."
1336,"analyzing gene expression data in terms of gene sets methodological issues",1111375,"Analyzing gene expression data in terms of gene sets: methodological issues.","MOTIVATION: Many statistical tests have been proposed in recent years for analyzing gene expression data in terms of gene sets, usually from Gene Ontology. These methods are based on widely different methodological assumptions. Some approaches test differential expression of each gene set against differential expression of the rest of the genes, whereas others test each gene set on its own. Also, some methods are based on a model in which the genes are the sampling units, whereas others treat the subjects as the sampling units. This article aims to clarify the assumptions behind different approaches and to indicate a preferential methodology of gene set testing. RESULTS: We identify some crucial assumptions which are needed by the majority of methods. P-values derived from methods that use a model which takes the genes as the sampling unit are easily misinterpreted, as they are based on a statistical model that does not resemble the biological experiment actually performed. Furthermore, because these models are based on a crucial and unrealistic independence assumption between genes, the P-values derived from such methods can be wildly anti-conservative, as a simulation experiment shows. We also argue that methods that competitively test each gene set against the rest of the genes create an unnecessary rift between single gene testing and gene set testing."
1337,"overexposed privacy patterns and considerations in online and mobile photo sharing",1116783,"Over-Exposed? Privacy Patterns and Considerations in Online and Mobile Photo Sharing","In a first-of-its-kind study, we use context-aware camerephone devices to examine privacy decisions in mobile and online photo sharing. We employed both quantitative and qualitative methods to reveal privacy patterns and common considerations users apply when making these privacy decisions. We performed data analysis on a corpus of privacy decisions and associated context data from a real-world system. We identified several relationships between context of photo capture and photo privacy settings. Our interviews with 15 users reveal common themes in privacy considerations: security, social disclosure, identity and convenience. While many users considered those in respect to themselves, some users were concerned about the privacy of others in the same manner. Interviews also indicate that users are largely unconcerned about long term, persistent exposure of the location information associated with photos. We highlight several implications for design of cameraphone applications, media content hosting, and photo sharing applications, including using past privacy patterns to prevent mistakes and increase awareness of privacy issues. user's personal and social environment. The persistent nature of such online media could reveal aggregate, rich information about the owner of the content. Digital cameras, and lately, a new class of cameraphone applications that can upload photos directly to the web, make it even easier for people to publish content online. The considerations made by users during the process are crucial for the design of systems that support the creation of such content."
1338,"a threshold selection method from graylevel histograms",1116982,"A threshold selection method from gray-level histograms","A nonparametric and unsupervised method of automatic threshold selection for picture segmentation is presented. An optimal threshold is selected by the discriminant criterion, namely, so as to maximize the separability of the resultant classes in gray levels. The procedure is very simple, utilizing only the zeroth- and the first-order cumulative moments of the gray-level histogram. It is straightforward to extend the method to multithreshold problems. Several experimental results are also presented to support the validity of the method."
1339,"distributed file systems concepts and examples",1117170,"Distributed file systems: concepts and examples","The purpose of a distributed file system (DFS) is to allow users of physically distributed computers to share data and storage resources by using a common file system. A typical configuration for a DFS is a collection of workstations and mainframes connected by a local area network (LAN). A DFS is implemented as part of the operating system of each of the connected computers. This paper establishes a viewpoint that emphasizes the dispersed structure and decentralization of both data and control in the design of such systems. It defines the concepts of transparency, fault tolerance, and scalability and discusses them in the context of DFSs. The paper claims that the principle of distributed operation is fundamental for a fault tolerant and scalable DFS design. It also presents   alternatives for the semantics of sharing and methods for providing access to remote files. A survey of contemporary UNIX-based systems, namely, UNIX United, Locus, Sprite, Sun's Network File System, and ITC's Andrew, illustrates the concepts and demonstrates various implementations and design alternatives. Based on the assessment of these systems, the paper makes the point that a departure from the extending centralized file systems over a communication network is necessary to accomplish sound distributed file system design."
1340,"the oring theory of economic development",1118438,"The O-Ring Theory of Economic Development","This paper proposes a production function describing processes subject to mistakes in any of several tasks. It shows that high-skill workers--those who make few mistakes--will be matched together in equilibrium, and that wages and output will rise steeply in skill. The model is consistent with large income differences between countries, the predominance of small firms in poor countries, and the positive correlation between the wages of workers in different occupations within enterprises. Imperfect observability of skill leads to imperfect matching and thus to spillovers, strategic complementarity, and multiple equilibria in education."
1341,"eigenbehaviors identifying structure in routine",1121438,"Eigenbehaviors: Identifying Structure in Routine","Abstract&nbsp;&nbsp;Longitudinal behavioral data generally contains a significant amount of structure. In this work, we identify the structure inherent in daily behavior with models that can accurately analyze, predict, and cluster multimodal data from individuals and communities within the social network of a population. We represent this behavioral structure by the principal components of the complete behavioral dataset, a set of characteristic vectors we have termed eigenbehaviors. In our model, an individual’s behavior over a specific day can be approximated by a weighted sum of his or her primary eigenbehaviors. When these weights are calculated halfway through a day, they can be used to predict the day’s remaining behaviors with 79% accuracy for our test subjects. Additionally, we demonstrate the potential for this dimensionality reduction technique to infer community affiliations within the subjects’ social network by clustering individuals into a “behavior space” spanned by a set of their aggregate eigenbehaviors. These behavior spaces make it possible to determine the behavioral similarity between both individuals and groups, enabling 96% classification accuracy of community affiliations within the population-level social network. Additionally, the distance between individuals in the behavior space can be used as an estimate for relational ties such as friendship, suggesting strong behavioral homophily amongst the subjects. This approach capitalizes on the large amount of rich data previously captured during the Reality Mining study from mobile phones continuously logging location, proximate phones, and communication of 100 subjects at MIT over the course of 9&nbsp;months. As wearable sensors continue to generate these types of rich, longitudinal datasets, dimensionality reduction techniques such as eigenbehaviors will play an increasingly important role in behavioral research."
1342,"a highperformance semisupervised learning method for text chunking",1122346,"A High-Performance Semi-Supervised Learning Method for Text Chunking","In machine learning, whether one can build a more accurate classiﬁer by using unlabeled data (semi-supervised learning) is an important issue. Although a num- ber of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. This paper presents a novel semi-supervised method that em- ploys a learning paradigm which we call structural learning. The idea is to ﬁnd “what good classiﬁers are like” by learn- ing from thousands of automatically gen- erated auxiliary classiﬁcation problems on unlabeled data. By doing so, the common predictive structure shared by the multiple classiﬁcation problems can be discovered, which can then be used to improve perfor- mance on the target problem. The method produces performance higher than the pre- vious best results on CoNLL’00 syntac- tic chunking and CoNLL’03 named entity chunking (English and German)."
1343,"remarkables a webbased research collaboration support system using social bookmarking tools",1123052,"ReMarkables: A Web-Based Research Collaboration Support System Using Social Bookmarking Tools","The paper describes design and implementation of a web-based Research Collaboration Support System (RCSS) using social bookmarking tools for scientific research communities. A key feature of the developed system ReMarkables is that it provides topic bookmarks function to build research communities in which the members can share bookmarks with specific tags and communicate with each other using mailing list and wiki pages associated with a topic bookmark list. Also, the system provides topic recommender function to promote the research collaboration using the topic bookmark list. The system is implemented as an extension of Connotea social bookmarking tools as open source software."
1344,"digital libraries and the future of the library profession",1125833,"Digital libraries and the future of the library profession","Abstract: Purpose – To argue that unique contemporary cultural shifts are leading to a new form of librarianship that can be characterised as “postmodern” in nature, and that this form of professional specialism will be increasingly influential in the decades to come. Design/methodology/approach – A theoretical piece based on ideas from cultural history. Findings – That postmodern library and information science {(LIS)} concepts will be a vital new strand to professional practice, but they will most likely subsist alongside more familiar concepts of practice which have proved readily applicable in the early years of “first wave” web technologies. Research limitations/implications – These are purely conceptual approaches to {LIS} and need to be investigated evidentially. Practical implications – The change from “first wave” web technologies to Web 2.0 information technologies may have a greater impact on future techniques in digital librarianship than the change from print to the first electronic libraries in the 1990s. Originality/value – This {LIS} paper is distinctive in that it borrows original ideas from the humanities to offer an understanding of {LIS} practice in the context of broad “cultural theory”, rather than in the narrower context of change in mechanical and technological processes"
1345,"building quality assurance into metadata creation an analysis based on the learning objects and eprints communities of practice",1126745,"Building Quality Assurance into Metadata Creation: an Analysis based on the Learning Objects and e-Prints Communities of Practice","This paper challenges some of the assumptions underlying the metadata creation process in the context of two communities of practice, based around learning object repositories and open e- Print archives. The importance of quality assurance for metadata creation is discussed and evidence from the literature, from the practical experiences of repositories and archives, and from related research and practices within other communities is presented. Issues for debate and further investigation are identified, formulated as a series of key research questions. Although there is much work to be done in the area of quality assurance for metadata creation, this paper represents an important first step towards a fuller understanding of the subject."
1346,"minimus a fast lightweight genome assembler",1126765,"Minimus: a fast, lightweight genome assembler","BACKGROUND: Genome assemblers have grown very large and complex in response to the need for algorithms to handle the challenges of large whole-genome sequencing projects. Many of the most common uses of assemblers, however, are best served by a simpler type of assembler that requires fewer software components, uses less memory, and is far easier to install and run. RESULTS: We have developed the Minimus assembler to address these issues, and tested it on a range of assembly problems. We show that Minimus performs well on several small assembly tasks, including the assembly of viral genomes, individual genes, and BAC clones. In addition, we evaluate Minimus' performance in assembling bacterial genomes in order to assess its suitability as a component of a larger assembly pipeline. We show that, unlike other software currently used for these tasks, Minimus produces significantly fewer assembly errors, at the cost of generating a more fragmented assembly. CONCLUSION: We find that for small genomes and other small assembly tasks, Minimus is faster and far more flexible than existing tools. Due to its small size and modular design Minimus is perfectly suited to be a component of complex assembly pipelines. Minimus is released as an open-source software project and the code is available as part of the AMOS project at Sourceforge."
1347,"human polymorphism at micrornas and microrna target sites",1130486,"Human polymorphism at microRNAs and microRNA target sites","10.1073/pnas.0611347104 MicroRNAs (miRNAs) function as endogenous translational repressors of protein-coding genes in animals by binding to target sites in the 3′ UTRs of mRNAs. Because a single nucleotide change in the sequence of a target site can affect miRNA regulation, naturally occurring SNPs in target sites are candidates for functional variation that may be of interest for biomedical applications and evolutionary studies. However, little is known to date about variation among humans at miRNAs and their target sites. In this study, we analyzed publicly available SNP data in context with miRNAs and their target sites throughout the human genome, and we found a relatively low level of variation in functional regions of miRNAs, but an appreciable level of variation at target sites. Approximately 400 SNPs were found at experimentally verified target sites or predicted target sites that are otherwise evolutionarily conserved across mammals. Moreover, ≈250 SNPs potentially create novel target sites for miRNAs in humans. If some variants have functional effects, they might confer phenotypic differences among humans. Although the majority of these SNPs appear to be evolving under neutrality, interestingly, some of these SNPs are found at relatively high population frequencies even in experimentally verified targets, and a few variants are associated with atypically long-range haplotypes that may have been subject to recent positive selection."
1348,"linear optical quantum computing with photonic qubits",1137074,"Linear optical quantum computing with photonic qubits","Linear optics with photon counting is a prominent candidate for practical quantum computing. The protocol by Knill, Laflamme, and Milburn [2001, Nature (London) 409, 46] explicitly demonstrates that efficient scalable quantum computing with single photons, linear optical elements, and projective measurements is possible. Subsequently, several improvements on this protocol have started to bridge the gap between theoretical scalability and practical implementation. The original theory and its improvements are reviewed, and a few examples of experimental two-qubit gates are given. The use of realistic components, the errors they induce in the computation, and how these errors can be corrected is discussed."
1349,"multi label classification an overview",1142586,"Multi Label Classification: An Overview","Nowadays, multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization and semantic scene classification. This paper introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative experimental results of certain multi-label classification methods. It also contributes the definition of concepts for the quantification of the multi-label nature of a data set."
1350,"the optical properties of metal nanoparticles the influence of size shape and dielectric environment",1144984,"The Optical Properties of Metal Nanoparticles:  The Influence of Size, Shape, and Dielectric Environment","Abstract: The optical properties of metal nanoparticles have long been of interest in physical chemistry, starting with Faraday's investigations of colloidal gold in the middle 1800s. More recently, new lithographic techniques as well as improvements to classical wet chemistry methods have made it possible to synthesize noble metal nanoparticles with a wide range of sizes, shapes, and dielectric environments. In this feature article, we describe recent progress in the theory of nanoparticle optical properties, particularly methods for solving Maxwell's equations for light scattering from particles of arbitrary shape in a complex environment. Included is a description of the qualitative features of dipole and quadrupole plasmon resonances for spherical particles; a discussion of analytical and numerical methods for calculating extinction and scattering cross-sections, local fields, and other optical properties for nonspherical particles; and a survey of applications to problems of recent interest involving triangular silver particles and related shapes."
1351,"comparative genome assembly",1145123,"Comparative genome assembly.","One of the most complex and computationally intensive tasks of genome sequence analysis is genome assembly. Even today, few centres have the resources, in both software and hardware, to assemble a genome from the thousands or millions of individual sequences generated in a whole-genome shotgun sequencing project. With the rapid growth in the number of sequenced genomes has come an increase in the number of organisms for which two or more closely related species have been sequenced. This has created the possibility of building a comparative genome assembly algorithm, which can assemble a newly sequenced genome by mapping it onto a reference genome. We describe here a novel algorithm for comparative genome assembly that can accurately assemble a typical bacterial genome in less than four minutes on a standard desktop computer. The software is available as part of the open-source AMOS project."
1352,"semipublic displays for small colocated groups",1145683,"Semi-public displays for small, co-located groups","The majority of systems using public displays to foster awareness have focused on providing information across remote locations or among people who are loosely connected and lack awareness of each other's activities or interests. We have, however, identified many potential benefits for an awareness system that displays information within a small, co-located group in which the members already possess some awareness of each other's activities. By using ""Semi-Public Displays,"" public displays scoped for small groups, we can make certain types of information visible in the environment, promoting collaboration and providing lightweight information about group activity. Compared to designing for large, loosely connected groups, designing for Semi-Public Displays mitigates typically problematic issues in sustaining relevant content for the display and minimizing privacy concerns. We are using these applications to support and enhance the interactions and information that group members utilize to maintain awareness and collaborate."
1353,"singlemolecule fluorescence resonance energy transfer",1146027,"Single-Molecule Fluorescence Resonance Energy Transfer","Fluorescent resonance energy transfer (FRET) is a powerful technique for studying conformational distribution and dynamics of biological molecules. Some conformational changes are difficult to synchronize or too rare to detect using ensemble FRET. FRET, detected at the single-molecule level, opens up new opportunities to probe the detailed kinetics of structural changes without the need for synchronization. Here, we discuss practical considerations for its implementation including experimental apparatus, fluorescent probe selection, surface immobilization, single-molecule FRET analysis schemes, and interpretation."
1354,"map learning with uninterpreted sensors and effectors",1146124,"Map learning with uninterpreted sensors and effectors","This paper presents a set of methods by which a learning agent can learn a sequence of increasingly abstract and powerful interfaces to control a robot whose sensorimotor apparatus and environment are initially unknown, The result of the learning is a rich hierarchical model of the robot's world (its sensorimotor apparatus and environment). The learning methods rely on generic properties of the robot's world such as almost-everywhere smooth effects of motor control signals on sensory features. At the lowest level of the hierarchy, the learning agent analyzes the effects of its motor control signals in order to define a new set of control signals, one for each of the robot's degrees of freedom. It uses a generate-and-test approach to define sensory features that capture important aspects of the environment, It uses Linear regression to learn models that characterize context-dependent effects of the control signals on the learned features. It uses these models to define high-level control laws for finding and following paths defined using constraints on the learned features, The agent abstracts these control laws, which interact with the continuous environment, to a finite set of actions that implement discrete state transitions. At this point, the agent has abstracted the robot's continuous world to a finite-state world and can use existing methods to learn its structure. The learning agent's methods are evaluated on several simulated robots with different sensorimotor systems and environments."
1355,"a human phenomeinteractome network of protein complexes implicated in genetic disorders",1146927,"A human phenome-interactome network of protein complexes implicated in genetic disorders","We performed a systematic, large-scale analysis of human protein complexes comprising gene products implicated in many different categories of human disease to create a phenome-interactome network. This was done by integrating quality-controlled interactions of human proteins with a validated, computationally derived phenotype similarity score, permitting identification of previously unknown complexes likely to be associated with disease. Using a phenomic ranking of protein complexes linked to human disease, we developed a Bayesian predictor that in 298 of 669 linkage intervals correctly ranks the known disease-causing protein as the top candidate, and in 870 intervals with no identified disease-causing gene, provides novel candidates implicated in disorders such as retinitis pigmentosa, epithelial ovarian cancer, inflammatory bowel disease, amyotrophic lateral sclerosis, Alzheimer disease, type 2 diabetes and coronary heart disease. Our publicly available draft of protein complexes associated with pathology comprises 506 complexes, which reveal functional relationships between disease-promoting genes that will inform future experimentation."
1356,"factors influencing publication choice why faculty choose open access",1152368,"Factors influencing publication choice: why faculty choose open access.","BACKGROUND: In an attempt to identify motivating factors involved in decisions to publish in open access and open archives (OA) journals, individual interviews with biomedical faculty members at the University of North Carolina at Chapel Hill (UNC-Chapel Hill) and Duke University, two major research universities, were conducted. The interviews focused on faculty identified as early adopters of OA/free full-text publishing. METHODS: Searches conducted in PubMed and PubMed Central identified faculty from the two institutions who have published works in OA/free full-text journals. The searches targeted authors with multiple OA citations during a specified 18 month period. Semi-structured interviews were conducted with the most prolific OA authors at each university. Individual interviews attempted to determine whether the authors were aware they published in OA journals, why they chose to publish in OA journals, what factors influenced their publishing decisions, and their general attitude towards OA publishing models. RESULTS & DISCUSSION: Fourteen interviews were granted and completed. Respondents included a fairly even mix of Assistant, Associate and Full professors. Results indicate that when targeting biomedical faculty at UNC-Chapel Hill and Duke, speed of publication and copyright retention are unlikely motivating factors or incentives for the promotion of OA publishing. In addition, author fees required by some open access journals are unlikely barriers or disincentives. CONCLUSION: It appears that publication quality is of utmost importance when choosing publication venues in general, while free access and visibility are specifically noted incentives for selection of OA journals. Therefore, free public availability and increased exposure may not be strong enough incentives for authors to choose open access over more traditional and respected subscription based publications, unless the quality issue is also addressed."
1357,"learning implicit user interest hierarchy for context in personalization",1153312,"Learning implicit user interest hierarchy for context in personalization","Abstract&nbsp;&nbsp; To provide a more robust context for personalization, we desire to extract a continuum of general to specific interests of a user, called a user interest hierarchy (UIH). The higher-level interests are more general, while the lower-level interests are more specific. A UIH can represent a user’s interests at different abstraction levels and can be learned from the contents (words/phrases) in a set of web pages bookmarked by a user. We propose a divisive hierarchical clustering (DHC) algorithm to group terms (topics) into a hierarchy where more general interests are represented by a larger set of terms. Our approach does not need user involvement and learns the UIH “implicitly”. To enrich features used in the UIH, we used phrases in addition to words. Our experiment indicates that DHC with the Augmented Expected Mutual Information (AEMI) correlation function and MaxChildren threshold-finding method built more meaningful UIHs than the other combinations on average; using words and phrases as features improved the quality of UIHs."
1358,"the protection of information in computer systems",1155153,"The protection of information in computer systems","This tutorial paper explores the mechanics of protecting computer-stored information from unauthorized use or modification. It concentrates on those architectural structures-whether hardware or software-that are necessary to support information protection. The paper develops in three main sections. Section I describes desired functions, design principles, and examples of elementary protection and authentication mechanisms. Any reader familiar with computers should find the first section to be reasonably accessible. Section II requires some familiarity with descriptor-based computer architecture. It examines in depth the principles of modern protection architectures and the relation between capability systems and access control list systems, and ends with a brief analysts of protected subsystems and protected objects. The reader who is dismayed by either the prerequisites or the level of detail in the second section may wish to skip to Section III, which reviews the state of the art and current research projects and provides suggestions for further reading."
1359,"random packings of frictionless particles",1155801,"Random Packings of Frictionless Particles","We conduct numerical simulations of random packings of frictionless particles at T = 0. The packing fraction where the pressure becomes nonzero is the same as the jamming threshold, where the static shear modulus becomes nonzero. The distribution of threshold packing fractions narrows, and its peak approaches random close packing as the system size increases. For packing fractions within the peak, there is no self-averaging, leading to exponential decay of the interparticle force distribution."
1360,"reconstructing genetic ancestry blocks in admixed individuals",1158212,"Reconstructing genetic ancestry blocks in admixed individuals.","A chromosome in an individual of recently admixed ancestry resembles a mosaic of chromosomal segments, or ancestry blocks, each derived from a particular ancestral population. We consider the problem of inferring ancestry along the chromosomes in an admixed individual and thereby delineating the ancestry blocks. Using a simple population model, we infer gene-flow history in each individual. Compared with existing methods, which are based on a hidden Markov model, the Markov–hidden Markov model (MHMM) we propose has the advantage of accounting for the background linkage disequilibrium (LD) that exists in ancestral populations. When there are more than two ancestral groups, we allow each ancestral population to admix at a different time in history. We use simulations to illustrate the accuracy of the inferred ancestry as well as the importance of modeling the background LD; not accounting for background LD between markers may mislead us to false inferences about mixed ancestry in an indigenous population. The MHMM makes it possible to identify genomic blocks of a particular ancestry by use of any high-density single-nucleotide–polymorphism panel. One application of our method is to perform admixture mapping without genotyping special ancestry-informative–marker panels."
1361,"automatic recognition of multiword terms the cvaluencvalue method",1158241,"Automatic recognition of multi-word terms:. the C-value/NC-value method","Abstract. &nbsp;&nbsp;Technical terms (henceforth called terms ), are important elements for digital libraries. In this paper we present a domain-independent method for the automatic extraction of multi-word terms, from machine-readable special language corpora. The method, (C-value/NC-value ), combines linguistic and statistical information. The first part, C-value, enhances the common statistical measure of frequency of occurrence for term extraction, making it sensitive to a particular type of multi-word terms, the nested terms. The second part, NC-value, gives: 1) a method for the extraction of term context words (words that tend to appear with terms); 2) the incorporation of information from term context words to the extraction of terms."
1362,"active shape modelstheir training and application",1158393,"Active Shape Models-Their Training and Application","Model-based vision is firmly established as a robust approach to recognizing and locating known rigid objects in the presence of noise, clutter, and occlusion. It is more problematic to apply model-based methods to images of objects whose appearance can vary, though a number of approaches based on the use of flexible templates have been proposed. The problem with existing methods is that they sacrifice model specificity in order to accommodate variability, thereby compromising robustness during image interpretation. We argue that a model should only be able to deform in ways characteristic of the class of objects it represents. We describe a method for building models by learning patterns of variability from a training set of correctly annotated images. These models can be used for image search in an iterative refinement algorithm analogous to that employed by Active Contour Models (Snakes). The key difference is that our  Active Shape Models  can only deform to fit the data in ways consistent with the training set. We show several practical examples where we have built such models and used them to locate partially occluded objects in noisy, cluttered images."
1363,"the epigenomics of cancer",1159111,"The Epigenomics of Cancer"," Aberrant gene function and altered patterns of gene expression are key features of cancer. Growing evidence shows that acquired epigenetic abnormalities participate with genetic alterations to cause this dysregulation. Here, we review recent advances in understanding how epigenetic alterations participate in the earliest stages of neoplasia, including stem/precursor cell contributions, and discuss the growing implications of these advances for strategies to control cancer."
1364,"a componentbased simulation layer for james",1159921,"A component-based simulation layer for {JAMES}","If a model shall be executed in a parallel, distributed instead of a sequential manner, typically the entire simulation engine has to be exchanged. To adapt the simulation layer more easily to the requirements of a concrete model to be run in a specific environment a component based simulation layer has been developed for JAMES. A set of different simulator components demonstrates that a component-based design facilitates the exchange of simulators and their combination."
1365,"stochsim modelling of stochastic biomolecular processes",1159939,"S{{TOCHSIM}}: modelling of stochastic biomolecular processes.","Summary: STOCHSIM is a stochastic simulator for chemical reactions. Molecules are represented as individual software objects that react according to probabilities derived from concentrations and rate constants. Version 1.2 of STOCHSIM provides a novel cross-platform graphical interface written in Perl/Tk. A simple two-dimensional spatial structure has also been implemented, in which nearest-neighbour interactions of molecules in a 2-D lattice can be simulated.  Availability: Various ports of the program can be retrieved at ftp://ftp.cds.caltech.edu/pub/dbray/  Contact: nl223@cus.cam.ac.uk; tss26@cam.ac.uk 10.1093/bioinformatics/17.6.575"
1366,"wholegenome chipchip analysis of dorsal twist and snail suggests integration of diverse patterning processes in the drosophila embryo",1161647,"Whole-genome ChIP-chip analysis of Dorsal, Twist, and Snail suggests integration of diverse patterning processes in the Drosophila embryo.","Genetic studies have identified numerous sequence-specific transcription factors that control development, yet little is known about their in vivo distribution across animal genomes. We determined the genome-wide occupancy of the dorsoventral (DV) determinants Dorsal, Twist, and Snail in the Drosophila embryo using chromatin immunoprecipitation coupled with microarray analysis (ChIP-chip). The in vivo binding of these proteins correlate tightly with the limits of known enhancers. Our analysis predicts substantially more target genes than previous estimates, and includes Dpp signaling components and anteroposterior (AP) segmentation determinants. Thus, the ChIP-chip data uncover a much larger than expected regulatory network, which integrates diverse patterning processes during development."
1367,"the difference how the power of diversity creates better groups firms schools and societies",1165718,"The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies","{<p>In this landmark book, Scott Page redefines the way we understand ourselves in relation to one another. <i>The Difference</i> is about how we think in groups--and how our collective wisdom exceeds the sum of its parts. Why can teams of people find better solutions than brilliant individuals working alone? And why are the best group decisions and predictions those that draw upon the very qualities that make each of us unique? The answers lie in diversity--not what we look like outside, but what we look like within, our distinct tools and abilities.</p><p><i>The Difference</i> reveals that progress and innovation may depend less on lone thinkers with enormous IQs than on diverse people working together and capitalizing on their individuality. Page shows how groups that display a range of perspectives outperform groups of like-minded experts. Diversity yields superior outcomes, and Page proves it using his own cutting-edge research. Moving beyond the politics that cloud standard debates about diversity, he explains why difference beats out homogeneity, whether you're talking about citizens in a democracy or scientists in the laboratory. He examines practical ways to apply diversity's logic to a host of problems, and along the way offers fascinating and surprising examples, from the redesign of the Chicago ""El"" to the truth about where we store our ketchup.</p><p>Page changes the way we understand diversity--how to harness its untapped potential, how to understand and avoid its traps, and how we can leverage our differences for the benefit of all.</p>}"
1368,"trading structure for randomness in wireless opportunistic routing",1167552,"Trading Structure for Randomness in Wireless Opportunistic Routing","Opportunistic routing is a recent technique that achieves high throughput in the face of lossy wireless links. The current opportunistic routing protocol, ExOR, ties the MAC with routing, imposing a strict schedule on routers’ access to the medium. Although the scheduler delivers opportunistic gains, it misses some of the inherent features of the 802.11 MAC. For example, it prevents spatial reuse and thus may underutilize the wireless medium. It also eliminates the layering abstraction, making the protocol less amenable to extensions to alternate traffic types such as multicast. This paper presents MORE, a MAC-independent opportunistic routing protocol. MORE randomly mixes packets before forwarding them. This randomness ensures that routers that hear the same transmission do not forward the same packets. Thus, MORE needs no special scheduler to coordinate routers and can run directly on top of 802.11. Experimental results from a 20-node wireless testbed show that MORE’s median unicast throughput is 22% higher than ExOR, and the gains rise to 45% over ExOR when there is a chance of spatial reuse. For multicast, MORE’s gains increase with the number of destinations, and are 35-200% greater than ExOR."
1369,"reactome a knowledge base of biologic pathways and processes",1169116,"Reactome: a knowledge base of biologic pathways and processes","Reactome http://www.reactome.org, an online curated resource for human pathway data, provides infrastructure for computation across the biologic reaction network. We use Reactome to infer equivalent reactions in multiple nonhuman species, and present data on the reliability of these inferred reactions for the distantly related eukaryote Saccharomyces cerevisiae. Finally, we describe the use of Reactome both as a learning resource and as a computational tool to aid in the interpretation of microarrays and similar large-scale datasets."
1370,"synonym search in wikipedia synarcher",1169701,"Synonym search in Wikipedia: Synarcher","The program Synarcher for synonym (and related terms) search in the text corpus of special structure (Wikipedia) was developed. The results of the search are presented in the form of graph. It is possible to explore the graph and search for graph elements interactively. Adapted HITS algorithm for synonym search, program architecture, and program work evaluation with test examples are presented in the paper. The proposed algorithm can be applied to a query expansion by synonyms (in a search engine) and a synonym dictionary forming."
1371,"multiple dna and protein sequence alignment based on segmenttosegment comparison",1169733,"Multiple {DNA} and protein sequence alignment based on segment-to-segment comparison","In this paper, a new way to think about, and to construct, pairwise as well as multiple alignments of DNA and protein sequences is proposed. Rather than forcing alignments to either align single residues or to introduce gaps by defining an alignment as a path running right from the source up to the sink in the associated dot-matrix diagram, we propose to consider alignments as consistent equivalence relations defined on the set of all positions occurring in all sequences under consideration. We also propose constructing alignments from whole segments exhibiting highly significant overall similarity rather than by aligning individual residues. Consequently, we present an alignment algorithm that (i) is based on segment-to-segment comparison instead of the commonly used residue-to-residue comparison and which (ii) avoids the well-known difficulties concerning the choice of appropriate gap penalties: gaps are not treated explicity, but remain as those parts of the sequences that do not belong to any of the aligned segments. Finally, we discuss the application of our algorithm to two test examples and compare it with commonly used alignment methods. As a first example, we aligned a set of 11 DNA sequences coding for functional helix-loop-helix proteins. Though the sequences show only low overall similarity, our program correctly aligned all of the 11 functional sites, which was a unique result among the methods tested. As a by-product, the reading frames of the sequences were identified. Next, we aligned a set of ribonuclease H proteins and compared our results with alignments produced by other programs as reported by McClure et al. [McClure, M. A., Vasi, T. K. & Fitch, W. M. (1994) Mol. Biol. Evol. 11, 571-592]. Our program was one of the best scoring programs. However, in contrast to other methods, our protein alignments are independent of user-defined parameters."
1372,"limits on the memory storage capacity of bounded synapses",1176143,"Limits on the memory storage capacity of bounded synapses.","Memories maintained in patterns of synaptic connectivity are rapidly overwritten and destroyed by ongoing plasticity related to the storage of new memories. Short memory lifetimes arise from the bounds that must be imposed on synaptic efficacy in any realistic model. We explored whether memory performance can be improved by allowing synapses to traverse a large number of states before reaching their bounds, or by changing the way these bounds are imposed. In the case of hard bounds, memory lifetimes grow proportional to the square of the number of synaptic states, but only if potentiation and depression are precisely balanced. Improved performance can be obtained without fine tuning by imposing soft bounds, but this improvement is only linear with respect to the number of synaptic states. We explored several other possibilities and conclude that improving memory performance requires a more radical modification of the standard model of memory storage."
1373,"network coding for wireless mesh networks a case study",1181456,"Network Coding for Wireless Mesh Networks: A Case Study","Network coding is a new transmission paradigm that proved its strength in optimizing the usage of network resources. In this paper, we evaluate the gain from using network coding for file sharing applications running on top of wireless mesh networks. With extensive simulations carried out on a simulator we developed specifically for this study, we confirm that network coding can improve the performance of the file sharing application, but not as in wired networks. The main reason is that nodes over wireless cannot listen to different neighbors simultaneously. Nevertheless, one can get more from network coding if the information transmission is made more diverse inside the network. We support this argument by varying the loss rate over wireless links and adding more sources. 1"
1374,"the folding and evolution of multidomain proteins",1181833,"The folding and evolution of multidomain proteins"," Analyses of genomes show that more than 70% of eukaryotic proteins are composed of multiple domains. However, most studies of protein folding focus on individual domains and do not consider how interactions between domains might affect folding. Here, we address this by analysing the three-dimensional structures of multidomain proteins that have been characterized experimentally and observe that where the interface is small and loosely packed, or unstructured, the folding of the domains is independent. Furthermore, recent studies indicate that multidomain proteins have evolved mechanisms to minimize the problems of interdomain misfolding."
1375,"crispr provides acquired resistance against viruses in prokaryotes",1181873,"CRISPR Provides Acquired Resistance Against Viruses in Prokaryotes","Clustered regularly interspaced short palindromic repeats (CRISPR) are a distinctive feature of the genomes of most Bacteria and Archaea and are thought to be involved in resistance to bacteriophages. We found that, after viral challenge, bacteria integrated new spacers derived from phage genomic sequences. Removal or addition of particular spacers modified the phage-resistance phenotype of the cell. Thus, CRISPR, together with associated cas genes, provided resistance against phages, and resistance specificity is determined by spacer-phage sequence similarity."
1376,"tunability and noise dependence in differentiation dynamics",1188027,"Tunability and noise dependence in differentiation dynamics.","The dynamic process of differentiation depends on the architecture, quantitative parameters, and noise of underlying genetic circuits. However, it remains unclear how these elements combine to control cellular behavior. We analyzed the probabilistic and transient differentiation of Bacillus subtilis cells into the state of competence. A few key parameters independently tuned the frequency of initiation and the duration of competence episodes and allowed the circuit to access different dynamic regimes, including oscillation. Altering circuit architecture showed that the duration of competence events can be made more precise. We used an experimental method to reduce global cellular noise and showed that noise levels are correlated with frequency of differentiation events. Together, the data reveal a noise-dependent circuit that is remarkably resilient and tunable in terms of its dynamic behavior."
1377,"collaborative filtering via gaussian probabilistic latent semantic analysis",1193410,"collaborative filtering via gaussian probabilistic latent semantic analysis","Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, i.e. a database of available user preferences. In this paper, we describe a new model-based algorithm designed for this task, which is based on a generalization of probabilistic latent semantic analysis to continuous-valued response variables. More specifically, we assume that the observed user ratings can be modeled as a mixture of user communities or interest groups, where users may participate probabilistically in one or more groups. Each community is characterized by a Gaussian distribution on the normalized ratings for each item. The normalization of ratings is performed in a user-specific manner to account for variations in absolute shift and variance of ratings. Experiments on the EachMovie data set show that the proposed approach compares favorably with other collaborative filtering techniques."
1378,"ten simple rules for a successful collaboration",1198575,"Ten Simple Rules for a Successful Collaboration","Excerpt: Scientific research has always been a collaborative undertaking, and this is particularly true today. For example, between 1981 and 2001, the average number of coauthors on a paper for the Proceedings of the National Academy of Sciences U S A rose from 3.9 to 8.4 [1]. Why the increase? Biology has always been considered the study of living systems; many of us now think of it as the study of complex systems. Understanding this complexity requires experts in many different domains. In short, these days success in being a biologist depends more on one's ability to collaborate than ever before. The Medical Research Centers in the United Kingdom figured this out long ago, and the new Janelia Farm research campus of the Howard Hughes Medical Institute in the United States has got the idea, as it strongly promotes intra- and inter-institutional collaborations [2]. Given that collaboration is crucial, how do you go about picking the right collaborators, and how can you best make the collaboration work? Here are ten simple rules based on our experience that we hope will help. Additional suggestions can be found in the references [3,4]. Above all, keep in mind that these rules are for both you and your collaborators. Always remember to treat your collaborators as you would want to be treated yourself—empathy is key"
1379,"the process of emotion inference",1201474,"The Process of Emotion Inference","Three experiments investigated the process of inferring emotions from brief descriptions of typical eliciting situations, using response time methodology. The initial hypothesis was that emotion inferences are mediated by inferred cognitive appraisals of the eliciting event (concerning e.g., its valence or the responsible agent). This hypothesis was contradicted by the finding of Experiment 1 that emotion judgments are typically made faster than appraisal judgments. To explain this finding, it was hypothesized that emotion judgments are based on automatized (proceduralized) appraisal inferences. This hypothesis was tested in Experiments 2 and 3 using a judgment facilitation paradigm. The results supported the proceduralization hypothesis by demonstrating that appraisal judgments are facilitated by prior emotion judgments."
1380,"educational data mining a survey from to",1201711,"Educational data mining: A survey from 1995 to 2005","Currently there is an increasing interest in data mining and educational systems, making educational data mining as a new growing research community. This paper surveys the application of data mining to traditional educational systems, particular web-based courses, well-known learning content management systems, and adaptive and intelligent web-based educational systems. Each of these systems has different data source and objectives for knowledge discovering. After preprocessing the available data in each case, data mining techniques can be applied: statistics and visualization; clustering, classification and outlier detection; association rule mining and pattern mining; and text mining. The success of the plentiful work needs much more specialized work in order for educational data mining to become a mature area."
1381,"timely reminders a case study of temporal guidance in pim and email tools usage",1204337,"Timely reminders: a case study of temporal guidance in PIM and email tools usage","We describe our research in progress that explores the use of personal information management (PIM) tools in time and attempts to establish temporal attributes of information. We report on a short field study undertaken to examine relations between tools and information life-cycle. We propose four information types: prospective, ephemeral, working and retrospective. We outline relationships between PIM tools, email and different types of information. We use this framework to explain problems observed with handling information."
1382,"continuous auctions and insider trading",1204515,"Continuous auctions and insider trading","A dynamic model of insider trading with sequential auctions, structured to resemble a sequential equilibrium, is used to examine the informational content of prices, the liquidity characteristics of a speculative market, and the value of private information to an insider. The model has three kinds of traders: a single risk neutral insider, random noise traders, and competitive risk neutral market makers. The insider makes positive profits by exploiting his monopoly power optimally in a dynamic context, where noise trading provides camouflage which conceals his trading from market makers. As the time interval between auctions goes to zero, a limiting model of continuous trading is obtained. In this equilibrium, prices follow Brownian motion, the depth of the market is constant over time, and all private information is incorporated into prices by the end of trading."
1383,"teaching and learning with the net generation",1205857,"Teaching and learning with the Net Generation","A decade ago, the first wave of the Net Generation began to enter college, forcing educational institutions to deal with a new population of learners with unique characteristics. With the Net Generation representing nearly 7% of the population today (Bartlett 2005) and with nearly 49.5 million students enrolled in schools in 2003 (Enrollment Management Report 2005), responding to the specific needs of this generation of learners is becoming increasingly important. The challenge of evolving pedagogy to meet the needs of Net-savvy students is daunting, but educators are assisted by the fact that this generation values education. These students learn in a different way than their predecessors did, but they do want to learn. In this article we will define the characteristics of Net Geners' learning styles and discuss how educators can make the most of these particular traits."
1384,"application of tetranucleotide frequencies for the assignment of genomic fragments",1205895,"Application of tetranucleotide frequencies for the assignment of genomic fragments","A basic problem of the metagenomic approach in microbial ecology is the assignment of genomic fragments to a certain species or taxonomic group, when suitable marker genes are absent. Currently, the (G +  C)-content together with phylogenetic information and codon adaptation for functional genes is mostly used to assess the relationship of different fragments. These methods, however, can produce ambiguous results. In order to evaluate sequence-based methods for fragment identification, we extensively compared (G + C)-contents and tetranucleotide usage patterns of 9054 fosmid-sized genomic fragments generated in silico from 118 completely sequenced bacterial genomes (40 982 931 fragment pairs were compared in total). The results of this systematic study show that the discriminatory power of correlations of tetranucleotide-derived z-scores is by far superior to that of differences in (G + C)-content and provides reasonable assignment probabilities when applied to metagenome libraries of small diversity. Using six fully sequenced fosmid inserts from a metagenomic analysis of microbial consortia mediating the anaerobic oxidation of methane (AOM), we demonstrate that discrimination based on tetranucleotide-derived z-score correlations was consistent with corresponding data from 16S ribosomal RNA sequence analysis and allowed us to discriminate between fosmid inserts that were indistinguishable with respect to their (G + C)-contents."
1385,"multimodal fast optical interrogation of neural circuitry",1208448,"Multimodal fast optical interrogation of neural circuitry.","Our understanding of the cellular implementation of systems-level neural processes like action, thought and emotion has been limited by the availability of tools to interrogate specific classes of neural cells within intact, living brain tissue. Here we identify and develop an archaeal light-driven chloride pump (NpHR) from Natronomonas pharaonis for temporally precise optical inhibition of neural activity. NpHR allows either knockout of single action potentials, or sustained blockade of spiking. NpHR is compatible with ChR2, the previous optical excitation technology we have described, in that the two opposing probes operate at similar light powers but with well-separated action spectra. NpHR, like ChR2, functions in mammals without exogenous cofactors, and the two probes can be integrated with calcium imaging in mammalian brain tissue for bidirectional optical modulation and readout of neural activity. Likewise, NpHR and ChR2 can be targeted together to Caenorhabditis elegans muscle and cholinergic motor neurons to control locomotion bidirectionally. NpHR and ChR2 form a complete system for multimodal, high-speed, genetically targeted, all-optical interrogation of living neural circuits."
1386,"lifespan regulation by evolutionarily conserved genes essential for viability",1217461,"Lifespan Regulation by Evolutionarily Conserved Genes Essential for Viability."," Evolutionarily conserved mechanisms that control aging are predicted to have pre-reproductive functions in order to be subject to natural selection. Genes that are essential for growth and development are highly conserved in evolution but their role in longevity has not previously been assessed. We screened 2700 genes essential for Caenorhabditis elegans development and identified 64 genes that extend lifespan when inactivated post-developmentally. These candidate lifespan regulators are highly conserved from yeast to humans. Classification of the candidate lifespan regulators into functional groups identified the expected insulin and metabolic pathways but also revealed enrichment for translation-, RNA-, and chromatin-factors. Many of these essential gene inactivations extend lifespan as much as the strongest known regulators of aging. Early gene inactivations of these essential genes caused growth arrest at larval stages, and some of these arrested animals live much longer than wild type adults. daf-16 is required for the enhanced survival of arrested larvae, suggesting that the increased longevity is a physiological response to the essential gene inactivation. These results suggest that insulin signaling pathways play a role in regulation of aging at any stage in life."
1387,"tagassist automatic tag suggestion for blog posts",1217723,"TagAssist: Automatic Tag Suggestion for Blog Posts","In this paper, we describe a system called TagAssist that provides tag suggestions for new blog posts by utilizing existing tagged posts. The system is able to increase the quality of suggested tags by performing lossless compression over existing tag data. In addition, the system employs a set of metrics to evaluate the quality of a potential tag suggestion. Coupled with the ability for users to manually add tags, TagAssist can ease the burden of tagging and increase the utility of retrieval and browsing systems built on top of tagging data."
1388,"binding site graphs a new graph theoretical framework for prediction of transcription factor binding sites",1219929,"Binding Site Graphs: A New Graph Theoretical Framework for Prediction of Transcription Factor Binding Sites","Computational prediction of nucleotide binding specificity for transcription factors remains a fundamental and largely unsolved problem. Determination of binding positions is a prerequisite for research in gene regulation, a major mechanism controlling phenotypic diversity. Furthermore, an accurate determination of binding specificities from high-throughput data sources is necessary to realize the full potential of systems biology. Unfortunately, recently performed independent evaluation showed that more than half the predictions from most widely used algorithms are false. We introduce a graph-theoretical framework to describe local sequence similarity as the pair-wise distances between nucleotides in promoter sequences, and hypothesize that densely connected subgraphs are indicative of transcription factor binding sites. Using a well-established sampling algorithm coupled with simple clustering and scoring schemes, we identify sets of closely related nucleotides and test those for known TF binding activity. Using an independent benchmark, we find our algorithm predicts yeast binding motifs considerably better than currently available techniques and without manual curation. Importantly, we reduce the number of false positive predictions in yeast to less than 30&#37;. We also develop a framework to evaluate the statistical significance of our motif predictions. We show that our approach is robust to the choice of input promoters, and thus can be used in the context of predicting binding positions from noisy experimental data. We apply our method to identify binding sites using data from genome scale ChIP&#8211;chip experiments. Results from these experiments are publicly available at http://cagt10.bu.edu/BSG. The graphical framework developed here may be useful when combining predictions from numerous computational and experimental measures. Finally, we discuss how our algorithm can be used to improve the sensitivity of computational predictions of transcription factor binding specificities."
1389,"bayesian adaptive user profiling with explicit implicit feedback",1222463,"Bayesian adaptive user profiling with explicit & implicit feedback","Research in information retrieval is now moving into a personalized scenario where a retrieval or filtering system maintains a separate user profile for each user. In this framework, information delivered to the user can be automatically personalized and catered to individual user's information needs. However, a practical concern for such a personalized system is the ""cold start problem"": any user new to the system must endure poor initial performance until sufficient feedback from that user is provided.To solve this problem, we use both explicit and implicit feedback to build a user's profile and use Bayesian hierarchical methods to borrow information from existing users. We analyze the usefulness of implicit feedback and the adaptive performance of the model on two data sets gathered from user studies where users' interaction with a document, or implicit feedback , were recorded along with explicit feedback. Our results are two-fold: first, we demonstrate that the Bayesian modeling approach effectively trades off between shared and user-specific information, alleviating poor initial performance for each user. Second, we find that implicit feedback has very limited unstable predictive value by itself and only marginal value when combined with explicit feedback."
1390,"evolutionary and biomedical insights from the rhesus macaque genome",1223530,"Evolutionary and Biomedical Insights from the Rhesus Macaque Genome","The rhesus macaque (Macaca mulatta) is an abundant primate species that diverged from the ancestors of Homo sapiens about 25 million years ago. Because they are genetically and physiologically similar to humans, rhesus monkeys are the most widely used nonhuman primate in basic and applied biomedical research. We determined the genome sequence of an Indian-origin Macaca mulatta female and compared the data with chimpanzees and humans to reveal the structure of ancestral primate genomes and to identify evidence for positive selection and lineage-specific expansions and contractions of gene families. A comparison of sequences from individual animals was used to investigate their underlying genetic diversity. The complete description of the macaque genome blueprint enhances the utility of this animal model for biomedical research and improves our understanding of the basic biology of the species. 10.1126/science.1139247"
1391,"analysis of epistatic interactions and fitness landscapes using a new geometric approach",1226694,"Analysis of epistatic interactions and fitness landscapes using a new geometric approach.","BACKGROUND: Understanding interactions between mutations and how they affect fitness is a central problem in evolutionary biology that bears on such fundamental issues as the structure of fitness landscapes and the evolution of sex. To date, analyses of fitness landscapes have focused either on the overall directional curvature of the fitness landscape or on the distribution of pairwise interactions. In this paper, we propose and employ a new mathematical approach that allows a more complete description of multi-way interactions and provides new insights into the structure of fitness landscapes. RESULTS: We apply the mathematical theory of gene interactions developed by Beerenwinkel et al. to a fitness landscape for Escherichia coli obtained by Elena and Lenski. The genotypes were constructed by introducing nine mutations into a wild-type strain and constructing a restricted set of 27 double mutants. Despite the absence of mutants higher than second order, our analysis of this genotypic space points to previously unappreciated gene interactions, in addition to the standard pairwise epistasis. Our analysis confirms Elena and Lenski's inference that the fitness landscape is complex, so that an overall measure of curvature obscures a diversity of interaction types. We also demonstrate that some mutations contribute disproportionately to this complexity. In particular, some mutations are systematically better than others at mixing with other mutations. We also find a strong correlation between epistasis and the average fitness loss caused by deleterious mutations. In particular, the epistatic deviations from multiplicative expectations tend toward more positive values in the context of more deleterious mutations, emphasizing that pairwise epistasis is a local property of the fitness landscape. Finally, we determine the geometry of the fitness landscape, which reflects many of these biologically interesting features. CONCLUSION: A full description of complex fitness landscapes requires more information than the average curvature or the distribution of independent pairwise interactions. We have proposed a mathematical approach that, in principle, allows a complete description and, in practice, can suggest new insights into the structure of real fitness landscapes. Our analysis emphasizes the value of non-independent genotypes for these inferences."
1392,"the origin of eukaryotes a reappraisal",1229624,"The origin of eukaryotes: a reappraisal.","Ever since the elucidation of the main structural and functional features of eukaryotic cells and subsequent discovery of the endosymbiotic origin of mitochondria and plastids, two opposing hypotheses have been proposed to account for the origin of eukaryotic cells. One hypothesis postulates that the main features of these cells, including their ability to capture food by endocytosis and to digest it intracellularly, were developed first, and later had a key role in the adoption of endosymbionts; the other proposes that the transformation was triggered by an interaction between two typical prokaryotic cells, one of which became the host and the other the endosymbiont. Re-examination of this question in the light of cell-biological and phylogenetic data leads to the conclusion that the first model is more likely to be the correct one."
1393,"understanding link quality in mobile ad hoc networks",1233025,"Understanding link quality in 802.11 mobile ad hoc networks","Mobile ad hoc wireless networks will extend the Internet into new territory, making Web services available ""anytime, anywhere."" This creates new markets in such areas as pervasive computing and traffic management. We show that the communication quality of current 802.11 ad hoc networks is low, and that users can experience strong fluctuations in link quality as a result. They identify key factors that cause these fluctuations and derive implications for application development. In particular, applications must tolerate frequent disconnections, network partitioning, and latency variations that are far more severe than in conventional networks."
1394,"a survey of automated web service composition methods",1233290,"A Survey of Automated Web Service Composition Methods","In today’s Web, Web services are created and updated on the fly. It’s already beyond the human ability to analysis them and generate the composition plan manually. A number of approaches have been proposed to tackle that problem. Most of them are inspired by the researches in cross-enterprise workflow and AI planning. This paper gives an overview of recent research efforts of automatic Web service composition both from the workflow and AI planning research community."
1395,"theory of ground state cooling of a mechanical oscillator using dynamical backaction",1247838,"Theory of Ground State Cooling of a Mechanical Oscillator Using Dynamical Backaction","A quantum theory of cooling of a mechanical oscillator by radiation pressure-induced dynamical backaction is developed, which is analogous to sideband cooling of trapped ions. We find that final occupancies well below unity can be attained when the mechanical oscillation frequency is larger than the optical cavity linewidth. It is shown that the final average occupancy can be retrieved directly from the optical output spectrum."
1396,"evolving virtual creatures and catapults",1259291,"Evolving Virtual Creatures and Catapults","doi: 10.1162/artl.2007.13.2.139 Abstract We present a system that can evolve the morphology and the controller of virtual walking and block-throwing creatures (catapults) using a genetic algorithm. The system is based on Sims' work, implemented as a flexible platform with an off-the-shelf dynamics engine. Experiments aimed at evolving Sims-type walkers resulted in the emergence of various realistic gaits while using fairly simple objective functions. Due to the flexibility of the system, drastically different morphologies and functions evolved with only minor modifications to the system and objective function. For example, various throwing techniques evolved when selecting for catapults that propel a block as far as possible. Among the strategies and morphologies evolved, we find the drop-kick strategy, as well as the systematic invention of the principle behind the wheel, when allowing mutations to the projectile."
1397,"social loafing a metaanalytic review and theoretical integration",1260763,"Social Loafing: A Meta-Analytic Review and Theoretical Integration","Social loafing is the tendency for individuals to expend less effort when working collectively than when working individually. A meta-analysis of 78 studies demonstrates that social loafing is robust and generalizes across tasks and S populations. A large number of variables were found to moderate social loafing. Evaluation potential, expectations of co-worker performance, task meaningfulness, and culture had espeically strong influence. These findings are interpreted in the light of a Collective Effort Model that integrates elements of expectancy-value, social identity, and self-validation theories."
1398,"nonlinear theory for relativistic plasma wakefields in the blowout regime",1261882,"Nonlinear Theory for Relativistic Plasma Wakefields in the Blowout Regime","We present a theory for nonlinear, multidimensional plasma waves with phase velocities near the speed of light. It is appropriate for describing plasma waves excited when all electrons are expelled out from a finite region by either the space charge of a short electron beam or the radiation pressure of a short intense laser. It works very well for the first bucket before phase mixing occurs. We separate the plasma response into a cavity or blowout region void of all electrons and a sheath of electrons just beyond the cavity. This simple model permits the derivation of a single equation for the boundary of the cavity. It works particularly well for narrow electron bunches and for short lasers with spot sizes matched to the radius of the cavity. It is also used to describe the structure of both the accelerating and focusing fields in the wake."
1399,"overview of the havc video coding standard",1269699,"Overview of the H.264/AVC video coding standard","H.264/AVC is newest video coding standard of the ITU-T Video Coding Experts Group and the ISO/IEC Moving Picture Experts Group. The main goals of the H.264/AVC standardization effort have been enhanced compression performance and provision of a ""network-friendly"" video representation addressing ""conversational"" (video telephony) and ""nonconversational"" (storage, broadcast, or streaming) applications. H.264/AVC has achieved a significant improvement in rate-distortion efficiency relative to existing standards. This article provides an overview of the technical features of H.264/AVC, describes profiles and applications for the standard, and outlines the history of the standardization process."
1400,"preprocessing agilent microarray data",1271262,"Pre-processing Agilent microarray data","BACKGROUND:Pre-processing methods for two-sample long oligonucleotide arrays, specifically the Agilent technology, have not been extensively studied. The goal of this study is to quantify some of the sources of error that affect measurement of expression using Agilent arrays and to compare Agilent's Feature Extraction software with pre-processing methods that have become the standard for normalization of cDNA arrays. These include log transformation followed by loess normalization with or without background subtraction and often a between array scale normalization procedure. The larger goal is to define best study design and pre-processing practices for Agilent arrays, and we offer some suggestions.RESULTS:Simple loess normalization without background subtraction produced the lowest variability. However, without background subtraction, fold changes were biased towards zero, particularly at low intensities. ROC analysis of a spike-in experiment showed that differentially expressed genes are most reliably detected when background is not subtracted. Loess normalization and no background subtraction yielded an AUC of 99.7% compared with 88.8% for Agilent processed fold changes. All methods performed well when error was taken into account by t- or z-statistics, AUCs [greater than or equal to] 99.8%. A substantial proportion of genes showed dye effects, 43 (99%CI : 39%, 47%). However, these effects were generally small regardless of the pre-processing method.CONCLUSION:Simple loess normalization without background subtraction resulted in low variance fold changes that more reliably ranked gene expression than the other methods. While t-statistics and other measures that take variation into account, including Agilent's z-statistic, can also be used to reliably select differentially expressed genes, fold changes are a standard measure of differential expression for exploratory work, cross platform comparison, and biological interpretation and can not be entirely replaced. Although dye effects are small for most genes, many array features are affected. Therefore, an experimental design that incorporates dye swaps or a common reference could be valuable."
1401,"towards zoomable multidimensional maps of the cell",1279064,"Towards zoomable multidimensional maps of the cell","The detailed structure of molecular networks, including their dependence on conditions and time, are now routinely assayed by various experimental techniques. Visualization is a vital aid in integrating and interpreting such data. We describe emerging approaches for representing and visualizing systems data and for achieving semantic zooming, or changes in information density concordant with scale. A central challenge is to move beyond the display of a static network to visualizations of networks as a function of time, space and cell state, which capture the adaptability of the cell. We consider approaches for representing the role of protein complexes in the cell cycle, displaying modules of metabolism in a hierarchical format, integrating experimental interaction data with structured vocabularies such as Gene Ontology categories and representing conserved interactions among orthologous groups of genes."
1402,"a familiar facebook profile elements as signals in an online social network",1279899,"A familiar face(book): profile elements as signals in an online social network","Using data from a popular online social network site, this paper explores the relationship between profile structure (namely, which fields are completed) and number of friends, giving designers insight into the importance of the profile and how it works to encourage connections and articulated relationships between users. We describe a theoretical framework that draws on aspects of signaling theory, common ground theory, and transaction costs theory to generate an understanding of why certain profile fields may be more predictive of friendship articulation on the site. Using a dataset consisting of 30,773 Facebook profiles, we determine which profile elements are most likely to predict friendship links and discuss the theoretical and design implications of our findings."
1403,"the language of covalent histone modifications",1281087,"The language of covalent histone modifications","Histone proteins and the nucleosomes they form with DNA are the fundamental building blocks of eukaryotic chromatin. A diverse array of post-translational modifications that often occur on tail domains of these proteins has been well documented. Although the function of these highly conserved modifications has remained elusive, converging biochemical and genetic evidence suggests functions in several chromatin-based processes. We propose that distinct histone modifications, on one or more tails, act sequentially or in combination to form a 'histone code' that is, read by other proteins to bring about distinct downstream events."
1404,"which transposable elements are active in the human genome",1282453,"Which transposable elements are active in the human genome?","Although a large proportion (44%) of the human genome is occupied by transposons and transposon-like repetitive elements, only a small proportion (<0.05%) of these elements remain active today. Recent evidence indicates that approximately 35-40 subfamilies of Alu, L1 and SVA elements (and possibly HERV-K elements) remain actively mobile in the human genome. These active transposons are of great interest because they continue to produce genetic diversity in human populations and also cause human diseases by integrating into genes. In this review, we examine these active human transposons and explore mechanistic factors that influence their mobilization."
1405,"fidelity and yield in a volcano monitoring sensor network",1284347,"Fidelity and Yield in a Volcano Monitoring Sensor Network","We present a science-centric evaluation of a 19-day sensor network deployment at Reventador, an active volcano in Ecuador. Each of the 16 sensors continuously sampled seismic and acoustic data at 100 Hz. Nodes used an event-detection algorithm to trigger on interesting volcanic activity and initiate reliable data transfer to the base station. During the deployment, the network recorded 229 earthquakes, eruptions, and other seismoacoustic events."
1406,"internetscale collection of humanreviewed data",1286356,"Internet-scale collection of human-reviewed data","Enterprise and web data processing and content aggregation systems often require extensive use of human-reviewed data (e.g. for training and monitoring machine learning-based applications). Today these needs are often met by in-house efforts or out-sourced offshore contracting. Emerging applications attempt to provide automated collection of human-reviewed data at Internet-scale. We conduct extensive experiments to study the effectiveness of one such application. We also study the feasibility of using Yahoo! Answers, a general question-answering forum, for human-reviewed data collection."
1407,"a microrna in a multipleturnover rnai enzyme complex",1287783,"A microRNA in a multiple-turnover RNAi enzyme complex.","In animals, the double-stranded RNA-specific endonuclease Dicer produces two classes of functionally distinct, tiny RNAs: microRNAs (miRNAs) and small interfering RNAs (siRNAs). miRNAs regulate mRNA translation, whereas siRNAs direct RNA destruction via the RNA interference (RNAi) pathway. Here we show that, in human cell extracts, the miRNA let-7 naturally enters the RNAi pathway, which suggests that only the degree of complementarity between a miRNA and its RNA target determines its function. Human let-7 is a component of a previously identified, miRNA-containing ribonucleoprotein particle, which we show is an RNAi enzyme complex. Each let-7-containing complex directs multiple rounds of RNA cleavage, which explains the remarkable efficiency of the RNAi pathway in human cells."
1408,"a picture of search",1288127,"A Picture of Search","We survey many of the measures used to describe and evaluate the efficiency and effectiveness of large-scale search services. These measures, herein visualized versus verbalized, reveal a domain rich in complexity and scale. We cover six principle facets of search: the query space, users' query sessions, user behavior, operational requirements, the content space, and user demographics. While this paper focuses on measures, the measurements themselves raise questions and suggest avenues of further investigation."
1409,"scaling up all pairs similarity search",1288910,"Scaling up all pairs similarity search","Given a large collection of sparse vector data in a high dimensional space, we investigate the problem of finding all pairs of vectors whose similarity score (as determined by a function such as cosine distance) is above a given threshold. We propose a simple algorithm based on novel indexing and optimization strategies that solves this problem without relying on approximation methods or extensive parameter tuning. We show the approach efficiently handles a variety of datasets across a wide setting of similarity thresholds, with large speedups over previous state-of-the-art approaches."
1410,"systematic discovery of regulatory motifs in conserved regions of the human genome including thousands of ctcf insulator sites",1289316,"Systematic discovery of regulatory motifs in conserved regions of the human genome, including thousands of CTCF insulator sites.","Conserved noncoding elements (CNEs) constitute the majority of sequences under purifying selection in the human genome, yet their function remains largely unknown. Experimental evidence suggests that many of these elements play regulatory roles, but little is known about regulatory motifs contained within them. Here we describe a systematic approach to discover and characterize regulatory motifs within mammalian CNEs by searching for long motifs (12-22 nt) with significant enrichment in CNEs and studying their biochemical and genomic properties. Our analysis identifies 233 long motifs (LMs), matching a total of approximately 60,000 conserved instances across the human genome. These motifs include 16 previously known regulatory elements, such as the histone 3'-UTR motif and the neuron-restrictive silencer element, as well as striking examples of novel functional elements. The most highly enriched motif (LM1) corresponds to the X-box motif known from yeast and nematode. We show that it is bound by the RFX1 protein and identify thousands of conserved motif instances, suggesting a broad role for the RFX family in gene regulation. A second group of motifs (LM2*) does not match any previously known motif. We demonstrate by biochemical and computational methods that it defines a binding site for the CTCF protein, which is involved in insulator function to limit the spread of gene activation. We identify nearly 15,000 conserved sites that likely serve as insulators, and we show that nearby genes separated by predicted CTCF sites show markedly reduced correlation in gene expression. These sites may thus partition the human genome into domains of expression."
1411,"phoneme recognition using timedelay neural networks",1290889,"Phoneme recognition using time-delay neural networks","In this paper we present a Time-Delay Neural Network (TDNN) approach to phoneme recognition which is characterized by two important properties. 1) Using a 3 layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces. The TDNN learns these decision surfaces automatically using error backpropagation [1]. 2) The time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independent of position in time and hence not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes ``B,'' ``D,'' and ``G'' in varying phonetic contexts was chosen. For comparison, several discrete Hidden Markov Models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5 percent correct while the rate obtained by the best of our HMM's was only 93.7 percent. Closer inspection reveals that the network ``invented'' well-known acoustic-phonetic features (e.g., F2-rise, F2-fall, vowel-onset) as useful abstractions. It also developed alternate internal representations to link different acoustic realizations to the same concept."
1412,"netprobe a fast and scalable system for fraud detection in online auction networks",1291533,"Netprobe: a fast and scalable system for fraud detection in online auction networks","Given a large online network of online auction users and their histories of transactions, how can we spot anomalies and auction fraud? This paper describes the design and implementation of NetProbe, a system that we propose for solving this problem. NetProbe models auction users and transactions as a Markov Random Field tuned to detect the suspicious patterns that fraudsters create, and employs a Belief Propagation mechanism to detect likely fraudsters. Our experiments show that NetProbe is both efficient and effective for fraud detection. We report experiments on synthetic graphs with as many as 7,000 nodes and 30,000 edges, where NetProbe was able to spot fraudulent nodes with over 90% precision and recall, within a matter of seconds. We also report experiments on a real dataset crawled from eBay, with nearly 700,000 transactions between more than 66,000users, where NetProbe was highly effective at unearthing hidden networks of fraudsters, within a realistic response time of about 6 minutes. For scenarios where the underlying data is dynamic in nature, we propose IncrementalNetProbe, which is an approximate, but fast, variant of NetProbe. Our experiments prove that Incremental NetProbe executes nearly doubly fast as compared to NetProbe, while retaining over 99% of its accuracy."
1413,"the development of scientific thinking skills in elementary and middle school",1297459,"The development of scientific thinking skills in elementary and middle school","The goal of this article is to provide an integrative review of research that has been conducted on the development of children's scientific reasoning. Broadly defined, scientific thinking includes the skills involved in inquiry, experimentation, evidence evaluation, and inference that are done in the service of conceptual change or scientific understanding. Therefore, the focus is on the thinking and reasoning skills that support the formation and modification of concepts and theories about the natural and social world. Recent trends include a focus on definitional, methodological and conceptual issues regarding what is normative and authentic in the context of the science lab and the science classroom, an increased focus on metacognitive and metastrategic skills, and explorations of different types of instructional and practice opportunities that are required for the development, consolidation and subsequent transfer of such skills."
1414,"multiscale conditional random fields for image labeling",1300254,"Multiscale conditional random fields for image labeling","We propose an approach to include contextual features for labeling images, in which each pixel is assigned to one of a finite set of labels. The features are incorporated into a probabilistic framework, which combines the outputs of several components. Components differ in the information they encode. Some focus on the image-label mapping, while others focus solely on patterns within the label field. Components also differ in their scale, as some focus on fine-resolution patterns while others on coarser, more global structure. A supervised version of the contrastive divergence algorithm is applied to learn these features from labeled image data. We demonstrate performance on two real-world image databases and compare it to a classifier and a Markov random field."
1415,"origins of major human infectious diseases",1302073,"Origins of major human infectious diseases","Many of the major human infectious diseases, including some now confined to humans and absent from animals, are ‘new’ ones that arose only after the origins of agriculture. Where did they come from? Why are they overwhelmingly of Old World origins? Here we show that answers to these questions are different for tropical and temperate diseases; for instance, in the relative importance of domestic animals and wild primates as sources. We identify five intermediate stages through which a pathogen exclusively infecting animals may become transformed into a pathogen exclusively infecting humans. We propose an initiative to resolve disputed origins of major diseases, and a global early warning system to monitor pathogens infecting individuals exposed to wild animals."
1416,"pseudopipe an automated pseudogene identification pipeline",1302950,"PseudoPipe: an automated pseudogene identification pipeline","Motivation: Mammalian genomes contain many  genomic fossils' i.e. pseudogenes. These are disabled copies of functional genes that have been retained in the genome by gene duplication or retrotransposition events. Pseudogenes are important resources in understanding the evolutionary history of genes and genomes.  Results: We have developed a homology-based computational pipeline ( PseudoPipe') that can search a mammalian genome and identify pseudogene sequences in a comprehensive and consistent manner. The key steps in the pipeline involve using BLAST to rapidly cross-reference potential ""parent"" proteins against the intergenic regions of the genome and then processing the resulting ""raw hits"" -- i.e. eliminating redundant ones, clustering together neighbors, and associating and aligning clusters with a unique parent. Finally, pseudogenes are classified based on a combination of criteria including homology, intron-exon structure, and existence of stop codons and frameshifts.  Availability: The PseudoPipe program is implemented in Python and can be downloaded at http://pseudogene.org/  Contact: Mark.Gerstein@yale.edu or zhaolei.zhang@utoronto.ca 10.1093/bioinformatics/btl116"
1417,"evolutionary theorizing in economics",1304918,"Evolutionary Theorizing in Economics","The article focuses on economic analysis oriented towards understanding the workings of economies. Attempts have been made to promote an evolutionary approach towards economics. Basic questions related to the coordination of economic activity depend upon a central authority guiding and commanding action. The questions concerned with the processes driving economic progress, pulls the theorizing of economics, towards an evolutionary orientation. The question of economic development has waxed and waned in centrality to the discipline, as has the importance of evolutionary theorizing. It became the standard view that microeconomic theory was about equilibrium conditions. Many neoclassical economists seem to hold the view that an evolutionary theory of firm and industry behavior and a neoclassical one really amount to the same thing. The problem of variety is that for the selection process to arrive at a neoclassical destination, the existing firms must represent a wide enough variety of s"
1418,"network motifs theory and experimental approaches",1307464,"Network motifs: theory and experimental approaches","Abstract   Transcription regulation networks control the expression of genes. The transcription networks of well-studied microorganisms appear to be made up of a small set of recurring regulation patterns, called network motifs. The same network motifs have recently been found in diverse organisms from bacteria to humans, suggesting that they serve as basic building blocks of transcription networks. Here I review network motifs and their functions, with an emphasis on experimental studies. Network motifs in other biological networks are also mentioned, including signalling and neuronal networks."
1419,"timing in the absence of clocks encoding time in neural network states",1314047,"Timing in the Absence of Clocks: Encoding Time in Neural Network States","Decisions based on the timing of sensory events are fundamental to sensory processing. However, the mechanisms by which the brain measures time over ranges of milliseconds to seconds remain unclear. The dominant model of temporal processing proposes that an oscillator emits events that are integrated to provide a linear metric of time. We examine an alternate model in which cortical networks are inherently able to tell time as a result of time-dependent changes in network state. Using computer simulations we show that within this framework, there is no linear metric of time, and that a given interval is encoded in the context of preceding events. Human psychophysical studies were used to examine the predictions of the model. Our results provide theoretical and experimental evidence that, for short intervals, there is no linear metric of time, and that time may be encoded in the high-dimensional state of local neural networks."
1420,"dna topoisomerases structure function and mechanism",1314313,"DNA TOPOISOMERASES: Structure, Function, and Mechanism","â–ª Abstractâ€‚ DNA topoisomerases solve the topological problems associated with DNA replication, transcription, recombination, and chromatin remodeling by introducing temporary single- or double-strand breaks in the DNA. In addition, these enzymes fine-tune the steady-state level of DNA supercoiling both to facilitate protein interactions with the DNA and to prevent excessive supercoiling that is deleterious. In recent years, the crystal structures of a number of topoisomerase fragments, representing nearly all the known classes of enzymes, have been solved. These structures provide remarkable insights into the mechanisms of these enzymes and complement previous conclusions based on biochemical analyses. Surprisingly, despite little or no sequence homology, both type IA and type IIA topoisomerases from prokaryotes and the type IIA enzymes from eukaryotes share structural folds that appear to reflect functional motifs within critical regions of the enzymes. The type IB enzymes are structurally distinct from all other known topoisomerases but are similar to a class of enzymes referred to as tyrosine recombinases. The structural themes common to all topoisomerases include hinged clamps that open and close to bind DNA, the presence of DNA binding cavities for temporary storage of DNA segments, and the coupling of protein conformational changes to DNA rotation or DNA movement. For the type II topoisomerases, the binding and hydrolysis of ATP further modulate conformational changes in the enzymes to effect changes in DNA topology."
1421,"coevolution of genomic intron number and splice sites",1318673,"Coevolution of genomic intron number and splice sites","Spliceosomal intron numbers and boundary sequences vary dramatically in eukaryotes. We found a striking correspondence between low intron number and strong sequence conservation of 5' splice sites (5'ss) across eukaryotic genomes. The phylogenetic pattern suggests that ancestral 5'ss were relatively weakly conserved, but that some lineages independently underwent both major intron loss and 5'ss strengthening. It seems that eukaryotic ancestors had relatively large intron numbers and `weak' 5'ss, a pattern associated with frequent alternative splicing in modern organisms."
1422,"adaptive intelligent presentation of information for the museum visitor in peach",1324984,"Adaptive, intelligent presentation of information for the museum visitor in PEACH","The study of intelligent user interfaces and user modeling and adaptation is well suited for augmenting educational visits to museums. We have defined a novel integrated framework for museum visits and claim that such a framework is essential in such a vast domain that inherently implies complex interactivity. We found that it requires a significant investment in software and hardware infrastructure, design and implementation of intelligent interfaces, and a systematic and iterative evaluation of the design and functionality of user interfaces, involving actual visitors at every stage. We defined and built a suite of interactive and user-adaptive technologies for museum visitors, which was then evaluated at the Buonconsiglio Castle in Trento, Italy: (1) animated agents that help motivate visitors and focus their attention when necessary, (2) automatically generated, adaptive video documentaries on mobile devices, and (3) automatically generated post-visit summaries that reflect the individual interests of visitors as determined by their behavior and choices during their visit. These components are supported by underlying user modeling and inference mechanisms that allow for adaptivity and personalization. Novel software infrastructure allows for agent connectivity and fusion of multiple positioning data streams in the museum space. We conducted several experiments, focusing on various aspects of PEACH. In one, conducted with 110 visitors, we found evidence that even older users are comfortable interacting with a major component of the system."
1423,"bridging from molecular simulation to biochemical networks",1324988,"Bridging from molecular simulation to biochemical networks","How can we make the connection between the three-dimensional structures of individual proteins and understanding how complex biological systems involving many proteins work? The modelling and simulation of protein structures can help to answer this question for systems ranging from multimacromolecular complexes to organelles and cells. On one hand, multiscale modelling and simulation techniques are advancing to permit the spatial and temporal properties of large systems to be simulated using atomic-detail structures. On the other hand, the estimation of kinetic parameters for the mathematical modelling of biochemical pathways using protein structure information provides a basis for iterative manipulation of biochemical pathways guided by protein structure. Recent advances include the structural modelling of protein complexes on the genomic level, novel coarse-graining strategies to increase the size of the system and the time span that can be simulated, and comparative molecular field analyses to estimate enzyme kinetic parameters."
1424,"new learning design in distance education the impact on student perception and motivation",1325178,"New learning design in distance education: The impact on student perception and motivation","Many forms of e-learning (such as online courses with authentic tasks and computer-supported collaborative learning) have become important in distance education. Very often, such e-learning courses or tasks are set up following constructivist design principles. Often, this leads to learning environments with authentic problems in ill-structured tasks that are supposed to motivate students. However, constructivist design principles are difficult to implement because developers must be able to predict how students perceive the tasks and whether the tasks motivate the students. The research in this article queries some of the assumed effects. It presents a study that provides increased insight into the actual perception of electronic authentic learning tasks. The main questions are how students learn in such e-learning environments with &ldquo;virtual&rdquo; reality and authentic problems and how they perceive them. To answer these questions, in two e-learning programs developed at the Open University of the Netherlands (OUNL) designers&rsquo; expectations were contrasted with student perceptions. The results show a gap between the two, for students experience much less authenticity than developers assume."
1425,"using encyclopedic knowledge for named entity disambiguation",1326435,"Using Encyclopedic Knowledge for Named Entity Disambiguation","contexts below are part of web documents refer- ring to different people who share the same name We present a new method for detecting and John Williams: disambiguating named entities in open do- main text. A disambiguation SVM kernel is trained to exploit the high coverage and rich structure of the knowledge encoded in an online encyclopedia. The resulting model significantly outperforms a less in- formed baseline."
1426,"a simple model of herd behavior",1341447,"A Simple Model of Herd Behavior","Experimental Evidence suggests that spectral techniques are valuable for a wide range of applications. A partial list of such applications includes (i) semantic analysis of documents used to cluster documents into areas of interest, (ii) collaborative filtering - the reconstruction of missing data items, and (iii) determining the relative importance of documents based on citation/link structure. Intuitive arguments can explain some of the phenomena that have been observed, but little theoretical study has been done. In this paper, we present a model for framing data mining tasks and a unified approach to solving the resulting data mining problems using spectral analysis. These results give strong justification to the use of spectral techniques for latent semantic indexing, collaborative filtering, and web site ranking."
1427,"face recognition based on fitting a d morphable model",1352715,"Face Recognition Based on Fitting a 3D Morphable Model","This paper presents a method for face recognition across variations in pose, ranging from frontal to profile views, and across a wide range of illuminations, including cast shadows and specular reflections. To account for these variations, the algorithm simulates the process of image formation in 3D space, using computer graphics, and it estimates 3D shape and texture of faces from single images. The estimate is achieved by fitting a statistical, morphable model of 3D faces to images. The model is learned from a set of textured 3D scans of heads. We describe the construction of the morphable model, an algorithm to fit the model to images, and a framework for face identification. In this framework, faces are represented by model parameters for 3D shape and texture. We present results obtained with 4,488 images from the publicly available CMU-PIE database and 1,940 images from the FERET database."
1428,"a note on platts probabilistic outputs for support vector machines",1355785,"A Note on Platt's Probabilistic Outputs for Support Vector Machines","Abstract&nbsp;&nbsp; Platt’s probabilistic outputs for Support Vector Machines (Platt, J. in Smola, A., et al. (eds.) Advances in large margin classifiers. Cambridge, 2000) has been popular for applications that require posterior class probabilities. In this note, we propose an improved algorithm that theoretically converges and avoids numerical difficulties. A simple and ready-to-use pseudo code is included."
1429,"multiobjective optimization in bioinformatics and computational biology",1356652,"Multiobjective optimization in bioinformatics and computational biology.","This paper reviews the application of multiobjective optimization in the fields of bioinformatics and computational biology. A survey of existing work, organized by application area, forms the main body of the review, following an introduction to the key concepts in multiobjective optimization. An original contribution of the review is the identification of five distinct ""contexts,"" giving rise to multiple objectives: These are used to explain the reasons behind the use of multiobjective optimization in each application area and also to point the way to potential future uses of the technique."
1430,"the neural basis of inhibition in cognitive control",1366162,"The Neural Basis of Inhibition in Cognitive Control","The concept of ""inhibition"" is widely used in synaptic, circuit, and systems neuroscience, where it has a clear meaning because it is clearly observable. The concept is also ubiquitous in psychology. One common use is to connote an active/willed process underlying cognitive control. Many authors claim that subjects execute cognitive control over unwanted stimuli, task sets, responses, memories, and emotions by inhibiting them, and that frontal lobe damage induces distractibility, impulsivity, and perseveration because of damage to an inhibitory mechanism. However, with the exception of the motor domain, the notion of an active inhibitory process underlying cognitive control has been heavily challenged. Alternative explanations have been provided that explain cognitive control without recourse to inhibition as concept, mechanism, or theory. This article examines the role that neuroscience can play when examining whether the psychological concept of active inhibition can be meaningfully applied in cognitive control research. NEUROSCIENTIST 13(3):214--228, 2007. 10.1177/1073858407299288"
1431,"rio analyzing proteomes by automated phylogenomics using resampled inference of orthologs",1373854,"RIO: analyzing proteomes by automated phylogenomics using resampled inference of orthologs.","BACKGROUND: When analyzing protein sequences using sequence similarity searches, orthologous sequences (that diverged by speciation) are more reliable predictors of a new protein's function than paralogous sequences (that diverged by gene duplication). The utility of phylogenetic information in high-throughput genome annotation (""phylogenomics"") is widely recognized, but existing approaches are either manual or not explicitly based on phylogenetic trees. RESULTS: Here we present RIO (Resampled Inference of Orthologs), a procedure for automated phylogenomics using explicit phylogenetic inference. RIO analyses are performed over bootstrap resampled phylogenetic trees to estimate the reliability of orthology assignments. We also introduce supplementary concepts that are helpful for functional inference. RIO has been implemented as Perl pipeline connecting several C and Java programs. It is available at http://www.genetics.wustl.edu/eddy/forester/. A web server is at http://www.rio.wustl.edu/. RIO was tested on the Arabidopsis thaliana and Caenorhabditis elegans proteomes. CONCLUSION: The RIO procedure is particularly useful for the automated detection of first representatives of novel protein subfamilies. We also describe how some orthologies can be misleading for functional inference."
1432,"application of the multimolecule and multiconformational resp methodology to biopolymers charge derivation for dna rna and proteins",1387480,"Application of the Multimolecule and Multiconformational RESP Methodology to Biopolymers: Charge Derivation for DNA, RNA, and Proteins","We present the derivation of charges of ribo- and deoxynucleosides, nucleotides, and peptide fragments using electrostatic potentials obtained from ab initio calculations with the 6-31G* basis set. For the nucleic acid fragments, we used electrostatic potentials of the four deoxyribonucleosides (A, G, C, T) and four ribonucleosides (A, G, C, U) and dimethylphosphate. The charges for the deoxyribose nucleosides and nucleotides are derived using multiple-molecule fitting and restrained electrostatic potential (RESP) fits,1,2 with Lagrangian multipliers ensuring a net charge of 0 or ± 1. We suggest that the preferred approach for deriving charges for nucleosides and nucleotides involves allowing only C1? and H1? of the sugar to vary as the nucleic acid base, with the remainder of sugar and backbone atoms forced to be equivalent. For peptide fragments, we have combined multiple conformation fitting, previously employed by Williams3 and Reynolds et al.,4 with the RESP approach1,2 to derive charges for blocked dipeptides appropriate for each of the 20 naturally occuring amino acids. Based on our results for propyl amine,1,2 we suggest that two conformations for each peptide suffice to give charges that represent well the conformationally dependent electrostatic properties of molecules, provided that these two conformations contain different values of the dihedral angles that terminate in heteroatoms or hydrogens attached to heteroatoms. In these blocked dipeptide models, it is useful to require equivalent N - H and C&dbond;O charges for all amino acids with a given net charge (except proline), and this is accomplished in a straightforward fashion with multiple-molecule fitting. Finally, the application of multiple Lagrangian constraints allows for the derivation of monomeric residues with the appropriate net charge from a chemically blocked version of the residue. The multiple Lagrange constraints also enable charges from two or more molecules to be spliced together in a well-defined fashion. Thus, the combined use of multiple molecules, multiple conformations, multiple Lagrangian constraints, and RESP fitting is shown to be a powerful approach to deriving electrostatic charges for biopolymers. {\\\\copyright} 1995 John Wiley & Sons, Inc."
1433,"a taskbased ontology approach to automate geospatial data retrieval",1389669,"A Task-Based Ontology Approach to Automate Geospatial Data Retrieval","Abstract This paper presents a task-based and Semantic Web approach to find geospatial data. The purpose of the project is to improve data discovery and facilitate automatic retrieval of data sources. The work presented here helps create the beginnings of a Geospatial Semantic Web. The intent is to create a system that provides appropriate results to application users who search for data when facing tasks such as emergency response or planning activities. In our task-based system, we formalize the relationships between types of tasks, including emergency response, and types of data sources needed for those tasks. Domain knowledge, including criteria describing data sources, is recorded in an ontology language. With the ontology, reasoning can be done to infer various types of information including which data sources meet specific criteria for use in particular tasks. The vision presented here is that in an emergency, for example, a user accesses a Web-based application and selects the type of emergency and the geographic area. The application then returns the types and locations (URLs) of the specific geospatial data needed. We explore the abilities and limitations of the OWL Web Ontology Language along with other Semantic Web technologies for this purpose."
1434,"statistical analysis of the genomic distribution and correlation of regulatory elements in the encode regions",1390187,"Statistical analysis of the genomic distribution and correlation of regulatory elements in the ENCODE regions","The comprehensive inventory of functional elements in 44 human genomic regions carried out by the ENCODE Project Consortium enables for the first time a global analysis of the genomic distribution of transcriptional regulatory elements. In this study we developed an intuitive and yet powerful approach to analyze the distribution of regulatory elements found in many different ChIP-chip experiments on a 10[~]100-kb scale. First, we focus on the overall chromosomal distribution of regulatory elements in the ENCODE regions and show that it is highly nonuniform. We demonstrate, in fact, that regulatory elements are associated with the location of known genes. Further examination on a local, single-gene scale shows an enrichment of regulatory elements near both transcription start and end sites. Our results indicate that overall these elements are clustered into regulatory rich ""islands"" and poor ""deserts."" Next, we examine how consistent the nonuniform distribution is between different transcription factors. We perform on all the factors a multivariate analysis in the framework of a biplot, which enhances biological signals in the experiments. This groups transcription factors into sequence-specific and sequence-nonspecific clusters. Moreover, with experimental variation carefully controlled, detailed correlations show that the distribution of sites was generally reproducible for a specific factor between different laboratories and microarray platforms. Data sets associated with histone modifications have particularly strong correlations. Finally, we show how the correlations between factors change when only regulatory elements far from the transcription start sites are considered. 10.1101/gr.5573107"
1435,"calculating scattering amplitudes efficiently",1395097,"Calculating scattering amplitudes efficiently","We review techniques for more efficient computation of perturbative scattering amplitudes in gauge theory, in particular tree and one-loop multi-parton amplitudes in QCD. We emphasize the advantages of (1) using color and helicity information to decompose amplitudes into smaller gauge-invariant pieces, and (2) exploiting the analytic properties of these pieces, namely their cuts and poles. Other useful tools include recursion relations, special gauges and supersymmetric rearrangements."
1436,"problems in systems neuroscience computational neuroscience series",1396810,"23 Problems in Systems Neuroscience (Computational Neuroscience Series)","The complexity of the brain and the protean nature of behavior remain the most elusive area of science, but also the most important. van Hemmen and Sejnowski invited 23 experts from the many areas--from evolution to qualia--of systems neuroscience to formulate one problem each. Although each chapter was written independently and can be read separately, together they provide a useful roadmap to the field of systems neuroscience and will serve as a source of inspirations for future explorers of the brain."
1437,"peertopeer support for massively multiplayer games",1401901,"Peer-to-peer support for massively multiplayer games","We present an approach to support massively multi-player games on peer-to-peer overlays. Our approach exploits the fact that players in MMGs display locality of interest, and therefore can form self-organizing groups based on their locations in the virtual world. To this end, we have designed scalable mechanisms to distribute the game state to the participating players and to maintain consistency in the face of node failures. The resulting system dynamically scales with the number of online players. It is more flexible and has a lower deployment cost than centralized games servers. We have implemented a simple game we call SimMud, and experimented with up to 4000 players to demonstrate the applicability of this approach."
1438,"pathologies of rational choice theory a critique of applications in political science",1405800,"Pathologies of Rational Choice Theory: A Critique of Applications in Political Science","This is the first comprehensive critical evaluation of the use of rational choice explanations in political science. Writing in an accessible and nontechnical style, Donald P. Green and Ian Shapiro assess rational choice theory where it is reputed to be most successful: the study of collective action, the behavior of political parties and politicians, and such phenomena as voting cycles and Prisoner's Dilemmas. In their hard-hitting critique, Green and Shapiro demonstrate that the much-heralded achievements of rational choice theory are in fact deeply suspect and that fundamental rethinking is needed if rational choice theorists are to contribute to the understanding of politics. Green and Shapiro show that empirical tests of rational choice theories are marred by a series of methodological defects. These defects flow from the characteristic rational choice impulse to defend universal theories of politics. As a result, many tests are so poorly conducted as to be irrelevant to evaluating rational choice models. Tests that are properly conducted either tend to undermine rational choice theories or to lend support for propositions that are banal. Green and Shapiro offer numerous suggestions as to how rational choice propositions might be reformulated as parts of testable hypotheses for the study of politics. In a final chapter they anticipate and respond to a variety of rational choice counterarguments, thereby initiating a dialogue that is bound to continue for some time"
1439,"flux balance analysis in the era of metabolomics",1409286,"Flux balance analysis in the era of metabolomics.","Flux balance analysis (FBA) has emerged as an effective means to analyse biological networks in a quantitative manner. Much progress has been made on the extension of FBA to incorporate a priori biological knowledge, provide more practical descriptions of observed cell behaviours, and predict the outcome of network perturbations. Metabolomics is independently advancing as a set of high-throughput data acquisition tools providing dynamic profiles of metabolites in an unbiased manner. These data sets are neither yet sufficiently comprehensive nor accurate enough for generating large-scale kinetic models. Thus, there is a pressing need to develop quantitative techniques that can make use of the emerging data and embrace the associated uncertainties. This article reviews recent advances in FBA to meet this need and discusses the utility of FBA as a complement to metabolomics and the expected synergy as a result of combining these two techniques."
1440,"discrete hierarchical organization of social group sizes",1413697,"{Discrete hierarchical organization of social group sizes}","The 'social brain hypothesis' for the evolution of large brains in primates has led to evidence for the coevolution of neocortical size and social group sizes, suggesting that there is a cognitive constraint on group size that depends, in some way, on the volume of neural material available for processing and synthesizing information on social relationships. More recently, work on both human and non-human primates has suggested that social groups are often hierarchically structured. We combine data on human grouping patterns in a comprehensive and systematic study. Using fractal analysis, we identify, with high statistical confidence, a discrete hierarchy of group sizes with a preferred scaling ratio close to three: rather than a single or a continuous spectrum of group sizes, humans spontaneously form groups of preferred sizes organized in a geometrical series approximating 3-5, 9-15, 30-45, etc. Such discrete scale invariance could be related to that identified in signatures of herding behaviour in financial markets and might reflect a hierarchical processing of social nearness by human brains. © 2005 The Royal Society."
1441,"protein folding then and now",1424521,"Protein folding: Then and now.","Over the past three decades the protein folding field has undergone monumental changes. Originally a purely academic question, how a protein folds has now become vital in understanding diseases and our abilities to rationally manipulate cellular life by engineering protein folding pathways. We review and contrast past and recent developments in the protein folding field. Specifically, we discuss the progress in our understanding of protein folding thermodynamics and kinetics, the properties of evasive intermediates, and unfolded states. We also discuss how some abnormalities in protein folding lead to protein aggregation and human diseases."
1442,"structurebased activity prediction for an enzyme of unknown function",1427756,"Structure-based activity prediction for an enzyme of unknown function","With many genomes sequenced, a pressing challenge in biology is predicting the function of the proteins that the genes encode. When proteins are unrelated to others of known activity, bioinformatics inference for function becomes problematic. It would thus be useful to interrogate protein structures for function directly. Here, we predict the function of an enzyme of unknown activity, Tm0936 from Thermotoga maritima, by docking high-energy intermediate forms of thousands of candidate metabolites. The docking hit list was dominated by adenine analogues, which appeared to undergo C6-deamination. Four of these, including 5-methylthioadenosine and S-adenosylhomocysteine (SAH), were tested as substrates, and three had substantial catalytic rate constants (105 M-1 s-1). The X-ray crystal structure of the complex between Tm0936 and the product resulting from the deamination of SAH, S-inosylhomocysteine, was determined, and it corresponded closely to the predicted structure. The deaminated products can be further metabolized by T. maritima in a previously uncharacterized SAH degradation pathway. Structure-based docking with high-energy forms of potential substrates may be a useful tool to annotate enzymes for function."
1443,"can internet videoondemand be profitable",1436865,"Can Internet Video-on-Demand be Profitable?","Video-on-demand in the Internet has become an immensely popular service in recent years. But due to its high bandwidth requirements and popularity, it is also a costly service to provide. We consider the design and potential benefits of peer-assisted video-on-demand, in which participating peers assist the server in delivering VoD content. The assistance is done in such a way that it provides the same user quality experience as pure client-server distribution. We focus on the single-video approach, whereby a peer only redistributes a video that it is currently watching."
1444,"microrna targeting specificity in mammals determinants beyond seed pairing",1439241,"MicroRNA Targeting Specificity in Mammals: Determinants beyond Seed Pairing"," Summary Mammalian microRNAs (miRNAs) pair to 3′UTRs of mRNAs to direct their posttranscriptional repression. Important for target recognition are 7 nt sites that match the seed region of the miRNA. However, these seed matches are not always sufficient for repression, indicating that other characteristics help specify targeting. By combining computational and experimental approaches, we uncovered five general features of site context that boost site efficacy: AU-rich nucleotide composition near the site, proximity to sites for coexpressed miRNAs (which leads to cooperative action), proximity to residues pairing to miRNA nucleotides 13–16, positioning within the 3′UTR at least 15 nt from the stop codon, and positioning away from the center of long UTRs. A model combining these context determinants quantitatively predicts site performance both for exogenously added miRNAs and for endogenous miRNA-message interactions. Because it predicts site efficacy without recourse to evolutionary conservation, the model also identifies effective nonconserved sites and siRNA off-targets."
1445,"highspeed imaging reveals neurophysiological links to behavior in an animal model of depression",1439666,"High-speed imaging reveals neurophysiological links to behavior in an animal model of depression.","The hippocampus is one of several brain areas thought to play a central role in affective behaviors, but the underlying local network dynamics are not understood. We used quantitative voltage-sensitive dye imaging to probe hippocampal dynamics with millisecond resolution in brain slices after bidirectional modulation of affective state in rat models of depression. We found that a simple measure of real-time activity-stimulus-evoked percolation of activity through the dentate gyrus relative to the hippocampal output subfield-accounted for induced changes in animal behavior independent of the underlying mechanism of action of the treatments. Our results define a circuit-level neurophysiological endophenotype for affective behavior and suggest an approach to understanding circuit-level substrates underlying psychiatric disease symptoms."
1446,"feedbackdirected random test generation",1446281,"Feedback-directed random test generation","We present a technique that improves random test generation by incorporating feedback obtained from executing test inputs as they are created. Our technique builds inputs incrementally by randomly selecting a method call to apply and finding arguments from among previously-constructed inputs. As soon as an input is built, it is executed and checked against a set of contracts and filters. The result of the execution determines whether the input is redundant, illegal, contract-violating, or useful for generating more inputs. The technique outputs a test suite consisting of unit tests for the classes under test. Passing tests can be used to ensure that code contracts are preserved across program changes; failing tests (that violate one or more contract) point to potential errors that should be corrected. Our experimental results indicate that feedback-directed random test generation can outperform systematic and undirected random test generation, in terms of coverage and error detection. On four small but nontrivial data structures (used previously in the literature), our technique achieves higher or equal block and predicate coverage than model checking (with and without abstraction) and undirected random generation. On 14 large, widely-used libraries (comprising 780KLOC), feedback-directed random test generation finds many previously-unknown errors, not found by either model checking or undirected random generation."
1447,"nanoelectromechanical systems",1464882,"{Nanoelectromechanical Systems}","Nanoelectromechanical systems (NEMS) are drawing interest from both technical and scientific communities. These are electromechanical systems, much like microelectromechanical systems, mostly operated in their resonant modes with dimensions in the deep submicron. In this size regime, they come with extremely high fundamental resonance frequencies, diminished active masses,and tolerable force constants; the quality (Q) factors of resonance are in the range Q ∼ 103–105—significantly higher than those of electrical resonant circuits. These attributes collectively make NEMS suitable for a multitude of technological applications such as ultrafast sensors, actuators, and signal processing components. Experimentally, NEMS are expected to open up investigations of phonon mediated mechanical processes and of the quantum behavior of mesoscopic mechanical systems. However, there still exist fundamental and technological challenges to NEMS optimization. In this review we shall provide a balanced introduction to NEMS by discussing the prospects and challenges in this rapidly developing field and outline an exciting emerging application, nanoelectromechanical mass detection."
1448,"the distribution of fitness effects of new mutations",1465977,"The distribution of fitness effects of new mutations","The distribution of fitness effects (DFE) of new mutations is a fundamental entity in genetics that has implications ranging from the genetic basis of complex disease to the stability of the molecular clock. It has been studied by two different approaches: mutation accumulation and mutagenesis experiments, and the analysis of DNA sequence data. The proportion of mutations that are advantageous, effectively neutral and deleterious varies between species, and the DFE differs between coding and non-coding DNA. Despite these differences between species and genomic regions, some general principles have emerged: advantageous mutations are rare, and those that are strongly selected are exponentially distributed; and the DFE of deleterious mutations is complex and multi-modal."
1449,"principled hybrids of generative and discriminative models",1467510,"Principled Hybrids of Generative and Discriminative Models","When labelled training data is plentiful, discriminative techniques are widely used since they give excellent generalization performance. However, for large-scale applications such as object recognition, hand labelling of data is expensive, and there is much interest in semi-supervised techniques based on generative models in which the majority of the training data is unlabelled. Although the generalization performance of generative models can often be improved by &#145;training them discriminatively&#146;, they can then no longer make use of unlabelled data. In an attempt to gain the benefit of both generative and discriminative approaches, heuristic procedure have been proposed [2, 3] which interpolate between these two extremes by taking a convex combination of the generative and discriminative objective functions. In this paper we adopt a new perspective which says that there is only one correct way to train a given model, and that a &#145;discriminatively trained&#146; generative model is fundamentally a new model [7]. From this viewpoint, generative and discriminative models correspond to specific choices for the prior over parameters. As well as giving a principled interpretation of &#145;discriminative training&#146;, this approach opens door to very general ways of interpolating between generative and discriminative extremes through alternative choices of prior. We illustrate this framework using both synthetic data and a practical example in the domain of multi-class object recognition. Our results show that, when the supply of labelled training data is limited, the optimum performance corresponds to a balance between the purely generative and the purely discriminative."
1450,"pirnasthe ancient hunters of genome invaders",1470445,"piRNAsâthe ancient hunters of genome invaders","In addition to miRNAs and siRNAs, a third small RNA silencing system has been uncovered that prevents the spreading of selfish genetic elements. Production of the Piwi-associated RNAs (piRNAs), which mediate the silencing activity in this pathway, is initiated at a few master control regions within the genome. The nature of the primary piRNA-generating transcript is still unknown, but RNA interference (RNAi)-like cleavage events are likely defining the 5â²-ends of mature piRNAs. We summarize the recent literature on piRNA biogenesis and function with an emphasis on work in Drosophila, where genetics and biochemistry have met very successfully."
1451,"widely distributed noncoding purifying selection in the human genome",1471614,"Widely distributed noncoding purifying selection in the human genome","10.1073/pnas.0705140104 It is widely assumed that human noncoding sequences comprise a substantial reservoir for functional variants impacting gene regulation and other chromosomal processes. Evolutionarily conserved noncoding sequences (CNSs) in the human genome have attracted considerable attention for their potential to simplify the search for functional elements and phenotypically important human alleles. A major outstanding question is whether functionally significant human noncoding variation is concentrated in CNSs or distributed more broadly across the genome. Here, we combine wholegenome sequence data from four nonhuman species (chimp, dog, mouse, and rat) with recently available comprehensive human polymorphism data to analyze selection at single-nucleotide resolution. We show that a substantial fraction of active purifying selection in human noncoding sequences occurs outside of CNSs and is diffusely distributed across the genome. This finding suggests the existence of a large complement of human noncoding variants that may impact gene expression and phenotypic traits, the majority of which will escape detection with current approaches to genome analysis."
1452,"development of an instrument to measure the perceptions of adopting an information technology innovation",1485773,"Development of an instrument to measure the perceptions of adopting an information technology innovation","This paper reports on the development of an instrument designed to measure the various perceptions that an individual may have of adopting an information technology (IT) innovation. This instrument is intended to be a tool for the study of the initial adoption and eventual diffusion of IT innovations within organizations. While the adoption of information technologies by individuals and organizations has been an area of substantial research interest since the early days of computerization, research efforts to date have led to mixed and inconclusive outcomes. The lack of a theoretical foundation for such research and inadequate definition and measurement of constructs have been identified as major causes for such outcomes. In a recent study examining the diffusion of new end-user IT, we decided to focus on measuring the potential adopters' perceptions of the technology. Measuring such perceptions has been termed a ""classic issue"" in the innovation diffusion literature, and a key to integrating the various findings of diffusion research. The perceptions of adopting were initially based on the five characteristics of innovations derived by Rogers (1983) from the diffusion of innovations literature, plus two developed specifically within this study. Of the existing scales for measuring these characteristics, very few had the requisite levels of validity and reliability. For this study, both newly created and existing items were placed in a common pool and subjected to four rounds of sorting by judges to establish which items should be in the various scales. The objective was to verify the convergent and discriminant validity of the scales by examining how the items were sorted into various construct categories. Analysis of interjudge agreement about item placement identified both bad items as well as weaknesses in some of the constructs' original definitions. These were subsequently redefined. Scales for the resulting constructs were subjected to three separate... ABSTRACT FROM AUTHOR Copyright of Information Systems Research is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts)"
1453,"computational genetics physiology metabolism neural systems learning vision and behavior or polyworld life in a new context",1505738,"Computational Genetics, Physiology, Metabolism, Neural Systems, Learning, Vision, and Behavior or PolyWorld: Life in a New Context","This paper discusses a computer model of living organisms and the ecology they exist in called PolyWorld. PolyWorld attempts to bring together all the principle components of real living systems into a single artificial (man-made) living system. PolyWorld brings together biologically motivated genetics, simple simulated physiologies and metabolisms, Hebbian learning in arbitrary neural network architectures, a visual perceptive mechanism, and a suite of primitive behaviors in artificial organisms grounded in an ecology just complex enough to foster speciation and inter-species competition. Predation, mimicry, sexual reproduction, and even communication are all supported in a straightforward fashion. The resulting survival strategies, both individual and group, are purely emergent, as are the functionalities embodied in their neural network &#034;brains&#034;. Complex behaviors resulting from the simulated neural activity are unpredictable, and change as natural selection acts over multiple generations. In many ways, PolyWorld may be thought of as a sort of electronic primordial soup experiment, in the vein of Urey and Miller&#039;s [33] classic experiment, only commencing at a much higher level of organization. While one could claim that Urey and Miller really just threw a bunch of ingredients in a pot and watched to see what happened, the reason these men made a contribution to science rather than ratatouille is that they put the right ingredients in the right pot ... and watched to see what happened. Here we start with software-coded genetics and various simple nerve cells (lightsensitive, motor, and unspecified neuronal) as the ingredients, and place them in a competitive ecological crucible which subjects them to an internally consistent physics and the process of natural selectio..."
1454,"alibaba pubmed as a graph",1506799,"AliBaba: PubMed as a graph.","The biomedical literature contains a wealth of information on associations between many different types of objects, such as protein-protein interactions, gene-disease associations and subcellular locations of proteins. When searching such information using conventional search engines, e.g. PubMed, users see the data only one-abstract at a time and 'hidden' in natural language text. AliBaba is an interactive tool for graphical summarization of search results. It parses the set of abstracts that fit a PubMed query and presents extracted information on biomedical objects and their relationships as a graphical network. AliBaba extracts associations between cells, diseases, drugs, proteins, species and tissues. Several filter options allow for a more focused search. Thus, researchers can grasp complex networks described in various articles at a glance. AVAILABILITY: http://alibaba.informatik.hu-berlin.de/"
1455,"evolution of chromosome organization driven by selection for reduced gene expression noise",1507853,"Evolution of chromosome organization driven by selection for reduced gene expression noise.","The distribution of genes on eukaryotic chromosomes is nonrandom, but the reasons behind this are not well understood. The commonly observed clustering of essential genes is a case in point. Here we model and test a new hypothesis. Essential proteins are unusual in that random fluctuations in abundance (noise) can be highly deleterious. We hypothesize that persistently open chromatin domains are sinks for essential genes, as they enable reduced noise by avoidance of transcriptional bursting associated with chromatin remodeling. Simulation of the model captures clustering and correctly predicts that (i) essential gene clusters are associated with low nucleosome occupancy (ii) noise-sensitive nonessential genes cluster with essential genes (iii) nonessential genes of similar knockout fitness are physically linked (iv) genes in domains rich in essential genes have low noise (v) essential genes are rare subtelomerically and (vi) essential gene clusters are preferentially conserved. We conclude that different noise characteristics of different genomic domains favors nonrandom gene positioning. This has implications for gene therapy and understanding transgenic phenotypes."
1456,"towards automatic extraction of event and place semantics from flickr tags",1510686,"Towards automatic extraction of event and place semantics from flickr tags","We describe an approach for extracting semantics of tags, unstructured text-labels assigned to resources on the Web, based on each tag’s usage patterns. In particular, we focus on the problem of extracting place and event semantics for tags that are assigned to photos on Flickr, a popular photo sharing website that supports time and location (latitude/longitude) metadata. We analyze two methods inspired by well-known burst-analysis techniques and one novel method: Scale-structure Identification. We evaluate the methods on a subset of Flickr data, and show that our Scale-structure Identification method outperforms the existing techniques. The approach and methods described in this work can be used in other domains such as geo-annotated web pages, where text terms can be extracted and associated with usage patterns."
1457,"stochastic simulation of chemical kinetics",1512273,"Stochastic simulation of chemical kinetics.","Abstract Stochastic chemical kinetics describes the time evolution of a well-stirred chemically reacting system in a way that takes into account the fact that molecules come in whole numbers and exhibit some degree of randomness in their dynamical behavior. Researchers are increasingly using this approach to chemical kinetics in the analysis of cellular systems in biology, where the small molecular populations of only a few reactant species can lead to deviations from the predictions of the deterministic differential equations of classical chemical kinetics. After reviewing the supporting theory of stochastic chemical kinetics, I discuss some recent advances in methods for using that theory to make numerical simulations. These include improvements to the exact stochastic simulation algorithm (SSA) and the approximate explicit tau-leaping procedure, as well as the development of two approximate strategies for simulating systems that are dynamically stiff: implicit tau-leaping and the slow-scale SSA."
1458,"cacore version implementation of a model driven serviceoriented architecture for semantic interoperability",1514425,"caCORE version 3: Implementation of a model driven, service-oriented architecture for semantic interoperability","One of the requirements for a federated information system is interoperability, the ability of one computer system to access and use the resources of another system. This feature is particularly important in biomedical research systems, which need to coordinate a variety of disparate types of data. In order to meet this need, the National Cancer Institute Center for Bioinformatics {(NCICB)} has created the cancer Common Ontologic Representation Environment {(caCORE),} an interoperability infrastructure based on Model Driven Architecture. The {caCORE} infrastructure provides a mechanism to create interoperable biomedical information systems. Systems built using the {caCORE} paradigm address both aspects of interoperability: the ability to access data (syntactic interoperability) and understand the data once retrieved (semantic interoperability). This infrastructure consists of an integrated set of three major components: a controlled terminology service {(Enterprise} Vocabulary Services), a standards-based metadata repository (the cancer Data Standards Repository) and an information system with an Application Programming Interface {(API)} based on Domain Model Driven Architecture. This infrastructure is being leveraged to create a Semantic {Service-Oriented} Architecture {(SSOA)} for cancer research by the National Cancer Institute's cancer Biomedical Informatics Grid {(caBIG).}"
1459,"a genomescale metabolic reconstruction for escherichia coli k mg that accounts for orfs and thermodynamic information",1524073,"A genome-scale metabolic reconstruction for Escherichia coli K-12 MG1655 that accounts for 1260 ORFs and thermodynamic information.","An updated genome-scale reconstruction of the metabolic network in Escherichia coli K-12 MG1655 is presented. This updated metabolic reconstruction includes: (1) an alignment with the latest genome annotation and the metabolic content of EcoCyc leading to the inclusion of the activities of 1260 ORFs, (2) characterization and quantification of the biomass components and maintenance requirements associated with growth of E. coli and (3) thermodynamic information for the included chemical reactions. The conversion of this metabolic network reconstruction into an in silico model is detailed. A new step in the metabolic reconstruction process, termed thermodynamic consistency analysis, is introduced, in which reactions were checked for consistency with thermodynamic reversibility estimates. Applications demonstrating the capabilities of the genome-scale metabolic model to predict high-throughput experimental growth and gene deletion phenotypic screens are presented. The increased scope and computational capability using this new reconstruction is expected to broaden the spectrum of both basic biology and applied systems biology studies of E. coli metabolism."
1460,"a synthetic gene network for tuning protein degradation in saccharomyces cerevisiae",1526396,"A synthetic gene network for tuning protein degradation in Saccharomyces cerevisiae","Protein decay rates are regulated by degradation machinery that clears unnecessary housekeeping proteins and maintains appropriate dynamic resolution for transcriptional regulators. Turnover rates are also crucial for fluorescence reporters that must strike a balance between sufficient fluorescence for signal detection and temporal resolution for tracking dynamic responses. Here, we use components of the Escherichia coli degradation machinery to construct a Saccharomyces cerevisiae strain that allows for tunable degradation of a tagged protein. Using a microfluidic platform tailored for single-cell fluorescence measurements, we monitor protein decay rates after repression using an ssrA-tagged fluorescent reporter. We observe a half-life ranging from 91 to 22 min, depending on the level of activation of the degradation genes. Computational modeling of the underlying set of enzymatic reactions leads to GFP decay curves that are in excellent agreement with the observations, implying that degradation is governed by Michaelis?Menten-type interactions. In addition to providing a reporter with tunable dynamic resolution, our findings set the stage for explorations of the effect of protein degradation on gene regulatory and signalling pathways.Keywords: gene regulation, temporal dynamics"
1461,"swoosh a generic approach to entity resolution",1528517,"Swoosh: A Generic Approach to Entity Resolution","We consider the entity resolution (ER) problem (also known as deduplication, or merge---purge), in which records determined to represent the same real-world entity are successively located and merged. We formalize the generic ER problem, treating the functions for comparing and merging records as black-boxes, which permits expressive and extensible ER solutions. We identify four important properties that, if satisfied by the match and merge functions, enable much more efficient ER algorithms. We develop three efficient ER algorithms: G-Swoosh for the case where the four properties do not hold, and R-Swoosh and F-Swoosh that exploit the four properties. F-Swoosh in addition assumes knowledge of the ""features"" (e.g., attributes) used by the match function. We experimentally evaluate the algorithms using comparison shopping data from Yahoo! Shopping and hotel information data from Yahoo! Travel. We also show that R-Swoosh (and F-Swoosh) can be used even when the four match and merge properties do not hold, if an ""approximate"" result is acceptable."
1462,"development of the human infant intestinal microbiota",1528747,"Development of the Human Infant Intestinal Microbiota","Almost immediately after a human being is born, so too is a new microbial ecosystem, one that resides in that person&#39;s gastrointestinal tract. Although it is a universal and integral part of human biology, the temporal progression of this process, the sources of the microbes that make up the ecosystem, how and why it varies from one infant to another, and how the composition of this ecosystem influences human physiology, development, and disease are still poorly understood. As a step toward systematically investigating these questions, we designed a microarray to detect and quantitate the small subunit ribosomal RNA (SSU rRNA) gene sequences of most currently recognized species and taxonomic groups of bacteria. We used this microarray, along with sequencing of cloned libraries of PCR-amplified SSU rDNA, to profile the microbial communities in an average of 26 stool samples each from 14 healthy, full-term human infants, including a pair of dizygotic twins, beginning with the first stool after birth and continuing at defined intervals throughout the first year of life. To investigate possible origins of the infant microbiota, we also profiled vaginal and milk samples from most of the mothers, and stool samples from all of the mothers, most of the fathers, and two siblings. The composition and temporal patterns of the microbial communities varied widely from baby to baby. Despite considerable temporal variation, the distinct features of each baby&#39;s microbial community were recognizable for intervals of weeks to months. The strikingly parallel temporal patterns of the twins suggested that incidental environmental exposures play a major role in determining the distinctive characteristics of the microbial community in each baby. By the end of the first year of life, the idiosyncratic microbial ecosystems in each baby, although still distinct, had converged toward a profile characteristic of the adult gastrointestinal tract."
1463,"lowcost multitouch sensing through frustrated total internal reflection",1530733,"Low-cost multi-touch sensing through frustrated total internal reflection","This paper describes a simple, inexpensive, and scalable technique for enabling high-resolution multi-touch sensing on rear-projected interactive surfaces based on frustrated total internal reflection. We review previous applications of this phenomenon to sensing, provide implementation details, discuss results from our initial prototype, and outline future directions. Copyright 2005 ACM."
1464,"social software im unternehmen wikis und weblogs fr wissensmanagement und kommunikation",1539479,"Social Software im Unternehmen. Wikis und Weblogs für Wissensmanagement und Kommunikation","""In den letzten Jahren hat sich im Internet eine neue Art von Software, sogenannte Social Software, zu der auch Wikis und Weblogs gehören, entwickelt. Gleichzeitig besteht bei vielen Unternehmen der Bedarf, das im Unternehmen vorhandene Wissen besser zu nutzen und die Kommunikation effizienter zu gestalten. Das Buch führt in die Thematiken Wissensmanagement und Unternehmenskommunikation ein, um anschlie{\\ss}end detailliert Social Software und deren Ausprägungen zu erörtern. Darauf folgend werden konkrete Einsatzmöglichkeiten insbesondere von Wikis und Weblogs in einem unternehmerischen Umfeld dargestellt. Die Nutzung und die entstehenden Lösungen werden dabei nicht als rein Technologie getrieben angesehen; daher werden für die Umsetzung relevante Aspekte berücksichtigt wie Erfolgsfaktoren im Wissensmanagement, die für einen Erfolg entscheidende Mitarbeitermotivation, Best Practices, Usability-Tests sowie Wiki- und Blog-Policies. Abschlie{\\ss}end wird ein umfassendes Rahmenwerk vorgestellt, das alle wichtigen Tätigkeiten, die für eine Einführung von Social Software-Anwendungen im Unternehmen notwendig sind, zusammenfassend darstellt. Das Buch richtet sich an Studenten, Wissenschaftler, Führungskräfte, Manager sowie alle am Thema Interessierten."""
1465,"etblast a web server to identify expert reviewers appropriate journals and similar publications",1542309,"eTBLAST: a web server to identify expert reviewers, appropriate journals and similar publications.","Authors, editors and reviewers alike use the biomedical literature to identify appropriate journals in which to publish, potential reviewers for papers or grants, and collaborators (or competitors) with similar interests. Traditionally, this process has either relied upon personal expertise and knowledge or upon a somewhat unsystematic and laborious process of manually searching through the literature for trends. To help with these tasks, we report three utilities that parse and summarize the results of an abstract similarity search to find appropriate journals for publication, authors with expertise in a given field, and documents similar to a submitted query. The utilities are based upon a program, eTBLAST, designed to identify similar documents within literature databases such as (but not limited to) MEDLINE. These services are freely accessible through the Internet at http://invention.swmed.edu/etblast/etblast.shtml, where users can upload a file or paste text such as an abstract into the browser interface."
1466,"three methods for optimization of crosslaboratory and crossplatform microarray expression data",1544270,"Three methods for optimization of cross-laboratory and cross-platform microarray expression data","Microarray gene expression data becomes more valuable as our confidence in the results grows. Guaranteeing data quality becomes increasingly important as microarrays are being used to diagnose and treat patients (1-4). The MAQC Quality Control Consortium, the FDA's Critical Path Initiative, NCI's caBIG and others are implementing procedures that will broadly enhance data quality. As GEO continues to grow, its usefulness is constrained by the level of correlation across experiments and general applicability. Although RNA preparation and array platform play important roles in data accuracy, pre-processing is a user-selected factor that has an enormous effect. Normalization of expression data is necessary, but the methods have specific and pronounced effects on precision, accuracy and historical correlation. As a case study, we present a microarray calibration process using normalization as the adjustable parameter. We examine the impact of eight normalizations across both Agilent and Affymetrix expression platforms on three expression readouts: (1) sensitivity and power, (2) functional/biological interpretation and (3) feature selection and classification error. The reader is encouraged to measure their own discordant data, whether cross-laboratory, cross-platform or across any other variance source, and to use their results to tune the adjustable parameters of their laboratory to ensure increased correlation."
1467,"subspace secure crossdomain communication for web mashups",1550730,"Subspace: secure cross-domain communication for web mashups","Combining data and code from third-party sources has enabled a new wave of web mashups that add creativity and functionality to web applications. However, browsers are poorly designed to pass data between domains, often forcing web developers to abandon security in the name of functionality. To address this deficiency, we developed Subspace, a cross-domain communication mechanism that allows efficient communication across domains without sacrificing security. Our prototype requires only a small JavaScript library, and works across all major browsers. We believe Subspace can serve as a new secure communication primitive for web mashups."
1468,"improving life sciences information retrieval using semantic web technology",1551015,"Improving life sciences information retrieval using semantic web technology.","The ability to retrieve relevant information is at the heart of every aspect of research and development in the life sciences industry. Information is often distributed across multiple systems and recorded in a way that makes it difficult to piece together the complete picture. Differences in data formats, naming schemes and network protocols amongst information sources, both public and private, must be overcome, and user interfaces not only need to be able to tap into these diverse information sources but must also assist users in filtering out extraneous information and highlighting the key relationships hidden within an aggregated set of information. The Semantic Web community has made great strides in proposing solutions to these problems, and many efforts are underway to apply Semantic Web techniques to the problem of information retrieval in the life sciences space. This article gives an overview of the principles underlying a Semantic Web-enabled information retrieval system: creating a unified abstraction for knowledge using the RDF semantic network model; designing semantic lenses that extract contextually relevant subsets of information; and assembling semantic lenses into powerful information displays. Furthermore, concrete examples of how these principles can be applied to life science problems including a scenario involving a drug discovery dashboard prototype called BioDash are provided."
1469,"distinguishing causal interactions in neural populations",1560459,"Distinguishing Causal Interactions in Neural Populations","doi: 10.1162/neco.2007.19.4.910 We describe a theoretical network analysis that can distinguish statistically causal interactions in population neural activity leading to a specific output. We introduce the concept of a causal core to refer to the set of neuronal interactions that are causally significant for the output, as assessed by Granger causality. Because our approach requires extensive knowledge of neuronal connectivity and dynamics, an illustrative example is provided by analysis of Darwin X, a brain-based device that allows precise recording of the activity of neuronal units during behavior. In Darwin X, a simulated neuronal model of the hippocampus and surrounding cortical areas supports learning of a spatial navigation task in a real environment. Analysis of Darwin X reveals that large repertoires of neuronal interactions contain comparatively small causal cores and that these causal cores become smaller during learning, a finding that may reflect the selection of specific causal pathways from diverse neuronal repertoires."
1470,"identification and characterization of cell typespecific and ubiquitous chromatin regulatory structures in the human genome",1572970,"Identification and Characterization of Cell Type–Specific and Ubiquitous Chromatin Regulatory Structures in the Human Genome","The identification of regulatory elements from different cell types is necessary for understanding the mechanisms controlling cell typeâ€“specific and housekeeping gene expression. Mapping DNaseI hypersensitive (HS) sites is an accurate method for identifying the location of functional regulatory elements. We used a high throughput method called DNase-chip to identify 3,904 DNaseI HS sites from six cell types across 1% of the human genome. A significant number (22%) of DNaseI HS sites from each cell type are ubiquitously present among all cell types studied. Surprisingly, nearly all of these ubiquitous DNaseI HS sites correspond to either promoters or insulator elements: 86% of them are located near annotated transcription start sites and 10% are bound by CTCF, a protein with known enhancer-blocking insulator activity. We also identified a large number of DNaseI HS sites that are cell type specific (only present in one cell type); these regions are enriched for enhancer elements and correlate with cell typeâ€“specific gene expression as well as cell typeâ€“specific histone modifications. Finally, we found that approximately 8% of the genome overlaps a DNaseI HS site in at least one the six cell lines studied, indicating that a significant percentage of the genome is potentially functional."
1471,"mechanistic approaches to the study of evolution the functional synthesis",1573176,"Mechanistic approaches to the study of evolution: the functional synthesis","An emerging synthesis of evolutionary biology and experimental molecular biology is providing much stronger and deeper inferences about the dynamics and mechanisms of evolution than were possible in the past. The new approach combines statistical analyses of gene sequences with manipulative molecular experiments to reveal how ancient mutations altered biochemical processes and produced novel phenotypes. This functional synthesis has set the stage for major advances in our understanding of fundamental questions in evolutionary biology. Here we describe this emerging approach, highlight important new insights that it has made possible, and suggest future directions for the field."
1472,"microrna sponges competitive inhibitors of small rnas in mammalian cells",1576654,"MicroRNA sponges: competitive inhibitors of small RNAs in mammalian cells","MicroRNAs are predicted to regulate thousands of mammalian genes, but relatively few targets have been experimentally validated and few microRNA loss-of-function phenotypes have been assigned. As an alternative to chemically modified antisense oligonucleotides, we developed microRNA inhibitors that can be expressed in cells, as RNAs produced from transgenes. Termed 'microRNA sponges', these competitive inhibitors are transcripts expressed from strong promoters, containing multiple, tandem binding sites to a microRNA of interest. When vectors encoding these sponges are transiently transfected into cultured cells, sponges derepress microRNA targets at least as strongly as chemically modified antisense oligonucleotides. They specifically inhibit microRNAs with a complementary heptameric seed, such that a single sponge can be used to block an entire microRNA seed family. RNA polymerase II promoter (Pol II)-driven sponges contain a fluorescence reporter gene for identification and sorting of sponge-treated cells. We envision the use of stably expressed sponges in animal models of disease and development."
1473,"the biological big bang model for the major transitions in evolution",1578346,"The Biological Big Bang model for the major transitions in evolution","ABSTRACT: BACKGROUND: Major transitions in biological evolution show the same pattern of sudden emergence of diverse forms at a new level of complexity. The relationships between major groups within an emergent new class of biological entities are hard to decipher and do not seem to fit the tree pattern that, following Darwin's original proposal, remains the dominant description of biological evolution. The cases in point include the origin of complex RNA molecules and protein folds; major groups of viruses; archaea and bacteria, and the principal lineages within each of these prokaryotic domains; eukaryotic supergroups; and animal phyla. In each of these pivotal nexuses in life's history, the principal ""types"" seem to appear rapidly and fully equipped with the signature features of the respective new level of biological organization. No intermediate ""grades"" or intermediate forms between different types are detectable. Usually, this pattern is attributed to cladogenesis compressed in time, combined with the inevitable erosion of the phylogenetic signal. HYPOTHESIS: I propose that most or all major evolutionary transitions that show the ""explosive"" pattern of emergence of new types of biological entities correspond to a boundary between two qualitatively distinct evolutionary phases. The first, inflationary phase is characterized by extremely rapid evolution driven by various processes of genetic information exchange, such as horizontal gene transfer, recombination, fusion, fission, and spread of mobile elements. These processes give rise to a vast diversity of forms from which the main classes of entities at the new level of complexity emerge independently, through a sampling process. In the second phase, evolution dramatically slows down, the respective process of genetic information exchange tapers off, and multiple lineages of the new type of entities emerge, each of them evolving in a tree-like fashion from that point on. This biphasic model of evolution incorporates the previously developed concepts of the emergence of protein folds by recombination of small structural units and origin of viruses and cells from a pre-cellular compartmentalized pool of recombining genetic elements. The model is extended to encompass other major transitions. It is proposed that bacterial and archaeal phyla emerged independently from two distinct populations of primordial cells that, originally, possessed leaky membranes, which made the cells prone to rampant gene exchange; and that the eukaryotic supergroups emerged through distinct, secondary endosymbiotic events (as opposed to the primary, mitochondrial endosymbiosis). This biphasic model of evolution is substantially analogous to the scenario of the origin of universes in the eternal inflation version of modern cosmology. Under this model, universes like ours emerge in the infinite multiverse when the eternal process of exponential expansion, known as inflation, ceases in a particular region as a result of false vacuum decay, a first order phase transition process. The result is the nucleation of a new universe, which is traditionally denoted Big Bang, although this scenario is radically different from the Big Bang of the traditional model of an expanding universe. Hence I denote the phase transitions at the end of each inflationary epoch in the history of life Biological Big Bangs (BBB). CONCLUSION: A Biological Big Bang (BBB) model is proposed for the major transitions in life's evolution. According to this model, each transition is a BBB such that new classes of biological entities emerge at the end of a rapid phase of evolution (inflation) that is characterized by extensive exchange of genetic information which takes distinct forms for different BBBs. The major types of new forms emerge independently, via a sampling process, from the pool of recombining entities of the preceding generation. This process is envisaged as being qualitatively different from tree-pattern cladogenesis. REVIEWERS: This article was reviewed by William Martin, Sergei Maslov, and Leonid Mirny."
1474,"a comparison of background correction methods for twocolour microarrays",1603668,"A comparison of background correction methods for two-colour microarrays.","MOTIVATION: Microarray data must be background corrected to remove the effects of non-specific binding or spatial heterogeneity across the array, but this practice typically causes other problems such as negative corrected intensities and high variability of low intensity log-ratios. Different estimators of background, and various model-based processing methods, are compared in this study in search of the best option for differential expression analyses of small microarray experiments. RESULTS: Using data where some independent truth in gene expression is known, eight different background correction alternatives are compared, in terms of precision and bias of the resulting gene expression measures, and in terms of their ability to detect differentially expressed genes as judged by two popular algorithms, SAM and limma eBayes. A new background processing method (normexp) is introduced which is based on a convolution model. The model-based correction methods are shown to be markedly superior to the usual practice of subtracting local background estimates. Methods which stabilize the variances of the log-ratios along the intensity range perform the best. The normexp+offset method is found to give the lowest false discovery rate overall, followed by morph and vsn. Like vsn, normexp is applicable to most types of two-colour microarray data. AVAILABILITY: The background correction methods compared in this article are available in the R package limma (Smyth, 2005) from http://www.bioconductor.org. SUPPLEMENTARY INFORMATION: Supplementary data are available from http://bioinf.wehi.edu.au/resources/webReferences.html."
1475,"the phusion assembler",1604373,"The Phusion Assembler","10.1101/gr.731003 The Phusion assembler has assembled the mouse genome from the whole-genome shotgun (WGS) dataset collected by the Mouse Genome Sequencing Consortium, at ∼7.5× sequence coverage, producing a high-quality draft assembly 2.6 gigabases in size, of which 90% of these bases are in 479 scaffolds. For the mouse genome, which is a large and repeat-rich genome, the input dataset was designed to include a high proportion of paired end sequences of various size selected inserts, from 2–200 kbp lengths, into various host vector templates. Phusion uses sequence data, called reads, and information about reads that share common templates, called read pairs, to drive the assembly of this large genome to highly accurate results. The preassembly stage, which clusters the reads into sensible groups, is a key element of the entire assembler, because it permits a simple approach to parallelization of the assembly stage, as each cluster can be treated independent of the others. In addition to the application of Phusion to the mouse genome, we will also present results from the WGS assembly ofCaenorhabditis briggsae sequenced to about 11× coverage. TheC. briggsae assembly was accessioned through EMBL,http://www.ebi.ac.uk/services/index.html, using the series CAAC01000001–CAAC01000578, however, the Phusion mouse assembly described here was not accessioned. The mouse data was generated by the Mouse Genome Sequencing Consortium. The C. briggsae sequence was generated at The Wellcome Trust Sanger Institute and the Genome Sequencing Center, Washington University School of Medicine."
1476,"minimization of boolean complexity in human concept learning",1616758,"{Minimization of Boolean complexity in human concept learning}","One of the unsolved problems in the field of human concept learning concerns the factors that determine the subjective difficulty of concepts: why are some concepts psychologically simple and easy to learn, while others seem difficult, complex or incoherent? This question was much studied in the 1960s1 but was never answered, and more recent characterizations of concepts as prototypes rather than logical rules [2, 3] leave it unsolved [4, 5, 6]. Here I investigate this question in the domain of Boolean concepts (categories defined by logical rules). A series of experiments measured the subjective difficulty of a wide range of logical varieties of concepts (41 mathematically distinct types in six families—a far wider range than has been tested previously). The data reveal a surprisingly simple empirical 'law': the subjective difficulty of a concept is directly proportional to its Boolean complexity (the length of the shortest logically equivalent propositional formula)—that is, to its logical incompressibility."
1477,"interpreting neuronal population activity by reconstruction unified framework with application to hippocampal place cells",1618886,"Interpreting Neuronal Population Activity by Reconstruction: Unified Framework With Application to Hippocampal Place Cells","Physical variables such as the orientation of a line in the visual field or the location of the body in space are coded as activity levels in populations of neurons. Reconstruction or decoding is an inverse problem in which the physical variables are estimated from observed neural activity. Reconstruction is useful first in quantifying how much information about the physical variables is present in the population and, second, in providing insight into how the brain might use distributed representations in solving related computational problems such as visual object recognition and spatial navigation. Two classes of reconstruction methods, namely, probabilistic or Bayesian methods and basis function methods, are discussed. They include important existing methods as special cases, such as population vector coding, optimal linear estimation, and template matching. As a representative example for the reconstruction problem, different methods were applied to multi-electrode spike train data from hippocampal place cells in freely moving rats. The reconstruction accuracy of the trajectories of the rats was compared for the different methods. Bayesian methods were especially accurate when a continuity constraint was enforced, and the best errors were within a factor of two of the information-theoretic limit on how accurate any reconstruction can be and were comparable with the intrinsic experimental errors in position tracking. In addition, the reconstruction analysis uncovered some interesting aspects of place cell activity, such as the tendency for erratic jumps of the reconstructed trajectory when the animal stopped running. In general, the theoretical values of the minimal achievable reconstruction errors quantify how accurately a physical variable is encoded in the neuronal population in the sense of mean square error, regardless of the method used for reading out the information. One related result is that the theoretical accuracy is independent of the width of the Gaussian tuning function only in two dimensions. Finally, all the reconstruction methods considered in this paper can be implemented by a unified neural network architecture, which the brain feasibly could use to solve related problems."
1478,"mining biological networks for unknown pathways",1621877,"Mining biological networks for unknown pathways.","Motivation: Biological pathways provide significant insights on the interaction mechanisms of molecules. Presently, many essential pathways still remain unknown or incomplete for newly sequenced organisms. Moreover, experimental validation of enormous numbers of possible pathway candidates in a wet-lab environment is time- and effort-extensive. Thus, there is a need for comparative genomics tools that help scientists predict pathways in an organism's biological network. Results: In this article, we propose a technique to discover unknown pathways in organisms. Our approach makes in-depth use of Gene Ontology (GO)-based functionalities of enzymes involved in metabolic pathways as follows:Model each pathway as a biological functionality graph of enzyme GO functions, which we call pathway functionality template. Locate frequent pathway functionality patterns so as to infer previously unknown pathways through pattern matching in metabolic networks of organisms. We have experimentally evaluated the accuracy of the presented technique for 30 bacterial organisms to predict around 1500 organism-specific versions of 50 reference pathways. Using cross-validation strategy on known pathways, we have been able to infer pathways with 86% precision and 72% recall for enzymes (i.e. nodes). The accuracy of the predicted enzyme relationships has been measured at 85\\ precision with 64% recall. Availability: Code upon request. Contact: ali.cakmak@case.edu Supplementary information: Supplementary data are available at Bioinformatics online."
1479,"navigability of complex networks",1624269,"Navigability of Complex Networks","Targeted or quasi-targeted propagation of information is a fundamental process running in complex networked systems. Optimal communication in a network is easy to achieve if all its nodes have a full view of the global topological structure of the network. However many complex networks manifest communication efficiency without nodes having a full view of the network, and yet there is no generally applicable explanation of what mechanisms may render efficient such communication in the dark. In this work we model this communication as an oblivious routing process greedily operating on top of a network and making its decisions based only on distances within a hidden metric space lying underneath. Abstracting intrinsic similarities among networked elements, this hidden metric space is not easily reconstructible from the visible network topology. Yet we find that the structure of complex networks observed in reality, characterized by strong clustering and specific values of exponents of power-law degree distributions, maximizes their navigability, i.e., the efficiency of the greedy path-finding strategy in this hidden framework. We explain this observation by showing that more navigable networks have more prominent hierarchical structures which are congruent with the optimal layout of routing paths through a network. This finding has potentially profound implications for constructing efficient routing and searching strategies in communication and social networks, such as the Internet, Web, etc., and merits further research that would explain whether navigability of complex networks does indeed follow naturally from specifics of their evolution."
1480,"temporal precision in the neural code and the timescales of natural vision",1624654,"Temporal precision in the neural code and the timescales of natural vision","The timing of action potentials relative to sensory stimuli can be precise down to milliseconds in the visual system, even though the relevant timescales of natural vision are much slower. The existence of such precision contributes to a fundamental debate over the basis of the neural code and, specifically, what timescales are important for neural computation. Using recordings in the lateral geniculate nucleus, here we demonstrate that the relevant timescale of neuronal spike trains depends on the frequency content of the visual stimulus, and that 'relative', not absolute, precision is maintained both during spatially uniform white-noise visual stimuli and naturalistic movies. Using information-theoretic techniques, we demonstrate a clear role of relative precision, and show that the experimentally observed temporal structure in the neuronal response is necessary to represent accurately the more slowly changing visual world. By establishing a functional role of precision, we link visual neuron function on slow timescales to temporal structure in the response at faster timescales, and uncover a straightforward purpose of fine-timescale features of neuronal spike trains."
1481,"mutational and selective effects on copynumber variants in the human genome",1629604,"Mutational and selective effects on copy-number variants in the human genome","Comprehensive descriptions of large insertion/deletion or segmental duplication polymorphisms (SDs) in the human genome have recently been generated. These annotations, known collectively as structural or copy-number variants (CNVs), include thousands of discrete genomic regions and span hundreds of millions of nucleotides. Here we review the genomic distribution of CNVs, which is strongly correlated with gene, repeat and segmental duplication content. We explore the evolutionary mechanisms giving rise to this nonrandom distribution, considering the available data on both human polymorphisms and the fixed changes that differentiate humans from other species. It is likely that mutational biases, selective effects and interactions between these forces all contribute substantially to the spectrum of human copy-number variation. Although defining these variants with nucleotide-level precision remains a largely unmet but critical challenge, our understanding of their potential medical impact and evolutionary importance is rapidly emerging."
1482,"strategies for theorizing from process data",1631086,"Strategies for Theorizing from Process Data","In this article I describe and compare ct number of alternative generic strategies for the analysis of process data, looking at the consequences of these strategies for emerging theories. I evaluate the strengths and weaknesses of the strategies in terms of their capacity to generate theory that is accurate, parsimonious, general, and useful and suggest that method and theory are inextricably intertwined, that multiple strategies are often advisable, and that no analysis strategy will produce theory without an uncodifiable creative leap, however small. Finally, I argue that there is room in the organizational research literature for more openness within the academic community toward a variety of forms of coupling between theory and data."
1483,"evolution in the social brain",1631927,"{Evolution in the Social Brain}","{The evolution of unusually large brains in some groups of animals, notably primates, has long been a puzzle. Although early explanations tended to emphasize the brain's role in sensory or technical competence (foraging skills, innovations, and way-finding), the balance of evidence now clearly favors the suggestion that it was the computational demands of living in large, complex societies that selected for large brains. However, recent analyses suggest that it may have been the particular demands of the more intense forms of pairbonding that was the critical factor that triggered this evolutionary development. This may explain why primate sociality seems to be so different from that found in most other birds and mammals: Primate sociality is based on bonded relationships of a kind that are found only in pairbonds in other taxa. 10.1126/science.1145463}"
1484,"humans have evolved specialized skills of social cognition the cultural intelligence hypothesis",1634025,"Humans Have Evolved Specialized Skills of Social Cognition: The Cultural Intelligence Hypothesis","Humans have many cognitive skills not possessed by their nearest primate relatives. The cultural intelligence hypothesis argues that this is mainly due to a species-specific set of social-cognitive skills, emerging early in ontogeny, for participating and exchanging knowledge in cultural groups. We tested this hypothesis by giving a comprehensive battery of cognitive tests to large numbers of two of humans' closest primate relatives, chimpanzees and orangutans, as well as to 2.5-year-old human children before literacy and schooling. Supporting the cultural intelligence hypothesis and contradicting the hypothesis that humans simply have more ""general intelligence,"" we found that the children and chimpanzees had very similar cognitive skills for dealing with the physical world but that the children had more sophisticated cognitive skills than either of the ape species for dealing with the social world."
1485,"a metagenomic survey of microbes in honey bee colony collapse disorder",1638951,"A Metagenomic Survey of Microbes in Honey Bee Colony Collapse Disorder","In colony collapse disorder (CCD), honey bee colonies inexplicably lose their workers. CCD has resulted in a loss of 50 to 90% of colonies in beekeeping operations across the United States. The observation that irradiated combs from affected colonies can be repopulated with naive bees suggests that infection may contribute to CCD. We used an unbiased metagenomic approach to survey microflora in CCD hives, normal hives, and imported royal jelly. Candidate pathogens were screened for significance of association with CCD by the examination of samples collected from several sites over a period of 3 years. One organism, Israeli acute paralysis virus of bees, was strongly correlated with CCD."
1486,"global pattern formation and ethniccultural violence",1655248,"Global Pattern Formation and Ethnic/Cultural Violence","We identify a process of global pattern formation that causes regions to differentiate by culture. Violence arises at boundaries between regions that are not sufficiently well defined. We model cultural differentiation as a separation of groups whose members prefer similar neighbors, with a characteristic group size at which violence occurs. Application of this model to the area of the former Yugoslavia and to India accurately predicts the locations of reported conflict. This model also points to imposed mixing or boundary clarification as mechanisms for promoting peace."
1487,"a survey of named entity recognition and classification",1657521,"A survey of named entity recognition and classification","This survey covers fifteen years of research in the Named Entity Recognition and Classification (NERC) field, from 1991 to 2006. We report observations about languages, named entity types, domains and textual genres studied in the literature. From the start, NERC systems have been developed using hand-made rules, but now machine learning techniques are widely used. These techniques are surveyed along with other critical aspects of NERC such as features and evaluation methods. Features are word-level, dictionary-level and corpus-level representations of words in a document. Evaluation techniques, ranging from intuitive exact match to very complex matching techniques with adjustable cost of errors, are an indisputable key to progress."
1488,"a capacity theory of comprehension individual differences in working memory",1668327,"A capacity theory of comprehension: individual differences in working memory.","A theory of the way working memory capacity constrains comprehension proposes that both processing and storage are mediated by activation and that the total amount of activation available in working memory varies among individuals. Individual differences in working memory capacity for language can account for qualitative and quantitative differences among college-age adults in several aspects of language comprehension. One aspect is syntactic modularity: The larger capacity of some individuals permits interaction among syntactic and pragmatic information, so that their syntactic processes are not informationally encapsulated. Another aspect is syntactic ambiguity: The larger capacity of some individuals permits them to maintain multiple interpretations. The theory is instantiated as a production system model in which the amount of activation available to the model affects how it adapts to the transient computational and storage demands that occur in comprehension. (PsycINFO Database Record (c) 2000 APA, all rights reserved)(unassigned)"
1489,"semantic web technologies for the adaptive web",1668986,"Semantic Web Technologies for the Adaptive Web","Ontologies and reasoning are the key terms brought into focus by the semantic web community. Formal representation of ontologies in a common data model on the web can be taken as a foundation for adaptive web technologies as well. This chapter describes how ontologies shared on the semantic web provide conceptualization for the links which are a main vehicle to access information on the web. The subject domain ontologies serve as constraints for generating only those links which are relevant for the domain a user is currently interested in. Furthermore, user model ontologies provide additional means for deciding which links to show, annotate, hide, generate, and reorder. The semantic web technologies provide means to formalize the domain ontologies and metadata created from them. The formalization enables reasoning for personalization decisions. This chapter describes which components are crucial to be formalized by the semantic web ontologies for adaptive web. We use examples from an eLearning domain to illustrate the principles which are broadly applicable to any information domain on the web."
1490,"gene conversion mechanisms evolution and human disease",1673083,"Gene conversion: mechanisms, evolution and human disease","Gene conversion, one of the two mechanisms of homologous recombination, involves the unidirectional transfer of genetic material from a 'donor' sequence to a highly homologous 'acceptor'. Considerable progress has been made in understanding the molecular mechanisms that underlie gene conversion, its formative role in human genome evolution and its implications for human inherited disease. Here we assess current thinking about how gene conversion occurs, explore the key part it has played in fashioning extant human genes, and carry out a meta-analysis of gene-conversion events that are known to have caused human genetic disease."
1491,"the evolution of genetic networks by nonadaptive processes",1673084,"The evolution of genetic networks by non-adaptive processes"," Although numerous investigators assume that the global features of genetic networks are moulded by natural selection, there has been no formal demonstration of the adaptive origin of any genetic network. This Analysis shows that many of the qualitative features of known transcriptional networks can arise readily through the non-adaptive processes of genetic drift, mutation and recombination, raising questions about whether natural selection is necessary or even sufficient for the origin of many aspects of gene-network topologies. The widespread reliance on computational procedures that are devoid of population-genetic details to generate hypotheses for the evolution of network configurations seems to be unjustified."
1492,"diet and the evolution of human amylase gene copy number variation",1674923,"Diet and the evolution of human amylase gene copy number variation","Starch consumption is a prominent characteristic of agricultural societies and hunter-gatherers in arid environments. In contrast, rainforest and circum-arctic hunter-gatherers and some pastoralists consume much less starch1, 2, 3. This behavioral variation raises the possibility that different selective pressures have acted on amylase, the enzyme responsible for starch hydrolysis4. We found that copy number of the salivary amylase gene (AMY1) is correlated positively with salivary amylase protein level and that individuals from populations with high-starch diets have, on average, more AMY1 copies than those with traditionally low-starch diets. Comparisons with other loci in a subset of these populations suggest that the extent of AMY1 copy number differentiation is highly unusual. This example of positive selection on a copy number–variable gene is, to our knowledge, one of the first discovered in the human genome. Higher AMY1 copy numbers and protein levels probably improve the digestion of starchy foods and may buffer against the fitness-reducing effects of intestinal disease."
1493,"classical electrostatics in biology and chemistry",1678574,"Classical Electrostatics in Biology and Chemistry","A major revival in the use of classical electrostatics as an approach to the study of charged and polar molecules in aqueous solution has been made possible through the development of fast numerical and computational methods to solve the Poisson-Boltzmann equation for solute molecules that have complex shapes and charge distributions. Graphical visualization of the calculated electrostatic potentials generated by proteins and nucleic acids has revealed insights into the role of electrostatic interactions in a wide range of biological phenomena. Classical electrostatics has also proved to be a successful quantitative tool yielding accurate descriptions of electrical potentials, diffusion limited processes, pH-dependent properties of proteins, ionic strength-dependent phenomena, and the solvation free energies of organic molecules."
1494,"jamming at zero temperature and zero applied stress the epitome of disorder",1681472,"Jamming at zero temperature and zero applied stress: The epitome of disorder","{We have studied how two- and three-dimensional systems made up of particles interacting with finite range, repulsive potentials jam (i.e., develop a yield stress in a disordered state) at zero temperature and zero applied stress. At low packing fractions phi, the system is not jammed and each particle can move without impediment from its neighbors. For each configuration, there is a unique jamming threshold phi(c) at which particles can no longer avoid each other, and the bulk and shear moduli simultaneously become nonzero. The distribution of phi(c) values becomes narrower as the system size increases, so that essentially all configurations jam at the same packing fraction in the thermodynamic limit. This packing fraction corresponds to the previously measured value for random close packing. In fact, our results provide a well-defined meaning for ``random close packing{''} in terms of the fraction of all phase space with inherent structures that jam. The jamming threshold, point J, occurring at zero temperature and applied stress and at the random-close-packing density, has properties reminiscent of an ordinary critical point. As point J is approached from higher packing fractions, power-law scaling is found for the divergence of the first peak in the pair correlation function and in the vanishing of the pressure, shear modulus, and excess number of overlapping neighbors. Moreover, near point J, certain quantities no longer self-average, suggesting the existence of a length scale that diverges at J. However, point J also differs from an ordinary critical point: the scaling exponents do not depend on dimension but do depend on the interparticle potential. Finally, as point J is approached from high packing fractions, the density of vibrational states develops a large excess of low-frequency modes. Indeed, at point J, the density of states is a constant all the way down to zero frequency. All of these results suggest that point J is a point of maximal disorder and may control behavior in its vicinity-perhaps even at the glass transition.}"
1495,"inhomogeneous electron gas",1693077,"Inhomogeneous Electron Gas","This work is a generalization of the Hohenberg—Kohn—Sham theory of the inhomogeneous electron gas; with emphasis on spin effects. An argument based on quantum electrodynamics is used to express the ground-state energy of a system of interacting electrons as a functional of the current density. Expressions are derived for coefficients appearing in an expansion of the correlation functional in terms of the linear-response functions of the homogeneous system; for a gas of almost constant four-current density. The current density contains a spin-dependent term which leads; in the nonrelativistic limit; to a local potential which is also spin dependent. This potential is applied to the problems of spin splitting of energy bands in ferromagnets and spin-density-wave antiferromagnets. The relations between the present approach; that of Slater; and the collective electron theory of ferromagnetism of Stoner are described."
1496,"issues of context in information retrieval ir an introduction to the special issue",1697751,"Issues of context in information retrieval ({IR}): an introduction to the special issue","The subject of context has received a great deal of attention in the information retrieval (IR) literature over the past decade, primarily in studies of information seeking and IR interactions. Recently, attention to context in IR has expanded to address new problems in new environments. In this paper we outline five overlapping dimensions of context which we believe to be important constituent elements and we discuss how they are related to different issues in IR research. The papers in this special issue are summarized with respect to how they represent work that is being conducted within these dimensions of context. We conclude with future areas of research which are needed in order to fully understand the multidimensional nature of context in IR."
1497,"genomescale reconstruction of metabolic network in bacillus subtilis based on highthroughput phenotyping and gene essentiality data",1713592,"Genome-scale Reconstruction of Metabolic Network in Bacillus subtilis Based on High-throughput Phenotyping and Gene Essentiality Data.","In this report, a genome-scale reconstruction of Bacillus subtilis metabolism and its iterative development based on the combination of genomic, biochemical, and physiological information and high-throughput phenotyping experiments is presented. The initial reconstruction was converted into an in silico model and expanded in a four-step iterative fashion. First, network gap analysis was used to identify 48 missing reactions that are needed for growth but were not found in the genome annotation. Second, the computed growth rates under aerobic conditions were compared with high-throughput phenotypic screen data, and the initial in silico model could predict the outcomes qualitatively in 140 of 271 cases considered. Detailed analysis of the incorrect predictions resulted in the addition of 75 reactions to the initial reconstruction, and 200 of 271 cases were correctly computed. Third, in silico computations of the growth phenotypes of knock-out strains were found to be consistent with experimental observations in 720 of 766 cases evaluated. Fourth, the integrated analysis of the large-scale substrate utilization and gene essentiality data with the genome-scale metabolic model revealed the requirement of 80 specific enzymes (transport, 53; intracellular reactions, 27) that were not in the genome annotation. Subsequent sequence analysis resulted in the identification of genes that could be putatively assigned to 13 intracellular enzymes. The final reconstruction accounted for 844 open reading frames and consisted of 1020 metabolic reactions and 988 metabolites. Hence, the in silico model can be used to obtain experimentally verifiable hypothesis on the metabolic functions of various genes."
1498,"a highresolution atlas of nucleosome occupancy in yeast",1713694,"A high-resolution atlas of nucleosome occupancy in yeast","We present the first complete high-resolution map of nucleosome occupancy across the whole Saccharomyces cerevisiae genome, identifying over 70,000 positioned nucleosomes occupying 81% of the genome. On a genome-wide scale, the persistent nucleosome-depleted region identified previously in a subset of genes demarcates the transcription start site. Both nucleosome occupancy signatures and overall occupancy correlate with transcript abundance and transcription rate. In addition, functionally related genes can be clustered on the basis of the nucleosome occupancy patterns observed at their promoters. A quantitative model of nucleosome occupancy indicates that DNA structural features may account for much of the global nucleosome occupancy."
1499,"inferring cellular networks a review",1715029,"Inferring cellular networks - a review","In this review we give an overview of computational and statistical methods to reconstruct cellular networks. Although this area of research is vast and fast developing, we show that most currently used methods can be organized by a few key concepts. The first part of the review deals with conditional independence models including Gaussian graphical models and Bayesian networks. The second part discusses probabilistic and graph-based methods for data from experimental interventions and perturbations."
1500,"the ncbi dbgap database of genotypes and phenotypes",1716134,"The NCBI dbGaP database of genotypes and phenotypes.","The National Center for Biotechnology Information has created the dbGaP public repository for individual-level phenotype, exposure, genotype and sequence data and the associations between them. dbGaP assigns stable, unique identifiers to studies and subsets of information from those studies, including documents, individual phenotypic variables, tables of trait data, sets of genotype data, computed phenotype-genotype associations, and groups of study subjects who have given similar consents for use of their data."
1501,"biological soap servers and web services provided by the public sequence data bank",1724599,"Biological SOAP servers and web services provided by the public sequence data bank.","A number of biological data resources (i.e. databases and data analytical tools) are searchable and usable on-line thanks to the internet and the World Wide Web (WWW) servers. The output from the web server is easy for us to browse. However, it is laborious and sometimes impossible for us to write a computer program that finds a useful data resource, sends a proper query and processes the output. It is a serious obstacle to the integration of distributed heterogeneous data resources. To solve the issue, we have implemented a SOAP (Simple Object Access Protocol) server and web services that provide a program-friendly interface. The web services are accessible at http://www.xml.nig.ac.jp/."
1502,"dbpedia a nucleus for a web of open data",1727461,"DBpedia: A Nucleus for a Web of Open Data","Abstract DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machineconsumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data. 1"
1503,"dynamo amazons highly available keyvalue store",1744606,"Dynamo: Amazon's Highly Available Key-Value Store","Reliability at massive scale is one of the biggest challenges we face at Amazon.com, one of the largest e-commerce operations in the world; even the slightest outage has significant financial consequences and impacts customer trust. The Amazon.com platform, which provides services for many web sites worldwide, is implemented on top of an infrastructure of tens of thousands of servers and network components located in many datacenters around the world. At this scale, small and large components fail continuously and the way persistent state is managed in the face of these failures drives the reliability and scalability of the software systems.  This paper presents the design and implementation of Dynamo, a highly available key-value storage system that some of Amazon’s core services use to provide an “always-on” experience. To achieve this level of availability, Dynamo sacrifices consistency under certain failure scenarios. It makes extensive use of object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use."
1504,"colloquium light scattering by particle and hole arrays",1756933,"Colloquium: Light scattering by particle and hole arrays","This Colloquium analyzes the interaction of light with two-dimensional periodic arrays of particles and holes. The enhanced optical transmission observed in the latter and the presence of surface modes in patterned metal surfaces is thoroughly discussed. A review of the most significant discoveries in this area is presented first. A simple tutorial model is then formulated to capture the essential physics involved in these phenomena, while allowing analytical derivations that provide deeper insight. Comparison with more elaborated calculations is offered as well. Finally, hole arrays in plasmon-supporting metals are compared to perforated perfect conductors, thus assessing the role of plasmons in these types of structures through analytical considerations. The developments that have been made in nanophotonics areas related to plasmons in nanostructures, extraordinary optical transmission in hole arrays, complete resonant absorption and emission of light, and invisibility in structured metals are illustrated in this Colloquium in a comprehensive, tutorial fashion."
1505,"living with noisy genes how cells function reliably with inherent variability in gene expression",1783204,"Living with Noisy Genes: How Cells Function Reliably with Inherent Variability in Gene Expression","Abstract Within a population of genetically identical cells there can be significant variation, or noise, in gene expression. Yet even with this inherent variability, cells function reliably. This review focuses on our understanding of noise at the level of both single genes and genetic regulatory networks, emphasizing comparisons between theoretical models and experimental results whenever possible. To highlight the importance of noise, we particularly emphasize examples in which a stochastic description of gene expression leads to a qualitatively different outcome than a deterministic one."
1506,"statistical physics of social dynamics",1785135,"Statistical physics of social dynamics","Statistical physics has proven to be a very fruitful framework to describe phenomena outside the realm of traditional physics. The last years have witnessed the attempt by physicists to study collective phenomena emerging from the interactions of individuals as elementary units in social structures. Here we review the state of the art by focusing on a wide list of topics ranging from opinion, cultural and language dynamics to crowd behavior, hierarchy formation, human dynamics, social spreading. We highlight the connections between these problems and other, more traditional, topics of statistical physics. We also emphasize the comparison of model results with empirical data from social systems."
1507,"comparing clusteringsan information based distance",1797414,"Comparing clusterings—an information based distance","This paper proposes an information theoretic criterion for comparing two partitions, or clusterings, of the same data set. The criterion, called variation of information (VI), measures the amount of information lost and gained in changing from clustering Click to view the MathML source to clustering Click to view the MathML source. The basic properties of VI are presented and discussed. We focus on two kinds of properties: (1) those that help one build intuition about the new criterion (in particular, it is shown the VI is a true metric on the space of clusterings), and (2) those that pertain to the comparability of VI values over different experimental conditions. As the latter properties have rarely been discussed explicitly before, other existing comparison criteria are also examined in their light. Finally we present the VI from an axiomatic point of view, showing that it is the only “sensible” criterion for comparing partitions that is both aligned to the lattice and convexely additive. As a consequence, we prove an impossibility result for comparing partitions: there is no criterion for comparing partitions that simultaneously satisfies the above two desirable properties and is bounded."
1508,"folksonomies and science communication a mashup of professional science databases and web services",1806502,"Folksonomies and science communication. A mash-up of professional science databases and Web 2.0 services","Folksonomies complete the methods of indexing scientific documents. Now scientists in their function as readers may play an active role in science communication as well, since they can tag documents with terms taken from their professional or personal environment. Folksonomies allow the indexing of documents by everyone without following any rules. Besides the benefits of folksonomies there are severe problems, e.g. the tags' lack of precision. In order to overcome the shortcomings of this collaborative indexing method we introduce natural language processing of tags and a relevance ranking algorithm which is based on specific tag distributions, on aspects of collaboration and on the actions of the ” prosumers”. This article is a plea for the combination of the ” old” science databases and the benefits of the folksonomies."
1509,"exploiting availability prediction in distributed systems",1817569,"Exploiting Availability Prediction in Distributed Systems","Loosely-coupled distributed systems have significant scale and cost advantages over more traditional architectures, but the availability of the nodes in these systems varies widely. Availability modeling is crucial for predicting per-machine resource burdens and understanding emergent, system-wide phenomena. We present new techniques for predicting availability and test them using traces taken from three distributed systems. We then describe three applications of availability prediction. The first, availability-guided replica placement, reduces object copying in a distributed data store while increasing data availability. The second shows how availability prediction can improve routing in delay-tolerant networks. The third combines availability prediction with virus modeling to improve forecasts of global infection dynamics."
1510,"decision theory what should the nervous system do",1826173,"Decision Theory: What ""Should"" the Nervous System Do?","The purpose of our nervous system is to allow us to successfully interact with our environment. This normative idea is formalized by decision theory that defines which choices would be most beneficial. We live in an uncertain world, and each decision may have many possible outcomes; choosing the best decision is thus complicated. Bayesian decision theory formalizes these problems in the presence of uncertainty and often provides compact models that predict observed behavior. With its elegant formalization of the problems faced by the nervous system, it promises to become a major inspiration for studies in neuroscience. 10.1126/science.1142998"
1511,"cooperation and quality in wikipedia",1840029,"Cooperation and quality in wikipedia","The rise of the Internet has enabled collaboration and cooperation on anunprecedentedly large scale. The online encyclopedia Wikipedia, which presently comprises 7.2 million articles created by 7.04 million distinct editors, provides a consummate example. We examined all 50 million edits made tothe 1.5 million English-language Wikipedia articles and found that the high-quality articles are distinguished by a marked increase in number of edits, number of editors, and intensity of cooperative behavior, as compared to other articles of similar visibility and age. This is significant because in other domains, fruitful cooperation has proven to be difficult to sustain as the size of the collaboration increases. Furthermore, in spite of the vagaries of human behavior, we show that Wikipedia articles accrete edits according to a simple stochastic mechanism in which edits beget edits. Topics of high interest or relevance are thus naturally brought to the forefront of quality."
1512,"does it matter who contributes a study on featured articles in the german wikipedia",1840109,"Does it matter who contributes: a study on featured articles in the german wikipedia","The considerable high quality of Wikipedia articles is often accredited to the large number of users who contribute to Wikipedia’s encyclopedia articles, who watch articles and correct errors immediately. In this paper, we are in particu- lar interested in a certain type of Wikipedia articles, namely, the featured articles – articles marked by a community’s vote as being of outstanding quality. The German Wikipedia has the nice property that it has two types of featured articles: excellent and worth reading. We explore on the German Wikipedia whether only the mere number of contributors makes the difference or whether the high quality of featured articles results from having experienced authors contribut- ing with a reputation for high quality contributions. Our results indicate that it does matter who contributes."
1513,"information literacy",1865961,"Information Literacy","[Autoren-Abstract] This paper aims to provide an overview of some of the most recent developments in concepts and practices associated with information literacy worldwide, revealing the paradox that, while information literacy is a key discipline of the information society and knowledge economy and is well-understood in its broader sense, it has made little progress educationally, save for a few exceptions in countries such as Australia, the USA, Canada and the UK. Deriving from the authors' background as university professors, the paper concentrates on approaches to promote information literacy in higher education. The paper concludes by pointing to the need to expand the debate on information literacy and how to raise ethical and moral concerns in the use of the Internet and the new technologies. It also explores the potential role that the European Commission eSafe (2003-2004) programme can play to encourage research and practice on information literacy in its widest sense, as an intrinsic competency in the fight against the effects of disseminating illegal and harmful content through online and other new technologies."
1514,"cultural recycling of cortical maps",1869121,"Cultural recycling of cortical maps.","Part of human cortex is specialized for cultural domains such as reading and arithmetic, whose invention is too recent to have influenced the evolution of our species. Representations of letter strings and of numbers occupy reproducible locations within large-scale macromaps, respectively in the left occipito-temporal and bilateral intraparietal cortex. Furthermore, recent fMRI studies reveal a systematic architecture within these areas. To explain this paradoxical cerebral invariance of cultural maps, we propose a neuronal recycling hypothesis, according to which cultural inventions invade evolutionarily older brain circuits and inherit many of their structural constraints."
1515,"publicly private and privately public social networking on youtube",1871475,"Publicly Private and Privately Public: Social Networking on YouTube","YouTube is a public video-sharing website where people can experience varying degrees of engagement with videos, ranging from casual viewing to sharing videos in order to maintain social relationships. Based on a one-year ethnographic project, this article analyzes how YouTube participants developed and maintained social networks by manipulating physical and interpretive access to their videos. The analysis reveals how circulating and sharing videos reflects different social relationships among youth. It also identifies varying degrees of ""publicness"" in video sharing. Some participants exhibited ""publicly private"" behavior, in which video makers' identities were revealed, but content was relatively private because it was not widely accessed. In contrast, ""privately public"" behavior involved sharing widely accessible content with many viewers, while limiting access to detailed information about video producers' identities."
1516,"way vertebrate alignment and conservation track in the ucsc genome browser",1873083,"28-Way vertebrate alignment and conservation track in the UCSC Genome Browser","This article describes a set of alignments of 28 vertebrate genome sequences that is provided by the UCSC Genome Browser. The alignments can be viewed on the Human Genome Browser (March 2006 assembly) at http://genome.ucsc.edu, downloaded in bulk by anonymous FTP from http://hgdownload.cse.ucsc.edu/goldenPath/hg18/multiz28way, or analyzed with the Galaxy server at http://g2.bx.psu.edu. This article illustrates the power of this resource for exploring vertebrate and mammalian evolution, using three examples. First, we present several vignettes involving insertions and deletions within protein-coding regions, including a look at some human-specific indels. Then we study the extent to which start codons and stop codons in the human sequence are conserved in other species, showing that start codons are in general more poorly conserved than stop codons. Finally, an investigation of the phylogenetic depth of conservation for several classes of functional elements in the human genome reveals striking differences in the rates and modes of decay in alignability. Each functional class has a distinctive period of stringent constraint, followed by decays that allow (for the case of regulatory regions) or reject (for coding regions and ultraconserved elements) insertions and deletions. 10.1101/gr.6761107"
1517,"exploring escience an introduction",1873815,"Exploring e-Science: An Introduction","A number of terms are in vogue that describe the transformation of science through utilization of Grid computing, Internet-based instrumentation, and global collaboration. For the purposes of this special theme section of the Journal of Computer-Mediated Communication, the term e-science serves as an umbrella for these initiatives. This article introduces the contributions to the collection and includes a number of suggestions for extending the exploratory work performed to date, including attention to disciplinary and contextual diversity and the importance of longitudinal research designs and historical awareness and of the social shaping of technology as a theoretical concept to understanding the changes currently underway in the scientific enterprise."
1518,"contributing to public document repositories a critical mass theory perspective",1875092,"Contributing to Public Document Repositories: A Critical Mass Theory Perspective","Public document repositories (PDRs) are valuable resources available on the Internet and are a component of the broader information commons freely accessible to the public. Instances of PDRs include the repository of reviews at Amazon.com and the online encyclopedia at Wikipedia. These repositories are created and sustained by the voluntary contributions of individuals who are not compensated for their inputs. This paper draws on and extends critical mass theory in the context of PDRs. Using data on the reviews written by prolific reviewers at Amazon.com and the text of their personal profiles, we find the critical mass of contributors at the PDR not only to be prolific and contributing high-quality reviews, but also to be among the earliest contributors of reviews on products. Reviewer profiles revealed the presence of multiple self-oriented motives (self expression, personal development, utilitarian motives, and enjoyment) and other-oriented motives (social affiliation, altruism, and reciprocity) for contribution. We find that the quality and quantity of contributions are inversely related and the motives for quantity of contribution are different from those related to the quality of contribution. The study highlights that PDRs are viewed by contributors as social contexts even though making contributions is an individual act that does not involve social interaction. 10.1177/0170840607076002"
1519,"models for synthetic biology",1877609,"Models for synthetic biology","Synthetic biological engineering is emerging from biology as a distinct discipline based on quantification. The technologies propelling synthetic biology are not new, nor is the concept of designing novel biological molecules. What is new is the emphasis on system behavior. The objective is the design and construction of new biological devices and systems to deliver useful applications. Numerous synthetic gene circuits have been created in the past decade, including bistable switches, oscillators, and logic gates, and possible applications abound, including biofuels, detectors for biochemical and chemical weapons, disease diagnosis, and gene therapies. More than fifty years after the discovery of the molecular structure of DNA, molecular biology is mature enough for real quantification that is useful for biological engineering applications, similar to the revolution in modeling in chemistry in the 1950s. With the excitement that synthetic biology is generating, the engineering and biological science communities appear remarkably willing to cross disciplinary boundaries toward a common goal."
1520,"distribution of node characteristics in complex networks",1884115,"Distribution of node characteristics in complex networks.","Our enhanced ability to map the structure of various complex networks is increasingly accompanied by the possibility of independently identifying the functional characteristics of each node. Although this led to the observation that nodes with similar characteristics have a tendency to link to each other, in general we lack the tools to quantify the interplay between node properties and the structure of the underlying network. Here we show that when nodes in a network belong to two distinct classes, two independent parameters are needed to capture the detailed interplay between the network structure and node properties. We find that the network structure significantly limits the values of these parameters, requiring a phase diagram to uniquely characterize the configurations available to the system. The phase diagram shows a remarkable independence from the network size, a finding that, together with a proposed heuristic algorithm, allows us to determine its shape even for large networks. To test the usefulness of the developed methods, we apply them to biological and socioeconomic systems, finding that protein functions and mobile phone usage occupy distinct regions of the phase diagram, indicating that the proposed parameters have a strong discriminating power."
1521,"some practical guidelines for effective sample size determination",1899371,"Some Practical Guidelines for Effective Sample Size Determination","Sample size determination is often an important step in planning a statistical study&#151;and it is usually a difficult one. Among the important hurdles to be surpassed, one must obtain an estimate of one or more error variances and specify an effect size of importance. There is the temptation to take some shortcuts. This article offers some suggestions for successful and meaningful sample size determination. Also discussed is the possibility that sample size may not be the main issue, that the real goal is to design a high-quality study. Finally, criticism is made of some ill-advised shortcuts relating to power and sample size."
1522,"web neue perspektiven fr marketing und medien",1906054,"Web 2.0. Neue Perspektiven für Marketing und Medien","{<P>Nach dem Platzen der „Internet-Blase"" und einigen spektakulären Dot-Com-Pleiten im Jahre 2001 setzte bei vielen Unternehmen, Kapitalgebern und Usern Ernüchterung hinsichtlich der Eignung des Internet als Handels- und Kommunikationsplattform ein. Mittlerweile wird ein neuer weltweiter Internet-Boom verzeichnet, in dessen Windschatten zahlreiche faszinierende – und häufig hoch-profitable – Internet-Angebote entstehen. Diese neue Phase des Internet wird als Web 2.0 bezeichnet. Die Herausgeber präsentieren eine praxisorientierte Einführung und einen systematischen Einblick in aktuelle Web 2.0-Fragestellungen. Erstmalig thematisieren renommierte Autoren aus Wissenschaft und Praxis die wichtigsten Trends und die Verflechtung privater Internetnutzung und kommerzieller Geschäftsmodelle, die so typisch für das web 2.0 ist. Das Buch wendet sich an Führungskräfte aus den Bereichen Marketing, E-Commerce und Neue Medien sowie an Wissenschaftler und Studierende auf diesen Gebieten.</P>}"
1523,"analysis of sequence conservation at nucleotide resolution",1916533,"Analysis of sequence conservation at nucleotide resolution","One of the major goals of comparative genomics is to understand the evolutionary history of each nucleotide in the human genome sequence, and the degree to which it is under selective pressure. Ascertainment of selective constraint at nucleotide resolution is particularly important for predicting the functional significance of human genetic variation and for analyzing the sequence substructure of cis-regulatory sequences and other functional elements. Current methods for analysis of sequence conservation are focused on delineation of conserved regions comprising tens or even hundreds of consecutive nucleotides. We therefore developed a novel computational approach designed specifically for scoring evolutionary conservation at individual base-pair resolution. Our approach estimates the rate at which each nucleotide position is evolving, computes the probability of neutrality given this rate estimate, and summarizes the result in a Sequence CONservation Evaluation (SCONE) score. We computed SCONE scores in a continuous fashion across 1&#37; of the human genome for which high-quality sequence information from up to 23 genomes are available. We show that SCONE scores are clearly correlated with the allele frequency of human polymorphisms in both coding and noncoding regions. We find that the majority of noncoding conserved nucleotides lie outside of longer conserved elements predicted by other conservation analyses, and are experiencing ongoing selection in modern humans as evident from the allele frequency spectrum of human polymorphism. We also applied SCONE to analyze the distribution of conserved nucleotides within functional regions. These regions are markedly enriched in individually conserved positions and short (&#60;15 bp) conserved &#8220;chunks.&#8221; Our results collectively suggest that the majority of functionally important noncoding conserved positions are highly fragmented and reside outside of canonically defined long conserved noncoding sequences. A small subset of these fragmented positions may be identified with high confidence."
1524,"the modular organization of domain structures insights into proteinprotein binding",1922175,"The modular organization of domain structures: insights into protein-protein binding","Domains are the building blocks of proteins and play a crucial role in protein&#8211;protein interactions. Here, we propose a new approach for the analysis and prediction of domain&#8211;domain interfaces. Our method, which relies on the representation of domains as residue-interacting networks, finds an optimal decomposition of domain structures into modules. The resulting modules comprise highly cooperative residues, which exhibit few connections with other modules. We found that non-overlapping binding sites in a domain, involved in different domain&#8211;domain interactions, are generally contained in different modules. This observation indicates that our modular decomposition is able to separate protein domains into regions with specialized functions. Our results show that modules with high modularity values identify binding site regions, demonstrating the predictive character of modularity. Furthermore, the combination of modularity with other characteristics, such as sequence conservation or surface patches, was found to improve our predictions. In an attempt to give a physical interpretation to the modular architecture of domains, we analyzed in detail six examples of protein domains with available experimental binding data. The modular configuration of the TEM1-&#946;-lactamase binding site illustrates the energetic independence of hotspots located in different modules and the cooperativity of those sited within the same modules. The energetic and structural cooperativity between intramodular residues is also clearly shown in the example of the chymotrypsin inhibitor, where non&#8211;binding site residues have a synergistic effect on binding. Interestingly, the binding site of the T cell receptor &#946; chain variable domain 2.1 is contained in one module, which includes structurally distant hot regions displaying positive cooperativity. These findings support the idea that modules possess certain functional and energetic independence. A modular organization of binding sites confers robustness and flexibility to the performance of the functional activity, and facilitates the evolution of protein interactions."
1525,"entanglement in manybody systems",1934660,"Entanglement in Many-Body Systems","The recent interest in aspects common to quantum information and condensed matter has prompted a prosperous activity at the border of these disciplines that were far distant until few years ago. Numerous interesting questions have been addressed so far. Here we review an important part of this field, the properties of the entanglement in many-body systems. We discuss the zero and finite temperature properties of entanglement in interacting spin, fermionic and bosonic model systems. Both bipartite and multipartite entanglement will be considered. At equilibrium we emphasize on how entanglement is connected to the phase diagram of the underlying model. The behavior of entanglement can be related, via certain witnesses, to thermodynamic quantities thus offering interesting possibilities for an experimental test. Out of equilibrium we discuss how to generate and manipulate entangled states by means of many-body Hamiltonians."
1526,"recent progress in understanding hydrophobic interactions",1963184,"Recent progress in understanding hydrophobic interactions","10.1073/pnas.0606422103 We present here a brief review of direct force measurements between hydrophobic surfaces in aqueous solutions. For almost 70 years, researchers have attempted to understand the hydrophobic effect (the low solubility of hydrophobic solutes in water) and the hydrophobic interaction or force (the unusually strong attraction of hydrophobic surfaces and groups in water). After many years of research into how hydrophobic interactions affect the thermodynamic properties of processes such as micelle formation (self-assembly) and protein folding, the results of direct force measurements between macroscopic surfaces began to appear in the 1980s. Reported ranges of the attraction between variously prepared hydrophobic surfaces in water grew from the initially reported value of 80â100 Ã to values as large as 3,000 Ã. Recent improved surface preparation techniques and the combination of surface force apparatus measurements with atomic force microscopy imaging have made it possible to explain the long-range part of this interaction (at separations >200 Ã) that is observed between certain surfaces. We tentatively conclude that only the short-range part of the attraction (<100 Ã) represents the true hydrophobic interaction, although a quantitative explanation for this interaction will require additional research. Although our force-measuring technique did not allow collection of reliable data at separations <10 Ã, it is clear that some stronger force must act in this regime if the measured interaction energy curve is to extrapolate to the measured adhesion energy as the surface separation approaches zero (i.e., as the surfaces come into molecular contact)."
1527,"personalized information retrieval based on context and ontological knowledge",2007189,"Personalized information retrieval based on context and ontological knowledge","Context modeling has long been acknowledged as a key aspect in a wide variety of problem domains. In this paper we focus on the combination of contextualization and personalization methods to improve the performance of personalized information retrieval. The key aspects in our proposed approach are (1) the explicit distinction between historic user context and live user context, (2) the use of ontology-driven representations of the domain of discourse, as a common, enriched representational ground for content meaning, user interests, and contextual conditions, enabling the definition of effective means to relate the three of them, and (3) the introduction of fuzzy representations as an instrument to properly handle the uncertainty and imprecision involved in the automatic interpretation of meanings, user attention, and user wishes. Based on a formal grounding at the representational level, we propose methods for the automatic extraction of persistent semantic user preferences, and live, ad-hoc user interests, which are combined in order to improve the accuracy and reliability of personalization for retrieval."
1528,"rnamediated epigenetic programming of a genomerearrangement pathway",2010561,"RNA-mediated epigenetic programming of a genome-rearrangement pathway","Genome-wide DNA rearrangements occur in many eukaryotes but are most exaggerated in ciliates, making them ideal model systems for epigenetic phenomena. During development of the somatic macronucleus, Oxytricha trifallax destroys 95% of its germ line, severely fragmenting its chromosomes, and then unscrambles hundreds of thousands of remaining fragments by permutation or inversion. Here we demonstrate that DNA or RNA templates can orchestrate these genome rearrangements in Oxytricha, supporting an epigenetic model for sequence-dependent comparison between germline and somatic genomes. A complete RNA cache of the maternal somatic genome may be available at a specific stage during development to provide a template for correct and precise DNA rearrangement. We show the existence of maternal RNA templates that could guide DNA assembly, and that disruption of specific RNA molecules disables rearrangement of the corresponding gene. Injection of artificial templates reprogrammes the DNA rearrangement pathway, suggesting that RNA molecules guide genome rearrangement."
1529,"the power of feedback",2057275,"The Power of Feedback","Feedback is one of the most powerful influences on learning and achievement, but this impact can be either positive or negative. Its power is frequently mentioned in articles about learning and teaching, but surprisingly few recent studies have systematically investigated its meaning. This article provides a conceptual analysis of feedback and reviews the evidence related to its impact on learning and achievement. This evidence shows that although feedback is among the major influences, the type of feedback and the way it is given can be differentially effective. A model of feedback is then proposed that identifies the particular properties and circumstances that make it effective, and some typically thorny issues are discussed, including the timing of feedback and the effects of positive and negative feedback. Finally, this analysis is used to suggest ways in which feedback can be used to enhance its effectiveness in classrooms. 10.3102/003465430298487"
1530,"noise propagation and signaling sensitivity in biological networks a role for positive feedback",2064778,"Noise propagation and signaling sensitivity in biological networks: A role for positive feedback","Interactions between genes and proteins are crucial for efficient processing of internal or external signals, but this connectivity also amplifies stochastic fluctuations by propagating noise between components. Linear (unbranched) cascades were shown to exhibit an interplay between the sensitivity to changes in input signals and the ability to buffer noise. We searched for biological circuits that can maintain signaling sensitivity while minimizing noise propagation, focusing on cases where the noise is characterized by rapid fluctuations. Negative feedback can buffer this type of noise, but this buffering comes at the expense of an even greater reduction in signaling sensitivity. By systematically analyzing all three-component circuits, we identify positive feedback as a central motif allowing for the buffering of propagated noise while maintaining sensitivity to long-term changes in input signals. We show analytically that noise reduction in the presence of positive feedback results from improved averaging of rapid fluctuations over time, and discuss in detail a particular implementation in the control of nutrient homeostasis in yeast. As the design of biological networks optimizes for multiple constrains, positive feedback can be used to improve sensitivity without a compromise the ability to buffer propagated noise."
1531,"the opls optimized potentials for liquid simulations potential functions for proteins energy minimizations for crystals of cyclic peptides and crambin",2094129,"The OPLS [optimized potentials for liquid simulations] potential functions for proteins, energy minimizations for crystals of cyclic peptides and crambin","A complete set of intermolecular potential functions has beendeveloped for use in computer simulations of proteins in their nativeenvironment. Parameters are reported for 25 peptide residues as wellas the common neutral and charged terminal groups. The potentialfunctions have the simple Coulomb plus Lennard-Jones form and arecompatible with the widely used models for water, TIP4P, TIP3P, andSPC. The parameters were obtained and tested primarily in conjunctionwith Monte Carlo statistical mechanics simulations of 36 pure organicliquids and numerous aqueous solutions of organic ions representativeof subunits in the side chains and backbones of proteins. Bondstretch, angle bend, and torsional terms have been adopted from theAMBER united-atom force field. As reported here, further testing hasinvolved studies of conformational energy surfaces and optimizationsof the crystal structures for four cyclic hexapeptides and a cyclicpentapeptide. The average root-mean-square deviation from the X-raystructures of the crystals is only 0.17 \AA for the atomic positions and 3% for the unit cell volumes. A more critical test was then providedby performing energy minimizations for the complete crystal of theprotein crambin, including 182 water molecules that were initiallyplaced via a Monte Carlo simulation. The resultant root-mean-squaredeviation for the non-hydrogen atoms is still ca. 0.2 \AA and thevariation in the errors for charged, polar, and nonpolar residues issmall. Improvement is apparent over the AMBER united-atom force fieldwhich has previously been demonstrated to be superior to manyalternatives."
1532,"dissipative particle dynamics bridging the gap between atomistic and mesoscopic simulation",2094310,"Dissipative particle dynamics: Bridging the gap between atomistic and mesoscopic simulation","We critically review dissipative particle dynamics (DPD) as a mesoscopic simulation method. We have established useful parameter ranges for simulations, and have made a link between these parameters and -parameters in Flory-Huggins-type models. This is possible because the equation of state of the DPD fluid is essentially quadratic in density. This link opens the way to do large scale simulations, effectively describing millions of atoms, by firstly performing simulations of molecular fragments retaining all atomistic details to derive -parameters, then secondly using these results as input to a DPD simulation to study the formation of micelles, networks, mesophases and so forth. As an example application, we have calculated the interfacial tension between homopolymer melts as a function of and N and have found a universal scaling collapse when /kBT0.4 is plotted against N for N>1. We also discuss the use of DPD to simulate the dynamics of mesoscopic systems, and indicate a possible problem with the timescale separation between particle diffusion and momentum diffusion (viscosity). {\\copyright}1997 American Institute of Physics."
1533,"a systematic interaction map of validated kinase inhibitors with serthr kinases",2097686,"A systematic interaction map of validated kinase inhibitors with Ser/Thr kinases","10.1073/pnas.0708800104 Protein kinases play a pivotal role in cell signaling, and dysregulation of many kinases has been linked to disease development. A large number of kinase inhibitors are therefore currently under investigation in clinical trials, and so far seven inhibitors have been approved as anti-cancer drugs. In addition, kinase inhibitors are widely used as specific probes to study cell signaling, but systematic studies describing selectivity of these reagents across a panel of diverse kinases are largely lacking. Here we evaluated the specificity of 156 validated kinase inhibitors, including inhibitors used in clinical trials, against 60 human Ser/Thr kinases using a thermal stability shift assay. Our analysis revealed many unexpected cross-reactivities for inhibitors thought to be specific for certain targets. We also found that certain combinations of active-site residues in the ATP-binding site correlated with the detected ligand promiscuity and that some kinases are highly sensitive to inhibition using diverse chemotypes, suggesting them as preferred intervention points. Our results uncovered also inhibitor cross-reactivities that may lead to alternate clinical applications. For example, LY333′531, a PKCβ inhibitor currently in phase III clinical trials, efficiently inhibited PIM1 kinase in our screen, a suggested target for treatment of leukemia. We determined the binding mode of this inhibitor by x-ray crystallography and in addition showed that LY333′531 induced cell death and significantly suppressed growth of leukemic cells from acute myeloid leukemia patients."
1534,"the biological impact of massspectrometrybased proteomics",2102115,"The biological impact of mass-spectrometry-based proteomics.","In the past decade, there have been remarkable advances in proteomic technologies. Mass spectrometry has emerged as the preferred method for in-depth characterization of the protein components of biological systems. Using mass spectrometry, key insights into the composition, regulation and function of molecular complexes and pathways have been gained. From these studies, it is clear that mass-spectrometry-based proteomics is now a powerful 'hypothesis-generating engine' that, when combined with complementary molecular, cellular and pharmacological techniques, provides a framework for translating large data sets into an understanding of complex biological processes."
1535,"computing topological parameters of biological networks",2111163,"Computing topological parameters of biological networks","Summary: Rapidly increasing amounts of molecular interaction data are being produced by various experimental techniques and computational prediction methods. In order to gain insight into the organization and structure of the resultant large complex networks formed by the interacting molecules, we have developed the versatile Cytoscape plugin NetworkAnalyzer. It computes and displays a comprehensive set of topological parameters, which includes the number of nodes, edges, and connected components, the network diameter, radius, density, centralization, heterogeneity, and clustering coefficient, the characteristic path length, and the distributions of node degrees, neighborhood connectivities, average clustering coefficients, and shortest path lengths. NetworkAnalyzer can be applied to both directed and undirected networks and also contains extra functionality to construct the intersection or union of two networks. It is an interactive and highly customizable application that requires no expert knowledge in graph theory from the user.  Availability: NetworkAnalyzer can be downloaded via the Cytoscape web site: http://www.cytoscape.org  Contact: mario.albrecht@mpi-inf.mpg.de  Supplementary information: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btm554"
1536,"reaching for highhanging fruit in drug discovery at protein",2111611,"Reaching for high-hanging fruit in drug discovery at protein","[ndash] protein interfaces Targeting the interfaces between proteins has huge therapeutic potential, but discovering small-molecule drugs that disrupt protein–protein interactions is an enormous challenge. Several recent success stories, however, indicate that protein–protein interfaces might be more tractable than has been thought. These studies discovered small molecules that bind with drug-like potencies to 'hotspots' on the contact surfaces involved in protein–protein interactions. Remarkably, these small molecules bind deeper within the contact surface of the target protein, and bind with much higher efficiencies, than do the contact atoms of the natural protein partner. Some of these small molecules are now making their way through clinical trials, so this high-hanging fruit might not be far out of reach."
1537,"the origin of protein interactions and allostery in colocalization",2111612,"The origin of protein interactions and allostery in colocalization.","Two fundamental principles can account for how regulated networks of interacting proteins originated in cells. These are the law of mass action, which holds that the binding of one molecule to another increases with concentration, and the fact that the colocalization of molecules vastly increases their local concentrations. It follows that colocalization can amplify the effect on one protein of random mutations in another protein and can therefore, through natural selection, lead to interactions between proteins and to a startling variety of complex allosteric controls. It also follows that allostery is common and that homologous proteins can have different allosteric mechanisms. Thus, the regulated protein networks of organisms seem to be the inevitable consequence of natural selection operating under physical laws."
1538,"the molecular sociology of the cell",2111613,"The molecular sociology of the cell","Proteomic studies have yielded detailed lists of the proteins present in a cell. Comparatively little is known, however, about how these proteins interact and are spatially arranged within the 'functional modules' of the cell: that is, the 'molecular sociology' of the cell. This gap is now being bridged by using emerging experimental techniques, such as mass spectrometry of complexes and single-particle cryo-electron microscopy, to complement traditional biochemical and biophysical methods. With the development of integrative computational methods to exploit the data obtained, such hybrid approaches will uncover the molecular architectures, and perhaps even atomic models, of many protein complexes. With these structures in hand, researchers will be poised to use cryo-electron tomography to view protein complexes in action within cells, providing unprecedented insights into protein-interaction networks."
1539,"dynamic programming algorithm optimization for spoken word recognition",2139257,"Dynamic Programming Algorithm Optimization for Spoken Word Recognition","Abstract-This paper reports on an optimum dynamic programming (DP) based time-normalization algorithm for spoken word recognition. First, a general principle of time-normalization is given using timewarping function. Then, two time-normalized distance definitions, ded symmetric and asymmetric forms, are derived from the principle. These two forms are compared with each other through theoretical discussions and experimental studies. The symmetric form algorithm superiority is established. A new technique, called slope constraint, is successfully introduced, in which the warping function slope is restricted so as to improve discrimination between words in different categories. AND SEIBI CHIBA vestigations were made, based on the assumption that speech patterns are time-sampled with a common and uniform sam-pling period, as in most general cases. One of the problems discussed in this paper involves the relative superiority of either a symmetric form of DP-matching or an asymmetric one. In the asymmetric form, time-normalization is achieved by trans-forming the time axis of a speech pattern onto that of the other. In the symmetric form, on the other hand, both time axes are transformed onto a temporarily defined common axis. Theoretical and experimental comparisons show that the sym-metric form gives better recognition than the asymmetric one. Another problem discussed concerns slope constraint technique. Since too much of the warping function flexibility sometimes results in poor discrimination between words in different The effective slope constraint characteristic is qualitatively analyzed, and the optimum slope constraint condition is determined through experiments. The optimized algorithm is then extensively subjected to experimentat comparison with various DP-algorithms, previously applied to spoken word recognition by different research groups. The experiment shows that the present algorithm gives no more than about twothirds errors, even compared to the best conventional algorithm. categories, a constraint is newly introduced on the warping I."
1540,"a biomolecular force field based on the free enthalpy of hydration and solvation the gromos forcefield parameter sets a and a",2139378,"A biomolecular force field based on the free enthalpy of hydration and solvation: the GROMOS force-field parameter sets 53A5 and 53A6.","Successive parameterizations of the GROMOS force field have been used successfully to simulate biomolecular systems over a long period of time. The continuing expansion of computational power with time makes it possible to compute ever more properties for an increasing variety of molecular systems with greater precision. This has led to recurrent parameterizations of the GROMOS force field all aimed at achieving better agreement with experimental data. Here we report the results of the latest, extensive reparameterization of the GROMOS force field. In contrast to the parameterization of other biomolecular force fields, this parameterization of the GROMOS force field is based primarily on reproducing the free enthalpies of hydration and apolar solvation for a range of compounds. This approach was chosen because the relative free enthalpy of solvation between polar and apolar environments is a key property in many biomolecular processes of interest, such as protein folding, biomolecular association, membrane formation, and transport over membranes. The newest parameter sets, 53A5 and 53A6, were optimized by first fitting to reproduce the thermodynamic properties of pure liquids of a range of small polar molecules and the solvation free enthalpies of amino acid analogs in cyclohexane (53A5). The partial charges were then adjusted to reproduce the hydration free enthalpies in water (53A6). Both parameter sets are fully documented, and the differences between these and previous parameter sets are discussed."
1541,"the hour work week escape live anywhere and join the new rich",2144477,"The 4-Hour work Week: Escape 9-5, Live Anywhere, and Join the New Rich","{Tim Ferriss is an extraordinary young man on a mission. The twenty-eight-year-old serial vagabond and successful entrepreneur has been teaching a wildly popular course at Princeton University for the past four years--a how-to and why-to guide to throwing out the old tools and methods for success (balancing life and work, retiring well, having a great nest egg) and replacing them with a whole new way of living. Readers can lead a rich life by working only four hours a week, freeing up the rest of their time to spend it living the lives they want.}"
1542,"game design and learning a conjectural analysis of how massively multiple online roleplaying games mmorpgs foster intrinsic motivation",2151562,"Game design and learning: a conjectural analysis of how massively multiple online role-playing games (MMORPGs) foster intrinsic motivation","Abstract&nbsp;&nbsp;During the past two decades, the popularity of computer and video games has prompted games to become a source of study for educational researchers and instructional designers investigating how various aspects of game design might be appropriated, borrowed, and re-purposed for the design of educational materials. The purpose of this paper is to present an analysis of how the structure in massively multiple online role-playing games (MMORPGs) might inform the design of interactive learning and game-based learning environments by looking at the elements which support intrinsic motivation. Specifically, this analysis presents (a) an overview of the two primary elements in MMORPGs game design: character design and narrative environment, (b) a discussion of intrinsic motivation in character role-playing, (c) a discussion of intrinsic motivational supports and cognitive support of the narrative structure of small quests, and (d) a discussion of how the narrative structure of MMORPGs might foster learning in various types of knowledge."
1543,"software design and architecture the once and future focus of software engineering",2189904,"Software Design and Architecture The once and future focus of software engineering","The design of software has been a focus of software engineering research since the field{'}s beginning. This paper explores key aspects of this research focus and shows why design will remain a principal focus. The intrinsic elements of software design, both process and product, are discussed: concept formation, use of experience, and means for representation, reasoning, and directing the design activity. Design is presented as being an activity engaged by a wide range of stakeholders, acting throughout most of a system{'}s lifecycle, making a set of key choices which constitute the application{'}s architecture. Directions for design research are outlined, including: (a) drawing lessons, inspiration, and techniques from design fields outside of computer science, (b) emphasizing the design of application ""character"" (functionality and style) as well as the application{'}s structure, and (c) expanding the notion of software to encompass the design of additional kinds of intangible complex artifacts."
1544,"drugbank a knowledgebase for drugs drug actions and drug targets",2191303,"DrugBank: a knowledgebase for drugs, drug actions and drug targets.","DrugBank is a richly annotated resource that combines detailed drug data with comprehensive drug target and drug action information. Since its first release in 2006, DrugBank has been widely used to facilitate in silico drug target discovery, drug design, drug docking or screening, drug metabolism prediction, drug interaction prediction and general pharmaceutical education. The latest version of DrugBank (release 2.0) has been expanded significantly over the previous release. With approximately 4900 drug entries, it now contains 60% more FDA-approved small molecule and biotech drugs including 10% more 'experimental' drugs. Significantly, more protein target data has also been added to the database, with the latest version of DrugBank containing three times as many non-redundant protein or drug target sequences as before (1565 versus 524). Each DrugCard entry now contains more than 100 data fields with half of the information being devoted to drug/chemical data and the other half devoted to pharmacological, pharmacogenomic and molecular biological data. A number of new data fields, including food-drug interactions, drug-drug interactions and experimental ADME data have been added in response to numerous user requests. DrugBank has also significantly improved the power and simplicity of its structure query and text query searches. DrugBank is available at http://www.drugbank.ca."
1545,"structural holes and good ideas",2207732,"Structural holes and good ideas","doi: 10.1086/421787 This article outlines the mechanism by which brokerage provides social capital. Opinion and behavior are more homogeneous within than between groups, so people connected across groups are more familiar with alternative ways of thinking and behaving. Brokerage across the structural holes between groups provides a vision of options otherwise unseen, which is the mechanism by which brokerage becomes social capital. I review evidence consistent with the hypothesis, then look at the networks around managers in a large American electronics company. The organization is rife with structural holes, and brokerage has its expected correlates. Compensation, positive performance evaluations, promotions, and good ideas are disproportionately in the hands of people whose networks span structural holes. The between‐group brokers are more likely to express ideas, less likely to have ideas dismissed, and more likely to have ideas evaluated as valuable. I close with implications for creativity and structural change."
1546,"a quantitative analysis of kinase inhibitor selectivity",2208356,"A quantitative analysis of kinase inhibitor selectivity.","Kinase inhibitors are a new class of therapeutics with a propensity to inhibit multiple targets. The biological consequences of multi-kinase activity are poorly defined, and an important step toward understanding the relationship between selectivity, efficacy and safety is the exploration of how inhibitors interact with the human kinome. We present interaction maps for 38 kinase inhibitors across a panel of 317 kinases representing >50% of the predicted human protein kinome. The data constitute the most comprehensive study of kinase inhibitor selectivity to date and reveal a wide diversity of interaction patterns. To enable a global analysis of the results, we introduce the concept of a selectivity score as a general tool to quantify and differentiate the observed interaction patterns. We further investigate the impact of panel size and find that small assay panels do not provide a robust measure of selectivity."
1547,"a correlated topic model of science",2219600,"A correlated topic model of Science","Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than X-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [J. Roy. Statist. Soc. Ser. B 44 (1982) 139–177]. We derive a fast variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. We apply the CTM to the articles from Science published from 1990–1999, a data set that comprises 57M words. The CTM gives a better fit of the data than LDA, and we demonstrate its use as an exploratory tool of large document collections. 1. Introduction. Large"
1548,"an iterative thresholding algorithm for linear inverse problems with a sparsity constraint",2226050,"An iterative thresholding algorithm for linear inverse problems with a sparsity constraint","We consider linear inverse problems where the solution is assumed to have a sparse expansion on an arbitrary preassigned orthonormal basis. We prove that replacing the usual quadratic regularizing penalties by weighted &lscr;p-penalties on the coefficients of such expansions, with 1 le p le 2, still regularizes the problem. Use of such &lscr;p-penalized problems with p &lt; 2 is often advocated when one expects the underlying ideal noiseless solution to have a sparse expansion with respect to the basis under consideration. To compute the corresponding regularized solutions, we analyze an iterative algorithm that amounts to a Landweber iteration with thresholding (or nonlinear shrinkage) applied at each iteration step. We prove that this algorithm converges in norm. © 2004 Wiley Periodicals, Inc."
1549,"programming biomolecular selfassembly pathways",2242051,"Programming biomolecular self-assembly pathways","In nature, self-assembling and disassembling complexes of proteins and nucleic acids bound to a variety of ligands perform intricate and diverse dynamic functions. In contrast, attempts to rationally encode structure and function into synthetic amino acid and nucleic acid sequences have largely focused on engineering molecules that self-assemble into prescribed target structures, rather than on engineering transient system dynamics1, 2. To design systems that perform dynamic functions without human intervention, it is necessary to encode within the biopolymer sequences the reaction pathways by which self-assembly occurs. Nucleic acids show promise as a design medium for engineering dynamic functions, including catalytic hybridization3, 4, 5, 6, triggered self-assembly7 and molecular computation8, 9. Here, we program diverse molecular self-assembly and disassembly pathways using a ‘reaction graph’ abstraction to specify complementarity relationships between modular domains in a versatile DNA hairpin motif. Molecular programs are executed for a variety of dynamic functions: catalytic formation of branched junctions, autocatalytic duplex formation by a cross-catalytic circuit, nucleated dendritic growth of a binary molecular ‘tree’, and autonomous locomotion of a bipedal walker."
1550,"the foundations of costsensitive learning",2266473,"The Foundations of Cost-Sensitive Learning","This paper revisits the problem of optimal learning and decision-making when different misclassification errors incur different penalties. We characterize precisely but intuitively when a cost matrix is reasonable, and we show how to avoid the mistake of defining a cost matrix that is economically incoherent. For the two-class case, we prove a theorem that shows how to change the proportion of negative examples in a training set in order to make optimal cost-sensitive classification decisions using a classifier learned by a standard non-costsensitive learning method. However, we then argue that changing the balance of negative and positive training examples has little effect on the classifiers produced by standard Bayesian and decision tree learning methods. Accordingly, the recommended way of applying one of these methods in a domain with differing misclassification costs is to learn a classifier from the training set as given, and then to compute optimal decisions ..."
1551,"the global record of memory in hippocampal neuronal activity",2269410,"The global record of memory in hippocampal neuronal activity","In humans the hippocampal region of the brain is crucial for declarative1 or episodic2 memory for a broad range of materials. In contrast, there has been controversy over whether the hippocampus mediates a similarly general memory function in other species, or whether it is dedicated to spatial memory processing3, 4, 5, 6. Evidence for the spatial view is derived principally from the observations of 'place cells'—hippocampal neurons that fire whenever the animal is in a particular location in its environment7, 8, or when it perceives a specific stimulus or performs a specific behaviour in a particular place3, 4. We trained rats to perform the same recognition memory task in several distinct locations in a rich spatial environment and found that the activity of many hippocampal neurons was related consistently to perceptual, behavioural or cognitive events, regardless of the location where these events occurred. These results indicate that non-spatial events are fundamental elements of hippocampal representation, and support the view that, across species, the hippocampus has a broad role in information processing associated with memory."
1552,"on testing the significance of sets of genes",2269925,"On testing the significance of sets of genes","This paper discusses the problem of identifying differentially expressed groups of genes from a microarray experiment. The groups of genes are externally defined, for example, sets of gene pathways derived from biological databases. Our starting point is the interesting Gene Set Enrichment Analysis (GSEA) procedure of Subramanian et al. [Proc. Natl. Acad. Sci. USA 102 (2005) 15545–15550]. We study the problem in some generality and propose two potential improvements to GSEA: the maxmean statistic for summarizing gene-sets, and restandardization for more accurate inferences. We discuss a variety of examples and extensions, including the use of gene-set scores for class predictions. We also describe a new R language package GSA that implements our ideas."
1553,"a statistical toolbox for metagenomics assessing functional diversity in microbial communities",2283005,"A statistical toolbox for metagenomics: assessing functional diversity in microbial communities","BACKGROUND: The 99% of bacteria in the environment that are recalcitrant to culturing have spurred the development of metagenomics, a culture-independent approach to sample and characterize microbial genomes. Massive datasets of metagenomic sequences have been accumulated, but analysis of these sequences has focused primarily on the descriptive comparison of the relative abundance of proteins that belong to specific functional categories. More robust statistical methods are needed to make inferences from metagenomic data. In this study, we developed and applied a suite of tools to describe and compare the richness, membership, and structure of microbial communities using peptide fragment sequences extracted from metagenomic sequence data. RESULTS: Application of these tools to acid mine drainage, soil, and whale fall metagenomic sequence collections revealed groups of peptide fragments with a relatively high abundance and no known function. When combined with analysis of 16S rRNA gene fragments from the same communities these tools enabled us to demonstrate that although there was no overlap in the types of 16S rRNA gene sequence observed, there was a core collection of operational protein families that was shared among the three environments. CONCLUSION: The results of comparisons between the three habitats were surprising considering the relatively low overlap of membership and the distinctively different characteristics of the three habitats. These tools will facilitate the use of metagenomics to pursue statistically sound genome-based ecological analyses."
1554,"modelbased variancestabilizing transformation for illumina microarray data",2286684,"Model-based variance-stabilizing transformation for Illumina microarray data.","Variance stabilization is a step in the preprocessing of microarray data that can greatly benefit the performance of subsequent statistical modeling and inference. Due to the often limited number of technical replicates for Affymetrix and cDNA arrays, achieving variance stabilization can be difficult. Although the Illumina microarray platform provides a larger number of technical replicates on each array (usually over 30 randomly distributed beads per probe), these replicates have not been leveraged in the current log2 data transformation process. We devised a variance-stabilizing transformation (VST) method that takes advantage of the technical replicates available on an Illumina microarray. We have compared VST with log2 and Variance-stabilizing normalization (VSN) by using the Kruglyak bead-level data (2006) and Barnes titration data (2005). The results of the Kruglyak data suggest that VST stabilizes variances of bead-replicates within an array. The results of the Barnes data show that VST can improve the detection of differentially expressed genes and reduce false-positive identifications. We conclude that although both VST and VSN are built upon the same model of measurement noise, VST stabilizes the variance better and more efficiently for the Illumina platform by leveraging the availability of a larger number of within-array replicates. The algorithms and Supplementary Data are included in the lumi package of Bioconductor, available at: www.bioconductor.org."
1555,"ezarray a webbased highly automated affymetrix expression array data management and analysis system",2288901,"EzArray: a web-based highly automated Affymetrix expression array data management and analysis system.","ABSTRACT: BACKGROUND: Though microarray experiments are very popular in life science research, managing and analyzing microarray data are still challenging tasks for many biologists. Most microarray programs require users to have sophisticated knowledge of mathematics, statistics and computer skills for usage. With accumulating microarray data deposited in public databases, easy-to-use programs to re-analyze previously published microarray data are in high demand. RESULTS: EzArray is a web-based Affymetrix expression array data management and analysis system for researchers who need to organize microarray data efficiently and get data analyzed instantly. EzArray organizes microarray data into projects that can be analyzed online with predefined or custom procedures. EzArray performs data preprocessing and detection of differentially expressed genes with statistical methods. All analysis procedures are optimized and highly automated so that even novice users with limited pre-knowledge of microarray data analysis can complete initial analysis quickly. Since all input files, analysis parameters, and executed scripts can be downloaded, EzArray provides maximum reproducibility for each analysis. In addition, EzArray integrates with Gene Expression Omnibus (GEO) and allows instantaneous re-analysis of published array data. CONCLUSIONS: EzArray is a novel Affymetrix expression array data analysis and sharing system. EzArray provides easy-to-use tools for re-analyzing published microarray data and will help both novice and experienced users perform initial analysis of their microarray data from the location of data storage. We believe EzArray will be a useful system for facilities with microarray services and laboratories with multiple members involved in microarray data analysis. EzArray is freely available from http://www.ezarray.com/."
1556,"wholegenome sequencing and variant discovery in c elegans",2289016,"Whole-genome sequencing and variant discovery in C. elegans.","Massively parallel sequencing instruments enable rapid and inexpensive DNA sequence data production. Because these instruments are new, their data require characterization with respect to accuracy and utility. To address this, we sequenced a Caernohabditis elegans N2 Bristol strain isolate using the Solexa Sequence Analyzer, and compared the reads to the reference genome to characterize the data and to evaluate coverage and representation. Massively parallel sequencing facilitates strain-to-reference comparison for genome-wide sequence variant discovery. Owing to the short-read-length sequences produced, we developed a revised approach to determine the regions of the genome to which short reads could be uniquely mapped. We then aligned Solexa reads from C. elegans strain CB4858 to the reference, and screened for single-nucleotide polymorphisms (SNPs) and small indels. This study demonstrates the utility of massively parallel short read sequencing for whole genome resequencing and for accurate discovery of genome-wide polymorphisms."
1557,"microrna induces expression of genes with complementary promoter sequences",2310535,"MicroRNA-373 induces expression of genes with complementary promoter sequences","10.1073/pnas.0707594105 Recent studies have shown that microRNA (miRNA) regulates gene expression by repressing translation or directing sequence-specific degradation of complementary mRNA. Here, we report new evidence in which miRNA may also function to induce gene expression. By scanning gene promoters  for sequences complementary to known miRNAs, we identified a putative miR-373 target site in the promoter of E-cadherin. Transfection of miR-373 and its precursor hairpin RNA (pre-miR-373) into PC-3 cells readily induced E-cadherin expression. Knockdown experiments confirmed that induction of E-cadherin by pre-miR-373 required the miRNA maturation protein Dicer. Further analysis revealed that cold-shock domain-containing protein C2 (CSDC2), which possesses a putative miR-373 target site within its promoter, was also readily induced in response to miR-373 and pre-miR-373. Furthermore, enrichment of RNA polymerase II was detected at both E-cadherin and CSDC2 promoters after miR-373 transfection. Mismatch mutations to miR-373 indicated that gene induction was specific to the miR-373 sequence. Transfection of promoter-specific dsRNAs revealed that the concurrent induction of E-cadherin and CSDC2 by miR-373 required the miRNA target sites in both promoters. In conclusion, we have identified a miRNA that targets promoter sequences and induces gene expression. These findings reveal a new mode by which miRNAs may regulate gene expression."
1558,"bacterial carbon processing by generalist species in the coastal ocean",2311249,"Bacterial carbon processing by generalist species in the coastal ocean","The assimilation and mineralization of dissolved organic carbon (DOC) by marine bacterioplankton is a major process in the ocean carbon cycle1. However, little information exists on the specific metabolic functions of participating bacteria and on whether individual taxa specialize on particular components of the marine DOC pool2. Here we use experimental metagenomics to show that coastal communities are populated by taxa capable of metabolizing a wide variety of organic carbon compounds. Genomic DNA captured from bacterial community subsets metabolizing a single model component of the DOC pool (either dimethylsulphoniopropionate or vanillate) showed substantial overlap in gene composition as well as a diversity of carbon-processing capabilities beyond the selected phenotypes. Our direct measure of niche breadth for bacterial functional assemblages indicates that, in accordance with ecological theory, heterogeneity in the composition and supply of organic carbon to coastal oceans may favour generalist bacteria. In the important interplay between microbial community structure and biogeochemical cycling, coastal heterotrophic communities may be controlled less by transient changes in the carbon reservoir that they process and more by factors such as trophic interactions and physical conditions."
1559,"qualifying the relationship between sequence conservation and molecular function",2320347,"Qualifying the relationship between sequence conservation and molecular function.","Quantification of evolutionary constraints via sequence conservation can be leveraged to annotate genomic functional sequences. Recent efforts addressing the converse of this relationship have identified many sites in metazoan genomes with molecular function but without detectable conservation between related species. Here, we discuss explanations and implications for these results considering both practical and theoretical issues. In particular, phylogenetic scope influences the relationship between sequence conservation and function. Comparisons of distantly related species can detect constraint with high specificity due to the loss of conserved neutral sequence, but sensitivity is sacrificed as a result of functional changes related to lineage-specific biology. The strength of natural selection operating on functional sequence is also important. Mutations to functional sequences that result in small fitness effects are subject to weaker constraints. Therefore, particularly when comparing highly divergent species, functional sequences that are degenerate or biologically redundant will be prone to turnover, wherein functional sequences are replaced by effectively equivalent, but nonorthologous counterparts. Finally, considering the size and complexity of metazoan genomes and the fact that many nonconserved sequences are associated with sequence-degenerate, low-level molecular functions, we find it likely that there exist many biochemically functional sequences that are not under constraint. This hypothesis does not lead to the conclusion that huge amounts of vertebrate genomes are functionally important, but rather that such ""functionality"" represents molecular noise that has weak or no effect on organismal phenotypes. 10.1101/gr.7205808"
1560,"opendmap an open source ontologydriven concept analysis engine with applications to capturing knowledge regarding protein transport protein interactions and celltypespecific gene expression",2321560,"OpenDMAP: An open source, ontology-driven concept analysis engine, with applications to capturing knowledge regarding protein transport, protein interactions and cell-type-specific gene expression","BACKGROUND:Information extraction (IE) efforts are widely acknowledged to be important in harnessing the rapid advance of biomedical knowledge, particularly in areas where important factual information is published in a diverse literature. Here we report on the design, implementation and several evaluations of OpenDMAP, an ontology-driven, integrated concept analysis system. It significantly advances the state of the art in information extraction by leveraging knowledge in ontological resources, integrating diverse text processing applications, and using an expanded pattern language that allows the mixing of syntactic and semantic elements and variable ordering.RESULTS:OpenDMAP information extraction systems were produced for extracting protein transport assertions (transport), protein-protein interaction assertions (interaction) and assertions that a gene is expressed in a cell type (expression). Evaluations were performed on each system, resulting in F-scores ranging from .26 - .72 (precision .39 - .85, recall .16 - .85). Additionally, each of these systems was run over all abstracts in MEDLINE, producing a total of 72,460 transport instances, 265,795 interaction instances and 176,153 expression instances. CONCLUSION:OpenDMAP advances the performance standards for extracting protein-protein interaction predications from the full texts of biomedical research articles. Furthermore, this level of performance appears to generalize to other information extraction tasks, including extracting information about predicates of more than two arguments. The output of the information extraction system is always constructed from elements of an ontology, ensuring that the knowledge representation is grounded with respect to a carefully constructed model of reality. The results of these efforts can be used to increase the efficiency of manual curation efforts and to provide additional features in systems that integrate multiple sources for information extraction. The open source OpenDMAP code library is freely available at http://bionlp.sourceforge.net/"
1561,"shot noise in mesoscopic conductors",2323708,"Shot Noise in Mesoscopic Conductors","Theoretical and experimental work concerned with dynamic fluctuations has developed into a very active and fascinating subfield of mesoscopic physics. We present a review of this development focusing on shot noise in small electric conductors. Shot noise is a consequence of the quantization of charge. It can be used to obtain information on a system which is not available through conductance measurements. In particular, shot noise experiments can determine the charge and statistics of the quasiparticles relevant for transport, and reveal information on the potential profile and internal energy scales of mesoscopic systems. Shot noise is generally more sensitive to the effects of electron–electron interactions than the average conductance. We present a discussion based on the conceptually transparent scattering approach and on the classical Langevin and Boltzmann–Langevin methods; in addition a discussion of results which cannot be obtained by these methods is provided. We conclude the review by pointing out a number of unsolved problems and an outlook on the likely future development of the field."
1562,"libsbml an api library for sbml",2342470,"LibSBML: an API Library for SBML","Summary: LibSBML is an application programming interface library for reading, writing, manipulating and validating content expressed in the Systems Biology Markup Language (SBML) format. It is written in ISO C and C++, provides language bindings for Common Lisp, Java, Python, Perl, MATLAB and Octave, and includes many features that facilitate adoption and use of both SBML and the li-brary. Developers can embed libSBML in their applications, saving themselves the work of implementing their own SBML parsing, ma-nipulation, and validation software.  Availability: LibSBML 3 was released in August 2007. Source code, binaries and documentation are freely available under LGPL open-source terms from http://sbml.org/software/libsbml.  Contact: sbml-team@caltech.edu 10.1093/bioinformatics/btn051"
1563,"comparison of likelihood and bayesian methods for estimating divergence times using multiple gene loci and calibration points with application to a radiation of cutelooking mouse lemur species",2345304,"Comparison of likelihood and Bayesian methods for estimating divergence times using multiple gene loci and calibration points, with application to a radiation of cute-looking mouse lemur species","Divergence time and substitution rate are seriously confounded in phylogenetic analysis, making it difficult to estimate divergence times when the molecular clock (rate constancy among lineages) is violated. This problem can be alleviated to some extent by analyzing multiple gene loci simultaneously and by using multiple calibration points. While different genes may have different patterns of evolutionary rate change, they share the same divergence times. Indeed, the fact that each gene may violate the molecular clock differently leads to the advantage of simultaneous analysis of multiple loci. Multiple calibration points provide the means for characterizing the local evolutionary rates on the phylogeny. In this paper, we extend previous likelihood models of local molecular clock for estimating species divergence times to accommodate multiple calibration points and multiple genes. Heterogeneity among different genes in evolutionary rate and in substitution process is accounted for by the models. We apply the likelihood models to analyze two mitochondrial protein-coding genes, cytochrome oxidase II and cytochrome b, to estimate divergence times of Malagasy mouse lemurs and related outgroups. The likelihood method is compared with the Bayes method of Thorne et al. (1998, Mol. Biol. Evol. 15: 1647-1657), which uses a probabilistic model to describe the change in evolutionary rate over time and uses the Markov chain Monte Carlo procedure to derive the posterior distribution of rates and times. Our likelihood implementation has the drawbacks of failing to accommodate uncertainties in fossil calibrations and of requiring the researcher to classify branches on the tree into different rate groups. Both problems are avoided in the Bayes method. Despite the differences in the two methods, however, data partitions and model assumptions had the greatest impact on date estimation. The three codon positions have very different substitution rates and evolutionary dynamics, and assumptions in the substitution model affect date estimation in both likelihood and Bayes analyses. The results demonstrate that the separate analysis is unreliable, with dates variable among codon positions and between methods, and that the combined analysis is much more reliable. When the three codon positions were analyzed simultaneously under the most realistic models using all available calibration information, the two methods produced similar results. The divergence of the mouse lemurs is dated to be around 7-10 million years ago, indicating a surprisingly early species radiation for such a morphologically uniform group of primates."
1564,"teens and social media",2349995,"Teens and Social Media","The use of social media – from blogging to online social networking to creation of all kinds of digital material – is central to many teenagers’ lives. Some 93% of teens use the internet, and more of them than ever are treating it as a venue for social interaction – a place where they can share creations, tell stories, and interact with others. The Pew Internet & American Life Project has found that 64% of online teens ages 12-17 have participated in one or more among a wide range of content-creating activities on the internet, up from 57% of online teens in a similar survey at the end of 2004. ô 39% of online teens share their own artistic creations online, such as artwork, photos, stories, or videos, up from 33% in 2004. ô 33% create or work on webpages or blogs for others, including those for groups they belong to, friends, or school assignments, basically unchanged from 2004 (32%). ô 28% have created their own online journal or blog, up from 19% in 2004. ô 27% maintain their own personal webpage, up from 22% in 2004. ô 26% remix content they find online into their own creations, up from 19% in 2004. The percentage of those ages 12-17 who said “yes” to at least one of those five content-creation activities is 64% of online teens, or 59% of all teens. In addition to those core elements of content creation, 55% of online teens ages 12-17 have created a profile on a social networking site such as Facebook or MySpace; 47% of online teens have uploaded photos where others can see them, though many restrict access to the photos in some way; and 14% of online teens have posted videos online. The current survey marks the first time questions about video posting and sharing were asked."
1565,"comparing patterns of natural selection across species using selective signatures",2354628,"Comparing patterns of natural selection across species using selective signatures.","Comparing gene expression profiles over many different conditions has led to insights that were not obvious from single experiments. In the same way, comparing patterns of natural selection across a set of ecologically distinct species may extend what can be learned from individual genome-wide surveys. Toward this end, we show how variation in protein evolutionary rates, after correcting for genome-wide effects such as mutation rate and demographic factors, can be used to estimate the level and types of natural selection acting on genes across different species. We identify unusually rapidly and slowly evolving genes, relative to empirically derived genome-wide and gene family-specific background rates for 744 core protein families in 30 &#947;-proteobacterial species. We describe the pattern of fast or slow evolution across species as the &#8220;selective signature&#8221; of a gene. Selective signatures represent a profile of selection across species that is predictive of gene function: pairs of genes with correlated selective signatures are more likely to share the same cellular function, and genes in the same pathway can evolve in concert. For example, glycolysis and phenylalanine metabolism genes evolve rapidly in Idiomarina loihiensis, mirroring an ecological shift in carbon source from sugars to amino acids. In a broader context, our results suggest that the genomic landscape is organized into functional modules even at the level of natural selection, and thus it may be easier than expected to understand the complex evolutionary pressures on a cell."
1566,"novelty and topicality in interactive information retrieval",2357901,"Novelty and topicality in interactive information retrieval","The information science research community is characterized by a paradigm split, with a system-centered cluster working on information retrieval (IR) algorithms and a user-centered cluster working on user behavior. The two clusters rarely leverage each other's insight and strength. One major suggestion from user-centered studies is to treat the relevance judgment of documents as a subjective, multidimensional, and dynamic concept rather than treating it as objective and based on topicality only. This study explores the possibility to enhance users' topicality-based relevance judgment with subjective novelty judgment in interactive IR. A set of systems is developed which differs in the way the novelty judgment is incorporated. In particular, this study compares systems which assume that users' novelty judgment is directed to a certain subtopic area and those which assume that users' novelty judgment is undirected. This study also compares systems which assume that users judge a document based on topicality first and then novelty in a stepwise, noncompensatory fashion and those which assume that users consider topicality and novelty simultaneously and as compensatory to each other. The user study shows that systems assuming directed novelty in general have higher relevance precision, but systems assuming a stepwise judgment process and systems assuming a compensatory judgment process are not significantly different. &copy; 2008 Wiley Periodicals, Inc."
1567,"representing word meaning and order information in a composite holographic lexicon",2357917,"Representing Word Meaning and Order Information in a Composite Holographic Lexicon","The authors present a computational model that builds a holographic lexicon representing both word meaning and word order from unsupervised experience with natural language. The model uses simple convolution and superposition mechanisms (cf. B. B. Murdock, 1982) to learn distributed holographic representations for words. The structure of the resulting lexicon can account for empirical data from classic experiments studying semantic typicality, categorization, priming, and semantic constraint in sentence completions. Furthermore, order information can be retrieved from the holographic representations, allowing the model to account for limited word transitions without the need for built-in transition rules. The model demonstrates that a broad range of psychological data can be accounted for directly from the structure of lexical representations learned in this way, without the need for complexity to be built into either the processing mechanisms or the representations. The holographic representations are an appropriate knowledge representation to be used by higher order models of language comprehension, relieving the complexity required at the higher level."
1568,"arb a software environment for sequence data",2366929,"ARB: a software environment for sequence data","The ARB (from Latin arbor, tree) project was initiated almost 10 years ago. The ARB program package comprises a variety of directly interacting software tools for sequence database maintenance and analysis which are controlled by a common graphical user interface. Although it was initially designed for ribosomal RNA data, it can be used for any nucleic and amino acid sequence data as well. A central database contains processed (aligned) primary structure data. Any additional descriptive data can be stored in database fields assigned to the individual sequences or linked via local or worldwide networks. A phylogenetic tree visualized in the main window can be used for data access and visualization. The package comprises additional tools for data import and export, sequence alignment, primary and secondary structure editing, profile and filter calculation, phylogenetic analyses, specific hybridization probe design and evaluation and other components for data analysis. Currently, the package is used by numerous working groups worldwide. 10.1093/nar/gkh293"
1569,"bioinformatics challenges of new sequencing technology",2369506,"Bioinformatics challenges of new sequencing technology.","New DNA sequencing technologies can sequence up to one billion bases in a single day at low cost, putting large-scale sequencing within the reach of many scientists. Many researchers are forging ahead with projects to sequence a range of species using the new technologies. However, these new technologies produce read lengths as short as 35–40 nucleotides, posing challenges for genome assembly and annotation. Here we review the challenges and describe some of the bioinformatics systems that are being proposed to solve them. We specifically address issues arising from using these technologies in assembly projects, both de novo and for resequencing purposes, as well as efforts to improve genome annotation in the fragmented assemblies produced by short read lengths."
1570,"quantum optics",2374088,"Quantum Optics","{Quantum optics has witnessed significant theoretical and experimental developments in recent years. This book provides an in-depth and wide-ranging introduction to the subject, emphasizing throughout the basic principles and their applications. The book begins by developing the basic tools of quantum optics, and goes on to show the application of these tools in a variety of quantum optical systems, including lasing without inversion, squeezed states, and atom optics. The final four chapters discuss quantum optical tests of the foundations of quantum mechanics, and particular aspects of measurement theory. Assuming only a background of standard quantum mechanics and electromagnetic theory, and containing many problems and references, this book will be invaluable to graduate students of quantum optics, as well as to researchers in this field.}"
1571,"evolution of the driving styles of anticipatory agent remotely operating a scaled model of racing car",2380446,"Evolution of the Driving Styles of Anticipatory Agent Remotely Operating a Scaled Model of Racing Car","We present an approach for automated evolutionary design of driving agent, able to remotely operate a scale model of racing car running in a fastest possible way. The agent's actions are conveyed to the car via standard radio control transmitter. The agent perceives the environment from a live video feedback of an overhead camera. In order to cope with the inherent video feed latency, which renders even the straightforward tasks of following simple routes unsolvable, we implement an anticipatory modeling - the agent considers its current actions based on anticipated intrinsic (rather than currently available, outdated) state of the car and its surrounding. The driving style (i.e. the driving line combined with the speed at which the car travels along this line) is first evolved offline on a software simulator of the car and then adapted online to the real world. Experimental results demonstrate that on long runs the agent-operated car is only marginally slower than a human-operated one, while the consistence of lap times posted by the evolved driving style of the agent is better than that of a human. This work can be viewed as a step towards the development of a framework for automated design of the controllers of remotely operated vehicles capable to find an optimal solution to various tasks in different traffic situations and road conditions."
1572,"conformational complexity of gproteincoupled receptors",2393670,"Conformational complexity of G-protein-coupled receptors.","G-protein-coupled receptors (GPCRs) are remarkably versatile signaling molecules. Members of this large family of membrane proteins respond to structurally diverse ligands and mediate most transmembrane signal transduction in response to hormones and neurotransmitters, and in response to the senses of sight, smell and taste. Individual GPCRs can signal through several G-protein subtypes and through G-protein-independent pathways, often in a ligand-specific manner. This functional plasticity can be attributed to structural flexibility of GPCRs and the ability of ligands to induce or to stabilize ligand-specific conformations. Here, we review what has been learned about the dynamic nature of the structure and mechanism of GPCR activation, primarily focusing on spectroscopic studies of purified human [beta]2 adrenergic receptor."
1573,"is learning the nth thing any easier than learning the first",2407908,"Is Learning The $n$-th Thing Any Easier Than Learning The First?","This paper investigates learning in a lifelong context. Lifelong learning addresses situations in which a learner faces a whole stream of learning tasks. Such scenarios provide the opportunity to transfer knowledge across multiple learning tasks, in order to generalize more accurately from less training data. In this paper, several different approaches to lifelong learning are described, and applied in an object recognition domain. It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks. 1"
1574,"modular cell biology retroactivity and insulation",2413970,"Modular cell biology: retroactivity and insulation.","Modularity plays a fundamental role in the prediction of the behavior of a system from the behavior of its components, guaranteeing that the properties of individual components do not change upon interconnection. Just as electrical, hydraulic, and other physical systems often do not display modularity, nor do many biochemical systems, and specifically, genetic networks. Here, we study the effect of interconnections on the input–output dynamic characteristics of transcriptional components, focusing on a property, which we call 'retroactivity', that plays a role analogous to non-zero output impedance in electrical systems. In transcriptional networks, retroactivity is large when the amount of transcription factor is comparable to, or smaller than, the amount of promoter-binding sites, or when the affinity of such binding sites is high. To attenuate the effect of retroactivity, we propose a feedback mechanism inspired by the design of amplifiers in electronics. We introduce, in particular, a mechanism based on a phosphorylation–dephosphorylation cycle. This mechanism enjoys a remarkable insulation property, due to the fast timescales of the phosphorylation and dephosphorylation reactions."
1575,"monolithic microfabricated valves and pumps by multilayer soft lithography",2417721,"Monolithic Microfabricated Valves and Pumps by Multilayer Soft Lithography","Soft lithography is an alternative to silicon-based micromachining that uses replica molding of nontraditional elastomeric materials to fabricate stamps and microfluidic channels. We describe here an extension to the soft lithography paradigm, multilayer soft lithography, with which devices consisting of multiple layers may be fabricated from soft materials. We used this technique to build active microfluidic systems containing on-off valves, switching valves, and pumps entirely out of elastomer. The softness of these materials allows the device areas to be reduced by more than two orders of magnitude compared with silicon-based devices. The other advantages of soft lithography, such as rapid prototyping, ease of fabrication, and biocompatibility, are retained."
1576,"symbiotic gut microbes modulate human metabolic phenotypes",2423543,"Symbiotic gut microbes modulate human metabolic phenotypes.","Humans have evolved intimate symbiotic relationships with a consortium of gut microbes (microbiome) and individual variations in the microbiome influence host health, may be implicated in disease etiology, and affect drug metabolism, toxicity, and efficacy. However, the molecular basis of these microbe–host interactions and the roles of individual bacterial species are obscure. We now demonstrate a""transgenomic"" approach to link gut microbiome and metabolic phenotype (metabotype) variation. We have used a combination of spectroscopic, microbiomic, and multivariate statistical tools to analyze fecal and urinary samples from seven Chinese individuals (sampled twice) and to model the microbial–host metabolic connectivities. At the species level, we found structural differences in the Chinese family gut microbiomes and those reported for American volunteers, which is consistent with population microbial cometabolic differences reported in epidemiological studies. We also introduce the concept of functional metagenomics, defined as ""the characterization of key functional members of the microbiome that most influence host metabolism and hence health."" For example, Faecalibacterium prausnitzii population variation is associated with modulation of eight urinary metabolites of diverse structure, indicating that this species is a highly functionally active member of the microbiome, influencing numerous host pathways. Other species were identified showing different and varied metabolic interactions. Our approach for understanding the dynamic basis of host–microbiome symbiosis provides a foundation for the development of functional metagenomics as a probe of systemic effects of drugs and diet that are of relevance to personal and public health care solutions."
1577,"selection to minimise noise in living systems and its implications for the evolution of gene expression",2466088,"Selection to minimise noise in living systems and its implications for the evolution of gene expression.","Gene expression, like many biological processes, is subject to noise. This noise has been measured on a global scale, but its general importance to the fitness of an organism is unclear. Here, I show that noise in gene expression in yeast has evolved to prevent harmful stochastic variation in the levels of genes that reduce fitness when their expression levels change. Therefore, there has probably been widespread selection to minimise noise in gene expression. Selection to minimise noise, because it results in gene expression that is stable to stochastic variation in cellular components, may also constrain the ability of gene expression to respond to non-stochastic variation. I present evidence that this has indeed been the case in yeast. I therefore conclude that gene expression noise is an important biological trait, and one that probably limits the evolvability of complex living systems."
1578,"strong dispersive coupling of a highfinesse cavity to a micromechanical membrane",2476650,"Strong dispersive coupling of a high-finesse cavity to a micromechanical membrane","Macroscopic mechanical objects and electromagnetic degrees of freedom can couple to each other through radiation pressure. Optomechanical systems in which this coupling is sufficiently strong are predicted to show quantum effects and are a topic of considerable interest. Devices in this regime would offer new types of control over the quantum state of both light and matter1, 2, 3, 4, and would provide a new arena in which to explore the boundary between quantum and classical physics5, 6, 7. Experiments so far have achieved sufficient optomechanical coupling to laser-cool mechanical devices8, 9, 10, 11, 12, but have not yet reached the quantum regime. The outstanding technical challenge in this field is integrating sensitive micromechanical elements (which must be small, light and flexible) into high-finesse cavities (which are typically rigid and massive) without compromising the mechanical or optical properties of either. A second, and more fundamental, challenge is to read out the mechanical element’s energy eigenstate. Displacement measurements (no matter how sensitive) cannot determine an oscillator’s energy eigenstate13, and measurements coupling to quantities other than displacement14, 15, 16 have been difficult to realize in practice. Here we present an optomechanical system that has the potential to resolve both of these challenges. We demonstrate a cavity which is detuned by the motion of a 50-nm-thick dielectric membrane placed between two macroscopic, rigid, high-finesse mirrors. This approach segregates optical and mechanical functionality to physically distinct structures and avoids compromising either. It also allows for direct measurement of the square of the membrane’s displacement, and thus in principle the membrane’s energy eigenstate. We estimate that it should be practical to use this scheme to observe quantum jumps of a mechanical system, an important goal in the field of quantum measurement."
1579,"merging two geneexpression studies via crossplatform normalization",2479514,"Merging two gene-expression studies via cross-platform normalization","Motivation: Gene-expression microarrays are currently being applied in a variety of biomedical applications. This article considers the problem of how to merge datasets arising from different gene-expression studies of a common organism and phenotype. Of particular interest is how to merge data from different technological platforms.  Results: The article makes two contributions to the problem. The first is a simple cross-study normalization method, which is based on linked gene/sample clustering of the given datasets. The second is the introduction and description of several general validation measures that can be used to assess and compare cross-study normalization methods. The proposed normalization method is applied to three existing breast cancer datasets, and is compared to several competing normalization methods using the proposed validation measures.  Availability: The supplementary materials and XPN Matlab code are publicly available at website: https://genome.unc.edu/xpn  Contact: shabalin@email.unc.edu  Supplementary information: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btn083"
1580,"mechanisms of maintenance of species diversity",2506264,"MECHANISMS OF MAINTENANCE OF SPECIES DIVERSITY","▪ Abstract  The focus of most ideas on diversity maintenance is species coexistence, which may be stable or unstable. Stable coexistence can be quantified by the long-term rates at which community members recover from low density. Quantification shows that coexistence mechanisms function in two major ways: They may be (a) equalizing because they tend to minimize average fitness differences between species, or (b) stabilizing because they tend to increase negative intraspecific interactions relative to negative interspecific interactions. Stabilizing mechanisms are essential for species coexistence and include traditional mechanisms such as resource partitioning and frequency-dependent predation, as well as mechanisms that depend on fluctuations in population densities and environmental factors in space and time. Equalizing mechanisms contribute to stable coexistence because they reduce large average fitness inequalities which might negate the effects of stabilizing mechanisms. Models of unstable coexitence, in which species diversity slowly decays over time, have focused almost exclusively on equalizing mechanisms. These models would be more robust if they also included stabilizing mechanisms, which arise in many and varied ways but need not be adequate for full stability of a system. Models of unstable coexistence invite a broader view of diversity maintenance incorporating species turnover."
1581,"how long will it take to fix this bug",2516366,"How Long Will It Take to Fix This Bug?","Predicting the time and effort for a software problem has long been a difficult task. We present an approach that automatically predicts the fixing effort, i.e., the person-hours spent on fixing an issue. Our technique leverages existing issue tracking systems: given a new issue report, we use the Lucene framework to search for similar, earlier reports and use their average time as a prediction. Our approach thus allows for early effort estimation, helping in assigning issues and scheduling stable releases. We evaluated our approach using effort data from the JBoss project. Given a sufficient number of issues reports, our automatic predictions are close to the actual effort; for issues that are bugs, we are off by only one hour, beating nai?ve predictions by a factor of four. © 2007 IEEE."
1582,"network properties of genes harboring inherited disease mutations",2518492,"Network properties of genes harboring inherited disease mutations","10.1073/pnas.0701722105 By analyzing, in parallel, large literature-derived and high-throughput experimental datasets we investigate genes harboring human inherited disease mutations in the context of molecular interaction networks. Our results demonstrate that network properties influence the likelihood and phenotypic consequences of disease mutations. Genes with intermediate connectivities have the highest probability of harboring germ-line disease mutations, suggesting that disease genes tend to occupy an intermediate niche in terms of their physiological and cellular importance. Our analysis of tissue expression profiles supports this view. We show that disease mutations are less likely to occur in essential genes compared with all human genes. Disease genes display significant functional clustering in the analyzed molecular network. For about one-third of known disorders with two or more associated genes we find physical clusters of genes with the same phenotype. These clusters are likely to represent disorder-specific functional modules and suggest a framework for identifying yet-undiscovered disease genes."
1583,"an evaluation of adaptive filtering in the context of realistic taskbased information exploration",2521939,"An evaluation of adaptive filtering in the context of realistic task-based information exploration","Exploratory search increasingly becomes an important research topic. Our interests focus on task-based information exploration, a specific type of exploratory search performed by a range of professional users, such as intelligence analysts. In this paper, we present an evaluation framework designed specifically for assessing and comparing performance of innovative information access tools created to support the work of intelligence analysts in the context of task-based information exploration. The motivation for the development of this framework came from our needs for testing systems in taskbased information exploration, which cannot be satisfied by existing frameworks. The new framework is closely tied with the kind of tasks that intelligence analysts perform: complex, dynamic, and multiple facets and multiple stages. It views the user rather than the information system as the center of the evaluation, and examines how well users are served by the systems in their tasks. The evaluation framework examines the support of the systems at users’ major information access stages, such as information foraging and sense-making. The framework is accompanied by a reference test collection that has 18 tasks scenarios and corresponding passage-level ground truth annotations. To demonstrate the usage of the framework and the reference test collection, we present a specific evaluation study on CAFE´ , an adaptive filtering engine designed for supporting task-based information exploration. This study is a successful use case of the framework, and the study indeed revealed various aspects of the information systems and their roles in supporting task-based information exploration."
1584,"the largedisplay user experience",2526029,"The Large-Display User Experience","As large displays become more affordable, researchers are investigating the effects on productivity, and techniques for making the large-display user experience more effective. Recent work has demonstrated significant productivity benefits, but has also identified numerous usability issues that inhibit productivity. Studies show that larger displays enable users to create and manage many windows, as well as to engage in complex multitasking behavior. This article describes various usability issues, including losing track of the cursor, accessing windows and icons at a distance, dealing with bezels in multimonitor displays, window management, and task management. It also presents several novel techniques that address these issues and make users more productive on large-display systems."
1585,"computational modeling of posttranscriptional gene regulation by micrornas",2527434,"Computational modeling of post-transcriptional gene regulation by microRNAs.","MicroRNAs (miRNAs) have recently emerged as a new complex layer of gene regulation. MiRNAs act post-transcriptionally, influencing the stability, compartmentalization, and translation of their target mRNAs. Computational efforts to understand the post-transcriptional gene regulation by miRNAs have been focused on the target prediction tools, while quantitative kinetic models of gene regulation by miRNAs have so far largely been overlooked. We here develop a kinetic model of post-transcriptional gene regulation by miRNAs, focusing on the miRNAs' effect on increasing the target mRNAs degradation rates. The model is fitted to a temporal microarray dataset where human mRNAs are measured upon transfection with a specific miRNA (miRNA124a). The proposed model exhibits good fit with many target mRNA profiles, indicating that such type of models can be used for studying post-transcriptional gene regulation by miRNA. In particular, the proposed kinetic model can be used for quantifying the miRNA-mediated effects on its targets in the miRNA mis-expression experiments. The model makes an experimentally verifiable prediction of the miRNA124a decay rate, quantifies the miRNA-mediated effect on the target mRNAs degradation, and yields a good correspondence between the inferred and experimentally measured decay rates of human target mRNAs."
1586,"a lightweight approach to semantic annotation of research papers",2535886,"A Lightweight Approach to Semantic Annotation of Research Papers","This paper presents a novel application of a semantic annotation system, named Cerno, to analyze research publications in electronic format. Specifically, we address the problem of providing automatic support for authors who need to deal with large volumes of research documents. To this end, we have developed Biblio, a user-friendly tool based on Cerno. The tool directs the user’s attention to the most important elements of the papers and provides assistance by generating automatically a list of references and an annotated bibliography given a collection of published research articles. The tool performance has been evaluated on a set of papers and preliminary evaluation results are promising. The backend of Biblio uses a standard relational database to store the results."
1587,"analyzing protein interaction networks using structural information",2545283,"Analyzing protein interaction networks using structural information.","Determining protein interaction networks and predicting network changes in time and space are crucial to understanding and modeling a biological system. In the past few years, the combination of experimental and computational tools has allowed great progress toward reaching this goal. Experimental methods include the large-scale determination of protein interactions using two-hybrid or pull-down analysis as well as proteomics. The latter one is especially valuable when changes in protein concentrations over time are recorded. Computational tools include methods to predict and validate protein interactions on the basis of structural information and bioinformatics tools that analyze and integrate data for the same purpose. In this review, we focus on the use of structural information in combination with computational tools to predict new protein interactions, to determine which interactions are compatible with each other, to obtain some functional insight into single and multiple mutations, and to estimate equilibrium and kinetic parameters. Finally, we discuss the importance of establishing criteria to biologically validate protein interactions. Expected final online publication date for the Annual Review of Biochemistry Volume 77 is June 02, 2008. Please see http://www.annualreviews.org/catalog/pubdates.aspx for revised estimates."
1588,"neural correlates of perceptual learning in a sensorymotor but not a sensory cortical area",2547218,"Neural correlates of perceptual learning in a sensory-motor, but not a sensory, cortical area","This study aimed to identify neural mechanisms that underlie perceptual learning in a visual-discrimination task. We trained two monkeys (Macaca mulatta) to determine the direction of visual motion while we recorded from their middle temporal area (MT), which in trained monkeys represents motion information that is used to solve the task, and lateral intraparietal area (LIP), which represents the transformation of motion information into a saccadic choice. During training, improved behavioral sensitivity to weak motion signals was accompanied by changes in motion-driven responses of neurons in LIP, but not in MT. The time course and magnitude of the changes in LIP correlated with the changes in behavioral sensitivity throughout training. Thus, for this task, perceptual learning does not appear to involve improvements in how sensory information is represented in the brain, but rather how the sensory representation is interpreted to form the decision that guides behavior."
1589,"de novo bacterial genome sequencing millions of very short reads assembled on a desktop computer",2547927,"De novo bacterial genome sequencing: Millions of very short reads assembled on a desktop computer","10.1101/gr.072033.107 Novel high-throughput DNA sequencing technologies allow researchers to characterize a bacterial genome during a single experiment and at a moderate cost. However, the increase in sequencing throughput that is allowed by using such platforms is obtained at the expense of individual sequence read length, which must be assembled into longer contigs to be exploitable. This study focuses on the Illumina sequencing platform that produces millions of very short sequences that are 35 bases in length. We propose a de novo assembler software that is dedicated to process such data. Based on a classical overlap graph representation and on the detection of potentially spurious reads, our software generates a set of accurate contigs of several kilobases that cover most of the bacterial genome. The assembly results were validated by comparing data sets that were obtained experimentally for Staphylococcus aureus strain MW2 and Helicobacter acinonychis strain Sheeba with that of their published genomes acquired by conventional sequencing of 1.5- to 3.0-kb fragments. We also provide indications that the broad coverage achieved by high-throughput sequencing might allow for the detection of clonal polymorphisms in the set of DNA molecules being sequenced."
1590,"unequal group variances in microarray data analyses",2568883,"Unequal group variances in microarray data analyses","Motivation: In searching for differentially expressed (DE) genes in microarray data, we often observe a fraction of the genes to have unequal variability between groups. This is not an issue in large samples, where a valid test exists that uses individual variances separately. The problem arises in the small-sample setting, where the approximately valid Welch test lacks sensitivity, while the more sensitive moderated t-test assumes equal variance.  Methods: We introduce a moderated Welch test (MWT) that allows unequal variance between groups. It is based on (i) weighting of pooled and unpooled standard errors and (ii) improved estimation of the gene-level variance that exploits the information from across the genes.  Results: When a non-trivial proportion of genes has unequal variability, false discovery rate (FDR) estimates based on the standard t and moderated t-tests are often too optimistic, while the standard Welch test has low sensitivity. The MWT is shown to (i) perform better than the standard t, the standard Welch and the moderated t-tests when the variances are unequal between groups and (ii) perform similarly to the moderated t, and better than the standard t and Welch tests when the group variances are equal. These results mean that MWT is more reliable than other existing tests over wider range of data conditions.  Availability: R package to perform MWT is available at http://www.meb.ki.se/~yudpaw  Contact: yudi.pawitan@ki.se  Supplementary information: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btn100"
1591,"archives records and power from postmodern theory to archival performance",2580334,"Archives, Records, and Power: From (Postmodern) Theory to (Archival) Performance","This article is the continuationand conclusion of our introduction, as theguest editors, that appeared in the first ofthese two special issues of ArchivalScience, which together are devoted to thetheme, “Archives, Records, and Power.” It argues that, inperforming their work, archivists follow ascript that has been naturalized by the routinerepetition of past practice. They act in waysthat they anticipate their various audienceswould desire. If archival practice is to beinfluenced by the postmodern ideas of theauthors of the essays in these two volumes,then archivists must see that the script,stage, and audiences have changed. Theory andpractice are not opposites, not evenpolarities, but integrated aspects of thearchivist's professional role andresponsibility. Transparency of process aboutthe archivist's performance will facilitatethis integration, stimulate the building ofarchival knowledge, and enable present andfuture generations to hold the professionaccountable for its choices in exercising powerover the making of modern memory."
1592,"semantic web for the working ontologist effective modeling in rdfs and owl",2584166,"Semantic Web for the Working Ontologist: Effective Modeling in RDFS and OWL","{The promise of the Semantic Web to provide a universal medium to exchange data information and knowledge has been well publicized.  There are many sources too for basic information on the extensions to the WWW that permit content to be expressed in natural language yet used by software agents to easily find, share and integrate information. Until now individuals engaged in creating ontologies-- formal descriptions of the concepts, terms, and relationships within a given knowledge domain-- have had no sources beyond the technical standards documents.    <br><br>Semantic Web for the Working Ontologist transforms this information into the practical knowledge that programmers and subject domain experts need.  Authors Allemang and Hendler begin with solutions to the basic problems, but dont stop there:  they demonstrate how to develop your own solutions to problems of increasing complexity and ensure that your skills will keep pace with the continued evolution of the Semantic Web.<br><br> Provides practical information for all programmers and subject matter experts engaged in modeling data to fit the requirements of the Semantic Web.<br> De-emphasizes algorithms and proofs, focusing instead on real-world problems, creative solutions, and highly illustrative examples. <br> Presents detailed, ready-to-apply recipes for use in many specific situations.<br> Shows how to create new recipes from RDF, RDFS, and OWL constructs.}"
1593,"gorouter an rdf model for providing semantic query and inference services for gene ontology and its associations",2587504,"GORouter: an RDF model for providing semantic query and inference services for Gene Ontology and its associations.","BACKGROUND: The most renowned biological ontology, Gene Ontology (GO) is widely used for annotations of genes and gene products of different organisms. However, there are shortcomings in the Resource Description Framework (RDF) data file provided by the GO consortium: 1) Lack of sufficient semantic relationships between pairs of terms coming from the three independent GO sub-ontologies, that limit the power to provide complex semantic queries and inference services based on it. 2) The term-centric view of GO annotation data and the fact that all information is stored in a single file. This makes attempts to retrieve GO annotations based on big volume datasets unmanageable. 3) No support of GOSlim. RESULTS: We propose a RDF model, GORouter, which encodes heterogeneous original data in a uniform RDF format, creates additional ontology mappings between GO terms, and introduces a set of inference rulebases. Furthermore, we use the Oracle Network Data Model (NDM) as the native RDF data repository and the table function RDF_MATCH to seamlessly combine the result of RDF queries with traditional relational data. As a result, the scale of GORouter is minimized; information not directly involved in semantic inference is put into relational tables. CONCLUSION: Our work demonstrates how to use multiple semantic web tools and techniques to provide a mixture of semantic query and inference solutions of GO and its associations. GORouter is licensed under Apache License Version 2.0, and is accessible via the website: http://www.scbit.org/gorouter/."
1594,"nutritional control of reproductive status in honeybees via dna methylation",2601345,"Nutritional Control of Reproductive Status in Honeybees via DNA Methylation","Fertile queens and sterile workers are alternative forms of the adult female honeybee that develop from genetically identical larvae following differential feeding with royal jelly. We show that silencing the expression of DNA methyltransferase Dnmt3, a key driver of epigenetic global reprogramming, in newly hatched larvae led to a royal jelly-like effect on the larval developmental trajectory; the majority of Dnmt3 small interfering RNA-treated individuals emerged as queens with fully developed ovaries. Our results suggest that DNA methylation in Apis is used for storing epigenetic information, that the use of that information can be differentially altered by nutritional input, and that the flexibility of epigenetic modifications underpins, profound shifts in developmental fates, with massive implications for reproductive and behavioral status. 10.1126/science.1153069"
1595,"predictably irrational the hidden forces that shape our decisions",2602869,"Predictably Irrational: The Hidden Forces That Shape Our Decisions","{<p> <ul> <li>Why do our headaches persist after taking a one-cent aspirin but disappear when we take a 50-cent aspirin? </li> </p> <p> <li>Why does recalling the Ten Commandments reduce our tendency to lie, even when we couldn't possibly be caught? </li> </p> <p> <li>Why do we splurge on a lavish meal but cut coupons to save twenty-five cents on a can of soup?</li> </p> <p> <li>Why do we go back for second helpings at the unlimited buffet, even when our stomachs are already full?</li> </p> <p> <li>And how did we ever start spending $4.15 on a cup of coffee when, just a few years ago, we used to pay less than a dollar?</li> </ul> </p> <p> When it comes to making decisions in our lives, we think we're in control. We think we're making smart, rational choices. But are we? </p> <p> In a series of illuminating, often surprising experiments, MIT behavioral economist Dan Ariely refutes the common assumption that we behave in fundamentally rational ways. Blending everyday experience with groundbreaking research, Ariely explains how expectations, emotions, social norms, and other invisible, seemingly illogical forces skew our reasoning abilities. </p> <p> Not only do we make astonishingly simple mistakes every day, but we make the same <i>types</i> of mistakes, Ariely discovers. We consistently overpay, underestimate, and procrastinate. We fail to understand the profound effects of our emotions on what we want, and we overvalue what we already own. Yet these misguided behaviors are neither random nor senseless. They're systematic and predictable—making us <i>predictably</i> irrational. </p> <p> From drinking coffee to losing weight, from buying a car to choosing a romantic partner, Ariely explains how to break through these systematic patterns of thought to make better decisions. <i>Predictably Irrational</i> will change the way we interact with the world—one small decision at a time. </p>}"
1596,"systematic identification of mammalian regulatory motifs target genes and functions",2607936,"Systematic identification of mammalian regulatory motifs' target genes and functions","We developed an algorithm, Lever, that systematically maps metazoan DNA regulatory motifs or motif combinations to sets of genes. Lever assesses whether the motifs are enriched in cis-regulatory modules (CRMs), predicted by our PhylCRM algorithm, in the noncoding sequences surrounding the genes. Lever analysis allows unbiased inference of functional annotations to regulatory motifs and candidate CRMs. We used human myogenic differentiation as a model system to statistically assess greater than 25,000 pairings of gene sets and motifs or motif combinations. We assigned functional annotations to candidate regulatory motifs predicted previously and identified gene sets that are likely to be co-regulated via shared regulatory motifs. Lever allows moving beyond the identification of putative regulatory motifs in mammalian genomes, toward understanding their biological roles. This approach is general and can be applied readily to any cell type, gene expression pattern or organism of interest."
1597,"relevance a review of the literature and a framework for thinking on the notion in information science part ii nature and manifestations of relevance",2616003,"Relevance: A review of the literature and a framework for thinking on the notion in information science. Part {II}: nature and manifestations of relevance","Abstract 10.1002/asi.20682.abs Relevant: Having significant and demonstrable bearing on the matter at hand.*A version of this article has been published in 2006 as a chapter in E.G. Abels & D.A. Nitecki (Eds.), Advances in Librarianship (Vol. 30, pp. 3–71). San Diego: Academic Press. (Saracevic, 2006). Relevance: The ability as of an information retrieval system to retrieve material that satisfies the needs of the user. —Merriam-Webster Dictionary 2005 Relevance is a, if not even the, key notion in information science in general and information retrieval in particular. This two-part critical review traces and synthesizes the scholarship on relevance over the past 30 years and provides an updated framework within which the still widely dissonant ideas and works about relevance might be interpreted and related. It is a continuation and update of a similar review that appeared in 1975 under the same title, considered here as being Part I. The present review is organized into two parts: Part II addresses the questions related to nature and manifestations of relevance, and Part III addresses questions related to relevance behavior and effects. In Part II, the nature of relevance is discussed in terms of meaning ascribed to relevance, theories used or proposed, and models that have been developed. The manifestations of relevance are classified as to several kinds of relevance that form an interdependent system of relevances. In Part III, relevance behavior and effects are synthesized using experimental and observational works that incorporate data. In both parts, each section concludes with a summary that in effect provides an interpretation and synthesis of contemporary thinking on the topic treated or suggests hypotheses for future research. Analyses of some of the major trends that shape relevance work are offered in conclusions."
1598,"protein networks in disease",2620752,"Protein networks in disease.","During a decade of proof-of-principle analysis in model organisms, protein networks have been used to further the study of molecular evolution, to gain insight into the robustness of cells to perturbation, and for assignment of new protein functions. Following these analyses, and with the recent rise of protein interaction measurements in mammals, protein networks are increasingly serving as tools to unravel the molecular basis of disease. We review promising applications of protein networks to disease in four major areas: identifying new disease genes; the study of their network properties; identifying disease-related subnetworks; and network-based disease classification. Applications in infectious disease, personalized medicine, and pharmacology are also forthcoming as the available protein network information improves in quality and coverage."
1599,"robust face recognition via sparse representation",2623645,"Robust Face Recognition via Sparse Representation","We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by \ell^{1}-minimization, we propose a general classification algorithm for (image-based) object recognition. This new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. Unconventional features such as downsampled images and random projections perform just as well as conventional features such as Eigenfaces and Laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. This framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. The theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. We conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims."
1600,"singlemolecule dna sequencing of a viral genome",2630042,"Single-Molecule DNA Sequencing of a Viral Genome","The full promise of human genomics will be realized only when the genomes of thousands of individuals can be sequenced for comparative analysis. A reference sequence enables the use of short read length. We report an amplification-free method for determining the nucleotide sequence of more than 280,000 individual DNA molecules simultaneously. A DNA polymerase adds labeled nucleotides to surface-immobilized primer-template duplexes in stepwise fashion, and the asynchronous growth of individual DNA molecules was monitored by fluorescence imaging. Read lengths of >25 bases and equivalent phred software program quality scores approaching 30 were achieved. We used this method to sequence the M13 virus to an average depth of >150x and with 100% coverage; thus, we resequenced the M13 genome with high-sensitivity mutation detection. This demonstrates a strategy for high-throughput low-cost resequencing. 10.1126/science.1150427"
1601,"entrainment of neuronal oscillations as a mechanism of attentional selection",2647614,"Entrainment of Neuronal Oscillations as a Mechanism of Attentional Selection","Whereas gamma-band neuronal oscillations clearly appear integral to visual attention, the role of lower-frequency oscillations is still being debated. Mounting evidence indicates that a key functional property of these oscillations is the rhythmic shifting of excitability in local neuronal ensembles. Here, we show that when attended stimuli are in a rhythmic stream, delta-band oscillations in the primary visual cortex entrain to the rhythm of the stream, resulting in increased response gain for task-relevant events and decreased reaction times. Because of hierarchical cross-frequency coupling, delta phase also determines momentary power in higher-frequency activity. These instrumental functions of low-frequency oscillations support a conceptual framework that integrates numerous earlier findings. 10.1126/science.1154735"
1602,"understanding the efficiency of social tagging systems using information theory",2647679,"Understanding the Efficiency of Social Tagging Systems using Information Theory","Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software."
1603,"coarsegrained md simulations of membrane proteinbilayer selfassembly",2649220,"Coarse-Grained MD Simulations of Membrane Protein-Bilayer Self-Assembly","Summary Complete determination of a membrane protein structure requires knowledge of the protein position within the lipid bilayer. As the number of determined structures of membrane proteins increases so does the need for computational methods which predict their position in the lipid bilayer. Here we present a coarse-grained molecular dynamics approach to lipid bilayer self-assembly around membrane proteins. We demonstrate that this method can be used to predict accurately the protein position in the bilayer for membrane proteins with a range of different sizes and architectures."
1604,"calorie restriction promotes mammalian cell survival by inducing the sirt deacetylase",2669725,"Calorie Restriction Promotes Mammalian Cell Survival by Inducing the SIRT1 Deacetylase","A major cause of aging is thought to result from the cumulative effects of cell loss over time. In yeast, caloric restriction (CR) delays aging by activating the Sir2 deacetylase. Here we show that expression of mammalian Sir2 (SIRT1) is induced in CR rats as well as in human cells that are treated with serum from these animals. Insulin and insulin-like growth factor 1 (IGF-1) attenuated this response. SIRT1 deacetylates the DNA repair factor Ku70, causing it to sequester the proapoptotic factor Bax away from mitochondria, thereby inhibiting stress-induced apoptotic cell death. Thus, CR could extend life-span by inducing SIRT1 expression and promoting the long-term survival of irreplaceable cells. 10.1126/science.1099196"
1605,"a map of human protein interactions derived from coexpression of human mrnas and their orthologs",2675151,"A map of human protein interactions derived from co-expression of human mRNAs and their orthologs.","The human protein interaction network will offer global insights into the molecular organization of cells and provide a framework for modeling human disease, but the network's large scale demands new approaches. We report a set of 7000 physical associations among human proteins inferred from indirect evidence: the comparison of human mRNA co-expression patterns with those of orthologous genes in five other eukaryotes, which we demonstrate identifies proteins in the same physical complexes. To evaluate the accuracy of the predicted physical associations, we apply quantitative mass spectrometry shotgun proteomics to measure elution profiles of 3013 human proteins during native biochemical fractionation, demonstrating systematically that putative interaction partners tend to co-sediment. We further validate uncharacterized proteins implicated by the associations in ribosome biogenesis, including WBSCR20C, associated with Williams-Beuren syndrome. This meta-analysis therefore exploits non-protein-based data, but successfully predicts associations, including 5589 novel human physical protein associations, with measured accuracies of 54+/-10%, comparable to direct large-scale interaction assays. The new associations' derivation from conserved in vivo phenomena argues strongly for their biological relevance."
1606,"realitybased interaction a framework for postwimp interfaces",2686437,"Reality-based interaction: a framework for post-WIMP interfaces","We are in the midst of an explosion of emerging humancomputer interaction techniques that redefine our understanding of both computers and interaction. We propose the notion of Reality-Based Interaction (RBI) as a unifying concept that ties together a large subset of these emerging interaction styles. Based on this concept of RBI, we provide a framework that can be used to understand, compare, and relate current paths of recent HCI research as well as to analyze specific interaction designs. We believe that viewing interaction through the lens of RBI provides insights for design and uncovers gaps or opportunities for future research. Author Keywords Reality-Based Interaction, interaction styles, virtual reality, ubiquitous computing, tangible interfaces, next-generation, multimodal, context-aware, post-WIMP interfaces."
1607,"crowdsourcing user studies with mechanical turk",2686454,"Crowdsourcing user studies with Mechanical Turk","User studies are important for many aspects of the design process and involve techniques ranging from informal surveys to rigorous laboratory studies. However, the costs involved in engaging users often requires practitioners to trade off between sample size, time requirements, and monetary costs. Micro-task markets, such as Amazon's Mechanical Turk, offer a potential paradigm for engaging a large number of users for low time and monetary costs. Here we investigate the utility of a micro-task market for collecting user measurements, and discuss design considerations for developing remote micro user evaluation tasks. Although micro-task markets have great potential for rapidly collecting user measurements at low costs, we found that special care is needed in formulating tasks in order to harness the capabilities of the approach."
1608,"current methods in medical image segmentation",2724724,"Current Methods in Medical Image Segmentation","Image segmentation plays a crucial role in many medical imaging applications by automating or facilitating the delineation of anatomical structures and other regions of interest. We present herein a critical appraisal of the current status of semi-automated and automated methods for the segmentation of anatomical medical images. Current segmentation approaches are reviewed with an emphasis placed on revealing the advantages and disadvantages of these methods for medical imaging applications. The use of image segmentation in different imaging modalities is also described along with the difficulties encountered in each modality. We conclude with a discussion on the future of image segmentation methods in biomedical research."
1609,"pagerank for product image search",2729940,"PageRank for Product Image Search","In this paper, we cast the image-ranking problem into the task of identifying “authority” nodes on an inferred visual similarity graph and propose an algorithm to analyze the visual link structure that can be created among a group of images. Through an iterative procedure based on the PageRank computation, a numerical weight is assigned to each image; this measures its relative importance to the other images being considered. The incorporation of visual signals in this process differs from the majority of large- scale commercial-search engines in use today. Commercial search-engines often solely rely on the text clues of the pages in which images are embedded to rank images, and often en- tirely ignore the content of the images themselves as a rank- ing signal. To quantify the performance of our approach in a real-world system, we conducted a series of experiments based on the task of retrieving images for 2000 of the most popular products queries. Our experimental results show significant improvement, in terms of user satisfaction and relevancy, in comparison to the most recent Google Image Search results."
1610,"interpreting principal component analyses of spatial population genetic variation",2730940,"Interpreting principal component analyses of spatial population genetic variation","Nearly 30 years ago, Cavalli-Sforza et al. pioneered the use of principal component analysis (PCA) in population genetics and used PCA to produce maps summarizing human genetic variation across continental regions1. They interpreted gradient and wave patterns in these maps as signatures of specific migration events1, 2, 3. These interpretations have been controversial4, 5, 6, 7, but influential8, and the use of PCA has become widespread in analysis of population genetics data9, 10, 11, 12, 13. However, the behavior of PCA for genetic data showing continuous spatial variation, such as might exist within human continental groups, has been less well characterized. Here, we find that gradients and waves observed in Cavalli-Sforza et al.'s maps resemble sinusoidal mathematical artifacts that arise generally when PCA is applied to spatial data, implying that the patterns do not necessarily reflect specific migration events. Our findings aid interpretation of PCA results and suggest how PCA can help correct for continuous population structure in association studies."
1611,"lifting the veil improving accountability and social transparency in wikipedia with wikidashboard",2733029,"Lifting the veil: improving accountability and social transparency in Wikipedia with wikidashboard","Wikis are collaborative systems in which virtually anyone can edit anything. Although wikis have become highly popular in many domains, their mutable nature often leads them to be distrusted as a reliable source of information. Here we describe a social dynamic analysis tool called {WikiDashboard} which aims to improve social transparency and accountability on Wikipedia articles. Early reactions from users suggest that the increased transparency afforded by the tool can improve the interpretation, communication, and trustworthiness of Wikipedia articles."
1612,"binning sequences using very sparse labels within a metagenome",2733191,"Binning sequences using very sparse labels within a metagenome","BACKGROUND:In metagenomic studies, a process called binning is necessary to assign contigs that belong to multiple species to their respective phylogenetic groups. Most of the current methods of binning, such as BLAST, k-mer and PhyloPythia, involve assigning sequence fragments by comparing sequence similarity or sequence composition with already-sequenced genomes that are still far from comprehensive. We propose a semi-supervised seeding method for binning that does not depend on knowledge of completed genomes. Instead, it extracts the flanking sequences of highly conserved 16S rRNA from the metagenome and uses them as seeds (labels) to assign other reads based on their compositional similarity.RESULTS:The proposed seeding method is implemented on an unsupervised Growing Self-Organising Map (GSOM), and called Seeded GSOM (S-GSOM). We compared it with four well-known semi-supervised learning methods in a preliminary test, separating random-length prokaryotic sequence fragments sampled from the NCBI genome database. We identified the flanking sequences of the highly conserved 16S rRNA as suitable seeds that could be used to group the sequence fragments according to their species. S-GSOM showed superior performance compared to the semi-supervised methods tested. Additionally, S-GSOM may also be used to visually identify some species that do not have seeds.The proposed method was then applied to simulated metagenomic datasets using two different confidence threshold settings and compared with PhyloPythia, k-mer and BLAST. At the reference taxonomic level Order, S-GSOM outperformed all k-mer and BLAST results and showed comparable results with PhyloPythia for each of the corresponding confidence settings, where S-GSOM performed better than PhyloPythia in the [greater than or equal to] 10 reads datasets and comparable in the [greater than or equal to] 8 kb benchmark tests.CONCLUSION:In the task of binning using semi-supervised learning methods, results indicate S-GSOM to be the best of the methods tested. Most importantly, the proposed method does not require knowledge from known genomes and uses only very few labels (one per species is sufficient in most cases), which are extracted from the metagenome itself. These advantages make it a very attractive binning method. S-GSOM outperformed the binning methods that depend on already-sequenced genomes, and compares well to the current most advanced binning method, PhyloPythia."
1613,"strategic management of technology and innovation",2739782,"Strategic Management of Technology and Innovation","The 5th Edition of Strategic Management of Technology and Innovation by Burgelman, Christensen, and Wheelwright continues its unmatched tradition of market leadership, by using a combination of text, readings, and cases to bring to life the latest business research on these critical business challenges. Strategic Management of Technology and Innovation takes the perspective of the general manager at the product line, business unit, and corporate levels. The book not only examines each of these levels in some detail, but also addresses the interaction between the different levels of general management - for example, the fit between product strategy and business unit strategy, and the link between business and corporate level technology strategy. Each part of the book starts with an introductory chapter laying out an overall framework and offering a brief discussion of key tools and findings from existing literature. The remainder of each part offers a selected handful of seminar readings and case studies. Almost all of the cases deal with recent events and situations, including several that are concerned with the impact of the Internet. A few ""classics"" have been retained, however, because they capture a timeless issue or problem in such a definitive way that the historical date of their writing is irrelevant."
1614,"the missing memristor found",2739855,"The missing memristor found","Anyone who ever took an electronics laboratory class will be familiar with the fundamental passive circuit elements: the resistor, the capacitor and the inductor. However, in 1971 Leon Chua reasoned from symmetry arguments that there should be a fourth fundamental element, which he called a memristor (short for memory resistor)1. Although he showed that such an element has many interesting and valuable circuit properties, until now no one has presented either a useful physical model or an example of a memristor. Here we show, using a simple analytical example, that memristance arises naturally in nanoscale systems in which solid-state electronic and ionic transport are coupled under an external bias voltage. These results serve as the foundation for understanding a wide range of hysteretic current–voltage behaviour observed in many nanoscale electronic devices2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19 that involve the motion of charged atomic or molecular species, in particular certain titanium dioxide cross-point switches20, 21, 22."
1615,"usability evaluation considered harmful some of the time",2752882,"Usability evaluation considered harmful (some of the time)","Current practice in Human Computer Interaction as encouraged by educational institutes, academic review processes, and institutions with usability groups advocate usability evaluation as a critical part of every design process. This is for good reason: usability evaluation has a significant role to play when conditions warrant it. Yet evaluation can be ineffective and even harmful if naively done 'by rule' rather than 'by thought'. If done during early stage design, it can mute creative ideas that do not conform to current interface norms. If done to test radical innovations, the many interface issues that would likely arise from an immature technology can quash what could have been an inspired vision. If done to validate an academic prototype, it may incorrectly suggest a design's scientific worthiness rather than offer a meaningful critique of how it would be adopted and used in everyday practice. If done without regard to how cultures adopt technology over time, then today's reluctant reactions by users will forestall tomorrow's eager acceptance. The choice of evaluation methodology - if any - must arise from and be appropriate for the actual problem or research question under consideration."
1616,"mapping the genetic architecture of gene expression in human liver",2760213,"Mapping the Genetic Architecture of Gene Expression in Human Liver","Genetic variants that are associated with common human diseases do not lead directly to disease, but instead act on intermediate, molecular phenotypes that in turn induce changes in higher-order disease traits. Therefore, identifying the molecular phenotypes that vary in response to changes in DNA and that also associate with changes in disease traits has the potential to provide the functional information required to not only identify and validate the susceptibility genes that are directly affected by changes in DNA, but also to understand the molecular networks in which such genes operate and how changes in these networks lead to changes in disease traits. Toward that end, we profiled more than 39,000 transcripts and we genotyped 782,476 unique single nucleotide polymorphisms (SNPs) in more than 400 human liver samples to characterize the genetic architecture of gene expression in the human liver, a metabolically active tissue that is important in a number of common human diseases, including obesity, diabetes, and atherosclerosis. This genome-wide association study of gene expression resulted in the detection of more than 6,000 associations between SNP genotypes and liver gene expression traits, where many of the corresponding genes identified have already been implicated in a number of human diseases. The utility of these data for elucidating the causes of common human diseases is demonstrated by integrating them with genotypic and expression data from other human and mouse populations. This provides much-needed functional support for the candidate susceptibility genes being identified at a growing number of genetic loci that have been identified as key drivers of disease from genome-wide association studies of disease. By using an integrative genomics approach, we highlight how the gene RPS26 and not ERBB3 is supported by our data as the most likely susceptibility gene for a novel type 1 diabetes locus recently identified in a large-scale, genome-wide association study. We also identify SORT1 and CELSR2 as candidate susceptibility genes for a locus recently associated with coronary artery disease and plasma low-density lipoprotein cholesterol levels in the process."
1617,"bayesian biclustering of gene expression data",2762934,"Bayesian biclustering of gene expression data.","BACKGROUND: Biclustering of gene expression data searches for local patterns of gene expression. A bicluster (or a two-way cluster) is defined as a set of genes whose expression profiles are mutually similar within a subset of experimental conditions/samples. Although several biclustering algorithms have been studied, few are based on rigorous statistical models. RESULTS: We developed a Bayesian biclustering model (BBC), and implemented a Gibbs sampling procedure for its statistical inference. We showed that Bayesian biclustering model can correctly identify multiple clusters of gene expression data. Using simulated data both from the model and with realistic characters, we demonstrated the BBC algorithm outperforms other methods in both robustness and accuracy. We also showed that the model is stable for two normalization methods, the interquartile range normalization and the smallest quartile range normalization. Applying the BBC algorithm to the yeast expression data, we observed that majority of the biclusters we found are supported by significant biological evidences, such as enrichments of gene functions and transcription factor binding sites in the corresponding promoter sequences. CONCLUSIONS: The BBC algorithm is shown to be a robust model-based biclustering method that can discover biologically significant gene-condition clusters in microarray data. The BBC model can easily handle missing data via Monte Carlo imputation and has the potential to be extended to integrated study of gene transcription networks."
1618,"predictive behavior within microbial genetic networks",2773606,"Predictive behavior within microbial genetic networks.","The homeostatic framework has dominated our understanding of cellular physiology. We question whether homeostasis alone adequately explains microbial responses to environmental stimuli, and explore the capacity of intracellular networks for predictive behavior in a fashion similar to metazoan nervous systems. We show that in silico biochemical networks, evolving randomly under precisely defined complex habitats, capture the dynamical, multidimensional structure of diverse environments by forming internal representations that allow prediction of environmental change. We provide evidence for such anticipatory behavior by revealing striking correlations of Escherichia coli transcriptional responses to temperature and oxygen perturbations--precisely mirroring the covariation of these parameters upon transitions between the outside world and the mammalian gastrointestinal tract. We further show that these internal correlations reflect a true associative learning paradigm, because they show rapid decoupling upon exposure to novel environments. 10.1126/science.1154456"
1619,"assessment of disease named entity recognition on a corpus of annotated sentences",2775897,"Assessment of disease named entity recognition on a corpus of annotated sentences.","BACKGROUND: In recent years, the recognition of semantic types from the biomedical scientific literature has been focused on named entities like protein and gene names (PGNs) and gene ontology terms (GO terms). Other semantic types like diseases have not received the same level of attention. Different solutions have been proposed to identify disease named entities in the scientific literature. While matching the terminology with language patterns suffers from low recall (e.g., Whatizit) other solutions make use of morpho-syntactic features to better cover the full scope of terminological variability (e.g., MetaMap). Currently, MetaMap that is provided from the National Library of Medicine (NLM) is the state of the art solution for the annotation of concepts from UMLS (Unified Medical Language System) in the literature. Nonetheless, its performance has not yet been assessed on an annotated corpus. In addition, little effort has been invested so far to generate an annotated dataset that links disease entities in text to disease entries in a database, thesaurus or ontology and that could serve as a gold standard to benchmark text mining solutions. RESULTS: As part of our research work, we have taken a corpus that has been delivered in the past for the identification of associations of genes to diseases based on the UMLS Metathesaurus and we have reprocessed and re-annotated the corpus. We have gathered annotations for disease entities from two curators, analyzed their disagreement (0.51 in the kappa-statistic) and composed a single annotated corpus for public use. Thereafter, three solutions for disease named entity recognition including MetaMap have been applied to the corpus to automatically annotate it with UMLS Metathesaurus concepts. The resulting annotations have been benchmarked to compare their performance. CONCLUSIONS: The annotated corpus is publicly available at ftp://ftp.ebi.ac.uk/pub/software/textmining/corpora/diseases and can serve as a benchmark to other systems. In addition, we found that dictionary look-up already provides competitive results indicating that the use of disease terminology is highly standardized throughout the terminologies and the literature. MetaMap generates precise results at the expense of insufficient recall while our statistical method obtains better recall at a lower precision rate. Even better results in terms of precision are achieved by combining at least two of the three methods leading, but this approach again lowers recall. Altogether, our analysis gives a better understanding of the complexity of disease annotations in the literature. MetaMap and the dictionary based approach are available through the Whatizit web service infrastructure (Rebholz-Schuhmann D, Arregui M, Gaudan S, Kirsch H, Jimeno A: Text processing through Web services: Calling Whatizit. Bioinformatics 2008, 24:296-298)."
1620,"what medical educators need to know about web",2793908,"What medical educators need to know about ""Web 2.0"".","‘‘Web 2.0’’ describes a collection of web-based technologies which share a user-focused approach to design and functionality, where users actively participate in content creation and editing through open collaboration between members of communities of practice. The current generation of students in medical school made Web 2.0 websites such as Facebook and MySpace some of the most popular on the Internet. Medical educators and designers of educational software applications can benefit from understanding and applying Web 2.0 concepts to the curriculum and related websites. Health science schools have begun experimenting with wikis, blogs and other Web 2.0 applications and have identified both advantages and potential problems with these relatively open, student-focused communication tools. This paper reviews the unique features of Web 2.0 technologies, addresses questions regarding potential pitfalls and suggests valuable applications in health science education."
1621,"patman rapid alignment of short sequences to large databases",2796032,"{PatMaN: rapid alignment of short sequences to large databases}","Summary: We present a tool suited for searching for many short nucleotide sequences in large databases, allowing for a predefined number of gaps and mismatches. The commandline-driven program implements a non-deterministic automata matching algorithm on a keyword tree of the search strings. Both queries with and without ambiguity codes can be searched. Search time is short for perfect matches, and retrieval time rises exponentially with the number of edits allowed.  Availability: The C++ source code for PatMaN is distributed under the GNU General Public License and has been tested on the GNU/Linux operating system. It is available from http://bioinf.eva.mpg.de/patman.  Contact: pruefer@eva.mpg.de  Supplementary information: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btn223"
1622,"discerning static and causal interactions in genomewide reverse engineering problems",2800564,"Discerning static and causal interactions in genome-wide reverse engineering problems.","Background: In the past years devising methods for discovering gene regulatory mechanisms at a genome-wide level has become a fundamental topic in the field of systems biology. The aim is to infer gene-gene interactions in an increasingly sophisticated and reliable way through the continuous improvement of reverse engineering algorithms exploiting microarray data. Motivation: This work is inspired by the several studies suggesting that coexpression is mostly related to static' stable binding relationships, like belonging to the same protein complex, rather than other types of interactions more of a causal' and transient nature (e.g. transcription factor-binding site interactions). The aim of this work is to verify if direct or conditional network inference algorithms (e.g. Pearson correlation for the former, partial Pearson correlation for the latter) are indeed useful in discerning static from causal dependencies in artificial and real gene networks (derived from Escherichia coli and Saccharomyces cerevisiae). Results: Even in the regime of weak inference power we have to work in, our analysis confirms the differences in the performances of the algorithms: direct methods are more robust in detecting stable interactions, conditional ones are better for causal interactions especially in presence of combinatorial transcriptional regulation. Contact: altafini@sissa.it Supplementary information: Supplementary data are available at Bioinformatics online."
1623,"polysearch a webbased text mining system for extracting relationships between human diseases genes mutations drugs and metabolites",2805405,"PolySearch: a web-based text mining system for extracting relationships between human diseases, genes, mutations, drugs and metabolites.","A particular challenge in biomedical text mining is to find ways of handling 'comprehensive' or 'associative' queries such as 'Find all genes associated with breast cancer'. Given that many queries in genomics, proteomics or metabolomics involve these kind of comprehensive searches we believe that a web-based tool that could support these searches would be quite useful. In response to this need, we have developed the PolySearch web server. PolySearch supports >50 different classes of queries against nearly a dozen different types of text, scientific abstract or bioinformatic databases. The typical query supported by PolySearch is 'Given X, find all Y's' where X or Y can be diseases, tissues, cell compartments, gene/protein names, SNPs, mutations, drugs and metabolites. PolySearch also exploits a variety of techniques in text mining and information retrieval to identify, highlight and rank informative abstracts, paragraphs or sentences. PolySearch's performance has been assessed in tasks such as gene synonym identification, protein-protein interaction identification and disease gene identification using a variety of manually assembled 'gold standard' text corpuses. Its f-measure on these tasks is 88, 81 and 79%, respectively. These values are between 5 and 50% better than other published tools. The server is freely available at http://wishart.biology.ualberta.ca/polysearch."
1624,"microbiology in the postgenomic era",2807037,"Microbiology in the post-genomic era","Genomics has revolutionized every aspect of microbiology. Now, 13 years after the first bacterial genome was sequenced, it is important to pause and consider what has changed in microbiology research as a consequence of genomics. In this article, we review the evolving field of bacterial typing and the genomic technologies that enable comparative analysis of multiple genomes and the metagenomes of complex microbial environments, and address the implications of the genomic era for the future of microbiology."
1625,"dynamic analysis of integrated signaling metabolic and regulatory networks",2810294,"Dynamic Analysis of Integrated Signaling, Metabolic, and Regulatory Networks","Extracellular cues affect signaling, metabolic, and regulatory processes to elicit cellular responses. Although intracellular signaling, metabolic, and regulatory networks are highly integrated, previous analyses have largely focused on independent processes (e.g., metabolism) without considering the interplay that exists among them. However, there is evidence that many diseases arise from multifunctional components with roles throughout signaling, metabolic, and regulatory networks. Therefore, in this study, we propose a flux balance analysis (FBA)â€“based strategy, referred to as integrated dynamic FBA (idFBA), that dynamically simulates cellular phenotypes arising from integrated networks. The idFBA framework requires an integrated stoichiometric reconstruction of signaling, metabolic, and regulatory processes. It assumes quasi-steady-state conditions for â€œfastâ€� reactions and incorporates â€œslowâ€� reactions into the stoichiometric formalism in a time-delayed manner. To assess the efficacy of idFBA, we developed a prototypic integrated system comprising signaling, metabolic, and regulatory processes with network features characteristic of actual systems and incorporating kinetic parameters based on typical time scales observed in literature. idFBA was applied to the prototypic system, which was evaluated for different environments and gene regulatory rules. In addition, we applied the idFBA framework in a similar manner to a representative module of the single-cell eukaryotic organism Saccharomyces cerevisiae . Ultimately, idFBA facilitated quantitative, dynamic analysis of systemic effects of extracellular cues on cellular phenotypes and generated comparable time-course predictions when contrasted with an equivalent kinetic model. Since idFBA solves a linear programming problem and does not require an exhaustive list of detailed kinetic parameters, it may be efficiently scaled to integrated intracellular systems that incorporate signaling, metabolic, and regulatory processes at the genome scale, such as the S. cerevisiae system presented here."
1626,"biosensing with plasmonic nanosensors",2841481,"Biosensing with plasmonic nanosensors","Recent developments have greatly improved the sensitivity of optical sensors based on metal nanoparticle arrays and single nanoparticles. We introduce the localized surface plasmon resonance (LSPR) sensor and describe how its exquisite sensitivity to size, shape and environment can be harnessed to detect molecular binding events and changes in molecular conformation. We then describe recent progress in three areas representing the most significant challenges: pushing sensitivity towards the single-molecule detection limit, combining LSPR with complementary molecular identification techniques such as surface-enhanced Raman spectroscopy, and practical development of sensors and instrumentation for routine use and high-throughput detection. This review highlights several exceptionally promising research directions and discusses how diverse applications of plasmonic nanoparticles can be integrated in the near future."
1627,"a robust framework for detecting structural variations in a genome",2841667,"A Robust Framework for Detecting Structural Variations in a Genome","Motivation: Recently, structural genomic variants have come to the forefront as a significant source of variation in the human population, but the identification of these variants in a large genome remains a challenge. The complete sequencing of a human individual is prohibitive at current costs, while current polymorphism detection technologies, such as SNP arrays, are not able to identify many of the large scale events. One of the most promising methods to detect such variants is the computational mapping of clone-end sequences to a reference genome.  Results: Here, we present a probabilistic framework for the identification of structural variants using clone-end sequencing. Unlike previous methods, our approach does not rely on an a priori determined mapping of all reads to the reference. Instead, we build a framework for finding the most probable assignment of sequenced clones to potential structural variants based on the other clones. We compare our predictions with the structural variants identified in three previous studies. While there is a statistically significant correlation between the predictions, we also find a significant number of previously uncharacterized structural variants. Furthermore, we identify a number of putative cross-chromosomal events, primarily located proximally to the centromeres of the chromosomes.  Availability: Our dataset, results and source code are available at http://compbio.cs.toronto.edu/structvar/  Contact:seunghak@cs.toronto.edu,echeran@cs.toronto.edu,brudno@cs.toronto.edu 10.1093/bioinformatics/btn176"
1628,"a phase diagram for jammed matter",2844577,"A phase diagram for jammed matter","{The problem of finding the most efficient way to pack spheres has a long history, dating back to the crystalline arrays conjectured(1) by Kepler and the random geometries explored(2) by Bernal. Apart from its mathematical interest, the problem has practical relevance(3) in a wide range of fields, from granular processing to fruit packing. There are currently numerous experiments showing that the loosest way to pack spheres ( random loose packing) gives a density of similar to 55 per cent(4-6). On the other hand, the most compact way to pack spheres ( random close packing) results in a maximum density of similar to 64 per cent(2,4,6). Although these values seem to be robust, there is as yet no physical interpretation for them. Here we present a statistical description of jammed states(7) in which random close packing can be interpreted as the ground state of the ensemble of jammed matter. Our approach demonstrates that random packings of hard spheres in three dimensions cannot exceed a density limit of similar to 63.4 per cent. We construct a phase diagram that provides a unified view of the hard- sphere packing problem and illuminates various data, including the random-loose-packed state.}"
1629,"a practical guide to singlemolecule fret",2845944,"A practical guide to single-molecule FRET","Single-molecule fluorescence resonance energy transfer (smFRET) is one of the most general and adaptable single-molecule techniques. Despite the explosive growth in the application of smFRET to answer biological questions in the last decade, the technique has been practiced mostly by biophysicists. We provide a practical guide to using smFRET, focusing on the study of immobilized molecules that allow measurements of single-molecule reaction trajectories from 1 ms to many minutes. We discuss issues a biologist must consider to conduct successful smFRET experiments, including experimental design, sample preparation, single-molecule detection and data analysis. We also describe how a smFRET-capable instrument can be built at a reasonable cost with off-the-shelf components and operated reliably using well-established protocols and freely available software."
1630,"assessing the evolutionary impact of amino acid mutations in the human genome",2851066,"Assessing the Evolutionary Impact of Amino Acid Mutations in the Human Genome","Quantifying the distribution of fitness effects among newly arising mutations in the human genome is key to resolving important debates in medical and evolutionary genetics. Here, we present a method for inferring this distribution using Single Nucleotide Polymorphism (SNP) data from a population with non-stationary demographic history (such as that of modern humans). Application of our method to 47,576 coding SNPs found by direct resequencing of 11,404 protein coding-genes in 35 individuals (20 European Americans and 15 African Americans) allows us to assess the relative contribution of demographic and selective effects to patterning amino acid variation in the human genome. We find evidence of an ancient population expansion in the sample with African ancestry and a relatively recent bottleneck in the sample with European ancestry. After accounting for these demographic effects, we find strong evidence for great variability in the selective effects of new amino acid replacing mutations. In both populations, the patterns of variation are consistent with a leptokurtic distribution of selection coefficients (e.g., gamma or log-normal) peaked near neutrality. Specifically, we predict 27‚Äì29% of amino acid changing (nonsynonymous) mutations are neutral or nearly neutral ( s <0.01%), 30‚Äì42% are moderately deleterious (0.01%< s <1%), and nearly all the remainder are highly deleterious or lethal ( s >1%). Our results are consistent with 10‚Äì20% of amino acid differences between humans and chimpanzees having been fixed by positive selection with the remainder of differences being neutral or nearly neutral. Our analysis also predicts that many of the alleles identified via whole-genome association mapping may be selectively neutral or (formerly) positively selected, implying that deleterious genetic variation affecting disease phenotype may be missed by this widely used approach for mapping genes underlying complex traits."
1631,"what is a gene an updated operational definition",2853154,"What is a gene? An updated operational definition","A crucial pre-requisite for large-scale annotation of eukaryotic genomes is the definition of what constitutes a gene. This issue is addressed here in the light of novel and surprising gene features that have recently emerged from large-scale genomic and transcriptomic analyses. The updated operational definition proposed here is: “a gene is a discrete genomic region whose transcription is regulated by one or more promoters and distal regulatory elements and which contains the information for the synthesis of functional proteins or non-coding RNAs, related by the sharing of a portion of genetic information at the level of the ultimate products (proteins or RNAs)”. This definition is specifically designed for eukaryotic chromosomal genes and emphasizes the commonality of the genetic material that gives rise to final, functional products (ncRNAs or proteins) derived from a single gene. It may be useful in several applications and should help in the provision of a comprehensive inventory of the genes of a given organism, finally allowing answers to the basic question of “how many genes” are encoded in its genome."
1632,"new knowledge from old data the role of standards in the sharing and reuse of ecological data",2855355,"New Knowledge from Old Data: The Role of Standards in the Sharing and Reuse of Ecological Data","This article analyzes the experiences of ecologists who used data they did not collect themselves. Specifically, the author examines the processes by which ecologists understand and assess the quality of the data they reuse, and investigates the role that standard methods of data collection play in these processes. Standardization is one means by which scientific knowledge is transported from local to public spheres. While standards can be helpful, the results show that knowledge of the local context is critical to ecologists' reuse of data. Yet, this information is often left behind as data move from the private to the public world. The knowledge that ecologists acquire through fieldwork enables them to recover the local details that are so critical to their comprehension of data collected by others. Social processes also play a role in ecologists' efforts to judge the quality of data they reuse. 10.1177/0162243907306704"
1633,"stem cell transcriptome profiling via massivescale mrna sequencing",2855780,"Stem cell transcriptome profiling via massive-scale mRNA sequencing","We developed a massive-scale RNA sequencing protocol, short quantitative random RNA libraries or SQRL, to survey the complexity, dynamics and sequence content of transcriptomes in a near-complete fashion. This method generates directional, random-primed, linear cDNA libraries that are optimized for next-generation short-tag sequencing. We surveyed the poly(A)(+) transcriptomes of undifferentiated mouse embryonic stem cells (ESCs) and embryoid bodies (EBs) at an unprecedented depth (10 Gb), using the Applied Biosystems SOLiD technology. These libraries capture the genomic landscape of expression, state-specific expression, single-nucleotide polymorphisms (SNPs), the transcriptional activity of repeat elements, and both known and new alternative splicing events. We investigated the impact of transcriptional complexity on current models of key signaling pathways controlling ESC pluripotency and differentiation, highlighting how SQRL can be used to characterize transcriptome content and dynamics in a quantitative and reproducible manner, and suggesting that our understanding of transcriptional complexity is far from complete."
1634,"internetbased tools for communication and collaboration in chemistry",2857666,"Internet-based tools for communication and collaboration in chemistry","{Web-based technologies, coupled with a drive for improved communication between scientists, have resulted in the proliferation of scientific opinion, data and knowledge at an ever-increasing rate. The availability of tools to host wikis and blogs has provided the necessary building blocks for scientists with only a rudimentary understanding of computer software science to communicate to the masses. This newfound freedom has the ability to speed up research and sharing of results, develop extensive collaborations, conduct science in public, and in near-real time. The technologies supporting chemistry, while immature, are fast developing to support chemical structures and reactions, analytical data support and integration to related data sources via supporting software technologies. Communication in chemistry is already witnessing a new revolution.}"
1635,"objectbased auditory and visual attention",2859483,"Object-based auditory and visual attention"," Theories of visual attention argue that attention operates on perceptual objects, and thus that interactions between object formation and selective attention determine how competing sources interfere with perception. In auditory perception, theories of attention are less mature and no comprehensive framework exists to explain how attention influences perceptual abilities. However, the same principles that govern visual perception can explain many seemingly disparate auditory phenomena. In particular, many recent studies of ‘informational masking’ can be explained by failures of either auditory object formation or auditory object selection. This similarity suggests that the same neural mechanisms control attention and influence perception across different sensory modalities."
1636,"forms of talk",2862245,"Forms of Talk","""What makes [Goffman] compelling is not just the aptness of his observations or the rigors of his theoretical scheme but also his considerable gifts for rendering the everyday as bizarre and amusing.""-New York Times Forms of Talk extends Erving Goffman's interactional analyses of face-to-face communication to ordinary conversations and vebal exchanges. In this, his most sociolinguistic work, Goffman relates to certain forms of talk some of the issues that concerned him in his work on frame analysis. This book brings together five of Goffman's essays: ""Replies and Responses,"" ""Response Cries,"" ""Footing,"" ""The Lecture,"" and ""Radio Talk."" Of lasting value in Goffman's work is his insistence that behavior-verbal or nonverbal-be examined along with the context of that behavior. In all of these classic essays, there is a ""topic"" at hand for discussion and analysis. In addition, as those familiar with Goffman's work have come to expect, there is the wider context in which the topic can be viewed and related to other topics-a characteristic move of Goffman's that has made his work so necessary for students of interaction in many disciplines."
1637,"large scale multiple kernel learning",2865447,"Large Scale Multiple Kernel Learning","While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classification, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classification. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with  sparse  feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox &lt;tt&gt;SHOGUN&lt;/tt&gt; for which the source code is publicly available at &lt;tt&gt;http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun&lt;/tt&gt;."
1638,"historical contingency and the evolution of a key innovation in an experimental population of escherichia coli",2865708,"Historical contingency and the evolution of a key innovation in an experimental population of Escherichia coli","10.1073/pnas.0803151105 The role of historical contingency in evolution has been much debated, but rarely tested. Twelve initially identical populations of Escherichia coli were founded in 1988 to investigate this issue. They have since evolved in a glucose-limited medium that also contains citrate, which E. coli cannot use as a carbon source under oxic conditions. No population evolved the capacity to exploit citrate for >30,000 generations, although each population tested billions of mutations. A citrate-using (Cit+) variant finally evolved in one population by 31,500 generations, causing an increase in population size and diversity. The long-delayed and unique evolution of this function might indicate the involvement of some extremely rare mutation. Alternately, it may involve an ordinary mutation, but one whose physical occurrence or phenotypic expression is contingent on prior mutations in that population. We tested these hypotheses in experiments that “replayed” evolution from different points in that population's history. We observed no Cit+ mutants among 8.4 × 1012 ancestral cells, nor among 9 × 1012 cells from 60 clones sampled in the first 15,000 generations. However, we observed a significantly greater tendency for later clones to evolve Cit+, indicating that some potentiating mutation arose by 20,000 generations. This potentiating change increased the mutation rate to Cit+ but did not cause generalized hypermutability. Thus, the evolution of this phenotype was contingent on the particular history of that population. More generally, we suggest that historical contingency is especially important when it facilitates the evolution of key innovations that are not easily evolved by gradual, cumulative selection."
1639,"pagerank without hyperlinks reranking with pubmed related article networks for biomedical text retrieval",2870248,"PageRank without hyperlinks: reranking with PubMed related article networks for biomedical text retrieval.","ABSTRACT: BACKGROUND: Graph analysis algorithms such as PageRank and HITS have been successful in Web environments because they are able to extract important inter-document relationships from manually-created hyperlinks. We consider the application of these techniques to biomedical text retrieval. In the current PubMed search interface, a MEDLINE citation is connected to a number of related citations, which are in turn connected to other citations. Thus, a MEDLINE record represents a node in a vast content-similarity network. This article explores the hypothesis that these networks can be exploited for text retrieval, in the same manner as hyperlink graphs on the Web. RESULTS: We conducted a number of reranking experiments using the TREC 2005 genomics track test collection in which scores extracted from PageRank and HITS analysis were combined with scores returned by an off-the-shelf retrieval engine. Experiments demonstrate that incorporating PageRank scores yields significant improvements in terms of standard ranked-retrieval metrics. CONCLUSIONS: The link structure of content-similarity networks can be exploited to improve the effectiveness of information retrieval systems. These results generalize the applicability of graph analysis algorithms to text retrieval in the biomedical domain."
1640,"disrupting class how disruptive innovation will change the way the world learns",2873914,"Disrupting Class: How Disruptive Innovation Will Change the Way the World Learns","**A crash course in the business of learning-from the bestselling author of _The Innovator's Dilemma_ and _The Innovator's Solution_…**  “A brilliant teacher, Christensen brings clarity to a muddled and chaotic world of education.”  **-Jim Collins, bestselling author of _Good to Great_**  According to recent studies in neuroscience, the way we learn doesn't always match up with the way we are taught. If we hope to stay competitive- academically, economically, and technologically-we need to rethink our understanding of intelligence, reevaluate our educational system, and reinvigorate our commitment to learning. In other words, we need “disruptive innovation.”  Now, in his long-awaited new book, Clayton M. Christensen and coauthors Michael B. Horn and Curtis W. Johnson take one of the most important issues of our time-education-and apply Christensen's now-famous theories of “disruptive” change using a wide range of real-life examples. Whether you're a school administrator, government official, business leader, parent, teacher, or entrepreneur, you'll discover surprising new ideas, outside-the-box strategies, and straight-A success stories.  You'll learn how  * Customized learning will help many more students succeed in school  * Student-centric classrooms will increase the demand for new technology  * Computers must be disruptively deployed to every student  * Disruptive innovation can circumvent roadblocks that have prevented other attempts at school reform  * We can compete in the global classroom-and get ahead in the global market  Filled with fascinating case studies, scientific findings, and unprecedented insights on how innovation must be managed, _Disrupting Class_ will open your eyes to new possibilities, unlock hidden potential, and get you to think differently. Professor Christensen and his coauthors provide a bold new lesson in innovation that will help you make the grade for years to come.  The future is now. Class is in session."
1641,"the evolutionary ecology of metacommunities",2877638,"The evolutionary ecology of metacommunities"," Research on the interactions between evolutionary and ecological dynamics has largely focused on local spatial scales and on relatively simple ecological communities. However, recent work demonstrates that dispersal can drastically alter the interplay between ecological and evolutionary dynamics, often in unexpected ways. We argue that a dispersal-centered synthesis of metacommunity ecology and evolution is necessary to make further progress in this important area of research. We demonstrate that such an approach generates several novel outcomes and substantially enhances understanding of both ecological and evolutionary phenomena in three core research areas at the interface of ecology and evolution."
1642,"discovering functional interaction patterns in proteinprotein interaction networks",2885336,"Discovering functional interaction patterns in protein-protein interaction networks","ABSTRACT: BACKGROUND: In recent years, a considerable amount of research effort has been directed to the analysis of biological networks with the availability of genome-scale networks of genes and/or proteins of an increasing number of organisms. A protein-protein interaction (PPI) network is a particular biological network which represents physical interactions between pairs of proteins of an organism. Major research on PPI networks has focused on understanding the topological organization of PPI networks, evolution of PPI networks and identification of conserved subnetworks across different species, discovery of modules of interaction, use of PPI networks for functional annotation of uncharacterized proteins, and improvement of the accuracy of currently available networks. RESULTS: In this article, we map known functional annotations of proteins onto a PPI network in order to identify frequently occurring interaction patterns in the functional space. We propose a new frequent pattern identification technique, PPISpan, adapted specifically for PPI networks from a well-known frequent subgraph identification method, gSpan. Existing module discovery techniques either look for specific clique-like highly interacting protein clusters or linear paths of interaction. However, our goal is different; instead of single clusters or pathways, we look for recurring functional interaction patterns in arbitrary topologies. We have applied PPISpan on PPI networks of Saccharomyces cerevisiae and identified a number of frequently occurring functional interaction patterns. CONCLUSIONS: With the help of PPISpan, recurring functional interaction patterns in an organism's PPI network can be identified. Such an analysis offers a new perspective on the modular organization of PPI networks. The complete list of identified functional interaction patterns is available at http://bioserver.ceng.metu.edu.tr/PPISpan/."
1643,"global identification of micrornatarget rna pairs by parallel analysis of rna ends",2890226,"Global identification of microRNA–target RNA pairs by parallel analysis of RNA ends","MicroRNAs (miRNAs) are important regulatory molecules in most eukaryotes and identification of their target mRNAs is essential for their functional analysis. Whereas conventional methods rely on computational prediction and subsequent experimental validation of target RNAs, we directly sequenced >28,000,000 signatures from the 5' ends of polyadenylated products of miRNA-mediated mRNA decay, isolated from inflorescence tissue of Arabidopsis thaliana, to discover novel miRNA-target RNA pairs. Within the set of approximately 27,000 transcripts included in the 8,000,000 nonredundant signatures, several previously predicted but nonvalidated targets of miRNAs were found. Like validated targets, most showed a single abundant signature at the miRNA cleavage site, particularly in libraries from a mutant deficient in the 5'-to-3' exonuclease AtXRN4. Although miRNAs in Arabidopsis have been extensively investigated, working in reverse from the cleaved targets resulted in the identification and validation of novel miRNAs. This versatile approach will affect the study of other aspects of RNA processing beyond miRNA-target RNA pairs."
1644,"biochemistry how do proteins interact",2890240,"Biochemistry. How do proteins interact?","PERSPECTIVE. 1st paragraph: Interactions between proteins are central to biology and are becoming increasingly important targets for drug design. Upon forming complexes, protein conformations usually change substantially compared to the unbound protein. Two main hypotheses have been advanced to explain these changes (see the figure). According to the ""induced fit"" hypothesis, the initial interaction between a protein and a binding partner induces a conformational change in the protein through a stepwise process (1). In the ""conformational selection"" model, it is assumed that, prior to the binding interaction, the unliganded protein exists as an ensemble of conformations in dynamic equilibrium. The binding partner interacts preferentially with a weakly populated, higher-energy conformation-causing the equilibrium to shift in favor of the selected conformation. This conformation then becomes the major conformation in the complex (2). Although biochemistry textbooks have championed the induced fit mechanism for more than 50 years, there is now growing support for the additional binding mechanism, including the seminal work by Lange, Lakomek, and co-workers on page 1471 of this issue. [Lange, Science (2008), 320, 1471]"
1645,"two bits the cultural significance of free software",2899290,"Two Bits: The Cultural Significance of Free Software","In Two Bits, Christopher M. Kelty investigates the history and cultural significance of Free Software, revealing the people and practices that have transformed not only software, but also music, film, science, and education. Free Software is a set of practices devoted to the collaborative creation of software source code that is made openly and freely available through an unconventional use of copyright law. Kelty shows how these specific practices have reoriented the relations of power around the creation, dissemination, and authorization of all kinds of knowledge after the arrival of the Internet. Two Bits also makes an important contribution to discussions of public spheres and social imaginaries by demonstrating how Free Software is a ""recursive public"" --a public organized around the ability to build, modify, and maintain the very infrastructure that gives it life in the first place.   <p>Drawing on ethnographic research that took him from an Internet healthcare start-up company in Boston to media labs in Berlin to young entrepreneurs in Bangalore, Kelty describes the technologies and the moral vision that binds together hackers, geeks, lawyers, and other Free Software advocates. In each case, he shows how their practices and way of life include not only the sharing of software source code but also ways of conceptualizing openness, writing copyright licenses, coordinating collaboration, and proselytizing for the movement. By exploring in detail how these practices came together as the Free Software movement from the 1970s to the 1990s, Kelty also shows how it is possible to understand the new movements that are emerging out of Free Software: projects such as Creative Commons, a nonprofit organization that creates copyright licenses, and Connexions, a project to create an online scholarly textbook commons."
1646,"structural specificity in coiledcoil interactions",2900445,"Structural specificity in coiled-coil interactions"," Coiled coils have a rich history in the field of protein design and engineering. Novel structures, such as the first seven-helix coiled coil, continue to provide surprises and insights. Large-scale datasets quantifying the influence of systematic mutations on coiled-coil stability are a valuable new asset to the area. Scoring methods based on sequence and/or structure can predict interaction preferences in coiled-coil-mediated bZIP transcription factor dimerization. Experimental and computational methods for dealing with the near-degeneracy of many coiled-coil structures appear promising for future design applications."
1647,"progress and challenges in protein structure prediction",2902291,"Progress and challenges in protein structure prediction","Depending on whether similar structures are found in the {PDB} library, the protein structure prediction can be categorized into template-based modeling and free modeling. Although threading is an efficient tool to detect the structural analogs, the advancements in methodology development have come to a steady state. Encouraging progress is observed in structure refinement which aims at drawing template structures closer to the native; this has been mainly driven by the use of multiple structure templates and the development of hybrid knowledge-based and physics-based force fields. For free modeling, exciting examples have been witnessed in folding small proteins to atomic resolutions. However, predicting structures for proteins larger than 150 residues still remains a challenge, with bottlenecks from both force field and conformational search."
1648,"ieee p towards an international standard for wireless access in vehicular environments",2904261,"IEEE 802.11p: Towards an International Standard for Wireless Access in Vehicular Environments","Vehicular environments impose a set of new requirements on today's wireless communication systems. Vehicular safety communications applications cannot tolerate long connection establishment delays before being enabled to communicate with other vehicles encountered on the road. Similarly, non-safety applications also demand efficient connection setup with roadside stations providing services (e.g. digital map update) because of the limited time it takes for a car to drive through the coverage area. Additionally, the rapidly moving vehicles and complex roadway environment present challenges at the PHY level. The IEEE 802.11 standard body is currently working on a new amendment, IEEE 802.1 lp, to address these concerns. This document is named wireless access in vehicular environment, also known as WAVE. As of writing, the draft document for IEEE 802.11p is making progress and moving closer towards acceptance by the general IEEE 802.11 working group. It is projected to pass letter ballot in the first half of 2008. This paper provides an overview of the latest draft proposed for IEEE 802.11p. It is intended to provide an insight into the reasoning and approaches behind the document."
1649,"singlecycle nonlinear optics",2909276,"Single-Cycle Nonlinear Optics","Nonlinear optics plays a central role in the advancement of optical science and laser-based technologies. We report on the confinement of the nonlinear interaction of light with matter to a single wave cycle and demonstrate its utility for time-resolved and strong-field science. The electric field of 3.3-femtosecond, 0.72-micron laser pulses with a controlled and measured waveform ionizes atoms near the crests of the central wave cycle, with ionization being virtually switched off outside this interval. Isolated sub-100-attosecond pulses of extreme ultraviolet light (photon energy [~] 80 electron volts), containing [~]0.5 nanojoule of energy, emerge from the interaction with a conversion efficiency of [~]10-6. These tools enable the study of the precision control of electron motion with light fields and electron-electron interactions with a resolution approaching the atomic unit of time ([~]24 attoseconds). 10.1126/science.1157846"
1650,"tuned responses of astrocytes and their influence on hemodynamic signals in the visual cortex",2910062,"Tuned Responses of Astrocytes and Their Influence on Hemodynamic Signals in the Visual Cortex","Astrocytes have long been thought to act as a support network for neurons, with little role in information representation or processing. We used two-photon imaging of calcium signals in the ferret visual cortex in vivo to discover that astrocytes, like neurons, respond to visual stimuli, with distinct spatial receptive fields and sharp tuning to visual stimulus features including orientation and spatial frequency. The stimulus-feature preferences of astrocytes were exquisitely mapped across the cortical surface, in close register with neuronal maps. The spatially restricted stimulus-specific component of the intrinsic hemodynamic mapping signal was highly sensitive to astrocyte activation, indicating that astrocytes have a key role in coupling neuronal organization to mapping signals critical for noninvasive brain imaging. Furthermore, blocking astrocyte glutamate transporters influenced the magnitude and duration of adjacent visually driven neuronal responses. 10.1126/science.1156120"
1651,"gmodweb a web framework for the generic model organism database",2911654,"GMODWeb: a web framework for the generic model organism database","The Generic Model Organism Database {(GMOD)} initiative provides species-agnostic data models and software tools for representing curated model organism data. Here we describe {GMODWeb,} a {GMOD} project designed to speed the development of Model Organism Database {(MOD)} websites. Sites created with {GMODWeb} provide integration with other {GMOD} tools and allow users to browse and search through a variety of data types. {GMODWeb} was built using the open source Turnkey web framework and is available from http://turnkey.sourceforge.net."
1652,"harvesting with sonar the value of aggregating social network information",2914632,"Harvesting with SONAR: the value of aggregating social network information","Web 2.0 gives people a substantial role in content and metadata creation. New interpersonal connections are formed and existing connections become evident through Web 2.0 services. This newly created social network (SN) spans across multiple services and aggregating it could bring great value. In this work we present SONAR, an API for gathering and sharing SN information. We give a detailed description of SONAR, demonstrate its potential value through user scenarios, and show results from experiments we conducted with a SONAR-based social networking application. These suggest that aggregating SN information across diverse data sources enriches the SN picture and makes it more complete and useful for the end user."
1653,"the exact description of biomedical protocols",2933156,"The EXACT description of biomedical protocols","Motivation: Many published manuscripts contain experiment protocols which are poorly described or deficient in information. This means that the published results are very hard or impossible to repeat. This problem is being made worse by the increasing complexity of high-throughput/automated methods. There is therefore a growing need to represent experiment protocols in an efficient and unambiguous way.  Results: We have developed the Experiment ACTions (EXACT) ontology as the basis of a method of representing biological laboratory protocols. We provide example protocols that have been formalized using EXACT, and demonstrate the advantages and opportunities created by using this formalization. We argue that the use of EXACT will result in the publication of protocols with increased clarity and usefulness to the scientific community.  Availability: The ontology, examples and code can be downloaded from http://www.aber.ac.uk/compsci/Research/bio/dss/EXACT/  Contact: Larisa Soldatova lss@aber.ac.uk 10.1093/bioinformatics/btn156"
1654,"coevolution of firm absorptive capacity and knowledge environment organizational forms and combinative capabilities",2933935,"Coevolution of firm absorptive capacity and knowledge environment: organizational forms and combinative capabilities","This paper advances the understanding of absorptive capacity for assimilating new knowledge as a mediating variable of organization adaptation. Many scholars suggest a firm's absorptive capacity plays a key role in the process of coevolution (Lewin et al., this issue). So far, most publications, in following Cohen and Levinthal (1990), have considered the level of prior related knowledge as the determinant of absorptive capacity. We suggest, however, that two specific organizational determinants of absorptive capacity should also be considered: organization forms and combinative capabilities. We will show how these organizational determinants influence the level of absorptive capacity, ceteris paribus the level of prior related knowledge. Subsequently, we will develop a framework in which absorptive capacity is related to both micro- and macrocoevolutionary effects. This framework offers an explanation of how knowledge environments coevolve with the emergence of organization forms and combinative capabilities that are suitable for absorbing knowledge. We will illustrate the framework by discussing two longitudinal case studies of traditional publishing firms moving into the turbulent knowledge environment of an emerging multimedia industrial complex. 10.1287/orsc.10.5.551"
1655,"predicting functional transcription factor binding through alignmentfree and affinitybased analysis of orthologous promoter sequences",2937498,"Predicting functional transcription factor binding through alignment-free and affinity-based analysis of orthologous promoter sequences","Motivation: The identification of transcription factor (TF) binding sites and the regulatory circuitry that they define is currently an area of intense research. Data from whole-genome chromatin immunoprecipitation (ChIP-chip), whole-genome expression microarrays, and sequencing of multiple closely related genomes have all proven useful. By and large, existing methods treat the interpretation of functional data as a classification problem (between bound and unbound DNA), and the analysis of comparative data as a problem of local alignment (to recover phylogenetic footprints of presumably functional elements). Both of these approaches suffer from the inability to model and detect low-affinity binding sites, which have recently been shown to be abundant and functional.  Results: We have developed a method that discovers functional regulatory targets of TFs by predicting the total affinity of each promoter for those factors and then comparing that affinity across orthologous promoters in closely related species. At each promoter, we consider the minimum affinity among orthologs to be the fraction of the affinity that is functional. Because we calculate the affinity of the entire promoter, our method is independent of local alignment. By comparing with functional annotation information and gene expression data in Saccharomyces cerevisiae, we have validated that this biophysically motivated use of evolutionary conservation gives rise to dramatic improvement in prediction of regulatory connectivity and factor-factor interactions compared to the use of a single genome. We propose novel biological functions for several yeast TFs, including the factors Snt2 and Stb4, for which no function has been reported. Our affinity-based approach towards comparative genomics may allow a more quantitative analysis of the principles governing the evolution of non-coding DNA.  Availability: The MatrixREDUCE software package is available from http://www.bussemakerlab.org/software/MatrixREDUCE  Contact: Harmen.Bussemaker@columbia.edu  Supplementary information: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btn154"
1656,"nextgeneration dna sequencing methods",2941039,"Next-generation DNA sequencing methods.","Recent scientific discoveries that resulted from the application of next-generation DNA sequencing technologies highlight the striking impact of these massively parallel platforms on genetics. These new methods have expanded previously focused readouts from a variety of DNA preparation protocols to a genome-wide scale and have fine-tuned their resolution to single base precision. The sequencing of RNA also has transitioned and now includes full-length cDNA analyses, serial analysis of gene expression (SAGE)-based methods, and noncoding RNA discovery. Next-generation sequencing has also enabled novel applications such as the sequencing of ancient DNA samples, and has substantially widened the scope of metagenomic analysis of environmentally derived samples. Taken together, an astounding potential exists for these technologies to bring enormous change in genetic and biological research and to enhance our fundamental biological knowledge."
1657,"the systems biology research tool evolvable opensource software",2943535,"The Systems Biology Research Tool: evolvable open-source software","ABSTRACT: BACKGROUND: Research in the field of systems biology requires software for a variety of purposes. Software must be used to store, retrieve, analyze, and sometimes even to collect the data obtained from system-level (often high-throughput) experiments. Software must also be used to implement mathematical models and algorithms required for simulation and theoretical predictions on the system-level. RESULTS: We introduce a free, easy-to-use, open-source, integrated software platform called the Systems Biology Research Tool (SBRT) to facilitate the computational aspects of systems biology. The SBRT currently performs 35 methods for analyzing stoichiometric networks and 16 methods from fields such as graph theory, geometry, algebra, and combinatorics. New computational techniques can be added to the SBRT via process plug-ins, providing a high degree of evolvability and a unifying framework for software development in systems biology. CONCLUSIONS: The Systems Biology Research Tool represents a technological advance for systems biology. This software can be used to make sophisticated computational techniques accessible to everyone (including those with no programming ability), to facilitate cooperation among researchers, and to expedite progress in the field of systems biology."
1658,"an epistemic dynamic model for tagging systems",2948484,"An epistemic dynamic model for tagging systems","In recent literature, several models were proposed for reproducing and understanding the tagging behavior of users. They all assume that the tagging behavior is influenced by the previous tag assignments of other users. But they are only partially successful in reproducing characteristic properties found in tag streams. We argue that this inadequacy of existing models results from their inability to include user&#8217;s background knowledge into their model of tagging behavior. This paper presents a generative tagging model that integrates both components, the background knowledge and the influence of previous tag assignments. Our model successfully reproduces characteristic properties of tag streams. It even explains effects of the user interface on the tag stream."
1659,"towards a natural system of organisms proposal for the domains archaea bacteria and eucarya",2949866,"Towards a natural system of organisms: proposal for the domains Archaea, Bacteria, and Eucarya","Molecular structures and sequences are generally more revealing of evolutionary relationships than are classical phenotypes (particularly so among microorganisms). Consequently, the basis for the definition of taxa has progressively shifted from the organismal to the cellular to the molecular level. Molecular comparisons show that life on this planet divides into three primary groupings, commonly known as the eubacteria, the archaebacteria, and the eukaryotes. The three are very dissimilar, the differences that separate them being of a more profound nature than the differences that separate typical kingdoms, such as animals and plants. Unfortunately, neither of the conventionally accepted views of the natural relationships among living systems--i.e., the five-kingdom taxonomy or the eukaryote-prokaryote dichotomy--reflects this primary tripartite division of the living world. To remedy this situation we propose that a formal system of organisms be established in which above the level of kingdom there exists a new taxon called a ""domain."" Life on this planet would then be seen as comprising three domains, the Bacteria, the Archaea, and the Eucarya, each containing two or more kingdoms. (The Eucarya, for example, contain Animalia, Plantae, Fungi, and a number of others yet to be defined). Although taxonomic structure within the Bacteria and Eucarya is not treated herein, Archaea is formally subdivided into the two kingdoms Euryarchaeota (encompassing the methanogens and their phenotypically diverse relatives) and Crenarchaeota (comprising the relatively tight clustering of extremely thermophilic archaebacteria, whose general phenotype appears to resemble most the ancestral phenotype of the Archaea."
1660,"efficient algorithms for accurate hierarchical clustering of huge datasets tackling the entire protein space",2966372,"Efficient algorithms for accurate hierarchical clustering of huge datasets: tackling the entire protein space","Motivation: UPGMA (average linking) is probably the most popular algorithm for hierarchical data clustering, especially in computational biology. However, UPGMA requires the entire dissimilarity matrix in memory. Due to this prohibitive requirement, UPGMA is not scalable to very large datasets. Application: We present a novel class of memory-constrained UPGMA (MC-UPGMA) algorithms. Given any practical memory size constraint, this framework guarantees the correct clustering solution without explicitly requiring all dissimilarities in memory. The algorithms are general and are applicable to any dataset. We present a data-dependent characterization of hardness and clustering efficiency. The presented concepts are applicable to any agglomerative clustering formulation. Results: We apply our algorithm to the entire collection of protein sequences, to automatically build a comprehensive evolutionary-driven hierarchy of proteins from sequence alone. The newly created tree captures protein families better than state-of-the-art large-scale methods such as CluSTr, ProtoNet4 or single-linkage clustering. We demonstrate that leveraging the entire mass embodied in all sequence similarities allows to significantly improve on current protein family clusterings which are unable to directly tackle the sheer mass of this data. Furthermore, we argue that non-metric constraints are an inherent complexity of the sequence space and should not be overlooked. The robustness of UPGMA allows significant improvement, especially for multidomain proteins, and for large or divergent families. Availability: A comprehensive tree built from all UniProt sequence similarities, together with navigation and classification tools will be made available as part of the ProtoNet service. A C++ implementation of the algorithm is available on request. Contact: lonshy@cs.huji.ac.il 10.1093/bioinformatics/btn174"
1661,"experience using web services for biological sequence analysis",3008520,"Experience using web services for biological sequence analysis","Programmatic access to data and tools through the web using so-called web services has an important role to play in bioinformatics. In this article, we discuss the most popular approaches based on SOAP/WS-I and REST and describe our, a cross section of the community, experiences with providing and using web services in the context of biological sequence analysis. We briefly review main technological approaches as well as best practice hints that are useful for both users and developers. Finally, syntactic and semantic data integration issues with multiple web services are discussed. 10.1093/bib/bbn029"
1662,"evaluating mapreduce for multicore and multiprocessor systems",3023017,"Evaluating MapReduce for Multi-core and Multiprocessor Systems","This paper evaluates the suitability of the MapReduce model for multi-core and multi-processor systems. MapReduce was created by Google for application development on data-centers with thousands of servers. It allows programmers to write functional-style code that is automaticatlly parallelized and scheduled in a distributed system. We describe Phoenix, an implementation of MapReduce for shared-memory systems that includes a programming API and an efficient runtime system. The Phoenix run-time automatically manages thread creation, dynamic task scheduling, data partitioning, and fault tolerance across processor nodes. We study Phoenix with multi-core and symmetric multiprocessor systems and evaluate its performance potential and error recovery features. We also compare MapReduce code to code written in lower-level APIs such as P-threads. Overall, we establish that, given a careful implementation, MapReduce is a promising model for scalable performance on shared-memory systems with simple parallel code."
1663,"biobayes a software package for bayesian inference in systems biology",3037500,"BioBayes: A Software Package for Bayesian Inference in Systems Biology","Motivation: There are several levels of uncertainty involved in the mathematical modelling of biochemical systems. There often may be a degree of uncertainty about the values of kinetic parameters, about the general structure of the model, and about the behaviour of biochemical species which cannot be observed directly. The methods of Bayesian inference provide a consistent framework for modelling and predicting in these uncertain conditions. We present a software package for applying the Bayesian inferential methodology to problems in Systems Biology.  Results: Described herein is a software package, BioBayes, which provides a framework for Bayesian parameter estimation and evidential model ranking over models of biochemical systems defined using ordinary differential equations. The package is extensible allowing additional modules to be included by developers. There are no other such packages available which provide this functionality.  Availability: http://www.dcs.gla.ac.uk/BioBayes/  Contact: vvv@dcs.gla.ac.uk 10.1093/bioinformatics/btn338"
1664,"selecting good expansion terms for pseudorelevance feedback",3053207,"Selecting good expansion terms for pseudo-relevance feedback","Pseudo-relevance feedback assumes that most frequent terms in the pseudo-feedback documents are useful for the retrieval. In this study, we re-examine this assumption and show that it does not hold in reality - many expansion terms identified in traditional approaches are indeed unrelated to the query and harmful to the retrieval. We also show that good expansion terms cannot be distinguished from bad ones merely on their distributions in the feedback documents and in the whole collection. We then propose to integrate a term classification process to predict the usefulness of expansion terms. Multiple additional features can be integrated in this process. Our experiments on three TREC collections show that retrieval effectiveness can be much improved when term classification is used. In addition, we also demonstrate that good terms should be identified directly according to their possible impact on the retrieval effectiveness, i.e. using supervised learning, instead of unsupervised learning."
1665,"tfidf uncovered a study of theories and probabilities",3056638,"TF-IDF uncovered: a study of theories and probabilities","Interpretations of TF-IDF are based on binary independence retrieval, Poisson, information theory, and language modelling. This paper contributes a review of existing interpretations, and then, TF-IDF is systematically related to the probabilities  P ( q   d ) and  P ( d   q ). Two approaches are explored: a space of  independent , and a space of  disjoint  terms. For  independent  terms, an ""extreme"" query/non-query term assumption uncovers TF-IDF, and an analogy of  P ( d   q ) and the probabilistic odds  O ( r   d ,  q ) mirrors relevance feedback. For  disjoint  terms, a relationship between probability theory and TF-IDF is established through the integral + 1/ x  d x  = log  x . This study uncovers components such as divergence from randomness and pivoted document length to be inherent parts of a document-query independence (DQI) measure, and interestingly, an integral of the DQI over the term occurrence probability leads to TF-IDF."
1666,"the cellml model repository",3058452,"The CellML Model Repository","Summary: The CellML Model Repository provides free access to over 330 biological models. The vast majority of these models are derived from published, peer-reviewed papers. Model curation is an important and ongoing process to ensure the CellML model is able to accurately reproduce the published results. As the CellML community grows, and more people add their models to the repository, model annotation will become increasingly important to facilitate data searches and information retrieval.  Availability: The CellML Model Repository is publicly accessible at http://www.cellml.org/models  Contact: c.lloyd@auckland.ac.nz 10.1093/bioinformatics/btn390"
1667,"realtime automatic tag recommendation",3063692,"Real-time automatic tag recommendation","Tags are user-generated labels for entities. Existing research on tag recommendation either focuses on improving its accuracy or on automating the process, while ignoring the efficiency issue. We propose a highly-automated novel framework for real-time tag recommendation. The tagged training documents are treated as triplets of (words, docs, tags), and represented in two bipartite graphs, which are partitioned into clusters by Spectral Recursive Embedding (SRE). Tags in each topical cluster are ranked by our novel ranking algorithm. A two-way Poisson Mixture Model (PMM) is proposed to model the document distribution into mixture components within each cluster and aggregate words into word clusters simultaneously. A new document is classified by the mixture model based on its posterior probabilities so that tags are recommended according to their ranks. Experiments on large-scale tagging datasets of scientific documents (CiteULike) and web pages (del.icio.us) indicate that our framework is capable of making tag recommendation efficiently and effectively. The average tagging time for testing a document is around 1 second, with over 88% test documents correctly labeled with the top nine tags we suggested."
1668,"efficient topk querying over socialtagging networks",3063696,"Efficient top-k querying over social-tagging networks","Online communities have become popular for publishing and searching content, as well as for finding and connecting to other users. User-generated content includes, for example, personal blogs, bookmarks, and digital photos. These items can be annotated and rated by different users, and these social tags and derived user-specific scores can be leveraged for searching relevant content and discovering subjectively interesting items. Moreover, the relationships among users can also be taken into consideration for ranking search results, the intuition being that you trust the recommendations of your close friends more than those of your casual acquaintances.  Queries for tag or keyword combinations that compute and rank the top-k results thus face a large variety of options that complicate the query processing and pose efficiency challenges. This paper addresses these issues by developing an incremental top-k algorithm with two-dimensional expansions: social expansion considers the strength of relations among users, and semantic expansion considers the relatedness of different tags. It presents a new algorithm, based on principles of threshold algorithms, by folding friends and related tags into the search space in an incremental on-demand manner. The excellent performance of the method is demonstrated by an experimental evaluation on three real-world datasets, crawled from deli.cio.us, Flickr, and LibraryThing."
1669,"accelerating the annotation of sparse named entities by dynamic sentence selection",3065672,"Accelerating the Annotation of Sparse Named Entities by Dynamic Sentence Selection","BACKGROUND: Previous studies of named entity recognition have shown that a reasonable level of recognition accuracy can be achieved by using machine learning models such as conditional random fields or support vector machines. However, the lack of training data (i.e. annotated corpora) makes it difficult for machine learning-based named entity recognizers to be used in building practical information extraction systems. RESULTS: This paper presents an active learning-like framework for reducing the human effort required to create named entity annotations in a corpus. In this framework, the annotation work is performed as an iterative and interactive process between the human annotator and a probabilistic named entity tagger. Unlike active learning, our framework aims to annotate all occurrences of the target named entities in the given corpus, so that the resulting annotations are free from the sampling bias which is inevitable in active learning approaches. CONCLUSION: We evaluate our framework by simulating the annotation process using two named entity corpora and show that our approach can reduce the number of sentences which need to be examined by the human annotator. The cost reduction achieved by the framework could be drastic when the target named entities are sparse."
1670,"altacyclic a selfoptimizing base caller for nextgeneration sequencing",3066965,"Alta-Cyclic: a self-optimizing base caller for next-generation sequencing","Next-generation sequencing is limited to short read lengths and by high error rates. We systematically analyzed sources of noise in the Illumina Genome Analyzer that contribute to these high error rates and developed a base caller, Alta-Cyclic, that uses machine learning to compensate for noise factors. Alta-Cyclic substantially improved the number of accurate reads for sequencing runs up to 78 bases and reduced systematic biases, facilitating confident identification of sequence variants."
1671,"why do hubs in the yeast protein interaction network tend to be essential reexamining the connection between the network topology and essentiality",3080312,"Why Do Hubs in the Yeast Protein Interaction Network Tend To Be Essential: Reexamining the Connection between the Network Topology and Essentiality","The centrality-lethality rule, which notes that high-degree nodes in a protein interaction network tend to correspond to proteins that are essential, suggests that the topological prominence of a protein in a protein interaction network may be a good predictor of its biological importance. Even though the correlation between degree and essentiality was confirmed by many independent studies, the reason for this correlation remains illusive. Several hypotheses about putative connections between essentiality of hubs and the topology of protein–protein interaction networks have been proposed, but as we demonstrate, these explanations are not supported by the properties of protein interaction networks. To identify the main topological determinant of essentiality and to provide a biological explanation for the connection between the network topology and essentiality, we performed a rigorous analysis of six variants of the genomewide protein interaction network for Saccharomyces cerevisiae obtained using different techniques. We demonstrated that the majority of hubs are essential due to their involvement in Essential Complex Biological Modules, a group of densely connected proteins with shared biological function that are enriched in essential proteins. Moreover, we rejected two previously proposed explanations for the centrality-lethality rule, one relating the essentiality of hubs to their role in the overall network connectivity and another relying on the recently published essential protein interactions model."
1672,"highprecision wholegenome sequencing of laboratory strains facilitates genetic studies",3080474,"High-Precision, Whole-Genome Sequencing of Laboratory Strains Facilitates Genetic Studies","Whole-genome sequencing is a powerful technique for obtaining the reference sequence information of multiple organisms. Its use can be dramatically expanded to rapidly identify genomic variations, which can be linked with phenotypes to obtain biological insights. We explored these potential applications using the emerging next-generation sequencing platform Solexa Genome Analyzer, and the well-characterized model bacterium Bacillus subtilis . Combining sequencing with experimental verification, we first improved the accuracy of the published sequence of the B. subtilis reference strain 168, then obtained sequences of multiple related laboratory strains and different isolates of each strain. This provides a framework for comparing the divergence between different laboratory strains and between their individual isolates. We also demonstrated the power of Solexa sequencing by using its results to predict a defect in the citrate signal transduction pathway of a common laboratory strain, which we verified experimentally. Finally, we examined the molecular nature of spontaneously generated mutations that suppress the growth defect caused by deletion of the stringent response mediator relA . Using whole-genome sequencing, we rapidly mapped these suppressor mutations to two small homologs of relA . Interestingly, stable suppressor strains had mutations in both genes, with each mutation alone partially relieving the relA growth defect. This supports an intriguing three-locus interaction module that is not easily identifiable through traditional suppressor mapping. We conclude that whole-genome sequencing can drastically accelerate the identification of suppressor mutations and complex genetic interactions, and it can be applied as a standard tool to investigate the genetic traits of model organisms."
1673,"the new institutionalism organizational factors in political life",3100446,"The New Institutionalism: Organizational Factors in Political Life","Contemporary theories of politics tend to portray politics as a reflection of society, political phenomena as the aggregate consequences of individual behavior, action as the result of choices based on calculated self-interest, history as efficient in reaching unique and appropriate outcomes, and decision making and the allocation of resources as the central foci of political life. Some recent theoretical thought in political science, however, blends elements of these theoretical styles into an older concern with institutions. This new institutionalism emphasizes the relative autonomy of political institutions, possibilities for inefficiency in history, and the importance of symbolic action to an understanding of politics. Such ideas have a resonable empirical basis, but they are not characterized by powerful theoretical forms. Some directions for theoretical research may, however, be identified in institutionalist conceptions of political order."
1674,"socialtrust tamperresilient trust establishment in online communities",3102124,"Socialtrust: tamper-resilient trust establishment in online communities","Web 2.0 promises rich opportunities for information sharing, electronic commerce, and new modes of social interaction, all centered around the ""social Web"" of user-contributed content, social annotations, and person-to-person social connections. But the increasing reliance on this ""social Web"" also places individuals and their computer systems at risk, creating opportunities for malicious participants to exploit the tight social fabric of these networks. With these problems in mind, we propose the SocialTrust framework for tamper-resilient trust establishment in online communities. SocialTrust provides community users with dynamic trust values by (i) distinguishing relationship quality from trust; (ii) incorporating a personalized feedback mechanism for adapting as the community evolves; and (iii) tracking user behavior. We experimentally evaluate the SocialTrust framework using real online social networking data consisting of millions of MySpace profiles and relationships. We find that SocialTrust supports robust trust establishment even in the presence of large-scale collusion by malicious participants."
1675,"optimal spliced alignments of short sequence reads",3111773,"Optimal spliced alignments of short sequence reads.","Motivation: Next generation sequencing technologies open exciting new possibilities for genome and transcriptome sequencing. While reads produced by these technologies are relatively short and error prone compared to the Sanger method their throughput is several magnitudes higher. To utilize such reads for transcriptome sequencing and gene structure identification, one needs to be able to accurately align the sequence reads over intron boundaries. This represents a significant challenge given their short length and inherent high error rate. Results: We present a novel approach, called QPALMA, for computing accurate spliced alignments which takes advantage of the read's quality information as well as computational splice site predictions. Our method uses a training set of spliced reads with quality information and known alignments. It uses a large margin approach similar to support vector machines to estimate its parameters to maximize alignment accuracy. In computational experiments, we illustrate that the quality information as well as the splice site predictions help to improve the alignment quality. Finally, to facilitate mapping of massive amounts of sequencing data typically generated by the new technologies, we have combined our method with a fast mapping pipeline based on enhanced suffix arrays. Our algorithms were optimized and tested using reads produced with the Illumina Genome Analyzer for the model plant Arabidopsis thaliana. Availability: Datasets for training and evaluation, additional results and a stand-alone alignment tool implemented in C++ and python are available at http://www.fml.mpg.de/raetsch/projects/qpalma. Contact: Gunnar.Raetsch@tuebingen.mpg.de 10.1093/bioinformatics/btn300"
1676,"a semantic web management model for integrative biomedical informatics",3123493,"A Semantic Web Management Model for Integrative Biomedical Informatics","BACKGROUND: Data, data everywhere. The diversity and magnitude of the data generated in the Life Sciences defies automated articulation among complementary efforts. The additional need in this field for managing property and access permissions compounds the difficulty very significantly. This is particularly the case when the integration involves multiple domains and disciplines, even more so when it includes clinical and high throughput molecular data. METHODOLOGY/PRINCIPAL FINDINGS: The emergence of Semantic Web technologies brings the promise of meaningful interoperation between data and analysis resources. In this report we identify a core model for biomedical Knowledge Engineering applications and demonstrate how this new technology can be used to weave a management model where multiple intertwined data structures can be hosted and managed by multiple authorities in a distributed management infrastructure. Specifically, the demonstration is performed by linking data sources associated with the Lung Cancer SPORE awarded to The University of Texas MD Anderson Cancer Center at Houston and the Southwestern Medical Center at Dallas. A software prototype, available with open source at www.s3db.org, was developed and its proposed design has been made publicly available as an open source instrument for shared, distributed data management. CONCLUSIONS/SIGNIFICANCE: The Semantic Web technologies have the potential to addresses the need for distributed and evolvable representations that are critical for systems Biology and translational biomedical research. As this technology is incorporated into application development we can expect that both general purpose productivity software and domain specific software installed on our personal computers will become increasingly integrated with the relevant remote resources. In this scenario, the acquisition of a new dataset should automatically trigger the delegation of its analysis."
1677,"impact of personal and cultural factors on knowledge sharing in china",3128959,"Impact of personal and cultural factors on knowledge sharing in China","Abstract&nbsp;&nbsp;Knowledge sharing has been the focus of research for more than a decade and it is widely recognized that it can contribute to the success of an organisation. However, in comparison with other countries, relatively little work on this topic has been done in the Chinese context. Knowledge sharing is particularly interesting to study in the Chinese context at the individual level, given the unique social and cultural characteristics of this environment. In this paper, we develop a theoretical model to explain how personal factors would affect people’s intention to share their knowledge. The Theory of Reasoned Action and Social Exchange Theory are used in this paper, as are the time dimension of national culture, face, and guanxi. A survey methodology is used to test the model. Face and guanxi orientation both exert a significant effect on the intention to share knowledge. Theoretical and practical implications, as well as directions for future research, are discussed."
1678,"applications of nextgeneration sequencing technologies in functional genomics",3129721,"Applications of next-generation sequencing technologies in functional genomics.","A new generation of sequencing technologies, from Illumina/Solexa, ABI/SOLiD, 454/Roche, and Helicos, has provided unprecedented opportunities for high-throughput functional genomic research. To date, these technologies have been applied in a variety of contexts, including whole-genome sequencing, targeted resequencing, discovery of transcription factor binding sites, and noncoding RNA expression profiling. This review discusses applications of next-generation sequencing technologies in functional genomics research and highlights the transforming potential these technologies offer."
1679,"rapid shifts in plant distribution with recent climate change",3141401,"Rapid shifts in plant distribution with recent climate change","10.1073/pnas.0802891105 A change in climate would be expected to shift plant distribution as species expand in newly favorable areas and decline in increasingly hostile locations. We compared surveys of plant cover that were made in 1977 and 2006‚Äì2007 along a 2,314-m elevation gradient in Southern California's Santa Rosa Mountains. Southern California's climate warmed at the surface, the precipitation variability increased, and the amount of snow decreased during the 30-year period preceding the second survey. We found that the average elevation of the dominant plant species rose by ‚âà65 m between the surveys. This shift cannot be attributed to changes in air pollution or fire frequency and appears to be a consequence of changes in regional climate."
1680,"a toolkit for analysing largescale plant small rna datasets",3145759,"A toolkit for analysing large-scale plant small RNA datasets","Summary: Recent developments in high-throughput sequencing technologies have generated considerable demand for tools to analyse large datasets of small RNA sequences. Here, we describe a suite of web-based tools for processing plant small RNA datasets. Our tools can be used to identify micro RNAs and their targets, compare expression levels in sRNA loci, and find putative trans-acting siRNA loci.  Availability: The tools are freely available for use at http://srna-tools.cmp.uea.ac.uk  Contact: vincent.moulton@cmp.uea.ac.uk 10.1093/bioinformatics/btn428"
1681,"detecting synonyms in social tagging systems to improve content retrieval",3175174,"Detecting Synonyms in Social Tagging Systems to Improve Content Retrieval","Collaborative tagging used in online social content systems is naturally characterized by many synonyms, causing low precision retrieval. We propose a mechanism based on user preference profiles to identify synonyms that can be used to retrieve more relevant documents by expanding the user’s query. Using a popular online book catalog we discuss the effectiveness of our method over usual similarity based ex- pansion methods."
1682,"merge a programming model for heterogeneous multicore systems",3177252,"Merge: a programming model for heterogeneous multi-core systems","In this paper we propose the Merge framework, a general purpose programming model for heterogeneous multi-core systems. The Merge framework replaces current ad hoc approaches to parallel programming on heterogeneous platforms with a rigorous, library-based methodology that can automatically distribute computation across heterogeneous cores to achieve increased energy and performance efficiency. The Merge framework provides (1) a predicate dispatch-based library system for managing and invoking function variants for multiple architectures; (2) a high-level, library-oriented parallel language based on map-reduce; and (3) a compiler and runtime which implement the map-reduce language pattern by dynamically selecting the best available function implementations for a given input and machine configuration. Using a generic sequencer architecture interface for heterogeneous accelerators, the Merge framework can integrate function variants for specialized accelerators, offering the potential for to-the-metal performance for a wide range of heterogeneous architectures, all transparent to the user. The Merge framework has been prototyped on a heterogeneous platform consisting of an Intel Core 2 Duo CPU and an 8-core 32-thread Intel Graphics and Media Accelerator X3000, and a homogeneous 32-way Unisys SMP system with Intel Xeon processors. We implemented a set of benchmarks using the Merge framework and enhanced the library with X3000 specific implementations, achieving speedups of 3.6x -- 8.5x using the X3000 and 5.2x -- 22x using the 32-way system relative to the straight C reference implementation on a single IA32 core."
1683,"macromolecular modeling with rosetta",3179558,"Macromolecular modeling with rosetta.","Advances over the past few years have begun to enable prediction and design of macromolecular structures at near-atomic accuracy. Progress has stemmed from the development of reasonably accurate and efficiently computed all-atom potential functions as well as effective conformational sampling strategies appropriate for searching a highly rugged energy landscape, both driven by feedback from structure prediction and design tests. A unified energetic and kinematic framework in the Rosetta program allows a wide range of molecular modeling problems, from fibril structure prediction to RNA folding to the design of new protein interfaces, to be readily investigated and highlights areas for improvement. The methodology enables the creation of novel molecules with useful functions and holds promise for accelerating experimental structural inference. Emerging connections to crystallographic phasing, NMR modeling, and lower-resolution approaches are described and critically assessed."
1684,"configurations of fluid membranes and vesicles",3191538,"Configurations of fluid membranes and vesicles","Vesicles consisting of a bilayer membrane of amphiphilic lipid molecules are remarkably flexible surfaces that show an amazing variety of shapes of different symmetry and topology. Owing to the fluidity of the membrane, shape transitions such as budding can be induced by temperature changes or the action of optical tweezers. Thermally excited shape fluctuations are both strong and slow enough to be visible by video microscopy. Depending on the physical conditions, vesicles adhere to and unbind from each other or a substrate. This article describes the systematic physical theory developed to understand the static and dynamic aspects of membrane and vesicle configurations. The preferred shapes arise from a competition between curvature energy, which derives from the bending elasticity of the membrane, geometrical constraints such as fixed surface area and fixed enclosed volume, and a signature of the bilayer aspect. These shapes of lowest energy are arranged into phase diagrams, which separate regions of different symmetry by continuous or discontinuous transitions. The geometrical constraints affect the fluctuations around these shapes by creating an effective tension. For vesicles of non-spherical topology, the conformal invariance of the curvature energy leads to conformal diffusion, which signifies a one-fold degeneracy of the ground state. Unbinding and adhesion transitions arise from the balance between attractive interactions and entropic repulsion or a cost in bending energy, respectively. Both the dynamics of equilibrium fluctuations and the dynamics of shape transformations are governed not only by viscous damping in the surrounding liquid but also by internal friction if the two monolayers slip over each other. More complex membranes such as that of the red blood cell exhibit a variety of new phenomena because of coupling between internal degrees of freedom and external geometry."
1685,"networkbased prediction of human tissuespecific metabolism",3196658,"Network-based prediction of human tissue-specific metabolism","Direct in vivo investigation of mammalian metabolism is complicated by the distinct metabolic functions of different tissues. We present a computational method that successfully describes the tissue specificity of human metabolism on a large scale. By integrating tissue-specific gene- and protein-expression data with an existing comprehensive reconstruction of the global human metabolic network, we predict tissue-specific metabolic activity in ten human tissues. This reveals a central role for post-transcriptional regulation in shaping tissue-specific metabolic activity profiles. The predicted tissue specificity of genes responsible for metabolic diseases and tissue-specific differences in metabolite exchange with biofluids extend markedly beyond tissue-specific differences manifest in enzyme-expression data, and are validated by large-scale mining of tissue-specificity data. Our results establish a computational basis for the genome-wide study of normal and abnormal human metabolism in a tissue-specific manner."
1686,"robust and efficient identification of biomarkers by classifying features on graphs",3200250,"Robust and efficient identification of biomarkers by classifying features on graphs.","MOTIVATION: A central problem in biomarker discovery from large-scale gene expression or single nucleotide polymorphism (SNP) data is the computational challenge of taking into account the dependence among all the features. Methods that ignore the dependence usually identify non-reproducible biomarkers across independent datasets. We introduce a new graph-based semi-supervised feature classification algorithm to identify discriminative disease markers by learning on bipartite graphs. Our algorithm directly classifies the feature nodes in a bipartite graph as positive, negative or neutral with network propagation to capture the dependence among both samples and features (clinical and genetic variables) by exploring bi-cluster structures in a graph. Two features of our algorithm are: (1) our algorithm can find a global optimal labeling to capture the dependence among all the features and thus, generates highly reproducible results across independent microarray or other high-thoughput datasets, (2) our algorithm is capable of handling hundreds of thousands of features and thus, is particularly useful for biomarker identification from high-throughput gene expression and SNP data. In addition, although designed for classifying features, our algorithm can also simultaneously classify test samples for disease prognosis/diagnosis. RESULTS: We applied the network propagation algorithm to study three large-scale breast cancer datasets. Our algorithm achieved competitive classification performance compared with SVMs and other baseline methods, and identified several markers with clinical or biological relevance with the disease. More importantly, our algorithm also identified highly reproducible marker genes and enriched functions from the independent datasets. AVAILABILITY: Supplementary results and source code are available at http://compbio.cs.umn.edu/Feature_Class. CONTACT: kuang@cs.umn.edu SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online."
1687,"systematic assessment of copy number variant detection via genomewide snp genotyping",3225800,"Systematic assessment of copy number variant detection via genome-wide SNP genotyping.","SNP genotyping has emerged as a technology to incorporate copy number variants (CNVs) into genetic analyses of human traits. However, the extent to which SNP platforms accurately capture CNVs remains unclear. Using independent, sequence-based CNV maps, we find that commonly used SNP platforms have limited or no probe coverage for a large fraction of CNVs. Despite this, in 9 samples we inferred 368 CNVs using Illumina SNP genotyping data and experimentally validated over two-thirds of these. We also developed a method (SNP-Conditional Mixture Modeling, SCIMM) to robustly genotype deletions using as few as two SNP probes. We find that HapMap SNPs are strongly correlated with 82% of common deletions, but the newest SNP platforms effectively tag about 50%. We conclude that currently available genome-wide SNP assays can capture CNVs accurately, but improvements in array designs, particularly in duplicated sequences, are necessary to facilitate more comprehensive analyses of genomic variation."
1688,"measuring micrornas comparisons of microarray and quantitative pcr measurements and of different total rna prep methods",3244398,"Measuring microRNAs:  Comparisons of microarray and quantitative PCR measurements, and of different total RNA prep methods","BACKGROUND: Determining the expression levels of microRNAs (miRNAs) is of great interest to researchers in many areas of biology, given the significant roles these molecules play in cellular regulation. Two common methods for measuring miRNAs in a total RNA sample are microarrays and quantitative RT-PCR (qPCR). To understand the results of studies that use these two different techniques to measure miRNAs, it is important to understand how well the results of these two analysis methods correlate. Since both methods use total RNA as a starting material, it is also critical to understand how measurement of miRNAs might be affected by the particular method of total RNA preparation used. RESULTS: We measured the expression of 470 human miRNAs in nine human tissues using Agilent microarrays, and compared these results to qPCR profiles of 61 miRNAs in the same tissues. Most expressed miRNAs (53/60) correlated well (R > 0.9) between the two methods. Using spiked-in synthetic miRNAs, we further examined the two miRNAs with the lowest correlations, and found the differences cannot be attributed to differential sensitivity of the two methods. We also tested three widely-used total RNA sample prep methods using miRNA microarrays. We found that while almost all miRNA levels correspond between the three methods, there were a few miRNAs whose levels consistently differed between the different prep techniques when measured by microarray analysis. These differences were corroborated by qPCR measurements. CONCLUSION: The correlations between Agilent miRNA microarray results and qPCR results are generally excellent, as are the correlations between different total RNA prep methods. However, there are a few miRNAs whose levels do not correlate between the microarray and qPCR measurements, or between different sample prep methods. Researchers should therefore take care when comparing results obtained using different analysis or sample preparation methods."
1689,"the balance of reproducibility sensitivity and specificity of lists of differentially expressed genes in microarray studies",3293311,"The balance of reproducibility, sensitivity, and specificity of lists of differentially expressed genes in microarray studies","BACKGROUND: Reproducibility is a fundamental requirement in scientific experiments. Some recent publications have claimed that microarrays are unreliable because lists of differentially expressed genes (DEGs) are not reproducible in similar experiments. Meanwhile, new statistical methods for identifying DEGs continue to appear in the scientific literature. The resultant variety of existing and emerging methods exacerbates confusion and continuing debate in the microarray community on the appropriate choice of methods for identifying reliable DEG lists. RESULTS: Using the data sets generated by the MicroArray Quality Control (MAQC) project, we investigated the impact on the reproducibility of DEG lists of a few widely used gene selection procedures. We present comprehensive results from inter-site comparisons using the same microarray platform, cross-platform comparisons using multiple microarray platforms, and comparisons between microarray results and those from TaqMan - the widely regarded ""standard"" gene expression platform. Our results demonstrate that (1) previously reported discordance between DEG lists could simply result from ranking and selecting DEGs solely by statistical significance (P) derived from widely used simple t-tests; (2) when fold change (FC) is used as the ranking criterion with a non-stringent P-value cutoff filtering, the DEG lists become much more reproducible, especially when fewer genes are selected as differentially expressed, as is the case in most microarray studies; and (3) the instability of short DEG lists solely based on P-value ranking is an expected mathematical consequence of the high variability of the t-values; the more stringent the P-value threshold, the less reproducible the DEG list is. These observations are also consistent with results from extensive simulation calculations. CONCLUSION: We recommend the use of FC-ranking plus a non-stringent P cutoff as a straightforward and baseline practice in order to generate more reproducible DEG lists. Specifically, the P-value cutoff should not be stringent (too small) and FC should be as large as possible. Our results provide practical guidance to choose the appropriate FC and P-value cutoffs when selecting a given number of DEGs. The FC criterion enhances reproducibility, whereas the P criterion balances sensitivity and specificity."
1690,"cavity optomechanics backaction at the mesoscale",3320298,"Cavity Optomechanics: Back-Action at the Mesoscale","The coupling of optical and mechanical degrees of freedom is the underlying principle of many techniques to measure mechanical displacement, from macroscale gravitational wave detectors to microscale cantilevers used in scanning probe microscopy. Recent experiments have reached a regime where the back-action of photons caused by radiation pressure can influence the optomechanical dynamics, giving rise to a host of long-anticipated phenomena. Here we review these developments and discuss the opportunities for innovative technology as well as for fundamental science. 10.1126/science.1156032"
1691,"direct inhibition of the longevitypromoting factor skn by insulinlike signaling in c elegans",3337104,"Direct inhibition of the longevity-promoting factor SKN-1 by insulin-like signaling in C. elegans.","Insulin/IGF-1-like signaling (IIS) is central to growth and metabolism and has a conserved role in aging. In C. elegans, reductions in IIS increase stress resistance and longevity, effects that require the IIS-inhibited FOXO protein DAF-16. The C. elegans transcription factor SKN-1 also defends against oxidative stress by mobilizing the conserved phase 2 detoxification response. Here we show that IIS not only opposes DAF-16 but also directly inhibits SKN-1 in parallel. The IIS kinases AKT-1, -2, and SGK-1 phosphorylate SKN-1, and reduced IIS leads to constitutive SKN-1 nuclear accumulation in the intestine and SKN-1 target gene activation. SKN-1 contributes to the increased stress tolerance and longevity resulting from reduced IIS and delays aging when expressed transgenically. Furthermore, SKN-1 that is constitutively active increases life span independently of DAF-16. Our findings indicate that the transcription network regulated by SKN-1 promotes longevity and is an important direct target of IIS."
1692,"digital ethnography",3338202,"Digital Ethnography","The rise of digital technologies has the potential to open new directions in ethnography. Despite the ubiquity of these technologies, their infiltration into popular sociological research methods is still limited compared to the insatiable uptake of online scholarly research portals. This article argues that social researchers cannot afford to continue this trend. Building upon pioneering work in `digital ethnography', I critically examine the possibilities and problems of four new technologies â online questionnaires, digital video, social networking websites, and blogs â and their potential impacts on the research relationship. The article concludes that a balanced combination of physical and digital ethnography not only gives researchers a larger and more exciting array of methods, but also enables them to demarginalize the voice of respondents. However, access to these technologies remains stratified by class, race, and gender of both researchers and respondents."
1693,"ecological models and data in r",3339772,"Ecological Models and Data in R","_Ecological Models and Data in R_ is the first truly practical introduction to modern statistical methods for ecology. In step-by-step detail, the book teaches ecology graduate students and researchers everything they need to know in order to use maximum likelihood, information-theoretic, and Bayesian techniques to analyze their own data using the programming language R. Drawing on extensive experience teaching these techniques to graduate students in ecology, Benjamin Bolker shows how to choose among and construct statistical models for data, estimate their parameters and confidence limits, and interpret the results. The book also covers statistical frameworks, the philosophy of statistical modeling, and critical mathematical functions and probability distributions. It requires no programming background--only basic calculus and statistics.  * Practical, beginner-friendly introduction to modern statistical techniques for ecology using the programming language R  * Step-by-step instructions for fitting models to messy, real-world data  * Balanced view of different statistical approaches  * Wide coverage of techniques--from simple (distribution fitting) to complex (state-space modeling)  * Techniques for data manipulation and graphical display  * Companion Web site with data and R code for all examples"
1694,"structural biology by nmr structure dynamics and interactions",3367094,"Structural biology by NMR: structure, dynamics, and interactions.","The function of bio-macromolecules is determined by both their 3D structure and conformational dynamics. These molecules are inherently flexible systems displaying a broad range of dynamics on time-scales from picoseconds to seconds. Nuclear Magnetic Resonance (NMR) spectroscopy has emerged as the method of choice for studying both protein structure and dynamics in solution. Typically, NMR experiments are sensitive both to structural features and to dynamics, and hence the measured data contain information on both. Despite major progress in both experimental approaches and computational methods, obtaining a consistent view of structure and dynamics from experimental NMR data remains a challenge. Molecular dynamics simulations have emerged as an indispensable tool in the analysis of NMR data."
1695,"the elements of statistical learning data mining inference and prediction second edition springer series in statistics",3388025,"The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition (Springer Series in Statistics)","During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book.  This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression & path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates.  Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting."
1696,"the development and impact of sequencing",3391361,"The development and impact of 454 sequencing","The 454 Sequencer has dramatically increased the volume of sequencing conducted by the scientific community and expanded the range of problems that can be addressed by the direct readouts of DNA sequence. Key breakthroughs in the development of the 454 sequencing platform included higher throughput, simplified all in vitro sample preparation and the miniaturization of sequencing chemistries, enabling massively parallel sequencing reactions to be carried out at a scale and cost not previously possible. Together with other recently released next-generation technologies, the 454 platform has started to democratize sequencing, providing individual laboratories with access to capacities that rival those previously found only at a handful of large sequencing centers. Over the past 18 months, 454 sequencing has led to a better understanding of the structure of the human genome, allowed the first non-Sanger sequence of an individual human and opened up new approaches to identify small RNAs. To make next-generation technologies more widely accessible, they must become easier to use and less costly. In the longer term, the principles established by 454 sequencing might reduce cost further, potentially enabling personalized genomics."
1697,"what would you do if you could sequence everything",3391362,"What would you do if you could sequence everything?","It could be argued that the greatest transformative aspect of the Human Genome Project has been not the sequencing of the genome itself, but the resultant development of new technologies. A host of new approaches has fundamentally changed the way we approach problems in basic and translational research. Now, a new generation of high-throughput sequencing technologies promises to again transform the scientific enterprise, potentially supplanting array-based technologies and opening up many new possibilities. By allowing DNA/RNA to be assayed more rapidly than previously possible, these next-generation platforms promise a deeper understanding of genome regulation and biology. Significantly enhancing sequencing throughput will allow us to follow the evolution of viral and bacterial resistance in real time, to uncover the huge diversity of novel genes that are currently inaccessible, to understand nucleic acid therapeutics, to better integrate biological information for a complete picture of health and disease at a personalized level and to move to advances that we cannot yet imagine."
1698,"highresolution mapping of expressionqtls yields insight into human gene regulation",3394146,"High-Resolution Mapping of Expression-QTLs Yields Insight into Human Gene Regulation","Recent studies of the HapMap lymphoblastoid cell lines have identified large numbers of quantitative trait loci for gene expression (eQTLs). Reanalyzing these data using a novel Bayesian hierarchical model, we were able to create a surprisingly high-resolution map of the typical locations of sites that affect mRNA levels in cis. Strikingly, we found a strong enrichment of eQTLs in the 250 bp just upstream of the transcription end site (TES), in addition to an enrichment around the transcription start site (TSS). Most eQTLs lie either within genes or close to genes; for example, we estimate that only 5% of eQTLs lie more than 20 kb upstream of the TSS. After controlling for position effects, SNPs in exons are approximately 2-fold more likely than SNPs in introns to be eQTLs. Our results suggest an important role for mRNA stability in determining steady-state mRNA levels, and highlight the potential of eQTL mapping as a high-resolution tool for studying the determinants of gene regulation."
1699,"personalizing navigation in folksonomies using hierarchical tag clustering",3403177,"Personalizing Navigation in Folksonomies Using Hierarchical Tag Clustering","The popularity of collaborative tagging, otherwise known as “folksonomies”, emanate from the flexibility they afford users in navigating large information spaces for resources, tags, or other users, unencumbered by a pre-defined navigational or conceptual hierarchy. Despite its advantages, social tagging also increases user overhead in search and navigation: users are free to apply any tag they wish to a resource, often resulting in a large number of tags that are redundant, ambiguous, or idiosyncratic. Data mining techniques such as clustering provide a means to overcome this problem by learning aggregate user models, and thus reducing noise. In this paper we propose a method to personalize search and navigation based on unsupervised hierarchical agglomerative tag clustering. Given a user profile, represented as a vector of tags, the learned tag clusters provide the nexus between the user and those resources that correspond more closely to the user’s intent. We validate this assertion through extensive evaluation of the proposed algorithm using data from a real collaborative tagging Web site."
1700,"forget time",3406993,"Forget Time","Following a line of research that I have developed for several years, I argue that the best strategy for understanding quantum gravity is to build a picture of the physical world where the notion of time plays no role at all. I summarize here this point of view, explaining why I think that in a fundamental description of nature we must “forget time”, and how this can be done in the classical and in the quantum theory. The idea is to develop a formalism that treats dependent and independent variables on the same footing. In short, I propose to interpret mechanics as a theory of relations between variables, rather than the theory of the evolution of variables in time."
1701,"an investigation of the statistical power of neutrality tests based on comparative and population genetic data",3426312,"An Investigation of the Statistical Power of Neutrality Tests Based on Comparative and Population Genetic Data","In this report, we investigate the statistical power of several tests of selective neutrality based on patterns of genetic diversity within and between species. The goal is to compare tests based solely on population genetic data with tests using comparative data or a combination of comparative and population genetic data. We show that in the presence of repeated selective sweeps on relatively neutral background, tests based on the dN/dS ratios in comparative data almost always have more power to detect selection than tests based on population genetic data, even if the overall level of divergence is low. Tests based solely on the distribution of allele frequencies or the site frequency spectrum, such as the Ewens-Watterson test or Tajima's D, have less power in detecting both positive and negative selection because of the transient nature of positive selection and the weak signal left by negative selection. The Hudson-Kreitman-Aguade test is the most powerful test for detecting positive selection among the population genetic tests investigated, whereas McDonald-Kreitman test typically has more power to detect negative selection. We discuss our findings in the light of the discordant results obtained in several recently published genomic scans. 10.1093/molbev/msn231"
1702,"mirdisease a manually curated database for microrna deregulation in human disease",3428472,"miR2Disease: a manually curated database for microRNA deregulation in human disease.","miR2Disease', a manually curated database, aims at providing a comprehensive resource of microRNA deregulation in various human diseases. The current version of miR2Disease documents 1939 curated relationships between 299 human microRNAs and 94 human diseases by reviewing more than 600 published papers. Around one-seventh of the microRNA-disease relationships represent the pathogenic roles of deregulated microRNA in human disease. Each entry in the miR2Disease contains detailed information on a microRNA-disease relationship, including a microRNA ID, the disease name, a brief description of the microRNA-disease relationship, an expression pattern of the microRNA, the detection method for microRNA expression, experimentally verified target gene(s) of the microRNA and a literature reference. miR2Disease provides a user-friendly interface for a convenient retrieval of each entry by microRNA ID, disease name, or target gene. In addition, miR2Disease offers a submission page that allows researchers to submit established microRNA-disease relationships that are not documented. Once approved by the submission review committee, the submitted records will be included in the database. miR2Disease is freely available at http://www.miR2Disease.org. 10.1093/nar/gkn714"
1703,"interpro the integrative protein signature database",3443713,"InterPro: the integrative protein signature database","The InterPro database (http://www.ebi.ac.uk/interpro/) integrates together predictive models or âsignaturesâ representing protein domains, families and functional sites from multiple, diverse source databases: Gene3D, PANTHER, Pfam, PIRSF, PRINTS, ProDom, PROSITE, SMART, SUPERFAMILY and TIGRFAMs. Integration is performed manually and approximately half of the total â¼58 000 signatures available in the source databases belong to an InterPro entry. Recently, we have started to also display the remaining un-integrated signatures via our web interface. Other developments include the provision of non-signature data, such as structural data, in new XML files on our FTP site, as well as the inclusion of matchless UniProtKB proteins in the existing match XML files. The web interface has been extended and now links out to the ADAN predicted proteinâprotein interaction database and the SPICE and Dasty viewers. The latest public release (v18.0) covers 79.8% of UniProtKB (v14.1) and consists of 16 549 entries. InterPro data may be accessed either via the web address above, via web services, by downloading files by anonymous FTP or by using the InterProScan search software (http://www.ebi.ac.uk/Tools/InterProScan/)."
1704,"smallscale copy number variation and largescale changes in gene expression",3443777,"Small-scale copy number variation and large-scale changes in gene expression","10.1073/pnas.0806239105 The expression dynamics of interacting genes depends, in part, on the structure of regulatory networks. Genetic regulatory networks include an overrepresentation of subgraphs commonly known as network motifs. In this article, we demonstrate that gene copy number is an omnipresent parameter that can dramatically modify the dynamical function of network motifs. We consider positive feedback, bistable feedback, and toggle switch motifs and show that variation in gene copy number, on the order of a single or few copies, can lead to multiple orders of magnitude change in gene expression and, in some cases, switches in deterministic control. Further, small changes in gene copy number for a 3-gene motif with successive inhibition (the “repressilator”) can lead to a qualitative switch in system behavior among oscillatory and equilibrium dynamics. In all cases, the qualitative change in expression is due to the nonlinear nature of transcriptional feedback in which duplicated motifs interact via common pools of transcription factors. We are able to implicitly determine the critical values of copy number which lead to qualitative shifts in system behavior. In some cases, we are able to solve for the sufficient condition for the existence of a bifurcation in terms of kinetic rates of transcription, translation, binding, and degradation. We discuss the relevance of our findings to ongoing efforts to link copy number variation with cell fate determination by viruses, dynamics of synthetic gene circuits, and constraints on evolutionary adaptation."
1705,"fusion and fission of genes define a metric between fungal genomes",3448578,"Fusion and Fission of Genes Define a Metric between Fungal Genomes","Gene fusion and fission events are key mechanisms in the evolution of gene architecture, whose effects are visible in protein architecture when they occur in coding sequences. Until now, the detection of fusion and fission events has been performed at the level of protein sequences with a post facto removal of supernumerary links due to paralogy, and often did not include looking for events defined only in single genomes. We propose a method for the detection of these events, defined on groups of paralogs to compensate for the gene redundancy of eukaryotic genomes, and apply it to the proteomes of 12 fungal species. We collected an inventory of 1,680 elementary fusion and fission events. In half the cases, both composite and element genes are found in the same species. Per-species counts of events correlate with the species genome size, suggesting a random mechanism of occurrence. Some biological functions of the genes involved in fusion and fission events are slightly over- or under-represented. As already noted in previous studies, the genes involved in an event tend to belong to the same functional category. We inferred the position of each event in the evolution tree of the 12 fungal species. The event localization counts for all the segments of the tree provide a metric that depicts the â€œrecombinationalâ€� phylogeny among fungi. A possible interpretation of this metric as distance in adaptation space is proposed."
1706,"comparative genomics and the study of evolution by natural selection",3458224,"Comparative genomics and the study of evolution by natural selection","Genomics profoundly affects most areas of biology, including ecology and evolutionary biology. By examining genome sequences from multiple species, comparative genomics offers new insight into genome evolution and the way natural selection moulds DNA sequence evolution. Functional divergence, as manifested in the accumulation of nonsynonymous substitutions in protein-coding genes, differs among lineages in a manner seemingly related to population size. For example, the ratio of nonsynonymous to synonymous substitution (d(N)/d(S)) is higher in apes than in rodents, compatible with Ohta's nearly neutral theory of molecular evolution, which suggests that the fixation of slightly deleterious mutations contributes to protein evolution at an extent negatively correlated with effective population size. While this supports the idea that functional evolution is not necessarily adaptive, comparative genomics is uncovering a role for positive Darwinian selection in 10-40% of all genes in different lineages, estimates that are likely to increase when the addition of more genomes gives increased power. Again, population size seems to matter also in this context, with a higher proportion of fixed amino acid changes representing advantageous mutations in large populations. Genes that are particularly prone to be driven by positive selection include those involved with reproduction, immune response, sensory perception and apoptosis. Genetic innovations are also frequently obtained by the gain or loss of complete gene sequences. Moreover, it is increasingly realized, from comparative genomics, that purifying selection conserves much more than just the protein-coding part of the genome, and this points at an important role for regulatory elements in trait evolution. Finally, genome sequencing using outbred or multiple individuals has provided a wealth of polymorphism data that gives information on population history, demography and marker evolution."
1707,"models of coding sequence evolution",3464173,"Models of coding sequence evolution.","Probabilistic models of sequence evolution are in widespread use in phylogenetics and molecular sequence evolution. These models have become increasingly sophisticated and combined with statistical model comparison techniques have helped to shed light on how genes and proteins evolve. Models of codon evolution have been particularly useful, because, in addition to providing a significant improvement in model realism for protein-coding sequences, codon models can also be designed to test hypotheses about the selective pressures that shape the evolution of the sequences. Such models typically assume a phylogeny and can be used to identify sites or lineages that have evolved adaptively. Recently some of the key assumptions that underlie phylogenetic tests of selection have been questioned, such as the assumption that the rate of synonymous changes is constant across sites or that a single phylogenetic tree can be assumed at all sites for recombining sequences. While some of these issues have been addressed through the development of novel methods, others remain as caveats that need to be considered on a case-by-case basis. Here, we outline the theory of codon models and their application to the detection of positive selection. We review some of the more recent developments that have improved their power and utility, laying a foundation for further advances in the modeling of coding sequence evolution. 10.1093/bib/bbn049"
1708,"boolean implication networks derived from large scale whole genome microarray datasets",3470400,"Boolean implication networks derived from large scale, whole genome microarray datasets","We describe a method for extracting Boolean implications (if-then relationships) in very large amounts of gene expression microarray data. A meta-analysis of data from thousands of microarrays for humans, mice, and fruit flies finds millions of implication relationships between genes that would be missed by other methods. These relationships capture gender differences, tissue differences, development, and differentiation. New relationships are discovered that are preserved across all three species."
1709,"micrornas in cancer",3480685,"MicroRNAs in cancer.","Within the past few years, studies on microRNA (miRNA) and cancer have burst onto the scene. Profiling of the miRNome (global miRNA expression levels) has become prevalent, and abundant miRNome data are currently available for various cancers. The pattern of miRNA expression can be correlated with cancer type, stage, and other clinical variables, so miRNA profiling can be used as a tool for cancer diagnosis and prognosis. miRNA expression analyses also suggest oncogenic (or tumor-suppressive) roles of miRNAs. miRNAs play roles in almost all aspects of cancer biology, such as proliferation, apoptosis, invasion/metastasis, and angiogenesis. Given that many miRNAs are deregulated in cancers but have not yet been further studied, it is expected that more miRNAs will emerge as players in the etiology and progression of cancer. Here we also discuss miRNAs as a tool for cancer therapy. Expected final online publication date for the Annual Review of Pathology: Mechanisms of Disease Volume 4 is January 24, 2009. Please see http://www.annualreviews.org/catalog/pubdates.aspx for revised estimates."
1710,"real world haskell",3482554,"Real World Haskell","This easy-to-use, fast-moving tutorial introduces you to functional programming with Haskell. You'll learn how to use Haskell in a variety of practical ways, from short scripts to large and demanding applications. _Real World Haskell_ takes you through the basics of functional programming at a brisk pace, and then helps you increase your understanding of Haskell in real- world issues like I/O, performance, dealing with data, concurrency, and more as you move through each chapter.   With this book, you will:  * Understand the differences between procedural and functional programming  * Learn the features of Haskell, and how to use it to develop useful programs  * Interact with filesystems, databases, and network services  * Write solid code with automated tests, code coverage, and error handling  * Harness the power of multicore systems via concurrent and parallel programming  You'll find plenty of hands-on exercises, along with examples of real Haskell programs that you can modify, compile, and run. Whether or not you've used a functional language before, if you want to understand why Haskell is coming into its own as a practical language in so many major organizations, _Real World Haskell_ is the best place to start."
1711,"the organization of the bacterial genome",3483523,"The Organization of the Bacterial Genome","Many bacterial cellular processes interact intimately with the chromosome. Such interplay is the major driving force of genome structure or organization. Interactions take place at different scales—local for gene expression, global for replication—and lead to the differentiation of the chromosome into organizational units such as operons, replichores, or macrodomains. These processes are intermingled in the cell and create complex higher-level organizational features that are adaptive because they favor the interplay between the processes. The surprising result of selection for genome organization is that gene repertoires change much more quickly than chromosomal structure. Comparative genomics and experimental genomic manipulations are untangling the different cellular and evolutionary mechanisms causing such resilience to change. Since organization results from cellular processes, a better understanding of chromosome organization will help unravel the underlying cellular processes and their diversity."
1712,"deep surveying of alternative splicing complexity in the human transcriptome by highthroughput sequencing",3484872,"Deep surveying of alternative splicing complexity in the human transcriptome by high-throughput sequencing","We carried out the first analysis of alternative splicing complexity in human tissues using mRNA-Seq data. New splice junctions were detected in approximately 20% of multiexon genes, many of which are tissue specific. By combining mRNA-Seq and EST-cDNA sequence data, we estimate that transcripts from approximately 95% of multiexon genes undergo alternative splicing and that there are approximately 100,000 intermediate- to high-abundance alternative splicing events in major human tissues. From a comparison with quantitative alternative splicing microarray profiling data, we also show that mRNA-Seq data provide reliable measurements for exon inclusion levels."
1713,"highthroughput chromatin information enables accurate tissuespecific prediction of transcription factor binding sites",3486430,"High-throughput chromatin information enables accurate tissue-specific prediction of transcription factor binding sites.","In silico prediction of transcription factor binding sites (TFBSs) is central to the task of gene regulatory network elucidation. Genomic DNA sequence information provides a basis for these predictions, due to the sequence specificity of TF-binding events. However, DNA sequence alone is an impoverished source of information for the task of TFBS prediction in eukaryotes, as additional factors, such as chromatin structure regulate binding events. We show that incorporating high-throughput chromatin modification estimates can greatly improve the accuracy of in silico prediction of in vivo binding for a wide range of TFs in human and mouse. This improvement is superior to the improvement gained by equivalent use of either transcription start site proximity or phylogenetic conservation information. Importantly, predictions made with the use of chromatin structure information are tissue specific. This result supports the biological hypothesis that chromatin modulates TF binding to produce tissue-specific binding profiles in higher eukaryotes, and suggests that the use of chromatin modification information can lead to accurate tissue-specific transcriptional regulatory network elucidation. 10.1093/nar/gkn866"
1714,"facilitated variation how evolution learns from past environments to generalize to new environments",3488831,"Facilitated variation: how evolution learns from past environments to generalize to new environments.","One of the striking features of evolution is the appearance of novel structures in organisms. Recently, Kirschner and Gerhart have integrated discoveries in evolution, genetics, and developmental biology to form a theory of facilitated variation (FV). The key observation is that organisms are designed such that random genetic changes are channeled in phenotypic directions that are potentially useful. An open question is how FV spontaneously emerges during evolution. Here, we address this by means of computer simulations of two well-studied model systems, logic circuits and RNA secondary structure. We find that evolution of FV is enhanced in environments that change from time to time in a systematic way: the varying environments are made of the same set of subgoals but in different combinations. We find that organisms that evolve under such varying goals not only remember their history but also generalize to future environments, exhibiting high adaptability to novel goals. Rapid adaptation is seen to goals composed of the same subgoals in novel combinations, and to goals where one of the subgoals was never seen in the history of the organism. The mechanisms for such enhanced generation of novelty (generalization) are analyzed, as is the way that organisms store information in their genomes about their past environments. Elements of facilitated variation theory, such as weak regulatory linkage, modularity, and reduced pleiotropy of mutations, evolve spontaneously under these conditions. Thus, environments that change in a systematic, modular fashion seem to promote facilitated variation and allow evolution to generalize to novel conditions."
1715,"an integrated software system for analyzing chipchip and chipseq data",3494817,"An integrated software system for analyzing ChIP-chip and ChIP-seq data","We present CisGenome, a software system for analyzing genome-wide chromatin immunoprecipitation (ChIP) data. CisGenome is designed to meet all basic needs of ChIP data analyses, including visualization, data normalization, peak detection, false discovery rate computation, gene-peak association, and sequence and motif analysis. In addition to implementing previously published ChIP–microarray (ChIP-chip) analysis methods, the software contains statistical methods designed specifically for ChlP sequencing (ChIP-seq) data obtained by coupling ChIP with massively parallel sequencing. The modular design of CisGenome enables it to support interactive analyses through a graphic user interface as well as customized batch-mode computation for advanced data mining. A built-in browser allows visualization of array images, signals, gene structure, conservation, and DNA sequence and motif information. We demonstrate the use of these tools by a comparative analysis of ChIP-chip and ChIP-seq data for the transcription factor NRSF/REST, a study of ChIP-seq analysis with or without a negative control sample, and an analysis of a new motif in Nanog- and Sox2-binding regions."
1716,"activity motifs reveal principles of timing in transcriptional control of the yeast metabolic network",3494821,"Activity motifs reveal principles of timing in transcriptional control of the yeast metabolic network","Significant insight about biological networks arises from the study of network motifs—overly abundant network subgraphs1, 2—but such wiring patterns do not specify when and how potential routes within a cellular network are used. To address this limitation, we introduce activity motifs, which capture patterns in the dynamic use of a network. Using this framework to analyze transcription in Saccharomyces cerevisiae metabolism, we find that cells use different timing activity motifs to optimize transcription timing in response to changing conditions: forward activation to produce metabolic compounds efficiently, backward shutoff to rapidly stop production of a detrimental product and synchronized activation for co-production of metabolites required for the same reaction. Measuring protein abundance over a time course reveals that mRNA timing motifs also occur at the protein level. Timing motifs significantly overlap with binding activity motifs, where genes in a linear chain have ordered binding affinity to a transcription factor, suggesting a mechanism for ordered transcription. Finely timed transcriptional regulation is therefore abundant in yeast metabolism, optimizing the organism's adaptation to new environmental conditions."
1717,"reactome knowledgebase of human biological pathways and processes",3497386,"Reactome knowledgebase of human biological pathways and processes.","Reactome (http://www.reactome.org) is an expert-authored, peer-reviewed knowledgebase of human reactions and pathways that functions as a data mining resource and electronic textbook. Its current release includes 2975 human proteins, 2907 reactions and 4455 literature citations. A new entity-level pathway viewer and improved search and data mining tools facilitate searching and visualizing pathway data and the analysis of user-supplied high-throughput data sets. Reactome has increased its utility to the model organism communities with improved orthology prediction methods allowing pathway inference for 22 species and through collaborations to create manually curated Reactome pathway datasets for species including Arabidopsis, Oryza sativa (rice), Drosophila and Gallus gallus (chicken). Reactome's data content and software can all be freely used and redistributed under open source terms."
1718,"empirical studies of agile software development a systematic review",3504163,"Empirical studies of agile software development: A systematic review","Agile software development represents a major departure from traditional, plan-based approaches to software engineering. A systematic review of empirical studies of agile software development up to and including 2005 was conducted. The search strategy identified 1996 studies, of which 36 were identified as empirical studies. The studies were grouped into four themes: introduction and adoption, human and social factors, perceptions on agile methods, and comparative studies. The review investigates what is currently known about the benefits and limitations of, and the strength of evidence for, agile methods. Implications for research and practice are presented. The main implication for research is a need for more and better empirical studies of agile software development within a common research agenda. For the industrial readership, the review provides a map of findings, according to topic, that can be compared for relevance to their own settings and situations."
1719,"identifying positioned nucleosomes with epigenetic marks in human from chipseq",3507817,"Identifying positioned nucleosomes with epigenetic marks in human from ChIP-Seq.","BACKGROUND: In vivo positioning and covalent modifications of nucleosomes play an important role in epigenetic regulation, but genome-wide studies of positioned nucleosomes and their modifications in human still remain limited. RESULTS: This paper describes a novel computational framework to efficiently identify positioned nucleosomes and their histone modification profiles from nucleosome-resolution histone modification ChIP-Seq data. We applied the algorithm to histone methylation ChIP-Seq data in human CD4+ T cells and identified over 438,000 positioned nucleosomes, which appear predominantly at functionally important regions such as genes, promoters, DNase I hypersensitive regions, and transcription factor binding sites. Our analysis shows the identified nucleosomes play a key role in epigenetic gene regulation within those functionally important regions via their positioning and histone modifications. CONCLUSION: Our method provides an effective framework for studying nucleosome positioning and epigenetic marks in mammalian genomes. The algorithm is open source and available at http://liulab.dfci.harvard.edu/NPS/."
1720,"arrayplex distributed interactive and programmatic access to genome sequence annotation ontology and analytical toolsets",3508967,"ArrayPlex: distributed, interactive and programmatic access to genome sequence, annotation, ontology, and analytical toolsets","ABSTRACT: ArrayPlex is a software package that centrally provides a large number of flexible toolsets useful for functional genomics including microarray data storage, quality assessments, data visualization, gene annotation retrieval, statistical tests, genomic sequence retrieval and motif analysis. It uses a client-server architecture based on open source components, provides graphical, command-line, and programmatic access to all needed resources, and is extensible by virtue of a documented application programming interface. ArrayPlex is available at http://sourceforge.net/projects/arrayplex/."
1721,"lead chemical porphyria and heme as a biological mediator",3574100,"Lead, Chemical Porphyria, and Heme as a Biological Mediator","One of the most well-characterized symptoms of lead poisoning is porphyria. The biochemical signs of lead intoxication related to porphyria are δ-aminolevulinic aciduria, coproporphyrinuria, and accumulation of free and zinc protoporphyrin in erythrocytes. From the 1970s to the early 80s, almost all of the enzymes in the heme pathway had been purified and characterized, and it was demonstrated that δ-aminolevulinic aciduria is due to inhibition of δ-aminolevulinate dehydratase by lead. Lead also inhibits purified ferrochelatase; however, the magnitude of inhibition was essentially nil even under pathological conditions. Further study proved the disturbance of iron-reducing activity by moderate lead exposure. Far different from these two enzymes, lead failed to inhibit purified coproporphyrinogen oxidase, i.e., the mechanism of coproporphyrinuria has not yet been understood. During the 80s to the 90s, the effects of environmental hazards including lead were elucidated through stress proteins, indicating the induction of some heme pathway enzymes as stress proteins. At that time, gene environment interaction was another focus of toxicology, since gene carriers of porphyrias are considered to be a high-risk group to chemical pollutants. Toxicological studies from the 70s to the 90s focused on the direct effect of hazards on biological molecules, such as the heme pathway enzymes, and many environmental pollutants were proved to affect cytosolic heme. Recently, we demonstrated the mechanism of the heme-controlled transcription system, which suggests that the indirect effects of environmental hazards are also important for elucidating toxicity, i.e., the hazards can affect cell functions through such biological mediators as regulatory heme. It is, therefore, probable that toxicology in the future will focus on biological systems such as gene regulation and signal transduction systems."
1722,"transcription factor nrf activation by inorganic arsenic in cultured keratinocytes involvement of hydrogen peroxide",3574337,"Transcription factor Nrf2 activation by inorganic arsenic in cultured keratinocytes: involvement of hydrogen peroxide.","Inorganic arsenic is a well-documented human carcinogen that targets the skin. The induction of oxidative stress, as shown with arsenic, may have a bearing on the carcinogenic mechanism of this metalloid. The transcription factor Nrf2 is a key player in the regulation of genes encoding for many antioxidative response enzymes. Thus, the effect of inorganic arsenic (as sodium arsenite) on Nrf2 expression and localization was studied in HaCaT cells, an immortalized human keratinocyte cell line. We found, for the first time, that arsenic enhanced cellular expression of Nrf2 at the transcriptional and protein levels and activated expression of Nrf2-related genes in these cells. In addition, arsenic exposure caused nuclear accumulation of Nrf2 in association with downstream activation of Nrf2-mediated oxidative response genes. Arsenic simultaneously increased the expression of Keap1, a regulator of Nrf2 activity. The coordinated induction of Keap1 expression and nuclear Nrf2 accumulation induced by arsenic suggests that Keap1 is important to arsenic-induced Nrf2 activation. Furthermore, when cells were pretreated with scavengers of hydrogen peroxide (H 2 O 2 ) such as catalase–polyethylene glycol (PEG-CAT) or Tiron, arsenic-induced nuclear Nrf2 accumulation was suppressed, whereas CuDIPSH, a cell-permeable superoxide dismutase (SOD) mimic compound that produces H 2 O 2 from superoxide (·O 2 − ), enhanced Nrf2 nuclear accumulation. These results indicate that H 2 O 2 , rather than ·O 2 − , is the mediator of nuclear Nrf2 accumulation. Additional study showed that arsenic causes increased cellular H 2 O 2 production and that H 2 O 2 itself has the ability to increase Nrf2 expression at both the transcription and protein levels in HaCaT cells. Taken together, these data clearly show that arsenic increases Nrf2 expression and activity at multiple levels and that H 2 O 2 is one of the mediators of this process."
1723,"rnaseq a revolutionary tool for transcriptomics",3614773,"RNA-Seq: a revolutionary tool for transcriptomics.","RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
1724,"design principles of biochemical oscillators",3655691,"Design principles of biochemical oscillators.","Cellular rhythms are generated by complex interactions among genes, proteins and metabolites. They are used to control every aspect of cell physiology, from signalling, motility and development to growth, division and death. We consider specific examples of oscillatory processes and discuss four general requirements for biochemical oscillations: negative feedback, time delay, sufficient 'nonlinearity' of the reaction kinetics and proper balancing of the timescales of opposing chemical reactions. Positive feedback is one mechanism to delay the negative-feedback signal. Biological oscillators can be classified according to the topology of the positive- and negative-feedback loops in the underlying regulatory mechanism."
1725,"disintegration of the ecological community",3665188,"Disintegration of the Ecological Community","Abstract: In this essay, I argue that the seemingly indestructible concept of the community as a local, interacting assemblage of species has hindered progress toward understanding species richness at local to regional scales. I suggest that the distributions of species within a region reveal more about the processes that generate diversity patterns than does the co‐occurrence of species at any given point. The local community is an epiphenomenon that has relatively little explanatory power in ecology and evolutionary biology. Local coexistence cannot provide insight into the ecogeographic distributions of species within a region, from which local assemblages of species derive, nor can local communities be used to test hypotheses concerning the origin, maintenance, and regulation of species richness, either locally or regionally. Ecologists are moving toward a community concept based on interactions between populations over a continuum of spatial and temporal scales within entire regions, including the population and evolutionary processes that produce new species."
1726,"articles by latin american authors in prestigious journals have fewer citations",3689841,"Articles by latin american authors in prestigious journals have fewer citations.","BACKGROUND: The journal Impact factor (IF) is generally accepted to be a good measurement of the relevance/quality of articles that a journal publishes. In spite of an, apparently, homogenous peer-review process for a given journal, we hypothesize that the country affiliation of authors from developing Latin American (LA) countries affects the IF of a journal detrimentally. METHODOLOGY/PRINCIPAL FINDINGS: Seven prestigious international journals, one multidisciplinary journal and six serving specific branches of science, were examined in terms of their IF in the Web of Science. Two subsets of each journal were then selected to evaluate the influence of author's affiliation on the IF. They comprised contributions (i) with authorship from four Latin American (LA) countries (Argentina, Brazil, Chile and Mexico) and (ii) with authorship from five developed countries (England, France, Germany, Japan and USA). Both subsets were further subdivided into two groups: articles with authorship from one country only and collaborative articles with authorship from other countries. Articles from the five developed countries had IF close to the overall IF of the journals and the influence of collaboration on this value was minor. In the case of LA articles the effect of collaboration (virtually all with developed countries) was significant. The IFs for non-collaborative articles averaged 66% of the overall IF of the journals whereas the articles in collaboration raised the IFs to values close to the overall IF. CONCLUSION/SIGNIFICANCE: The study shows a significantly lower IF in the group of the subsets of non-collaborative LA articles and thus that country affiliation of authors from non-developed LA countries does affect the IF of a journal detrimentally. There are no data to indicate whether the lower IFs of LA articles were due to their inherent inferior quality/relevance or psycho-social trend towards under-citation of articles from these countries. However, further study is required since there are foreseeable consequences of this trend as it may stimulate strategies by editors to turn down articles that tend to be under-cited."
1727,"coordinate regulation of phase i and ii xenobiotic metabolisms by the ah receptor and nrf",3729371,"Coordinate regulation of Phase I and II xenobiotic metabolisms by the Ah receptor and Nrf2.","The aryl hydrocarbon receptor (AhR) is a ligand-activated transcription factor with important roles in metabolic adaptation, normal physiology and dioxin toxicology. Metabolic adaptation is based on coordinate regulation of a set of xenobiotic-metabolizing enzymes (XMEs), termed AhR battery. Coordination is achieved by AhR/Arnt-binding to XREs (xenobiotic response elements), identified in the 5' upstream region of AhR target genes. The AhR battery encodes Phase I and II enzymes. Interestingly, these Phase II genes are linked to the Nrf2 gene battery that encodes enzymes that are essential in protection against oxidative/electrophile stress. Nrf2 binds to AREs (antioxidant response elements) in the regulatory region of a large and distinct set of target genes. Functionally characterized response elements such as XREs and AREs in the regulatory region of target genes may provide a genetic basis to understand AhR- and Nrf2-induced genes. Linkage between AhR and Nrf2 batteries is probably achieved by multiple mechanisms, including Nrf2 as a target gene of the AhR, indirect activation of Nrf2 via CYP1A1-generated reactive oxygen species, and direct cross-interaction of AhR/XRE and Nrf2/ARE signaling. Linkage appears to be species- and cell-dependent. However, mechanisms linking XRE- and ARE-controlled Phase II genes need further investigation. Tightened coupling between Phases I and II by AhR- and Nrf2-induced XMEs may greatly attenuate health risks posed by CYP1A1-generated toxic intermediates and reactive oxygen species. Better recognition of coordinate Phase I and II metabolisms may improve risk assessment of reactive toxic intermediates in the extrapolation to low level endo- and xenobiotic exposure."
1728,"cell shape and cellwall organization in gramnegative bacteria",3741343,"Cell shape and cell-wall organization in Gram-negative bacteria","10.1073/pnas.0805309105 In bacterial cells, the peptidoglycan cell wall is the stress-bearing structure that dictates cell shape. Although many molecular details of the composition and assembly of cell-wall components are known, how the network of peptidoglycan subunits is organized to give the cell shape during normal growth and how it is reorganized in response to damage or environmental forces have been relatively unexplored. In this work, we introduce a quantitative physical model of the bacterial cell wall that predicts the mechanical response of cell shape to peptidoglycan damage and perturbation in the rod-shaped Gram-negative bacterium . To test these predictions, we use time-lapse imaging experiments to show that damage often manifests as a bulge on the sidewall, coupled to large-scale bending of the cylindrical cell wall around the bulge. Our physical model also suggests a surprising robustness of cell shape to peptidoglycan defects, helping explain the observed porosity of the cell wall and the ability of cells to grow and maintain their shape even under conditions that limit peptide crosslinking. Finally, we show that many common bacterial cell shapes can be realized within the same model via simple spatial patterning of peptidoglycan defects, suggesting that minor patterning changes could underlie the great diversity of shapes observed in the bacterial kingdom."
1729,"global analysis of the insulator binding protein ctcf in chromatin barrier regions reveals demarcation of active and repressive domains",3749174,"Global analysis of the insulator binding protein CTCF in chromatin barrier regions reveals demarcation of active and repressive domains","Insulators are DNA elements that prevent inappropriate interactions between the neighboring regions of the genome. They can be functionally classified as either enhancer blockers or domain barriers. CTCF (CCCTC-binding factor) is the only known major insulator-binding protein in the vertebrates and has been shown to bind many enhancer-blocking elements. However, it is not clear whether it plays a role in chromatin domain barriers between active and repressive domains. Here, we used ChIP-seq to map the genome-wide binding sites of CTCF in three cell types and identified significant binding of CTCF to the boundaries of repressive chromatin domains marked by H3K27me3. Although we find an extensive overlapping of CTCF-binding sites across the three cell types, its association with the domain boundaries is cell-type-specific. We further show that the nucleosomes flanking CTCF-binding sites are well positioned. Interestingly, we found a complementary pattern between the repressive H3K27me3 and the active H2AK5ac regions, which are separated by CTCF. Our data indicate that CTCF may play important roles in the barrier activity of insulators, and this study provides a resource for further investigation of the CTCF function in organizing chromatin in the human genome."
1730,"extracting information from textual documents in the electronic health record a review of recent research",3749296,"Extracting information from textual documents in the electronic health record: a review of recent research.","OBJECTIVES: We examine recent published research on the extraction of information from textual documents in the Electronic Health Record (EHR). METHODS: Literature review of the research published after 1995, based on PubMed, conference proceedings, and the ACM Digital Library, as well as on relevant publications referenced in papers already included. RESULTS: 174 publications were selected and are discussed in this review in terms of methods used, pre-processing of textual documents, contextual features detection and analysis, extraction of information in general, extraction of codes and of information for decision-support and enrichment of the EHR, information extraction for surveillance, research, automated terminology management, and data mining, and de-identification of clinical text. CONCLUSIONS: Performance of information extraction systems with clinical text has improved since the last systematic review in 1995, but they are still rarely applied outside of the laboratory they have been developed in. Competitive challenges for information extraction from clinical text, along with the availability of annotated clinical text corpora, and further improvements in system performance are important factors to stimulate advances in this field and to increase the acceptance and usage of these systems in concrete clinical and biomedical research contexts."
1731,"dynamic spread of happiness in a large social network longitudinal analysis over years in the framingham heart study",3749403,"Dynamic spread of happiness in a large social network: longitudinal analysis over 20 years in the Framingham Heart Study","Objectives To evaluate whether happiness can spread from person to person and whether niches of happiness form within social networks. Design Longitudinal social network analysis. Setting Framingham Heart Study social network. Participants 4739 individuals followed from 1983 to 2003. Main outcome measures Happiness measured with validated four item scale; broad array of attributes of social networks and diverse social ties. Results Clusters of happy and unhappy people are visible in the network, and the relationship between people's happiness extends up to three degrees of separation (for example, to the friends of one's friends' friends). People who are surrounded by many happy people and those who are central in the network are more likely to become happy in the future. Longitudinal statistical models suggest that clusters of happiness result from the spread of happiness and not just a tendency for people to associate with similar individuals. A friend who lives within a mile (about 1.6 km) and who becomes happy increases the probability that a person is happy by 25{%} (95{%} confidence interval 1{%} to 57{%}). Similar effects are seen in coresident spouses (8{%}, 0.2{%} to 16{%}), siblings who live within a mile (14{%}, 1{%} to 28{%}), and next door neighbours (34{%}, 7{%} to 70{%}). Effects are not seen between coworkers. The effect decays with time and with geographical separation. Conclusions People's happiness depends on the happiness of others with whom they are connected. This provides further justification for seeing happiness, like health, as a collective phenomenon."
1732,"divergent transcription from active promoters",3750481,"Divergent transcription from active promoters.","Transcription initiation by RNA polymerase II (RNAPII) is thought to occur unidirectionally from most genes. Here, we present evidence of widespread divergent transcription at protein-encoding gene promoters. Transcription start site-associated RNAs (TSSa-RNAs) nonrandomly flank active promoters, with peaks of antisense and sense short RNAs at 250 nucleotides upstream and 50 nucleotides downstream of TSSs, respectively. Northern analysis shows that TSSa-RNAs are subsets of an RNA population 20 to 90 nucleotides in length. Promoter-associated RNAPII and H3K4-trimethylated histones, transcription initiation hallmarks, colocalize at sense and antisense TSSa-RNA positions; however, H3K79-dimethylated histones, characteristic of elongating RNAPII, are only present downstream of TSSs. These results suggest that divergent transcription over short distances is common for active promoters and may help promoter regions maintain a state poised for subsequent regulation."
1733,"targeted screening of cisregulatory variation in human haplotypes",3757771,"Targeted screening of cis-regulatory variation in human haplotypes.","Regulatory cis-acting variants account for a large proportion of gene expression variability in populations. Cis-acting differences can be specifically measured by comparing relative levels of allelic transcripts within a sample. Allelic expression (AE) mapping for cis-regulatory variant discovery has been hindered by the requirements of having informative or heterozygous single nucleotide polymorphisms (SNPs) within genes in order to assign the allelic origin of each transcript. In this study we have developed an approach to systematically screen for heritable cis-variants in common human haplotypes across >1,000 genes. In order to achieve the highest level of information per haplotype studied, we carried out allelic expression measurements by using both intronic and exonic SNPs in primary transcripts. We used a novel RNA pooling strategy in immortalized lymphoblastoid cell lines (LCLs) and primary human osteoblast cell lines (HObs) to allow for high-throughput AE. Screening hits from RNA pools were further validated by performing allelic expression mapping in individual samples. Our results indicate that >10% of expressed genes in human LCLs show genotype-linked AE. In addition, we have validated cis-acting variants in over 20 genes linked with common disease susceptibility in recent genome-wide studies. More generally, our results indicate that RNA pooling coupled with AE read-out by second generation sequencing or by other methods provides a high-throughput tool for cataloging the impact of common noncoding variants in the human genome."
1734,"biodiversity informatics managing and applying primary biodiversity data",3760539,"Biodiversity informatics: managing and applying primary biodiversity data","Recently, advances in information technology and an increased willingness to share primary biodiversity data are enabling unprecedented access to it. By combining presences of species data with electronic cartography via a number of algorithms, estimating niches of species and their areas of distribution becomes feasible at resolutions one to three orders of magnitude higher than it was possible a few years ago. Some examples of the power of that technique are presented. For the method to work, limitations such as lack of high-quality taxonomic determination, precise georeferencing of the data and availability of high-quality and updated taxonomic treatments of the groups must be overcome. These are discussed, together with comments on the potential of these biodiversity informatics techniques not only for fundamental studies but also as a way for developing countries to apply state of the art bioinformatic methods and large quantities of data, in practical ways, to tackle issues of biodiversity management."
1735,"ethnographic approaches to the internet and computermediated communication",3801011,"Ethnographic Approaches to the Internet and Computer-Mediated Communication","In this article we review ethnographic research on the Internet and computer-mediated communication. The technologically mediated environment prevents researchers from directly observing research participants and often makes the interaction anonymous. In addition, in the online environment direct interaction with participants is replaced by computer-screen data that are largely textual, but may include combinations of textual, visual, aural, and kinetic components. We show how the online environment requires adjustments in how ethnographers define the setting of their research, conduct participant observation and interviews, obtain access to settings and research subjects, and deal with the ethical dilemmas posed by the medium. 10.1177/0891241607310839"
1736,"microscopy and its focal switch",3837640,"Microscopy and its focal switch","Until not very long ago, it was widely accepted that lens-based (far-field) optical microscopes cannot visualize details much finer than about half the wavelength of light. The advent of viable physical concepts for overcoming the limiting role of diffraction in the early 1990s set off a quest that has led to readily applicable and widely accessible fluorescence microscopes with nanoscale spatial resolution. Here I discuss the principles of these methods together with their differences in implementation and operation. Finally, I outline potential developments."
1737,"syntactical negation detection in clinical practice guidelines",3857936,"Syntactical negation detection in clinical practice guidelines.","In clinical practice guidelines (CPGs) the medical information is stored in a narrative way. A large part of this information occurs in a negated form. The detection of negation in CPGs is an important task since it helps medical personnel to identify not occurring symptoms and diseases as well as treatment actions that should not be accomplished. We developed algorithms capable of Negation Detection in this kind of medical documents. According to our results, we are convinced that the involvement of syntactical methods can improve Negation Detection, not only in medical writings but also in arbitrary narrative texts."
1738,"biomart biological queries made easy",3886758,"BioMart - biological queries made easy","ABSTRACT: BACKGROUND: Biologists need to perform complex queries, often across a variety of databases. Typically, each data resource provides an advanced query interface, each of which must be learnt by the biologist before they can begin to query them. Frequently, more than one data source is required and for high-throughput analysis, cutting and pasting results between websites is certainly very time consuming. Therefore, many groups rely on local bioinformatics support to process queries by accessing the resource's programmatic interfaces if they exist. This is not an efficient solution in terms of cost and time. Instead, it would be better if the biologist only had to learn one generic interface. BioMart provides such a solution. RESULTS: BioMart enables scientists to perform advanced querying of biological data sources through a single web interface. The power of the system comes from integrated querying of data sources regardless of their geographical locations. Once these queries have been defined, they may be automated with its ""scripting at the click of a button"" functionality. BioMart's capabilities are extended by integration with several widely used software packages such as BioConductor, DAS, Galaxy, Cytoscape, Taverna. In this paper, we describe all aspects of BioMart from a user's perspective and demonstrate how it can be used to solve real biological use cases such as SNP selection for candidate gene screening or annotation of microarray results. CONCLUSIONS: BioMart is an easy to use, generic and scalable system and therefore, has become an integral part of large data resources including Ensembl, UniProt, HapMap, Wormbase, Gramene, Dictybase, PRIDE, MSD and Reactome. BioMart is freely accessible to use at www.biomart.org."
1739,"small silencing rnas an expanding universe",3891587,"Small silencing RNAs: an expanding universe.","Since the discovery in 1993 of the first small silencing RNA, a dizzying number of small RNA classes have been identified, including microRNAs (miRNAs), small interfering RNAs (siRNAs) and Piwi-interacting RNAs (piRNAs). These classes differ in their biogenesis, their modes of target regulation and in the biological pathways they regulate. There is a growing realization that, despite their differences, these distinct small RNA pathways are interconnected, and that small RNA pathways compete and collaborate as they regulate genes and protect the genome from external and internal threats."
1740,"generalized linear mixed models a practical guide for ecology and evolution",3979340,"Generalized linear mixed models: a practical guide for ecology and evolution.","How should ecologists and evolutionary biologists analyze nonnormal data that involve random effects? Nonnormal data such as counts or proportions often defy classical statistical procedures. Generalized linear mixed models (GLMMs) provide a more flexible approach for analyzing nonnormal data when random effects are present. The explosion of research on GLMMs in the last decade has generated considerable uncertainty for practitioners in ecology and evolution. Despite the availability of accurate techniques for estimating GLMM parameters in simple cases, complex GLMMs are challenging to fit and statistical inference such as hypothesis testing remains difficult. We review the use (and misuse) of GLMMs in ecology and evolution, discuss estimation and inference and summarize [`]best-practice' data analysis procedures for scientists facing this challenge."
1741,"computations of standard binding free energies with molecular dynamics simulations",3981307,"Computations of Standard Binding Free Energies with Molecular Dynamics Simulations","doi: 10.1021/jp807701h An increasing number of studies have reported computations of the standard (absolute) binding free energy of small ligands to proteins using molecular dynamics (MD) simulations and explicit solvent molecules that are in good agreement with experiments. This encouraging progress suggests that physics-based approaches hold the promise of making important contributions to the process of drug discovery and optimization in the near future. Two types of approaches are principally used to compute binding free energies with MD simulations. The most widely known is the alchemical double decoupling method, in which the interaction of the ligand with its surroundings are progressively switched off. It is also possible to use a potential of mean force (PMF) method, in which the ligand is physically separated from the protein receptor. For both of these computational approaches, restraining potentials may be activated and released during the simulation for sampling efficiently the changes in translational, rotational, and conformational freedom of the ligand and protein upon binding. Because such restraining potentials add bias to the simulations, it is important that their effects be rigorously removed to yield a binding free energy that is properly unbiased with respect to the standard state. A review of recent results is presented, and differences in computational methods are discussed. Examples of computations with T4-lysozyme mutants, FKBP12, SH2 domain, and cytochrome P450 are discussed and compared. Remaining difficulties and challenges are highlighted."
1742,"multiple wholegenome alignments without a reference organism",3981325,"Multiple whole-genome alignments without a reference organism","10.1101/gr.081778.108 Multiple sequence alignments have become one of the most commonly used resources in genomics research. Most algorithms for multiple alignment of whole genomes rely either on a reference genome, against which all of the other sequences are laid out, or require a one-to-one mapping between the nucleotides of the genomes, preventing the alignment of recently duplicated regions. Both approaches have drawbacks for whole-genome comparisons. In this paper we present a novel symmetric alignment algorithm. The resulting alignments not only represent all of the genomes equally well, but also include all relevant duplications that occurred since the divergence from the last common ancestor. Our algorithm, implemented as a part of the VISTA Genome Pipeline (VGP), was used to align seven vertebrate and six  genomes. The resulting whole-genome alignments demonstrate a higher sensitivity and specificity than the pairwise alignments previously available through the VGP and have higher exon alignment accuracy than comparable public whole-genome alignments. Of the multiple alignment methods tested, ours performed the best at aligning genes from multigene families—perhaps the most challenging test for whole-genome alignments. Our whole-genome multiple alignments are available through the VISTA Browser at ."
1743,"itunes university and the classroom can podcasts replace professors",3994145,"iTunes University and the classroom: Can podcasts replace Professors?","iTunes University, a website with downloadable educational podcasts, can provide students the opportunity to obtain professors’ lectures when students are unable to attend class. To determine the effectiveness of audio lectures in higher education, undergraduate general psychology students participated in one of two conditions. In the lecture condition, participants listened to a 25-min lecture given in person by a professor using PowerPoint slides. Copies of the slides were given to aid note-taking. In the podcast condition, participants received a podcast of the same lecture along with the PowerPoint handouts. Participants in both conditions were instructed to keep a running log of study time and activities used in preparing for an exam. One week from the initial session students returned to take an exam on lecture content. Results indicated that students in the podcast condition who took notes while listening to the podcast scored significantly higher than the lecture condition. The impact of mobile learning on classroom performance is discussed."
1744,"comparison of small n statistical tests of differential expression applied to microarrays",4000622,"Comparison of small n statistical tests of differential expression applied to microarrays.","BACKGROUND: DNA microarrays provide data for genome wide patterns of expression between observation classes. Microarray studies often have small samples sizes, however, due to cost constraints or specimen availability. This can lead to poor random error estimates and inaccurate statistical tests of differential expression. We compare the performance of the standard t-test, fold change, and four small n statistical test methods designed to circumvent these problems. We report results of various normalization methods for empirical microarray data and of various random error models for simulated data. RESULTS: Three Empirical Bayes methods (CyberT, BRB, and limma t-statistics) were the most effective statistical tests across simulated and both 2-colour cDNA and Affymetrix experimental data. The CyberT regularized t-statistic in particular was able to maintain expected false positive rates with simulated data showing high variances at low gene intensities, although at the cost of low true positive rates. The Local Pooled Error (LPE) test introduced a bias that lowered false positive rates below theoretically expected values and had lower power relative to the top performers. The standard two-sample t-test and fold change were also found to be sub-optimal for detecting differentially expressed genes. The generalized log transformation was shown to be beneficial in improving results with certain data sets, in particular high variance cDNA data. CONCLUSION: Pre-processing of data influences performance and the proper combination of pre-processing and statistical testing is necessary for obtaining the best results. All three Empirical Bayes methods assessed in our study are good choices for statistical tests for small n microarray studies for both Affymetrix and cDNA data. Choice of method for a particular study will depend on software and normalization preferences."
1745,"computational social science",4015011,"Computational Social Science","The social sciences investigate human and social dynamics and organization at all levels of analysis (consilience), including cognition, decision making, behavior, groups, organizations, societies, and the world system. Computational social science is the integrated, interdisciplinary pursuit of social inquiry with emphasis on information processing and through the medium of advanced computation. The main computational social science areas are automated information extraction systems, social network analysis, social geographic information systems (GIS), complexity modeling, and social simulation models. Just like Galileo exploited the telescope as the key instrument for observing and gaining a deeper and empirically truthful understanding of the physical universe, computational social scientists are learning to exploit the advanced and increasingly powerful instruments of computation to see beyond the visible spectrum of more traditional disciplinary analyses. Copyright © 2010 John Wiley & Sons, Inc.For further resources related to this article, please visit the"
1746,"classifying tags using open content resources",4037235,"Classifying tags using open content resources","Tagging has emerged as a popular means to annotate on-line objects such as bookmarks, photos and videos. Tags vary in semantic meaning and can describe different aspects of a media object. Tags describe the content of the media as well as locations, dates, people and other associated meta-data. Being able to automatically classify tags into semantic categories allows us to understand better the way users annotate media objects and to build tools for viewing and browsing the media objects. In this paper we present a generic method for classifying tags using third party open content resources, such as Wikipedia and the Open Directory. Our method uses structural patterns that can be extracted from resource meta-data. We describe the implementation of our method on Wikipedia using WordNet categories as our classification schema and ground truth. Two structural patterns found in Wikipedia are used for training and classification: categories and templates. We apply our system to classifying Flickr tags. Compared to a WordNet baseline our method increases the coverage of the Flickr vocabulary by 115%. We can classify many important entities that are not covered by WordNet, such as,  London Eye, Big Island, Ronaldinho, geocaching  and  wii ."
1747,"ab initio construction of a eukaryotic transcriptome by massively parallel mrna sequencing",4042280,"Ab initio construction of a eukaryotic transcriptome by massively parallel mRNA sequencing.","Defining the transcriptome, the repertoire of transcribed regions encoded in the genome, is a challenging experimental task. Current approaches, relying on sequencing of ESTs or cDNA libraries, are expensive and labor-intensive. Here, we present a general approach for ab initio discovery of the complete transcriptome of the budding yeast, based only on the unannotated genome sequence and millions of short reads from a single massively parallel sequencing run. Using novel algorithms, we automatically construct a highly accurate transcript catalog. Our approach automatically and fully defines 86% of the genes expressed under the given conditions, and discovers 160 previously undescribed transcription units of 250 bp or longer. It correctly demarcates the 5' and 3' UTR boundaries of 86 and 77% of expressed genes, respectively. The method further identifies 83% of known splice junctions in expressed genes, and discovers 25 previously uncharacterized introns, including 2 cases of condition-dependent intron retention. Our framework is applicable to poorly understood organisms, and can lead to greater understanding of the transcribed elements in an explored genome."
1748,"a groupwise association test for rare mutations using a weighted sum statistic",4044723,"A groupwise association test for rare mutations using a weighted sum statistic.","Resequencing is an emerging tool for identification of rare disease-associated mutations. Rare mutations are difficult to tag with SNP genotyping, as genotyping studies are designed to detect common variants. However, studies have shown that genetic heterogeneity is a probable scenario for common diseases, in which multiple rare mutations together explain a large proportion of the genetic basis for the disease. Thus, we propose a weighted-sum method to jointly analyse a group of mutations in order to test for groupwise association with disease status. For example, such a group of mutations may result from resequencing a gene. We compare the proposed weighted-sum method to alternative methods and show that it is powerful for identifying disease-associated genes, both on simulated and Encode data. Using the weighted-sum method, a resequencing study can identify a disease-associated gene with an overall population attributable risk (PAR) of 2%, even when each individual mutation has much lower PAR, using 1,000 to 7,000 affected and unaffected individuals, depending on the underlying genetic model. This study thus demonstrates that resequencing studies can identify important genetic associations, provided that specialised analysis methods, such as the weighted-sum method, are used."
1749,"toward a unified ontology of cloud computing",4050881,"Toward a Unified Ontology of Cloud Computing","Progress of research efforts in a novel tech- nology is contingent on having a rigorous organization of its knowledge domain and a comprehensive understanding of  all  the  relevant  components  of  this  technology  and their relationships. Cloud Computing is one contemporary technology in which the research community has recently embarked. Manifesting itself as the descendant of several other computing research areas such as Service-Oriented Architecture, distributed and grid computing, and virtual- ization, cloud computing inherits their advancements and limitations. Towards the end-goal of a thorough comprehension of the field of cloud computing, and a more rapid adoption from the scientific community, we propose in this paper an ontology of this area which demonstrates a dissection of the cloud into five main layers, and illustrates their inter- relations as well as their inter-dependency on preceding technologies. The contribution of this paper lies in being one of the first attempts to establish a detailed ontology of the cloud. Better comprehension of the technology would enable the community to design more efficient portals and gateways for the cloud, and facilitate the adoption of this novel computing approach in scientific environments. In turn, this will assist the scientific community to expedite its contributions and insights into this evolving computing field."
1750,"modulation of dopamine transporter function by alphasynuclein is altered by impairment of cell adhesion and by induction of oxidative stress",4067975,"Modulation of dopamine transporter function by &alpha;-synuclein is altered by impairment of cell adhesion and by induction of oxidative stress","Human -synuclein accumulates in dopaminergic neurons as intraneuronal inclusions, Lewy bodies, which are characteristic of idiopathic Parkinson's disease (PD). Here, we suggest that modulation of the functional activity of the dopamine transporter (DAT) by -synuclein may be a key factor in the preferential degeneration of mesencephalic dopamine (DA)-synthesizing neurons in PD. In cotransfected Ltk-, HEK 293, and SK-N-MC cells, -synuclein induced a 35% decrease in [3H]DA uptake. Biotinylated DAT levels were decreased by 40% in cotransfected cells relative to cells expressing only DAT. DAT was colocalized with -synuclein in mesencephalic neurons and cotransfected Ltk- cells. Coimmunoprecipitation studies showed the existence of a complex between -synuclein and DAT, in specific rat brain regions and cotransfected cells, through specific amino acid motifs of both proteins. The attenuation of DAT function by -synuclein was cytoprotective, because DA-mediated oxidative stress and cell death were reduced in cotransfected cells. The neurotoxin MPP+ (1-methyl-4-phenylpyridinium), oxidative stress, or impairment of cell adhesion ablated the -synuclein-mediated inhibition of DAT activity, which caused increased uptake of DA and increased biotinylated DAT levels, in both mesencephalic neurons and cotransfected cells. These studies suggest a novel normative role for -synuclein in regulating DA synaptic availability and homeostasis, which is relevant to the pathophysiology of PD. Key words: Parkinson's disease  synucleinopathies  MPP+  neurodegeneration  Lewy bodies 10.1096/fj.03-0152fje"
1751,"stressinducible regulation of heat shock factor by the deacetylase sirt",4073078,"Stress-inducible regulation of heat shock factor 1 by the deacetylase SIRT1.","Heat shock factor 1 (HSF1) is essential for protecting cells from protein-damaging stress associated with misfolded proteins and regulates the insulin-signaling pathway and aging. Here, we show that human HSF1 is inducibly acetylated at a critical residue that negatively regulates DNA binding activity. Activation of the deacetylase and longevity factor SIRT1 prolonged HSF1 binding to the heat shock promoter Hsp70 by maintaining HSF1 in a deacetylated, DNA-binding competent state. Conversely, down-regulation of SIRT1 accelerated the attenuation of the heat shock response (HSR) and release of HSF1 from its cognate promoter elements. These results provide a mechanistic basis for the requirement of HSF1 in the regulation of life span and establish a role for SIRT1 in protein homeostasis and the HSR. 10.1126/science.1165946"
1752,"allosteric communication occurs via networks of tertiary and quaternary motions in proteins",4075274,"Allosteric Communication Occurs via Networks of Tertiary and Quaternary Motions in Proteins","Allosteric proteins bind an effector molecule at one site resulting in a functional change at a second site. We hypothesize that allosteric communication in proteins relies upon networks of quaternary (collective, rigid-body) and tertiary (residueâ€“residue contact) motions. We argue that cyclic topology of these networks is necessary for allosteric communication. An automated algorithm identifies rigid bodies from the displacement between the inactive and the active structures and constructs â€œquaternary networksâ€� from these rigid bodies and the substrate and effector ligands. We then integrate quaternary networks with a coarse-grained representation of contact rearrangements to form â€œglobal communication networksâ€� (GCNs). The GCN reveals allosteric communication among all substrate and effector sites in 15 of 18 multidomain and multimeric proteins, while tertiary and quaternary networks exhibit such communication in only 4 and 3 of these proteins, respectively. Furthermore, in 7 of the 15 proteins connected by the GCN, 50% or more of the substrate-effector paths via the GCN are â€œinterdependentâ€� paths that do not exist via either the tertiary or the quaternary network. Substrate-effector â€œpathwaysâ€� typically are not linear but rather consist of polycyclic networks of rigid bodies and clusters of rearranging residue contacts. These results argue for broad applicability of allosteric communication based on structural changes and demonstrate the utility of the GCN. Global communication networks may inform a variety of experiments on allosteric proteins as well as the design of allostery into non-allosteric proteins."
1753,"continuous base identification for singlemolecule nanopore dna sequencing",4087454,"Continuous base identification for single-molecule nanopore DNA sequencing","A single-molecule method for sequencing DNA that does not require fluorescent labelling could reduce costs and increase sequencing speeds. An exonuclease enzyme might be used to cleave individual nucleotide molecules from the DNA, and when coupled to an appropriate detection system, these nucleotides could be identified in the correct order. Here, we show that a protein nanopore with a covalently attached adapter molecule can continuously identify unlabelled nucleoside 5'-monophosphate molecules with accuracies averaging 99.8%. Methylated cytosine can also be distinguished from the four standard DNA bases: guanine, adenine, thymine and cytosine. The operating conditions are compatible with the exonuclease, and the kinetic data show that the nucleotides have a high probability of translocation through the nanopore and, therefore, of not being registered twice. This highly accurate tool is suitable for integration into a system for sequencing nucleic acids and for analysing epigenetic modifications."
1754,"personalized interactive tag recommendation for flickr",4087856,"Personalized, interactive tag recommendation for flickr","We study the problem of personalized, interactive tag recommendation for Flickr: While a user enters/selects new tags for a particular picture, the system suggests related tags to her, based on the tags that she or other people have used in the past along with (some of) the tags already entered. The suggested tags are dynamically updated with every additional tag entered/selected. We describe a new algorithm, called Hybrid, which can be applied to this problem, and show that it outperforms previous algorithms. It has only a single tunable parameter, which we found to be very robust."
1755,"pemer a computational framework with simulationbased error models for inferring genomic structural variants from massive pairedend sequencing data",4089107,"PEMer: a computational framework with simulation-based error models for inferring genomic structural variants from massive paired-end sequencing data","ABSTRACT: Personal-genomics endeavors, such as the 1000 Genomes project, are generating maps of genomic structural variants by analyzing ends of massively sequenced genome fragments. To process these we developed Paired-End Mapper (PEMer; http://sv.gersteinlab.org/pemer). This comprises an analysis pipeline, compatible with several next-generation sequencing platforms; simulation-based error models, yielding confidence-values for each structural variant; and a back-end database. The simulations demonstrated high structural variant reconstruction efficiency for PEMer's coverage-adjusted multi-cutoff scoring-strategy and showed its relative insensitivity to base-calling errors."
1756,"mapping the worlds photos",4091149,"Mapping the World's Photos","We investigate how to organize a large collection of geotagged photos, working with a dataset of about 35 million images collected from Flickr. Our approach combines content analysis based on text tags and image data with structural analysis based on geospatial data. We use the spatial distribution of where people take photos to define a relational structure between the photos that are taken at popular places. We then study the interplay between this structure and the content, using classification methods for predicting such locations from visual, textual and temporal features of the photos. We find that visual and temporal features improve the ability to estimate the location of a photo, compared to using just textual features. We illustrate using these techniques to organize a large photo collection, while also revealing various interesting properties about popular cities and landmarks at a global scale."
1757,"a roadmap of clustering algorithms finding a match for a biomedical application",4096561,"A roadmap of clustering algorithms: finding a match for a biomedical application","Clustering is ubiquitously applied in bioinformatics with hierarchical clustering and k-means partitioning being the most popular methods. Numerous improvements of these two clustering methods have been introduced, as well as completely different approaches such as grid-based, density-based and model-based clustering. For improved bioinformatics analysis of data, it is important to match clusterings to the requirements of a biomedical application. In this article, we present a set of desirable clustering features that are used as evaluation criteria for clustering algorithms. We review 40 different clustering algorithms of all approaches and datatypes. We compare algorithms on the basis of desirable clustering features, and outline algorithms' benefits and drawbacks as a basis for matching them to biomedical applications. 10.1093/bib/bbn058"
1758,"google scholar search performance comparative recall and precision",4122655,"Google Scholar Search Performance: Comparative Recall and Precision","This paper presents a comparative evaluation of Google Scholar and 11 other bibliographic databases (Academic Search Elite, AgeLine, ArticleFirst, EconLit, GEOBASE, MEDLINE, PAIS International, POPLINE, Social Sciences Abstracts, Social Sciences Citation Index, and SocINDEX), focusing on search performance within the multidisciplinary field of later-life migration. The results of simple keyword searches are evaluated with reference to a set of 155 relevant articles identified in advance. In terms of both recall and precision, Google Scholar performs better than most of the subscription databases. This finding, based on a rigorous evaluation procedure, is contrary to the impressions of many early reviewers. The paper concludes with a discussion of a new approach to document relevance in educational settings--an approach that accounts for the instructors' goals as well as the students' assessments of relevance. (Contains 4 tables, 2 figures, and 29 notes.)"
1759,"comparative functional analysis of the caenorhabditis elegans and drosophila melanogaster proteomes",4129082,"Comparative Functional Analysis of the Caenorhabditis elegans and Drosophila melanogaster Proteomes","The nematode Caenorhabditis elegans is a popular model system in genetics, not least because a majority of human disease genes are conserved in C. elegans . To generate a comprehensive inventory of its expressed proteome, we performed extensive shotgun proteomics and identified more than half of all predicted C. elegans proteins. This allowed us to confirm and extend genome annotations, characterize the role of operons in C. elegans , and semiquantitatively infer abundance levels for thousands of proteins. Furthermore, for the first time to our knowledge, we were able to compare two animal proteomes ( C. elegans and Drosophila melanogaster ). We found that the abundances of orthologous proteins in metazoans correlate remarkably well, better than protein abundance versus transcript abundance within each organism or transcript abundances across organisms; this suggests that changes in transcript abundance may have been partially offset during evolution by opposing changes in protein abundance."
1760,"big genomes facilitate the comparative identification of regulatory elements",4133229,"Big Genomes Facilitate the Comparative Identification of Regulatory Elements","<p>The identification of regulatory sequences in animal genomes remains a significant challenge. Comparative genomic methods that use patterns of evolutionary conservation to identify non-coding sequences with regulatory function have yielded many new vertebrate enhancers. However, these methods have not contributed significantly to the identification of regulatory sequences in sequenced invertebrate taxa. We demonstrate here that this differential success, which is often attributed to fundamental differences in the nature of vertebrate and invertebrate regulatory sequences, is instead primarily a product of the relatively small size of sequenced invertebrate genomes. We sequenced and compared loci involved in early embryonic patterning from four species of true fruit flies (family Tephritidae) that have genomes four to six times larger than those of <italic>Drosophila melanogaster</italic>. Unlike in <italic>Drosophila</italic>, where virtually all non-coding DNA is highly conserved, blocks of conserved non-coding sequence in tephritids are flanked by large stretches of poorly conserved sequence, similar to what is observed in vertebrate genomes. We tested the activities of nine conserved non-coding sequences flanking the <italic>even-skipped</italic> gene of the teprhitid <italic>Ceratis capitata</italic> in transgenic <italic>D. melanogaster</italic> embryos, six of which drove patterns that recapitulate those of known <italic>D. melanogaster</italic> enhancers. In contrast, none of the three non-conserved tephritid non-coding sequences that we tested drove expression in <italic>D. melanogaster</italic> embryos. Based on the landscape of non-coding conservation in tephritids, and our initial success in using conservation in tephritids to identify <italic>D. melanogaster</italic> regulatory sequences, we suggest that comparison of tephritid genomes may provide a systematic means to annotate the non-coding portion of the <italic>D. melanogaster</italic> genome. We also propose that large genomes be given more consideration in the selection of species for comparative genomics projects, to provide increased power to detect functional non-coding DNAs and to provide a less biased view of the evolution and function of animal genomes.</p>"
1761,"intrinsic variability of gene expression encoded in nucleosome positioning sequences",4141636,"Intrinsic variability of gene expression encoded in nucleosome positioning sequences.","Variation in gene expression is an essential material for biological diversity among single cells, individuals and populations or species. Here we show that expression variability is an intrinsic property that persists at those different levels. Each promoter seems to have a unique capacity to respond to external signals that can be environmental, genetic or even stochastic. Our investigation into nucleosome organization of variably responding promoters revealed a commonly positioned nucleosome at a critical regulatory region where most transcription start sites and TATA elements are located, a deviation from typical nucleosome-free status. The nucleotide sequences in this region of variable promoters showed a high propensity for DNA bending and a periodic distribution of particular dinucleotides, encoding preferences for DNA-nucleosome interaction. Variable expression is likely to occur during removal of this nucleosome for gene activation. This is a unique example of how promoter sequences intrinsically encode regulatory flexibility, which is vital for biological processes such as adaptation, development and evolution."
1762,"learning to tag",4154311,"Learning to Tag","Social tagging provides valuable and crucial information for large-scale web image retrieval. It is ontology-free and easy to obtain; however, irrelevant tags frequently appear, and users typically will not tag all semantic objects in the image, which is also called semantic loss. To avoid noises and compensate for the semantic loss, tag recommendation is proposed in literature. However, current recommendation simply ranks the related tags based on the single modality of tag co-occurrence on the whole dataset, which ignores other modalities, such as visual correlation. This paper proposes a multi-modality recommendation based on both tag and visual correlation, and formulates the tag recommendation as a learning problem. Each modality is used to generate a ranking feature, and Rankboost algorithm is applied to learn an optimal combination of these ranking features from different modalities. Experiments on Flickr data demonstrate the effectiveness of this learning-based multi-modality recommendation strategy."
1763,"computational systems biology of the cell cycle",4164691,"Computational systems biology of the cell cycle","One of the early success stories of computational systems biology was the work done on cell-cycle regulation. The earliest mathematical descriptions of cell-cycle control evolved into very complex, detailed computational models that describe the regulation of cell division in many different cell types. On the way these models predicted several dynamical properties and unknown components of the system that were later experimentally verified/identified. Still, research on this field is far from over. We need to understand how the core cell-cycle machinery is controlled by internal and external signals, also in yeast cells and in the more complex regulatory networks of higher eukaryotes. Furthermore, there are many computational challenges what we face as new types of data appear thanks to continuing advances in experimental techniques. We have to deal with cell-to-cell variations, revealed by single cell measurements, as well as the tremendous amount of data flowing from high throughput machines. We need new computational concepts and tools to handle these data and develop more detailed, more precise models of cell-cycle regulation in various organisms. Here we review past and present of computational modeling of cell-cycle regulation, and discuss possible future directions of the field. 10.1093/bib/bbp005"
1764,"line graphs link partitions and overlapping communities",4170452,"Line graphs, link partitions, and overlapping communities","In this paper, we use a partition of the links of a network in order to uncover its community structure. This approach allows for communities to overlap at nodes, so that nodes may be in more than one community. We do this by making a node partition of the line graph of the original network. In this way we show that any algorithm which produces a partition of nodes can be used to produce a partition of links. We discuss the role of the degree heterogeneity and propose a weighted version of the line graph in order to account for this."
1765,"harnessing genomics for evolutionary insights",4182419,"Harnessing genomics for evolutionary insights"," Next-generation DNA sequencing technologies can generate unprecedented amounts of genomic data, even for non-model organisms. Here we describe how these new technologies have facilitated recent key advances in ecology and evolutionary biology, and highlight several outstanding ecological and evolutionary questions that are distinctly suited to the innovations they provide. Importantly, using these technologies to their full potential requires careful experimental design and critical consideration of several caveats associated with them. Although several significant challenges remain to be resolved before the integration of next-generation sequencing technologies into single-investigator research programs, we argue that they will soon transform ecology and evolution by fundamentally changing the ranges and types of questions that can be addressed."
1766,"take two aspirin and tweet me in the morning how twitter facebook and other social media are reshaping health care",4186671,"Take two aspirin and tweet me in the morning: how Twitter, Facebook, and other social media are reshaping health care.","If you want a glimpse of what health care could look like a few years from now, consider ""Hello Health,"" the Brooklyn-based primary care practice that is fast becoming an emblem of modern medicine. A paperless, concierge practice that eschews the limitations of insurance-based medicine, Hello Health is popular and successful, largely because of the powerful and cost-effective communication tools it employs: Web-based social media. Indeed, across the health care industry, from large hospital networks to patient support groups, new media tools like weblogs, instant messaging platforms, video chat, and social networks are reengineering the way doctors and patients interact."
1767,"semantic web for integrated network analysis in biomedicine",4199626,"Semantic web for integrated network analysis in biomedicine","The Semantic Web technology enables integration of heterogeneous data on the World Wide Web by making the semantics of data explicit through formal ontologies. In this article, we survey the feasibility and state of the art of utilizing the Semantic Web technology to represent, integrate and analyze the knowledge in various biomedical networks. We introduce a new conceptual framework, semantic graph mining, to enable researchers to integrate graph mining with ontology reasoning in network data analysis. Through four case studies, we demonstrate how semantic graph mining can be applied to the analysis of disease-causal genes, Gene Ontology category cross-talks, drug efficacy analysis and herb-drug interactions analysis. 10.1093/bib/bbp002"
1768,"optical deconstruction of parkinsonian neural circuitry",4202153,"Optical Deconstruction of Parkinsonian Neural Circuitry","Deep brain stimulation (DBS) is a therapeutic option for intractable neurological and psychiatric disorders, including Parkinson's disease and major depression. Because of the heterogeneity of brain tissues where electrodes are placed, it has been challenging to elucidate the relevant target cell types or underlying mechanisms of DBS. We employed optogenetics and solid-state optics to systematically drive or inhibit an array of distinct circuit elements in freely moving Parkinsonian rodents, and found that therapeutic effects within the subthalamic nucleus can be accounted for by direct selective stimulation of afferent axons projecting to this region. In addition to providing insight into DBS mechanisms, these results demonstrate an optical approach for dissection of disease circuitry, and define the technological toolbox needed for systematic deconstruction of disease circuits by selectively controlling individual components. 10.1126/science.1167093"
1769,"evaluating similarity measures for emergent semantics of social tagging",4202177,"Evaluating Similarity Measures for Emergent Semantics of Social Tagging","Social bookmarking systems are becoming increasingly important data sources for bootstrapping and maintaining Semantic Web applications. Their emergent information structures have become known as folksonomies. A key question for harvesting semantics from these systems is how to extend and adapt traditional notions of similarity to folksonomies, and which measures are best suited for applications such as community detection, navigation support, semantic search, user profiling and ontology learning. Here we build an evaluation framework to compare various general folksonomy-based similarity measures, which are derived from several established information-theoretic, statistical, and practical measures. Our framework deals generally and symmetrically with users, tags, and resources. For evaluation purposes we focus on similarity between tags and between resources and consider different methods to aggregate annotations across users. After comparing the ability of several tag similarity measures to predict user-created tag relations, we provide an external grounding by user-validated semantic proxies based on WordNet and the Open Directory Project. We also investigate the issue of scalability. We find that mutual information with distributional micro-aggregation across users yields the highest accuracy, but is not scalable; per-user projection with collaborative aggregation provides the best scalable approach via incremental computations. The results are consistent across resource and tag similarity."
1770,"an introduction to artificial neural networks in bioinformaticsapplication to complex microarray and mass spectrometry datasets in cancer studies",4214142,"An introduction to artificial neural networks in bioinformatics--application to complex microarray and mass spectrometry datasets in cancer studies.","Applications of genomic and proteomic technologies have seen a major increase, resulting in an explosion in the amount of highly dimensional and complex data being generated. Subsequently this has increased the effort by the bioinformatics community to develop novel computational approaches that allow for meaningful information to be extracted. This information must be of biological relevance and thus correlate to disease phenotypes of interest. Artificial neural networks are a form of machine learning from the field of artificial intelligence with proven pattern recognition capabilities and have been utilized in many areas of bioinformatics. This is due to their ability to cope with highly dimensional complex datasets such as those developed by protein mass spectrometry and DNA microarray experiments. As such, neural networks have been applied to problems such as disease classification and identification of biomarkers. This review introduces and describes the concepts related to neural networks, the advantages and caveats to their use, examples of their applications in mass spectrometry and microarray research (with a particular focus on cancer studies), and illustrations from recent literature showing where neural networks have performed well in comparison to other machine learning methods. This should form the necessary background knowledge and information enabling researchers with an interest in these methodologies, but not necessarily from a machine learning background, to apply the concepts to their own datasets, thus maximizing the information gain from these complex biological systems."
1771,"web page classification features and algorithms",4214865,"Web page classification: Features and algorithms","Classification of Web page content is essential to many tasks in Web information retrieval such as maintaining Web directories and focused crawling. The uncontrolled nature of Web content presents additional challenges to Web page classification as compared to traditional text classification, but the interconnected nature of hypertext also provides features that can assist the process."
1772,"human mutation rate associated with dna replication timing",4233160,"Human mutation rate associated with DNA replication timing.","Eukaryotic DNA replication is highly stratified, with different genomic regions shown to replicate at characteristic times during S phase. Here we observe that mutation rate, as reflected in recent evolutionary divergence and human nucleotide diversity, is markedly increased in later-replicating regions of the human genome. All classes of substitutions are affected, suggesting a generalized mechanism involving replication time-dependent DNA damage. This correlation between mutation rate and regionally stratified replication timing may have substantial evolutionary implications."
1773,"a bipedal dna brownian motor with coordinated legs",4267889,"A Bipedal DNA Brownian Motor with Coordinated Legs","A substantial challenge in engineering molecular motors is designing mechanisms to coordinate the motion between multiple domains of the motor so as to bias random thermal motion. For bipedal motors, this challenge takes the form of coordinating the movement of the biped's legs so that they can move in a synchronized fashion. To address this problem, we have constructed an autonomous DNA bipedal walker that coordinates the action of its two legs by cyclically catalyzing the hybridization of metastable DNA fuel strands. This process leads to a chemically ratcheted walk along a directionally polar DNA track. By covalently cross-linking aliquots of the walker to its track in successive walking states, we demonstrate that this Brownian motor can complete a full walking cycle on a track whose length could be extended for longer walks. We believe that this study helps to uncover principles behind the design of unidirectional devices that can function without intervention. This device should be able to fulfill roles that entail the performance of useful mechanical work on the nanometer scale. 10.1126/science.1170336"
1774,"cosmology of the lifshitz universe",4281711,"Cosmology of the Lifshitz universe","We study the ultraviolet complete non-relativistic theory recently proposed by Horava. After introducing a Lifshitz scalar for a general background, we analyze the cosmology of the model in Lorentzian and Euclidean signature. Vacuum solutions are found and it is argued the existence of non-singular bouncing profiles. We find a general qualitative agreement with both the picture of Causal Dynamical Triangulations and Quantum Einstein Gravity. However, inflation driven by a Lifshitz scalar field on a classical background might not produce a scale-invariant spectrum when the principle of detailed balance is assumed."
1775,"webgbrowsea web server for gbrowse",4293788,"WebGBrowse--a web server for GBrowse","Summary: The Generic Genome Browser (GBrowse) is one of the most widely used tools for visualizing genomic features along a reference sequence. However, the installation and configuration of GBrowse is not trivial for biologists. We have developed a web server, WebGBrowse that allows users to upload genome annotation in the GFF3 format, configure the display of each genomic feature by simply using a web browser and visualize the configured genomic features with the integrated GBrowse software.  Availability: WebGBrowse is accessible via http://webgbrowse.cgb.indiana.edu/ and the system is also freely available for local installations.  Contact: dongq@indiana.edu 10.1093/bioinformatics/btp239"
1776,"trapping moving targets with small molecules",4295674,"Trapping Moving Targets with Small Molecules","Structure-based drug design traditionally uses static protein models as inspirations for focusing on ""active"" site targets. Allosteric regulation of biological macromolecules, however, is affected by both conformational and dynamic properties of the protein or protein complex and can potentially lead to more avenues for therapeutic development. We discuss the advantages of searching for molecules that conformationally trap a macromolecule in its inactive state. Although multiple methodologies exist to probe protein dynamics and ligand binding, our current discussion highlights the use of nuclear magnetic resonance spectroscopy in the drug discovery and design process. 10.1126/science.1169378"
1777,"low cost scalable proteomics data analysis using amazons cloud computing services and open source search algorithms",4298633,"Low Cost, Scalable Proteomics Data Analysis Using Amazon’s Cloud Computing Services and Open Source Search Algorithms","PMID: 19358578 We describe a system combining cloud computing and open source software that allows individual laboratories or users to create scalable virtual proteomics analysis clusters and have large-scale computational resources at their disposal at a very low cost without the investment in computational hardware or software licensing fees. We provide detailed step-by-step instructions on using these virtual proteomics analysis clusters at the Medical College of Wisconsin Proteomics Center Web site ( http://proteomics.mcw.edu/vipdac )."
1778,"an introduction to quantum error correction and faulttolerant quantum computation",4340822,"An Introduction to Quantum Error Correction and Fault-Tolerant Quantum Computation","Quantum states are very delicate, so it is likely some sort of quantum error correction will be necessary to build reliable quantum computers. The theory of quantum error-correcting codes has some close ties to and some striking differences from the theory of classical error-correcting codes. Many quantum codes can be described in terms of the stabilizer of the codewords. The stabilizer is a finite Abelian group, and allows a straightforward characterization of the error-correcting properties of the code. The stabilizer formalism for quantum codes also illustrates the relationships to classical coding theory, particularly classical codes over GF(4), the finite field with four elements. To build a quantum computer which behaves correctly in the presence of errors, we also need a theory of fault-tolerant quantum computation, instructing us how to perform quantum gates on qubits which are encoded in a quantum error-correcting code. The threshold theorem states that it is possible to create a quantum computer to perform an arbitrary quantum computation provided the error rate per physical gate or time step is below some constant threshold value."
1779,"d molecular graphics a flattened world of chemistry and biology",4381383,"2D molecular graphics: a flattened world of chemistry and biology","Molecular graphics provides an intuitive way for representation, modeling and analysis of complex chemical and biological systems. It is now widely used in the theoretical chemistry, structural biology, molecular modeling and drug design communities. Traditional molecular graphics techniques mainly dedicate to showing molecular architectures at three-dimensional (3D) level. However, in some occasions the two-dimensional (2D) representation of molecular configurations, profiles, behaviors and interactions may be more readily acceptable for audiences, especially when we need to describe abstract information in a straightforward way or to present numerous data in schematic diagrams. In recent years, 2D representation methods/tools have been developed rapidly for various purposes, ranging from the aesthetic depiction of atomic arrangement for small organic molecules to schematic layout of complicated nonbonding network across the biomolecular binding interfaces, and have received considerable interest in the fields of chemistry, biology and medicine. In this article we first propose the term of 2D molecular graphics to cover the spectrum of 2D representing chemical and biological systems, we also give a comprehensive review on the methods, tools and applications of 2D molecular graphics. 10.1093/bib/bbp013"
1780,"assembling the marine metagenome one cell at a time",4381405,"Assembling the marine metagenome, one cell at a time.","The difficulty associated with the cultivation of most microorganisms and the complexity of natural microbial assemblages, such as marine plankton or human microbiome, hinder genome reconstruction of representative taxa using cultivation or metagenomic approaches. Here we used an alternative, single cell sequencing approach to obtain high-quality genome assemblies of two uncultured, numerically significant marine microorganisms. We employed fluorescence-activated cell sorting and multiple displacement amplification to obtain hundreds of micrograms of genomic DNA from individual, uncultured cells of two marine flavobacteria from the Gulf of Maine that were phylogenetically distant from existing cultured strains. Shotgun sequencing and genome finishing yielded 1.9 Mbp in 17 contigs and 1.5 Mbp in 21 contigs for the two flavobacteria, with estimated genome recoveries of about 91% and 78%, respectively. Only 0.24% of the assembling sequences were contaminants and were removed from further analysis using rigorous quality control. In contrast to all cultured strains of marine flavobacteria, the two single cell genomes were excellent Global Ocean Sampling (GOS) metagenome fragment recruiters, demonstrating their numerical significance in the ocean. The geographic distribution of GOS recruits along the Northwest Atlantic coast coincided with ocean surface currents. Metabolic reconstruction indicated diverse potential energy sources, including biopolymer degradation, proteorhodopsin photometabolism, and hydrogen oxidation. Compared to cultured relatives, the two uncultured flavobacteria have small genome sizes, few non-coding nucleotides, and few paralogous genes, suggesting adaptations to narrow ecological niches. These features may have contributed to the abundance of the two taxa in specific regions of the ocean, and may have hindered their cultivation. We demonstrate the power of single cell DNA sequencing to generate reference genomes of uncultured taxa from a complex microbial community of marine bacterioplankton. A combination of single cell genomics and metagenomics enabled us to analyze the genome content, metabolic adaptations, and biogeography of these taxa."
1781,"how confident can we be that orthologs are similar but paralogs differ",4404756,"How confident can we be that orthologs are similar, but paralogs differ?","Homologous genes are classified into orthologs and paralogs, depending on whether they arose by speciation or duplication. It is widely assumed that orthologs share similar functions, whereas paralogs are expected to diverge more from each other. But does this assumption hold up on further examination? We present evidence that orthologs and paralogs are not so different in either their evolutionary rates or their mechanisms of divergence. We emphasize the importance of appropriately designed studies to test models of gene evolution between orthologs and between paralogs. Thus, functional change between orthologs might be as common as between paralogs, and future studies should be designed to test the impact of duplication against this alternative model."
1782,"the genetic structure and history of africans and african americans",4447374,"The genetic structure and history of Africans and African Americans.","Africa is the source of all modern humans, but characterization of genetic variation and of relationships among populations across the continent has been enigmatic. We studied 121 African populations, four African American populations, and 60 non-African populations for patterns of variation at 1327 nuclear microsatellite and insertion/deletion markers. We identified 14 ancestral population clusters in Africa that correlate with self-described ethnicity and shared cultural and/or linguistic properties. We observed high levels of mixed ancestry in most populations, reflecting historical migration events across the continent. Our data also provide evidence for shared ancestry among geographically diverse hunter-gatherer populations (Khoesan speakers and Pygmies). The ancestry of African Americans is predominantly from Niger-Kordofanian (approximately 71%), European (approximately 13%), and other African (approximately 8%) populations, although admixture levels varied considerably among individuals. This study helps tease apart the complex evolutionary history of Africans and African Americans, aiding both anthropological and genetic epidemiologic studies."
1783,"the gut flora as a forgotten organ",4462135,"The gut flora as a forgotten organ.","The intestinal microflora is a positive health asset that crucially influences the normal structural and functional development of the mucosal immune system. Mucosal immune responses to resident intestinal microflora require precise control and an immunosensory capacity for distinguishing commensal from pathogenic bacteria. In genetically susceptible individuals, some components of the flora can become a liability and contribute to the pathogenesis of various intestinal disorders, including inflammatory bowel diseases. It follows that manipulation of the flora to enhance the beneficial components represents a promising therapeutic strategy. The flora has a collective metabolic activity equal to a virtual organ within an organ, and the mechanisms underlying the conditioning influence of the bacteria on mucosal homeostasis and immune responses are beginning to be unravelled. An improved understanding of this hidden organ will reveal secrets that are relevant to human health and to several infectious, inflammatory and neoplastic disease processes."
1784,"contextaware recommender systems",4486449,"Context-aware recommender systems","The importance of contextual information has been recognized by researchers and practitioners in many disciplines, including e-commerce personalization, information retrieval, ubiquitous and mobile computing, data mining, marketing, and management. While a substantial amount of research has already been performed in the area of recommender systems, most existing approaches focus on recommending the most relevant items to users without taking into account any additional contextual information, such as time, location, or the company of other people (e.g., for watching movies or dining out). In this chapter we argue that relevant contextual information does matter in recommender systems and that it is important to take this information into account when providing recommendations. We discuss the general notion of context and how it can be modeled in recommender systems. Furthermore, we introduce three different algorithmic paradigms – contextual pre-filtering, post-filtering, and modeling – for incorporating contextual information into the recommendation process, discuss the possibilities of combining several context-aware recommendation techniques into a single unifying approach, and provide a case study of one such combined approach. Finally, we present additional capabilities for context-aware recommenders and discuss important and promising directions for future research."
1785,"indelible a flexible simulator of biological sequence evolution",4510581,"INDELible: a flexible simulator of biological sequence evolution.","Many methods exist for reconstructing phylogenies from molecular sequence data, but few phylogenies are known and can be used to check their efficacy. Simulation remains the most important approach to testing the accuracy and robustness of phylogenetic inference methods. However, current simulation programs are limited, especially concerning realistic models for simulating insertions and deletions. We implement a portable and flexible application, named INDELible, for generating nucleotide, amino acid and codon sequence data by simulating insertions and deletions (indels) as well as substitutions. Indels are simulated under several models of indel-length distribution. The program implements a rich repertoire of substitution models, including the general unrestricted model and nonstationary nonhomogeneous models of nucleotide substitution, mixture, and partition models that account for heterogeneity among sites, and codon models that allow the nonsynonymous/synonymous substitution rate ratio to vary among sites and branches. With its many unique features, INDELible should be useful for evaluating the performance of many inference methods, including those for multiple sequence alignment, phylogenetic tree inference, and ancestral sequence, or genome reconstruction."
1786,"bowiki an ontologybased wiki for annotation of data and integration of knowledge in biology",4525735,"BOWiki: an ontology-based wiki for annotation of data and integration of knowledge in biology.","Ontology development and the annotation of biological data using ontologies are time-consuming exercises that currently requires input from expert curators. Open, collaborative platforms for biological data annotation enable the wider scientific community to become involved in developing and maintaining such resources. However, this openness raises concerns regarding the quality and correctness of the information added to these knowledge bases. The combination of a collaborative web-based platform with logic-based approaches and Semantic Web technology can be used to address some of these challenges and concerns. We have developed the BOWiki, a web-based system that includes a biological core ontology. The core ontology provides background knowledge about biological types and relations. Against this background, an automated reasoner assesses the consistency of new information added to the knowledge base. The system provides a platform for research communities such as wikis for the description, discussion and annotation of the functions of genes and gene products [Wang, 2006, Hoehndorf et al., 2006, Giles, 2007]. However, an open approach like wikis frequently raises concerns regarding the quality of the information captured. The information represented in the wiki should adhere to particular quality criteria such as internal consistency (the wiki content does not contain contradictory information) and consistency with biological background knowledge (the wiki content should be semantically correct). To address some of these concerns, logic-based tools can be employed. We have developed the BOWiki, a wiki system that uses a core ontology together with an automated reasoner to maintain a consistent knowledge base. It is specifically targeted at small- to medium-sized communities. to collaboratively integrate information and annotate data. The BOWiki and supplementary material is available at http://www. bowiki.net/. The source code is available under the GNU GPL from http://onto.eva.mpg.de/trac/BoWiki. Contact: bowiki-users@lists.informatik.uni-leipzig.de"
1787,"beautiful data the stories behind elegant data solutions",4532761,"Beautiful Data: The Stories Behind Elegant Data Solutions","In this insightful book, you'll learn from the best data practitioners in the field just how wide-ranging -- and beautiful -- working with data can be. Join 39 contributors as they explain how they developed simple and elegant solutions on projects ranging from the Mars lander to a Radiohead video. With Beautiful Data, you will: Explore the opportunities and challenges involved in working with the vast number of datasets made available by the Web Learn how to visualize trends in urban crime, using maps and data mashups Discover the challenges of designing a data processing system that works within the constraints of space travel Learn how crowdsourcing and transparency have combined to advance the state of drug research Understand how new data can automatically trigger alerts when it matches or overlaps pre-existing data Learn about the massive infrastructure required to create, capture, and process DNA data  That's only small sample of what you'll find in Beautiful Data. For anyone who handles data, this is a truly fascinating book. Contributors include: Nathan Yau Jonathan Follett and Matt Holm J.M. Hughes Raghu Ramakrishnan, Brian Cooper, and Utkarsh Srivastava Jeff Hammerbacher Jason Dykes and Jo Wood Jeff Jonas and Lisa Sokol Jud Valeski Alon Halevy and Jayant Madhavan Aaron Koblin and Valdean Klump Michal Migurski Jeff Heer Coco Krumme Peter Norvig Matt Wood and Ben Blackburne Jean-Claude Bradley, Rajarshi Guha, Andrew Lang, Pierre Lindenbaum, Cameron Neylon, Antony Williams, and Egon Willighagen Lukas Biewald and Brendan O'Connor Hadley Wickham, Deborah Swayne, and David Poole Andrew Gelman, Jonathan P. Kastellec, and Yair Ghitza Toby Segaran"
1788,"issues in learning an ontology from text",4537934,"Issues in learning an ontology from text","BACKGROUND: Ontology construction for any domain is a labour intensive and complex process. Any methodology that can reduce the cost and increase efficiency has the potential to make a major impact in the life sciences. This paper describes an experiment in ontology construction from text for the animal behaviour domain. Our objective was to see how much could be done in a simple and relatively rapid manner using a corpus of journal papers. We used a sequence of pre-existing text processing steps, and here describe the different choices made to clean the input, to derive a set of terms and to structure those terms in a number of hierarchies. We describe some of the challenges, especially that of focusing the ontology appropriately given a starting point of a heterogeneous corpus. RESULTS: Using mainly automated techniques, we were able to construct an 18055 term ontology-like structure with 73% recall of animal behaviour terms, but a precision of only 26%. We were able to clean unwanted terms from the nascent ontology using lexico-syntactic patterns that tested the validity of term inclusion within the ontology. We used the same technique to test for subsumption relationships between the remaining terms to add structure to the initially broad and shallow structure we generated. All outputs are available at http://thirlmere.aston.ac.uk/~kiffer/animalbehaviour/. CONCLUSION: We present a systematic method for the initial steps of ontology or structured vocabulary construction for scientific domains that requires limited human effort and can make a contribution both to ontology learning and maintenance. The method is useful both for the exploration of a scientific domain and as a stepping stone towards formally rigourous ontologies. The filtering of recognised terms from a heterogeneous corpus to focus upon those that are the topic of the ontology is identified to be one of the main challenges for research in ontology learning."
1789,"hierarchical hidden markov model with application to joint analysis of chipchip and chipseq data",4544030,"Hierarchical hidden Markov model with application to joint analysis of ChIP-chip and ChIP-seq data","Motivation: Chromatin immunoprecipitation (ChIP) experiments followed by array hybridization, or ChIP-chip, is a powerful approach for identifying transcription factor binding sites (TFBS) and has been widely used. Recently, massively parallel sequencing coupled with ChIP experiments (ChIP-seq) has been increasingly used as an alternative to ChIP-chip, offering cost-effective genome-wide coverage and resolution up to a single base pair. For many well-studied TFs, both ChIP-seq and ChIP-chip experiments have been applied and their data are publicly available. Previous analyses have revealed substantial technology-specific binding signals despite strong correlation between the two sets of results. Therefore, it is of interest to see whether the two data sources can be combined to enhance the detection of TFBS. Results: In this work, hierarchical hidden Markov model (HHMM) is proposed for combining data from ChIP-seq and ChIP-chip. In HHMM, inference results from individual HMMs in ChIP-seq and ChIP-chip experiments are summarized by a higher level HMM. Simulation studies show the advantage of HHMM when data from both technologies co-exist. Analysis of two well-studied TFs, NRSF and CCCTC-binding factor (CTCF), also suggests that HHMM yields improved TFBS identification in comparison to analyses using individual data sources or a simple merger of the two. Availability: Source code for the software ChIPmeta is freely available for download at http://www.umich.edu/[~]hwchoi/HHMMsoftware.zip, implemented in C and supported on linux. Contact: ghoshd@psu.edu; qin@umich.edu Supplementary information: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btp312"
1790,"fast and accurate short read alignment with burrowswheeler transform",4544032,"Fast and accurate short read alignment with Burrows-Wheeler transform.","Motivation: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals. Results: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows-Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is [~]10-20x faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package. Availability: http://maq.sourceforge.net Contact: rd@sanger.ac.uk"
1791,"make new friends but keep the old recommending people on social networking sites",4545636,"Make new friends, but keep the old: recommending people on social networking sites","This paper studies people recommendations designed to help users find known, offline contacts and discover new friends on social networking sites. We evaluated four recommender algorithms in an enterprise social networking site using a personalized survey of 500 users and a field study of 3,000 users. We found all algorithms effective in expanding users' friend lists. Algorithms based on social network information were able to produce better-received recommendations and find more known contacts for users, while algorithms using similarity of user-created content were stronger in discovering new friends. We also collected qualitative feedback from our survey users and draw several meaningful design implications."
1792,"keller estimating timevarying interactions between genes",4673368,"KELLER: estimating time-varying interactions between genes.","MOTIVATION: Gene regulatory networks underlying temporal processes, such as the cell cycle or the life cycle of an organism, can exhibit significant topological changes to facilitate the underlying dynamic regulatory functions. Thus, it is essential to develop methods that capture the temporal evolution of the regulatory networks. These methods will be an enabling first step for studying the driving forces underlying the dynamic gene regulation circuitry and predicting the future network structures in response to internal and external stimuli. RESULTS: We introduce a kernel-reweighted logistic regression method (KELLER) for reverse engineering the dynamic interactions between genes based on their time series of expression values. We apply the proposed method to estimate the latent sequence of temporal rewiring networks of 588 genes involved in the developmental process during the life cycle of Drosophila melanogaster. Our results offer the first glimpse into the temporal evolution of gene networks in a living organism during its full developmental course. Our results also show that many genes exhibit distinctive functions at different stages along the developmental cycle. AVAILABILITY: Source codes and relevant data will be made available at http://www.sailing.cs.cmu.edu/keller."
1793,"the computation of social behavior",4674775,"The Computation of Social Behavior","Neuroscientists are beginning to advance explanations of social behavior in terms of underlying brain mechanisms. Two distinct networks of brain regions have come to the fore. The first involves brain regions that are concerned with learning about reward and reinforcement. These same reward-related brain areas also mediate preferences that are social in nature even when no direct reward is expected. The second network focuses on regions active when a person must make estimates of another person's intentions. However, it has been difficult to determine the precise roles of individual brain regions within these networks or how activities in the two networks relate to one another. Some recent studies of reward-guided behavior have described brain activity in terms of formal mathematical models; these models can be extended to describe mechanisms that underlie complex social exchange. Such a mathematical formalism defines explicit mechanistic hypotheses about internal computations underlying regional brain activity, provides a framework in which to relate different types of activity and understand their contributions to behavior, and prescribes strategies for performing experiments under strong control. 10.1126/science.1169694"
1794,"highfrequency longrange coupling between prefrontal and visual cortex during attention",4680712,"High-frequency, long-range coupling between prefrontal and visual cortex during attention.","Electrical recordings in humans and monkeys show attentional enhancement of evoked responses and gamma synchrony in ventral stream cortical areas. Does this synchrony result from intrinsic activity in visual cortex or from inputs from other structures? Using paired recordings in the frontal eye field (FEF) and area V4, we found that attention to a stimulus in their joint receptive field leads to enhanced oscillatory coupling between the two areas, particularly at gamma frequencies. This coupling appeared to be initiated by FEF and was time-shifted by about 8 to 13 milliseconds across a range of frequencies. Considering the expected conduction and synaptic delays between the areas, this time-shifted coupling at gamma frequencies may optimize the postsynaptic impact of spikes from one area upon the other, improving cross-area communication with attention."
1795,"brain anatomical network and intelligence",4680784,"Brain Anatomical Network and Intelligence","Intuitively, higher intelligence might be assumed to correspond to more efficient information transfer in the brain, but no direct evidence has been reported from the perspective of brain networks. In this study, we performed extensive analyses to test the hypothesis that individual differences in intelligence are associated with brain structural organization, and in particular that higher scores on intelligence tests are related to greater global efficiency of the brain anatomical network. We constructed binary and weighted brain anatomical networks in each of 79 healthy young adults utilizing diffusion tensor tractography and calculated topological properties of the networks using a graph theoretical method. Based on their IQ test scores, all subjects were divided into general and high intelligence groups and significantly higher global efficiencies were found in the networks of the latter group. Moreover, we showed significant correlations between IQ scores and network properties across all subjects while controlling for age and gender. Specifically, higher intelligence scores corresponded to a shorter characteristic path length and a higher global efficiency of the networks, indicating a more efficient parallel information transfer in the brain. The results were consistently observed not only in the binary but also in the weighted networks, which together provide convergent evidence for our hypothesis. Our findings suggest that the efficiency of brain structural organization may be an important biological basis for intelligence."
1796,"aspirin in the primary and secondary prevention of vascular disease collaborative metaanalysis of individual participant data from randomised trials",4688553,"Aspirin in the primary and secondary prevention of vascular disease: collaborative meta-analysis of individual participant data from randomised trials.","{SummaryBackground} Low-dose aspirin is of definite and substantial net benefit for many people who already have occlusive vascular disease. We have assessed the benefits and risks in primary {prevention.Methods} We undertook meta-analyses of serious vascular events (myocardial infarction, stroke, or vascular death) and major bleeds in six primary prevention trials (95[punctuation space]000 individuals at low average risk, 660[punctuation space]000 person-years, 3554 serious vascular events) and 16 secondary prevention trials (17[punctuation space]000 individuals at high average risk, 43[punctuation space]000 person-years, 3306 serious vascular events) that compared long-term aspirin versus control. We report intention-to-treat analyses of first events during the scheduled treatment {period.Findings} In the primary prevention trials, aspirin allocation yielded a 12% proportional reduction in serious vascular events (0·51% aspirin vs 0·57% control per year, p=0·0001), due mainly to a reduction of about a fifth in non-fatal myocardial infarction (0·18% vs 0·23% per year, p{\\textless}0·0001). The net effect on stroke was not significant (0·20% vs 0·21% per year, p=0·4: haemorrhagic stroke 0·04\\ vs 0·03%, p=0·05; other stroke 0·16% vs 0·18% per year, p=0·08). Vascular mortality did not differ significantly (0·19% vs 0·19% per year, p=0·7). Aspirin allocation increased major gastrointestinal and extracranial bleeds (0·10% vs 0·07% per year, p{\\textless}0·0001), and the main risk factors for coronary disease were also risk factors for bleeding. In the secondary prevention trials, aspirin allocation yielded a greater absolute reduction in serious vascular events (6·7% vs 8·2% per year, p{\\textless}0.0001), with a non-significant increase in haemorrhagic stroke but reductions of about a fifth in total stroke (2·08% vs 2·54% per year, p=0·002) and in coronary events (4·3% vs 5·3% per year, p{\\textless}0·0001). In both primary and secondary prevention trials, the proportional reductions in the aggregate of all serious vascular events seemed similar for men and {women.Interpretation} In primary prevention without previous disease, aspirin is of uncertain net value as the reduction in occlusive events needs to be weighed against any increase in major bleeds. Further trials are in {progress.Funding} {UK} Medical Research Council, British Heart Foundation, Cancer Research {UK,} and the European Community Biomed Programme."
1797,"pscan finding overrepresented transcription factor binding site motifs in sequences from coregulated or coexpressed genes",4743344,"Pscan: finding over-represented transcription factor binding site motifs in sequences from co-regulated or co-expressed genes.","The first step in gene expression, transcription, is modulated by the interaction of transcription factors with their corresponding binding sites on the DNA sequence. Pscan is a software tool that scans a set of sequences (e.g. promoters) from co-regulated or co-expressed genes with motifs describing the binding specificity of known transcription factors and assesses which motifs are significantly over- or under-represented, providing thus hints on which transcription factors could be common regulators of the genes studied, together with the location of their candidate binding sites in the sequences. Pscan does not resort to comparisons with orthologous sequences and experimental results show that it compares favorably to other tools for the same task in terms of false positive predictions and computation time. The website is free and open to all users and there is no login requirement. Address: http://www.beaconlab.it/pscan."
1798,"late pleistocene demography and the appearance of modern human behavior",4750782,"Late Pleistocene Demography and the Appearance of Modern Human Behavior","The origins of modern human behavior are marked by increased symbolic and technological complexity in the archaeological record. In western Eurasia this transition, the Upper Paleolithic, occurred about 45,000 years ago, but many of its features appear transiently in southern Africa about 45,000 years earlier. We show that demography is a major determinant in the maintenance of cultural complexity and that variation in regional subpopulation density and/or migratory activity results in spatial structuring of cultural skill accumulation. Genetic estimates of regional population size over time show that densities in early Upper Paleolithic Europe were similar to those in sub-Saharan Africa when modern behavior first appeared. Demographic factors can thus explain geographic variation in the timing of the first appearance of modern behavior without invoking increased cognitive capacity. 10.1126/science.1170165"
1799,"detecting snps and estimating allele frequencies in clonal bacterial populations by sequencing pooled dna",4774373,"Detecting SNPs and estimating allele frequencies in clonal bacterial populations by sequencing pooled DNA","Summary: Here, we present a method for estimating the frequencies of SNP alleles present within pooled samples of DNA using high-throughput short-read sequencing. The method was tested on real data from six strains of the highly monomorphic pathogen Salmonella Paratyphi A, sequenced individually and in a pool. A variety of read mapping and quality-weighting procedures were tested to determine the optimal parameters, which afforded [&ge;]80% sensitivity of SNP detection and strong correlation with true SNP frequency at poolwide read depth of 40x, declining only slightly at read depths 20-40x.  Availability: The method was implemented in Perl and relies on the opensource software Maq for read mapping and SNP calling. The Perl script is freely available from ftp://ftp.sanger.ac.uk/pub/pathogens/pools/.  Contact: kh2@sanger.ac.uk  Supplementary information: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btp344"
1800,"approaches to neuroscience data integration",4774507,"Approaches to neuroscience data integration","As the number of neuroscience databases increases, the need for neuroscience data integration grows. This paper reviews and compares several approaches, including the Neuroscience Database Gateway (NDG), Neuroscience Information Framework (NIF) and Entrez Neuron, which enable neuroscience database annotation and integration. These approaches cover a range of activities spanning from registry, discovery and integration of a wide variety of neuroscience data sources. They also provide different user interfaces for browsing, querying and displaying query results. In Entrez Neuron, for example, four different facets or tree views (neuron, neuronal property, gene and drug) are used to hierarchically organize concepts that can be used for querying a collection of ontologies. The facets are also used to define the structure of the query results. 10.1093/bib/bbp029"
1801,"de novo transcriptome assembly with abyss",4867928,"De novo transcriptome assembly with ABySS","MOTIVATION: Whole transcriptome shotgun sequencing data from non-normalized samples offer unique opportunities to study the metabolic states of organisms. One can deduce gene expression levels using sequence coverage as a surrogate, identify coding changes or discover novel isoforms or transcripts. Especially for discovery of novel events, de novo assembly of transcriptomes is desirable. RESULTS: Transcriptome from tumor tissue of a patient with follicular lymphoma was sequenced with 36 base-pair (bp) single- and paired-end reads on the Illumina Genome Analyzer II platform. We assembled approximately 194 million reads using ABySS into 66,921 contigs 100bp or longer, with a maximum contig length of 10,951bp, representing over 30 million base pairs of unique transcriptome sequence, or roughly 1% of the genome. Availability and Implementation: Source code and binaries of ABySS are freely available for download at http: // www.bcgsc.ca / platform / bioinfo / software / abyss. Assembler tool is implemented in C++. The parallel version uses Open MPI. Explorer tool is implemented in Java using the Java universal network/graph framework. CONTACT: Software help: abyss@bcgsc.ca, authors {ibirol, sjackman, cydneyn, jqian, sjones}@bcgsc.ca."
1802,"hadoop the definitive guide",4882841,"Hadoop: The Definitive Guide","Hadoop: The Definitive Guide helps you harness the power of your data. Ideal for processing large datasets, the Apache Hadoop framework is an open source implementation of the MapReduce algorithm on which Google built its empire. This comprehensive resource demonstrates how to use Hadoop to build reliable, scalable, distributed systems: programmers will find details for analyzing large datasets, and administrators will learn how to set up and run Hadoop clusters. Complete with case studies that illustrate how Hadoop solves specific problems, this book helps you:  Use the Hadoop Distributed File System (HDFS) for storing large datasets, and run distributed computations over those datasets using MapReduce Become familiar with Hadoop's data and I/O building blocks for compression, data integrity, serialization, and persistence Discover common pitfalls and advanced features for writing real-world MapReduce programs Design, build, and administer a dedicated Hadoop cluster, or run Hadoop in the cloud Use Pig, a high-level query language for large-scale data processing Take advantage of HBase, Hadoop's database for structured and semi-structured data Learn ZooKeeper, a toolkit of coordination primitives for building distributed systems  If you have lots of data -- whether it's gigabytes or petabytes -- Hadoop is the perfect solution. Hadoop: The Definitive Guide is the most thorough book available on the subject. ""Now you have the opportunity to learn about Hadoop from a master-not only of the technology, but also of common sense and plain talk."" -- Doug Cutting, Hadoop Founder, Yahoo!"
1803,"autodock vina improving the speed and accuracy of docking with a new scoring function efficient optimization and multithreading",4891923,"AutoDock Vina: Improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading","AutoDock Vina, a new program for molecular docking and virtual screening, is presented. AutoDock Vina achieves an approximately two orders of magnitude speed-up compared with the molecular docking software previously developed in our lab (AutoDock 4), while also significantly improving the accuracy of the binding mode predictions, judging by our tests on the training set used in AutoDock 4 development. Further speed-up is achieved from parallelism, by using multithreading on multicore machines. AutoDock Vina automatically calculates the grid maps and clusters the results in a way transparent to the user. © 2009 Wiley Periodicals, Inc. J Comput Chem 2010"
1804,"backup in gene regulatory networks explains differences between binding and knockout results",4892239,"Backup in gene regulatory networks explains differences between binding and knockout results.","The complementarity of gene expression and {protein-DNA} interaction data led to several successful models of biological systems. However, recent studies in multiple species raise doubts about the relationship between these two datasets. These studies show that the overwhelming majority of genes bound by a particular transcription factor {(TF)} are not affected when that factor is knocked out. Here, we show that this surprising result can be partially explained by considering the broader cellular context in which {TFs} operate. Factors whose functions are not backed up by redundant paralogs show a fourfold increase in the agreement between their bound targets and the expression levels of those targets. In addition, we show that incorporating protein interaction networks provides physical explanations for knockout effects. New double knockout experiments support our conclusions. Our results highlight the robustness provided by redundant {TFs} and indicate that in the context of diverse cellular systems, binding is still largely functional."
1805,"untangling the web of eresearch towards a sociology of online knowledge",4909050,"Untangling the web of e-Research: Towards a sociology of online knowledge","e-Research is a rapidly growing research area, both in terms of publications and in terms of funding. In this article we argue that it is necessary to reconceptualize the ways in which we seek to measure and understand e-Research by developing a sociology of knowledge based on our understanding of how science has been transformed historically and shifted into online forms. Next, we report data which allows the examination of e-Research through a variety of traces in order to begin to understand how knowledge in the realm of e-Research has been and is being constructed. These data indicate that e-Research has had a variable impact in different fields of research. We argue that only an overall account of the scale and scope of e-Research within and between different fields makes it possible to identify the organizational coherence and diffuseness of e-Research in terms of its socio-technical networks, and thus to identify the contributions of e-Research to various research fronts in the online production of knowledge. © 2009 Elsevier Ltd. All rights reserved."
1806,"youtube online video and participatory culture",4923889,"YouTube: Online Video and Participatory Culture","YouTube is one of the most well-known and widely discussed sites of participatory media in the contemporary online environment, and it is the first genuinely mass-popular platform for user-created video. In this timely and comprehensive introduction to how YouTube is being used and why it matters, Burgess and Green discuss the ways that it relates to wider transformations in culture, society and the economy.  The book critically examines the public debates surrounding the site, demonstrating how it is central to struggles for authority and control in the new media environment. Drawing on a range of theoretical sources and empirical research, the authors discuss how YouTube is being used by the media industries, by audiences and amateur producers, and by particular communities of interest, and the ways in which these uses challenge existing ideas about cultural ‘production’ and ‘consumption’.  Rich with both concrete examples and featuring specially commissioned chapters by Henry Jenkins and John Hartley, the book is essential reading for anyone interested in the contemporary and future implications of online media. It will be particularly valuable for students and scholars in media, communication and cultural studies."
1807,"cloud computing and emerging it platforms vision hype and reality for delivering computing as the th utility",4970791,"Cloud computing and emerging IT platforms: Vision, hype, and reality for delivering computing as the 5th utility","With the significant advances in Information and Communications Technology (ICT) over the last half century, there is an increasingly perceived vision that computing will one day be the 5th utility (after water, electricity, gas, and telephony). This computing utility, like all other four existing utilities, will provide the basic level of computing service that is considered essential to meet the everyday needs of the general community. To deliver this vision, a number of computing paradigms have been proposed, of which the latest one is known as Cloud computing. Hence, in this paper, we define Cloud computing and provide the architecture for creating Clouds with market-oriented resource allocation by leveraging technologies such as Virtual Machines (VMs). We also provide insights on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain Service Level Agreement (SLA)-oriented resource allocation. In addition, we reveal our early thoughts on interconnecting Clouds for dynamically creating global Cloud exchanges and markets. Then, we present some representative Cloud platforms, especially those developed in industries, along with our current work towards realizing market-oriented resource allocation of Clouds as realized in Aneka enterprise Cloud technology. Furthermore, we highlight the difference between High Performance Computing (HPC) workload and Internet-based services workload. We also describe a meta-negotiation infrastructure to establish global Cloud exchanges and markets, and illustrate a case study of harnessing ‘Storage Clouds’ for high performance content delivery. Finally, we conclude with the need for convergence of competing IT paradigms to deliver our 21st century vision."
1808,"jbrowse a nextgeneration genome browser",5038173,"JBrowse: a next-generation genome browser.","We describe an open source, portable, JavaScript-based genome browser, JBrowse, that can be used to navigate genome annotations over the web. JBrowse helps preserve the user's sense of location by avoiding discontinuous transitions, instead offering smoothly animated panning, zooming, navigation, and track selection. Unlike most existing genome browsers, where the genome is rendered into images on the webserver and the role of the client is restricted to displaying those images, JBrowse distributes work between the server and client and therefore uses significantly less server overhead than previous genome browsers. We report benchmark results empirically comparing server- and client-side rendering strategies, review the architecture and design considerations of JBrowse, and describe a simple wiki plug-in that allows users to upload and share annotation tracks."
1809,"the gene ontologys reference genome project a unified framework for functional annotation across species",5044135,"The Gene Ontology's Reference Genome Project: A Unified Framework for Functional Annotation across Species","The Gene Ontology (GO) is a collaborative effort that provides structured vocabularies for annotating the molecular function, biological role, and cellular location of gene products in a highly systematic way and in a species-neutral manner with the aim of unifying the representation of gene function across different organisms. Each contributing member of the GO Consortium independently associates GO terms to gene products from the organism(s) they are annotating. Here we introduce the Reference Genome project, which brings together those independent efforts into a unified framework based on the evolutionary relationships between genes in these different organisms. The Reference Genome project has two primary goals: to increase the depth and breadth of annotations for genes in each of the organisms in the project, and to create data sets and tools that enable other genome annotation efforts to infer GO annotations for homologous genes in their organisms. In addition, the project has several important incidental benefits, such as increasing annotation consistency across genome databases, and providing important improvements to the GO's logical structure and biological content. © 2009 Gaudet et al."
1810,"a synthetic genetic edge detection program",5057521,"A synthetic genetic edge detection program.","Edge detection is a signal processing algorithm common in artificial intelligence and image recognition programs. We have constructed a genetically encoded edge detection algorithm that programs an isogenic community of E. coli to sense an image of light, communicate to identify the light-dark edges, and visually present the result of the computation. The algorithm is implemented using multiple genetic circuits. An engineered light sensor enables cells to distinguish between light and dark regions. In the dark, cells produce a diffusible chemical signal that diffuses into light regions. Genetic logic gates are used so that only cells that sense light and the diffusible signal produce a positive output. A mathematical model constructed from first principles and parameterized with experimental measurements of the component circuits predicts the performance of the complete program. Quantitatively accurate models will facilitate the engineering of more complex biological behaviors and inform bottom-up studies of natural genetic regulatory networks."
1811,"tagommenders connecting users to items through tags",5148747,"Tagommenders: connecting users to items through tags","Tagging has emerged as a powerful mechanism that enables users to ﬁnd, organize, and understand online entities. Recommender systems similarly enable users to efﬁciently navigate vast collections of items. Algorithms combining tags with recommenders may deliver both the automation inherent in recommenders, and the ﬂexibility and conceptual comprehensibility inherent in tagging systems. In this paper we explore tagommenders, recommender algorithms that predict users’ preferences for items based on their inferred preferences for tags. We describe tag preference inference algorithms based on users’ interactions with tags and movies, and evaluate these algorithms based on tag preference ratings collected from 995 {MovieLens} users. We design and evaluate algorithms that predict users’ ratings for movies based on their inferred tag preferences. Our tag-based algorithms generate better recommendation rankings than state-of-the-art algorithms, and they may lead to ﬂexible recommender systems that leverage the characteristics of items users ﬁnd most important."
1812,"mechanisms of change in gene copy number",5175610,"Mechanisms of change in gene copy number"," Deletions and duplications of chromosomal segments (copy number variants, CNVs) are a major source of variation between individual humans and are an underlying factor in human evolution and in many diseases, including mental illness, developmental disorders and cancer. CNVs form at a faster rate than other types of mutation, and seem to do so by similar mechanisms in bacteria, yeast and humans. Here we review current models of the mechanisms that cause copy number variation. Non-homologous end-joining mechanisms are well known, but recent models focus on perturbation of DNA replication and replication of non-contiguous DNA segments. For example, cellular stress might induce repair of broken replication forks to switch from high-fidelity homologous recombination to non-homologous repair, thus promoting copy number change."
1813,"search for a tree of life in the thicket of the phylogenetic forest",5183264,"{Search for a 'Tree of Life' in the thicket of the phylogenetic forest.}","{BACKGROUND: Comparative genomics has revealed extensive horizontal gene transfer among prokaryotes, a development that is often considered to undermine the 'tree of life' concept. However, the possibility remains that a statistical central trend still exists in the phylogenetic 'forest of life'. RESULTS: A comprehensive comparative analysis of a 'forest' of 6,901 phylogenetic trees for prokaryotic genes revealed a consistent phylogenetic signal, particularly among 102 nearly universal trees, despite high levels of topological inconsistency, probably due to horizontal gene transfer. Horizontal transfers seemed to be distributed randomly and did not obscure the central trend. The nearly universal trees were topologically similar to numerous other trees. Thus, the nearly universal trees might reflect a significant central tendency, although they cannot represent the forest completely. However, topological consistency was seen mostly at shallow tree depths and abruptly dropped at the level of the radiation of archaeal and bacterial phyla, suggesting that early phases of evolution could be non-tree-like (Biological Big Bang). Simulations of evolution under compressed cladogenesis or Biological Big Bang yielded a better fit to the observed dependence between tree inconsistency and phylogenetic depth for the compressed cladogenesis model. CONCLUSIONS: Horizontal gene transfer is pervasive among prokaryotes: very few gene trees are fully consistent, making the original tree of life concept obsolete. A central trend that most probably represents vertical inheritance is discernible throughout the evolution of archaea and bacteria, although compressed cladogenesis complicates unambiguous resolution of the relationships between the major archaeal and bacterial clades.}"
1814,"rapid detection classification and accurate alignment of up to a million or more related protein sequences",5195365,"Rapid detection, classification and accurate alignment of up to a million or more related protein sequences","Motivation: The patterns of sequence similarity and divergence present within functionally diverse, evolutionarily related proteins contain implicit information about corresponding biochemical similarities and differences. A first step toward accessing such information is to statistically analyze these patterns, which, in turn, requires that one first identify and accurately align a very large set of protein sequences. Ideally, the set should include many distantly related, functionally divergent subgroups. Because it is extremely difficult, if not impossible for fully automated methods to align such sequences correctly, researchers often resort to manual curation based on detailed structural and biochemical information. However, multiply-aligning vast numbers of sequences in this way is clearly impractical.  Results: This problem is addressed using Multiply-Aligned Profiles for Global Alignment of Protein Sequences (MAPGAPS). The MAPGAPS program uses a set of multiply-aligned profiles both as a query to detect and classify related sequences and as a template to multiply-align the sequences. It relies on Karlin-Altschul statistics for sensitivity and on PSI-BLAST (and other) heuristics for speed. Using as input a carefully curated multiple-profile alignment for P-loop GTPases, MAPGAPS correctly aligned weakly conserved sequence motifs within 33 distantly related GTPases of known structure. By comparison, the sequence- and structurally based alignment methods hmmalign and PROMALS3D misaligned at least 11 and 23 of these regions, respectively. When applied to a dataset of 65 million protein sequences, MAPGAPS identified, classified and aligned (with comparable accuracy) nearly half a million putative P-loop GTPase sequences.  Availability: A C++ implementation of MAPGAPS is available at http://mapgaps.igs.umaryland.edu.  Contact: aneuwald@som.umaryland.edu  Supplementary information: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btp342"
1815,"the pathologies of big data",5249602,"The pathologies of big data","What is big DATA anyway? Gigabytes? Terabytes? Petabytes? A brief personal memory may provide some perspective. In the late 1980s at Columbia University, I had the chance to play around with what at the time was a truly enormous disk: the IBM 3850 MSS (Mass Storage System). The MSS was actually a fully automatic robotic tape library and associated staging disks to make random access, if not exactly instantaneous, at least fully transparent. In Columbias configuration, it stored a total of around 100GB. It was already on its way out by the time I got my hands on it, but in its heyday, the early- to mid-1980s, it had been used to support access by social scientists to what was unquestionably big data at the time: the entire 1980 U.S. Census database. Presumably, there was no other practical way to provide the researchers with ready access to a dataset that large - at close to $40K per GB,3 a 100GB disk"
1816,"pathway databases and tools for their exploitation benefits current limitations and challenges",5296845,"Pathway databases and tools for their exploitation: benefits, current limitations and challenges","In past years, comprehensive representations of cell signalling pathways have been developed by manual curation from literature, which requires huge effort and would benefit from information stored in databases and from automatic retrieval and integration methods. Once a reconstruction of the network of interactions is achieved, analysis of its structural features and its dynamic behaviour can take place. Mathematical modelling techniques are used to simulate the complex behaviour of cell signalling networks, which ultimately sheds light on the mechanisms leading to complex diseases or helps in the identification of drug targets. A variety of databases containing information on cell signalling pathways have been developed in conjunction with methodologies to access and analyse the data. In principle, the scenario is prepared to make the most of this information for the analysis of the dynamics of signalling pathways. However, are the knowledge repositories of signalling pathways ready to realize the systems biology promise? In this article we aim to initiate this discussion and to provide some insights on this issue."
1817,"upcoming challenges for multiple sequence alignment methods in the highthroughput era",5308512,"Upcoming challenges for multiple sequence alignment methods in the high-throughput era","This review focuses on recent trends in multiple sequence alignment tools. It describes the latest algorithmic improvements including the extension of consistency-based methods to the problem of template-based multiple sequence alignments. Some results are presented suggesting that template-based methods are significantly more accurate than simpler alternative methods. The validation of existing methods is also discussed at length with the detailed description of recent results and some suggestions for future validation strategies. The last part of the review addresses future challenges for multiple sequence alignment methods in the genomic era, most notably the need to cope with very large sequences, the need to integrate large amounts of experimental data, the need to accurately align non-coding and non-transcribed sequences and finally, the need to integrate many alternative methods and approaches.  Contact: cedric.notredame@crg.es 10.1093/bioinformatics/btp452"
1818,"replica exchange with nonequilibrium switches",5321670,"Replica exchange with nonequilibrium switches","10.1073/pnas.0900406106 We introduce a replica exchange (parallel tempering) method in which attempted configuration swaps are generated using nonequilibrium work simulations. By effectively increasing phase space overlap, this approach mitigates the need for many replicas. We illustrate our method by using a model system and show that it is able to achieve the computational efficiency of ordinary replica exchange, using fewer replicas."
1819,"scaling laws of human interaction activity",5362916,"Scaling laws of human interaction activity","10.1073/pnas.0902667106 Even though people in our contemporary technological society are depending on communication, our understanding of the underlying laws of human communicational behavior continues to be poorly understood. Here we investigate the communication patterns in 2 social Internet communities in search of statistical laws in human interaction activity. This research reveals that human communication networks dynamically follow scaling laws that may also explain the observed trends in economic growth. Specifically, we identify a generalized version of Gibrat's law of social activity expressed as a scaling law between the fluctuations in the number of messages sent by members and their level of activity. Gibrat's law has been essential in understanding economic growth patterns, yet without an underlying general principle for its origin. We attribute this scaling law to long-term correlation patterns in human activity, which surprisingly span from days to the entire period of the available data of more than 1 year. Further, we provide a mathematical framework that relates the generalized version of Gibrat's law to the long-term correlated dynamics, which suggests that the same underlying mechanism could be the source of Gibrat's law in economics, ranging from large firms, research and development expenditures, gross domestic product of countries, to city population growth. These findings are also of importance for designing communication networks and for the understanding of the dynamics of social systems in which communication plays a role, such as economic markets and political systems."
1820,"shortread a bioconductor package for input quality assessment and exploration of highthroughput sequence data",5369385,"ShortRead: a bioconductor package for input, quality assessment and exploration of high-throughput sequence data.","Summary: ShortRead is a package for input, quality assessment, manipulation and output of high-throughput sequencing data. ShortRead is provided in the R and Bioconductor environments, allowing ready access to additional facilities for advanced statistical analysis, data transformation, visualization and integration with diverse genomic resources. Availability and Implementation: This package is implemented in R and available at the Bioconductor web site; the package contains a vignette' outlining typical work flows. Contact: mtmorgan@fhcrc.org"
1821,"systematic artifacts in metagenomes from complex microbial communities",5395168,"Systematic artifacts in metagenomes from complex microbial communities","Metagenomics is providing an unprecedented view of the taxonomic diversity, metabolic potential and ecological role of microbial communities in biomes as diverse as the mammalian gastrointestinal tract, the marine water column and soils. However, we have found a systematic error in metagenomes generated by 454-based pyrosequencing that leads to an overestimation of gene and taxon abundance; between 11% and 35% of sequences in a typical metagenome are artificial replicates. Here we document the error in several published and original datasets and offer a web-based solution (http://microbiomes.msu.edu/replicates) for identifying and removing these artifacts."
1822,"opinion and community formation in coevolving networks",5403625,"Opinion and community formation in coevolving networks","In human societies, opinion formation is mediated by social interactions, consequently taking place on a network of relationships and at the same time influencing the structure of the network and its evolution. To investigate this coevolution of opinions and social interaction structure, we develop a dynamic agent-based network model by taking into account short range interactions like discussions between individuals, long range interactions like a sense for overall mood modulated by the attitudes of individuals, and external field corresponding to outside influence. Moreover, individual biases can be naturally taken into account. In addition, the model includes the opinion-dependent link-rewiring scheme to describe network topology coevolution with a slower time scale than that of the opinion formation. With this model, comprehensive numerical simulations and mean field calculations have been carried out and they show the importance of the separation between fast and slow time scales resulting in the network to organize as well-connected small communities of agents with the same opinion."
1823,"biological pathways as communicating computer systems",5405522,"Biological pathways as communicating computer systems","Time and cost are the enemies of cell biology. The number of experiments required to rigorously dissect and comprehend a pathway of even modest complexity is daunting. Methods are needed to formulate biological pathways in a machine-analysable fashion, which would automate the process of considering all possible experiments in a complex pathway and identify those that command attention. In this Essay, we describe a method that is based on the exploitation of computational tools that were originally developed to analyse reactive communicating computer systems such as mobile phones and web browsers. In this approach, the biological process is articulated as an executable computer program that can be interrogated using methods that were developed to analyse complex software systems. Using case studies of the FGF, MAPK and Delta/Notch pathways, we show that the application of this technology can yield interesting insights into the behaviour of signalling pathways, which have subsequently been corroborated by experimental data. 10.1242/jcs.039701"
1824,"learning to rank for information retrieval",5413127,"Learning to Rank for Information Retrieval","Learning to rank for Information Retrieval (IR) is a task to automatically construct a ranking model using training data, such that the model can sort new objects according to their degrees of relevance, preference, or importance. Many IR problems are by nature ranking problems, and many IR technologies can be potentially enhanced by using learning-to-rank techniques. The objective of this tutorial is to give an introduction to this research direction. Specifically, the existing learning-to-rank algorithms are reviewed and categorized into three approaches: the pointwise, pairwise, and listwise approaches. The advantages and disadvantages with each approach are analyzed, and the relationships between the loss functions used in these approaches and IR evaluation measures are discussed. Then the empirical evaluations on typical learning-to-rank methods are shown, with the LETOR collection as a benchmark dataset, which seems to suggest that the listwise approach be the most effective one among all the approaches. After that, a statistical ranking theory is introduced, which can describe different learning-to-rank algorithms, and be used to analyze their query-level generalization abilities. At the end of the tutorial, we provide a summary and discuss potential future work on learning to rank."
1825,"breakdancer an algorithm for highresolution mapping of genomic structural variation",5415605,"BreakDancer: an algorithm for high-resolution mapping of genomic structural variation","Detection and characterization of genomic structural variation are important for understanding the landscape of genetic variation in human populations and in complex diseases such as cancer. Recent studies demonstrate the feasibility of detecting structural variation using next-generation, short-insert, paired-end sequencing reads. However, the utility of these reads is not entirely clear, nor are the analysis methods with which accurate detection can be achieved. The algorithm BreakDancer predicts a wide variety of structural variants including insertion-deletions (indels), inversions and translocations. We examined BreakDancer's performance in simulation, in comparison with other methods and in analyses of a sample from an individual with acute myeloid leukemia and of samples from the 1,000 Genomes trio individuals. BreakDancer sensitively and accurately detected indels ranging from 10 base pairs to 1 megabase pair that are difficult to detect via a single conventional approach."
1826,"effective field theory past and future",5433696,"Effective Field Theory, Past and Future","This is a written version of the opening talk at the 6th International Workshop on Chiral Dynamics, at the University of Bern, Switzerland, July 6, 2009, to be published in the proceedings of the Workshop. In it, I reminisce about the early development of effective field theories of the strong interactions, comment briefly on some other applications of effective field theories, and then take up the idea that the Standard Model and General Relativity are the leading terms in an effective field theory. Finally, I cite recent calculations that suggest that the effective field theory of gravitation and matter is asymptotically safe."
1827,"demonstration of a spaserbased nanolaser",5451459,"Demonstration of a spaser-based nanolaser","One of the most rapidly growing areas of physics and nanotechnology focuses on plasmonic effects on the nanometre scale, with possible applications ranging from sensing and biomedicine to imaging and information technology1, 2. However, the full development of nanoplasmonics is hindered by the lack of devices that can generate coherent plasmonic fields. It has been proposed3 that in the same way as a laser generates stimulated emission of coherent photons, a ‘spaser’ could generate stimulated emission of surface plasmons (oscillations of free electrons in metallic nanostructures) in resonating metallic nanostructures adjacent to a gain medium. But attempts to realize a spaser face the challenge of absorption loss in metal, which is particularly strong at optical frequencies. The suggestion4, 5, 6 to compensate loss by optical gain in localized and propagating surface plasmons has been implemented recently7, 8, 9, 10 and even allowed the amplification of propagating surface plasmons in open paths11. Still, these experiments and the reported enhancement of the stimulated emission of dye molecules in the presence of metallic nanoparticles12, 13, 14 lack the feedback mechanism present in a spaser. Here we show that 44-nm-diameter nanoparticles with a gold core and dye-doped silica shell allow us to completely overcome the loss of localized surface plasmons by gain and realize a spaser. And in accord with the notion that only surface plasmon resonances are capable of squeezing optical frequency oscillations into a nanoscopic cavity to enable a true nanolaser15, 16, 17, 18, we show that outcoupling of surface plasmon oscillations to photonic modes at a wavelength of 531 nm makes our system the smallest nanolaser reported to date—and to our knowledge the first operating at visible wavelengths. We anticipate that now it has been realized experimentally, the spaser will advance our fundamental understanding of nanoplasmonics and the development of practical applications."
1828,"genetics in geographically structured populations defining estimating and interpreting fst",5467858,"Genetics in geographically structured populations: defining, estimating and interpreting FST","Wright's F-statistics, and especially F ST, provide important insights into the evolutionary processes that influence the structure of genetic variation within and among populations, and they are among the most widely used descriptive statistics in population and evolutionary genetics. Estimates of F ST can identify regions of the genome that have been the target of selection, and comparisons of F ST from different parts of the genome can provide insights into the demographic history of populations. For these reasons and others, F ST has a central role in population and evolutionary genetics and has wide applications in fields that range from disease association mapping to forensic science. This Review clarifies how F ST is defined, how it should be estimated, how it is related to similar statistics and how estimates of F ST should be interpreted."
1829,"targetminer microrna target prediction with systematic identification of tissuespecific negative examples",5489191,"TargetMiner: microRNA target prediction with systematic identification of tissue-specific negative examples","Motivation: Prediction of microRNA (miRNA) target mRNAs using machine learning approaches is an important area of research. However, most of the methods suffer from either high false positive or false negative rates. One reason for this is the marked deficiency of negative examples or miRNA non-target pairs. Systematic identification of non-target mRNAs is still not addressed properly, and therefore, current machine learning approaches are compelled to rely on artificially generated negative examples for training.  Results: In this article, we have identified [~]300 tissue-specific negative examples using a novel approach that involves expression profiling of both miRNAs and mRNAs, miRNA-mRNA structural interactions and seed-site conservation. The newly generated negative examples are validated with pSILAC dataset, which elucidate the fact that the identified non-targets are indeed non-targets.These high-throughput tissue-specific negative examples and a set of experimentally verified positive examples are then used to build a system called TargetMiner, a support vector machine (SVM)-based classifier. In addition to assessing the prediction accuracy on cross-validation experiments, TargetMiner has been validated with a completely independent experimental test dataset. Our method outperforms 10 existing target prediction algorithms and provides a good balance between sensitivity and specificity that is not reflected in the existing methods. We achieve a significantly higher sensitivity and specificity of 69% and 67.8% based on a pool of 90 feature set and 76.5% and 66.1% using a set of 30 selected feature set on the completely independent test dataset.  In order to establish the effectiveness of the systematically generated negative examples, the SVM is trained using a different set of negative data generated using the method in Yousef et al. A significantly higher false positive rate (70.6%) is observed when tested on the independent set, while all other factors are kept the same. Again, when an existing method (NBmiRTar) is executed with the our proposed negative data, we observe an improvement in its performance. These clearly establish the effectiveness of the proposed approach of selecting the negative examples systematically.  Availability: TargetMiner is now available as an online tool at www.isical.ac.in/[~]bioinfo_miu  Contact: sanghami@isical.ac.in; rmitra_t@isical.ac.in  Supplementary information: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btp503"
1830,"parametric complexity of sequence assembly theory and applications to next generation sequencing",5490410,"Parametric Complexity of Sequence Assembly: Theory and Applications to Next Generation Sequencing","In recent years, a flurry of new DNA sequencing technologies have altered the landscape of genomics, providing a vast amount of sequence information at a fraction of the costs that were previously feasible. The task of assembling these sequences into a genome has, however, still remained an algorithmic challenge that is in practice answered by heuristic solutions. In order to design better assembly algorithms and exploit the characteristics of sequence data from new technologies, we need an improved understanding of the parametric complexity of the assembly problem. In this article, we provide a first theoretical study in this direction, exploring the connections between repeat complexity, read lengths, overlap lengths and coverage in determining the ""hard"" instances of the assembly problem. Our work suggests at least two ways in which existing assemblers can be extended in a rigorous fashion, in addition to delineating directions for future theoretical investigations."
1831,"cooperative behavior cascades in human social networks",5649721,"Cooperative Behavior Cascades in Human Social Networks","Theoretical models suggest that social networks influence the evolution ofcooperation, but to date there have been few experimental studies.Observational data suggest that a wide variety of behaviors may spread in humansocial networks, but subjects in such studies can choose to befriend peoplewith similar behaviors, posing difficulty for causal inference. Here, weexploit a seminal set of laboratory experiments that originally showed thatvoluntary costly punishment can help sustain cooperation. In these experiments,subjects were randomly assigned to a sequence of different groups in order toplay a series of single-shot public goods games with strangers; this featureallowed us to draw networks of interactions to explore how cooperative anduncooperative behavior spreads from person to person to person. We show that,in both an ordinary public goods game and in a public goods game withpunishment, focal individuals are influenced by fellow group members'contribution behavior in future interactions with other individuals who werenot a party to the initial interaction. Furthermore, this influence persistsfor multiple periods and spreads up to three degrees of separation (from personto person to person to person). The results suggest that each additionalcontribution a subject makes to the public good in the first period is tripledover the course of the experiment by other subjects who are directly orindirectly influenced to contribute more as a consequence. These are the firstresults to show experimentally that cooperative behavior cascades in humansocial networks."
1832,"subpathwayminer a software package for flexible identification of pathways",5655355,"SubpathwayMiner: a software package for flexible identification of pathways","With the development of high-throughput experimental techniques such as microarray, mass spectrometry and large-scale mutagenesis, there is an increasing need to automatically annotate gene sets and identify the involved pathways. Although many pathway analysis tools are developed, new tools are still needed to meet the requirements for flexible or advanced analysis purpose. Here, we developed an R-based software package (SubpathwayMiner) for flexible pathway identification. SubpathwayMiner facilitates sub-pathway identification of metabolic pathways by using pathway structure information. Additionally, SubpathwayMiner also provides more flexibility in annotating gene sets and identifying the involved pathways (entire pathways and sub-pathways): (i) SubpathwayMiner is able to provide the most up-to-date pathway analysis results for users; (ii) SubpathwayMiner supports multiple species ([~]100 eukaryotes, 714 bacteria and 52 Archaea) and different gene identifiers (Entrez Gene IDs, NCBI-gi IDs, UniProt IDs, PDB IDs, etc.) in the KEGG GENE database; (iii) the system is quite efficient in cooperating with other R-based tools in biology. SubpathwayMiner is freely available at http://cran.r-project.org/web/packages/SubpathwayMiner/. 10.1093/nar/gkp667"
1833,"assisted assembly how to improve a de novo genome assembly by using related species",5657848,"Assisted assembly: how to improve a de novo genome assembly by using related species","ABSTRACT: Genome assembly has long been one of the major challenges in genome sequencing, where the product is dependent on genome coverage, data quality, the properties of the sequenced genome as well as the efficiency and accuracy of the algorithms employed for assembly. We describe a new assembly concept, where a genome assembly with low sequence coverage, either throughout the genome or locally, due to cloning bias, is considerably improved through assisted assembly. In the assisted assembly process a related reference genome is used to validate and leverage the information captured in the low coverage reads. We show that the information provided by aligning the WGS reads of the target against a reference genome can be effectively used to substantially improve the assembly of the target, both by covering more of the genome (5 to 15%) and long range connectivity (in some cases more than 500%), and by improving the quality of the resulting assembly. We then show the validity of the assisting process by testing the algorithm on a low coverage assembly for which a high quality draft exists, that of Canis familiaris. In this paper we describe the algorithms, the methodological validation using the dog genome, as well as some real applications such as the 2-fold coverage assemblies of the low coverage mammals used to inform the human genome, and the 8-fold coverage assembly of Plasmodium falciparum HB3."
1834,"mrna expression profiles show differential regulatory effects of micrornas between estrogen receptorpositive and estrogen receptornegative breast cancer",5700677,"mRNA expression profiles show differential regulatory effects of microRNAs between estrogen receptor-positive and estrogen receptor-negative breast cancer.","BACKGROUND : Recent studies have shown that the regulatory effect of microRNAs can be investigated by examining expression changes of their target genes. Given this, it is useful to define an overall metric of regulatory effect for a specific microRNA and see how this changes across different conditions. RESULTS : Here, we define a regulatory effect score (RE-score) to measure the inhibitory effect of a microRNA in a sample, essentially the average difference in expression of its targets versus non-targets. Then we compare the RE-scores of various microRNAs between two breast cancer subtypes: estrogen receptor positive (ER+) and negative (ER-). We applied this approach to five microarray breast cancer datasets and found that the expression of target genes of most microRNAs was more repressed in ER- than ER+; that is, microRNAs appear to have higher RE-scores in ER- breast cancer. These results are robust to the microRNA target prediction method. To interpret these findings, we analyzed the level of microRNA expression in previous studies and found that higher microRNA expression was not always accompanied by higher inhibitory effects. However, several key microRNA processing genes, especially Ago2 and Dicer, were differentially expressed between ER- and ER+ breast cancer, which may explain the different regulatory effects of microRNAs in these two breast cancer subtypes. CONCLUSIONS : The RE-score is a promising indicator to measure microRNAs' inhibitory effects. Most microRNAs exhibit higher RE-scores in ER- than in ER+ samples, suggesting that they have stronger inhibitory effects in ER- breast cancers."
1835,"comparative study of gene set enrichment methods",5705840,"Comparative study of gene set enrichment methods.","BACKGROUND: The analysis of high-throughput gene expression data with respect to sets of genes rather than individual genes has many advantages. A variety of methods have been developed for assessing the enrichment of sets of genes with respect to differential expression. In this paper we provide a comparative study of four of these methods: Fisher's exact test, Gene Set Enrichment Analysis (GSEA), Random-Sets (RS), and Gene List Analysis with Prediction Accuracy (GLAPA). The first three methods use associative statistics, while the fourth uses predictive statistics. We first compare all four methods on simulated data sets to verify that Fisher's exact test is markedly worse than the other three approaches. We then validate the other three methods on seven real data sets with known genetic perturbations and then compare the methods on two cancer data sets where our a priori knowledge is limited. RESULTS: The simulation study highlights that none of the three method outperforms all others consistently. GSEA and RS are able to detect weak signals of deregulation and they perform differently when genes in a gene set are both differentially up and down regulated. GLAPA is more conservative and large differences between the two phenotypes are required to allow the method to detect differential deregulation in gene sets. This is due to the fact that the enrichment statistic in GLAPA is prediction error which is a stronger criteria than classical two sample statistic as used in RS and GSEA. This was reflected in the analysis on real data sets as GSEA and RS were seen to be significant for particular gene sets while GLAPA was not, suggesting a small effect size. We find that the rank of gene set enrichment induced by GLAPA is more similar to RS than GSEA. More importantly, the rankings of the three methods share significant overlap. CONCLUSION: The three methods considered in our study recover relevant gene sets known to be deregulated in the experimental conditions and pathologies analyzed. There are differences between the three methods and GSEA seems to be more consistent in finding enriched gene sets, although no method uniformly dominates over all data sets. Our analysis highlights the deep difference existing between associative and predictive methods for detecting enrichment and the use of both to better interpret results of pathway analysis. We close with suggestions for users of gene set methods."
1836,"comparison of registered and published primary outcomes in randomized controlled trials",5706026,"Comparison of Registered and Published Primary Outcomes in Randomized Controlled Trials","CONTEXT: As of 2005, the International Committee of Medical Journal Editors required investigators to register their trials prior to participant enrollment as a precondition for publishing the trial's findings in member journals. OBJECTIVE: To assess the proportion of registered trials with results recently published in journals with high impact factors; to compare the primary outcomes specified in trial registries with those reported in the published articles; and to determine whether primary outcome reporting bias favored significant outcomes. DATA SOURCES AND STUDY SELECTION: MEDLINE via PubMed was searched for reports of randomized controlled trials (RCTs) in 3 medical areas (cardiology, rheumatology, and gastroenterology) indexed in 2008 in the 10 general medical journals and specialty journals with the highest impact factors. DATA EXTRACTION: For each included article, we obtained the trial registration information using a standardized data extraction form. RESULTS: Of the 323 included trials, 147 (45.5%) were adequately registered (ie, registered before the end of the trial, with the primary outcome clearly specified). Trial registration was lacking for 89 published reports (27.6%), 45 trials (13.9%) were registered after the completion of the study, 35 (10.8%) were registered with no or an unclear description of the primary outcome, 39 (12%) were registered with no or an unclear description of the primary outcome, and 3 (0.9%) were registered after the completion of the study and had an unclear description of the primary outcome. Among articles with trials adequately registered, 31% (46 of 147) showed some evidence of discrepancies between the outcomes registered and the outcomes published. The influence of these discrepancies could be assessed in only half of them and in these statistically significant results were favored in 82.6% (19 of 23). CONCLUSION: Comparison of the primary outcomes of RCTs registered with their subsequent publication indicated that selective outcome reporting is prevalent."
1837,"the information paradox a pedagogical introduction",5738683,"The information paradox: A pedagogical introduction","The black hole information paradox is a very poorly understood problem. It is often believed that Hawking's argument is not precisely formulated, and a more careful accounting of naturally occurring quantum corrections will allow the radiation process to become unitary. We show that such is not the case, by proving that small corrections to the leading order Hawking computation cannot remove the entanglement between the radiation and the hole. We formulate Hawking's argument as a `theorem': assuming `traditional' physics at the horizon and usual assumptions of locality we will be forced into mixed states or remnants. We also argue that one cannot explain away the problem by invoking AdS/CFT duality. We conclude with recent results on the quantum physics of black holes which show the the interior of black holes have a `fuzzball' structure. This nontrivial structure of microstates resolves the information paradox, and gives a qualitative picture of how classical intuition can break down in black hole physics."
1838,"updates to the rmap shortread mapping software",5738975,"Updates to the RMAP short-read mapping software","Summary: We report on a major new version of the RMAP software for mapping reads from short-read sequencing technology. General improvements to accuracy and space requirements are included, along with novel functionality. Included in the RMAP software package are tools for mapping paired-end reads, mapping using more sophisticated use of quality scores, collecting ambiguous mapping locations and mapping bisulfite-treated reads.  Availability: The applications described in this note are available for download at http://www.cmb.usc.edu/people/andrewds/rmap and are distributed as Open Source software under the GPLv3.0. The software has been tested on Linux and OS X platforms.  Contact: andrewds@usc.edu; mzhang@cshl.edu  The RMAP algorithm was introduced by (Smith et al., 2008) as one of the earliest available programs for mapping reads from the Illumina second-generation sequencing technology. One important contribution of RMAP was to incorporate the use of quality scores directly into the mapping process: read positions with too low a quality score were not considered while mapping, and that quality score cutoff could be adjusted by the user. Subsequently, numerous mapping algorithm have appeared (Langmead et al., 2009; Li,H. et al., 2008; Li,R. et al., 2008; Lin et al., 2008; Schatz, 2009; Yanovsky et al., 2008), with improvements in both efficiency and breadth of functionality (e.g. ability to map paired-end reads; integrated SNP calling). Investigators requiring solutions to mapping problems now have many options. As new applications of short-read sequencing emerge, many variations on the analysis task of read mapping emerge. Diversity in performance characteristics of existing mapping tools becomes potentially valuable.  We report the first major update to RMAP. The basic algorithmic framework in RMAP is still to preprocess reads and scan the genome, but several modifications have been made and much additional functionality has been included. Importantly, RMAP has a memory footprint that depends on the number of reads being mapped. This feature allows RMAP to be used effectively in cluster environments with commodity nodes, because partitioning the reads allows natural parallelizations with linear reduction in memory requirements per processor core used.  Included in this release of the RMAP software package is functionality for mapping paired-end reads, making more sophisticated use of quality scores, collecting mapping locations for ambiguously mapping reads and mapping bisulfite-treated reads. 10.1093/bioinformatics/btp533"
1839,"data clustering years beyond kmeans",5777260,"Data clustering: 50 years beyond K-means","Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering."
1840,"comparative genomics of protoploid saccharomycetaceae",5779456,"Comparative genomics of protoploid Saccharomycetaceae.","Our knowledge of yeast genomes remains largely dominated by the extensive studies on Saccharomyces cerevisiae and the consequences of its ancestral duplication, leaving the evolution of the entire class of hemiascomycetes only partly explored. We concentrate here on five species of Saccharomycetaceae, a large subdivision of hemiascomycetes, that we call ""protoploid"" because they diverged from the S. cerevisiae lineage prior to its genome duplication. We determined the complete genome sequences of three of these species: Kluyveromyces (Lachancea) thermotolerans and Saccharomyces (Lachancea) kluyveri (two members of the newly described Lachancea clade), and Zygosaccharomyces rouxii. We included in our comparisons the previously available sequences of Kluyveromyces lactis and Ashbya (Eremothecium) gossypii. Despite their broad evolutionary range and significant individual variations in each lineage, the five protoploid Saccharomycetaceae share a core repertoire of approximately 3300 protein families and a high degree of conserved synteny. Synteny blocks were used to define gene orthology and to infer ancestors. Far from representing minimal genomes without redundancy, the five protoploid yeasts contain numerous copies of paralogous genes, either dispersed or in tandem arrays, that, altogether, constitute a third of each genome. Ancient, conserved paralogs as well as novel, lineage-specific paralogs were identified."
1841,"selective suppression of hippocampal ripples impairs spatial memory",5802477,"Selective suppression of hippocampal ripples impairs spatial memory","Sharp wave-ripple (SPW-R) complexes in the hippocampus-entorhinal cortex are believed to be important for transferring labile memories from the hippocampus to the neocortex for long-term storage. We found that selective elimination of SPW-Rs during post-training consolidation periods resulted in performance impairment in rats trained on a hippocampus-dependent spatial memory task. Our results provide evidence for a prominent role of hippocampal SPW-Rs in memory consolidation."
1842,"reconstructing indian population history",5832760,"Reconstructing Indian population history","India has been underrepresented in genome-wide surveys of human variation. We analyse 25 diverse groups in India to provide strong evidence for two ancient populations, genetically divergent, that are ancestral to most Indians today. One, the 'Ancestral North Indians' (ANI), is genetically close to Middle Easterners, Central Asians, and Europeans, whereas the other, the 'Ancestral South Indians' (ASI), is as distinct from ANI and East Asians as they are from each other. By introducing methods that can estimate ancestry without accurate ancestral populations, we show that ANI ancestry ranges from 39–71% in most Indian groups, and is higher in traditionally upper caste and Indo-European speakers. Groups with only ASI ancestry may no longer exist in mainland India. However, the indigenous Andaman Islanders are unique in being ASI-related groups without ANI ancestry. Allele frequency differences between groups in India are larger than in Europe, reflecting strong founder effects whose signatures have been maintained for thousands of years owing to endogamy. We therefore predict that there will be an excess of recessive diseases in India, which should be possible to screen and map genetically."
1843,"sswap a simple semantic web architecture and protocol for semantic web services",5833633,"SSWAP: A Simple Semantic Web Architecture and Protocol for semantic web services.","BACKGROUND: SSWAP (Simple Semantic Web Architecture and Protocol; pronounced ""swap"") is an architecture, protocol, and platform for using reasoning to semantically integrate heterogeneous disparate data and services on the web. SSWAP was developed as a hybrid semantic web services technology to overcome limitations found in both pure web service technologies and pure semantic web technologies. RESULTS: There are currently over 2400 resources published in SSWAP. Approximately two dozen are custom-written services for QTL (Quantitative Trait Loci) and mapping data for legumes and grasses (grains). The remaining are wrappers to Nucleic Acids Research Database and Web Server entries. As an architecture, SSWAP establishes how clients (users of data, services, and ontologies), providers (suppliers of data, services, and ontologies), and discovery servers (semantic search engines) interact to allow for the description, querying, discovery, invocation, and response of semantic web services. As a protocol, SSWAP provides the vocabulary and semantics to allow clients, providers, and discovery servers to engage in semantic web services. The protocol is based on the W3C-sanctioned first-order description logic language OWL DL. As an open source platform, a discovery server running at http://sswap.info (as in to ""swap info"") uses the description logic reasoner Pellet to integrate semantic resources. The platform hosts an interactive guide to the protocol at http://sswap.info/protocol.jsp, developer tools at http://sswap.info/developer.jsp, and a portal to third-party ontologies at http://sswapmeet.sswap.info (a ""swap meet""). CONCLUSION: SSWAP addresses the three basic requirements of a semantic web services architecture (i.e., a common syntax, shared semantic, and semantic discovery) while addressing three technology limitations common in distributed service systems: i.e., i) the fatal mutability of traditional interfaces, ii) the rigidity and fragility of static subsumption hierarchies, and iii) the confounding of content, structure, and presentation. SSWAP is novel by establishing the concept of a canonical yet mutable OWL DL graph that allows data and service providers to describe their resources, to allow discovery servers to offer semantically rich search engines, to allow clients to discover and invoke those resources, and to allow providers to respond with semantically tagged data. SSWAP allows for a mix-and-match of terms from both new and legacy third-party ontologies in these graphs."
1844,"finding structure with randomness stochastic algorithms for constructing approximate matrix decompositions",5841463,"Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions","Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys recent research which demonstrates that \emph{randomization} offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. In particular, these techniques offer a route toward principal component analysis (PCA) for petascale data.   This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed - either explicitly or implicitly - to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, speed, and robustness. These claims are supported by extensive numerical experiments and a detailed error analysis."
1845,"coexpression network based on natural variation in human gene expression reveals gene interactions and functions",5870912,"Coexpression network based on natural variation in human gene expression reveals gene interactions and functions","10.1101/gr.097600.109 Genes interact in networks to orchestrate cellular processes. Analysis of these networks provides insights into gene interactions and functions. Here, we took advantage of normal variation in human gene expression to infer gene networks, which we constructed using correlations in expression levels of more than 8.5 million gene pairs in immortalized B cells from three independent samples. The resulting networks allowed us to identify biological processes and gene functions. Among the biological pathways, we found processes such as translation and glycolysis that co-occur in the same subnetworks. We predicted the functions of poorly characterized genes, including  and , and provided experimental evidence that  is part of the endoplasmic reticulum-associated secretory pathway. We also found that , a susceptibility gene of type 1 diabetes, interacts with , which plays a role in glucose transport. Furthermore, genes that predispose to the same diseases are clustered nonrandomly in the coexpression network, suggesting that networks can provide candidate genes that influence disease susceptibility. Therefore, our analysis of gene coexpression networks offers information on the role of human genes in normal and disease processes."
1846,"the performance of modularity maximization in practical contexts",5876649,"The performance of modularity maximization in practical contexts","Although widely used in practice, the behavior and accuracy of the popular module identification technique called modularity maximization is not well understood. Here, we present a broad and systematic characterization of its performance in practical situations. First, we generalize and clarify the recently identified resolution limit phenomenon. Second, we show that the modularity function Q exhibits extreme degeneracies: that is, the modularity landscape admits an exponential number of distinct high-scoring solutions and does not typically exhibit a clear global maximum. Third, we derive the limiting behavior of the maximum modularity Q_max for infinitely modular networks, showing that it depends strongly on the size of the network and the number of module-like subgraphs it contains. Finally, using three real-world examples of metabolic networks, we show that the degenerate solutions can fundamentally disagree on the composition of even the largest modules. Together, these results significantly extend and clarify our understanding of this popular method. In particular, they explain why so many heuristics perform well in practice at finding high-scoring partitions, why these heuristics can disagree on the composition of the identified modules, and how the estimated value of Q_max should be interpreted. Further, they imply that the output of any modularity maximization procedure should be interpreted cautiously in scientific contexts. We conclude by discussing avenues for mitigating these behaviors, such as combining information from many degenerate solutions or using generative models."
1847,"how to identify essential genes from molecular networks",5933057,"How to identify essential genes from molecular networks?","BACKGROUND: The prediction of essential genes from molecular networks is a way to test the understanding of essentiality in the context of what is known about the network. However, the current knowledge on molecular network structures is incomplete yet, and consequently the strategies aimed to predict essential genes are prone to uncertain predictions. We propose that simultaneously evaluating different network structures and different algorithms representing gene essentiality (centrality measures) may identify essential genes in networks in a reliable fashion. RESULTS: By simultaneously analyzing 16 different centrality measures on 18 different reconstructed metabolic networks for Saccharomyces cerevisiae, we show that no single centrality measure identifies essential genes from these networks in a statistically significant way; however, the combination of at least 2 centrality measures achieves a reliable prediction of most but not all of the essential genes. No improvement is achieved in the prediction of essential genes when 3 or 4 centrality measures were combined. CONCLUSION: The method reported here describes a reliable procedure to predict essential genes from molecular networks. Our results show that essential genes may be predicted only by combining centrality measures, revealing the complex nature of the function of essential genes."
1848,"windshield splatter analysis with the galaxy metagenomic pipeline",5938416,"Windshield splatter analysis with the Galaxy metagenomic pipeline","10.1101/gr.094508.109 How many species inhabit our immediate surroundings? A straightforward collection technique suitable for answering this question is known to anyone who has ever driven a car at highway speeds. The windshield of a moving vehicle is subjected to numerous insect strikes and can be used as a collection device for representative sampling. Unfortunately the analysis of biological material collected in that manner, as with most metagenomic studies, proves to be rather demanding due to the large number of required tools and considerable computational infrastructure. In this study, we use organic matter collected by a moving vehicle to design and test a comprehensive pipeline for phylogenetic profiling of metagenomic samples that includes all steps from processing and quality control of data generated by next-generation sequencing technologies to statistical analyses and data visualization. To the best of our knowledge, this is also the first publication that features a live online supplement providing access to exact analyses and workflows used in the article."
1849,"seaview version a multiplatform graphical user interface for sequence alignment and phylogenetic tree building",6008522,"SeaView Version 4: A Multiplatform Graphical User Interface for Sequence Alignment and Phylogenetic Tree Building","We present SeaView version 4, a multiplatform program designed to facilitate multiple alignment and phylogenetic tree building from molecular sequence data through the use of a graphical user interface. SeaView version 4 combines all the functions of the widely used programs SeaView (in its previous versions) and Phylo_win, and expands them by adding network access to sequence databases, alignment with arbitrary algorithm, maximum-likelihood tree building with PhyML, and display, printing, and copy-to-clipboard of rooted or unrooted, binary or multifurcating phylogenetic trees. In relation to the wide present offer of tools and algorithms for phylogenetic analyses, SeaView is especially useful for teaching and for occasional users of such software. SeaView is freely available at http://pbil.univ-lyon1.fr/software/seaview. 10.1093/molbev/msp259"
1850,"nextgeneration gap",6054102,"Next-generation gap","via @mndoci : There is a growing gap between the generation of massively parallel sequencing output and the ability to process and analyze the resulting data. New users are left to navigate a bewildering maze of base calling, alignment, assembly and analysis tools with often incomplete documentation and no idea how to compare and validate their outputs. Bridging this gap is essential, or the coveted $1,000 genome will come with a $20,000 analysis price tag."
1851,"prediction of hot spot residues at proteinprotein interfaces by combining machine learning and energybased methods",6055159,"Prediction of hot spot residues at protein-protein interfaces by combining machine learning and energy-based methods","BACKGROUND:Alanine scanning mutagenesis is a powerful experimental methodology for investigating the structural and energetic characteristics of protein complexes. Individual amino-acids are systematically mutated to alanine and changes in free energy of binding (DeltaDeltaG) measured. Several experiments have shown that protein-protein interactions are critically dependent on just a few residues (""hot spots"") at the interface. Hot spots make a dominant contribution to the free energy of binding and if mutated they can disrupt the interaction. As mutagenesis studies require significant experimental efforts, there is a need for accurate and reliable computational methods. Such methods would also add to our understanding of the determinants of affinity and specificity in protein-protein recognition.RESULTS:We present a novel computational strategy to identify hot spot residues, given the structure of a complex. We consider the basic energetic terms that contribute to hot spot interactions, i.e. van der Waals potentials, solvation energy, hydrogen bonds and Coulomb electrostatics. We treat them as input features and use machine learning algorithms such as Support Vector Machines and Gaussian Processes to optimally combine and integrate them, based on a set of training examples of alanine mutations. We show that our approach is effective in predicting hot spots and it compares favourably to other available methods. In particular we find the best performances using Transductive Support Vector Machines, a semi-supervised learning scheme. When hot spots are defined as those residues for which DeltaDeltaG [greater than or equal to] 2 kcal/mol, our method achieves a precision and a recall respectively of 56% and 65%.CONCLUSION:We have developed an hybrid scheme in which energy terms are used as input features of machine learning models. This strategy combines the strengths of machine learning and energy-based methods. Although so far these two types of approaches have mainly been applied separately to biomolecular problems, the results of our investigation indicate that there are substantial benefits to be gained by their integration."
1852,"predicting new molecular targets for known drugs",6056800,"Predicting new molecular targets for known drugs.","Although drugs are intended to be selective, at least some bind to several physiological targets, explaining side effects and efficacy. Because many drug-target combinations exist, it would be useful to explore possible interactions computationally. Here we compared 3,665 US Food and Drug Administration (FDA)-approved and investigational drugs against hundreds of targets, defining each target by its ligands. Chemical similarities between drugs and ligand sets predicted thousands of unanticipated associations. Thirty were tested experimentally, including the antagonism of the beta(1) receptor by the transporter inhibitor Prozac, the inhibition of the 5-hydroxytryptamine (5-HT) transporter by the ion channel drug Vadilex, and antagonism of the histamine H(4) receptor by the enzyme inhibitor Rescriptor. Overall, 23 new drug-target associations were confirmed, five of which were potent (<100 nM). The physiological relevance of one, the drug N,N-dimethyltryptamine (DMT) on serotonergic receptors, was confirmed in a knockout mouse. The chemical similarity approach is systematic and comprehensive, and may suggest side-effects and new indications for many drugs."
1853,"using twitter to recommend realtime topical news",6058169,"Using twitter to recommend real-time topical news","Recommending news stories to users, based on their preferences, has long been a favourite domain for recommender systems research. In this paper, we describe a novel approach to news recommendation that harnesses real-time micro-blogging activity, from a service such as Twitter, as the basis for promoting news stories from a user's favourite RSS feeds. A preliminary evaluation is carried out on an implementation of this technique that shows promising results."
1854,"energetics of displacing water molecules from protein binding sites consequences for ligand optimization",6064489,"Energetics of Displacing Water Molecules from Protein Binding Sites: Consequences for Ligand Optimization","PMID: 19778066 A strategy in drug design is to consider enhancing the affinity of lead molecules with structural modifications that displace water molecules from a protein binding site. Because success of the approach is uncertain, clarification of the associated energetics was sought in cases where similar structural modifications yield qualitatively different outcomes. Specifically, free-energy perturbation calculations were carried out in the context of Monte Carlo statistical mechanics simulations to investigate ligand series that feature displacement of ordered water molecules in the binding sites of scytalone dehydratase, p38-αMAP kinase, and EGFR kinase. The change in affinity for a ligand modification is found to correlate with the ease of displacement of the ordered water molecule. However, as in the EGFR example, the binding affinity may diminish if the free-energy increase due to the removal of the bound water molecule is not more than compensated by the additional interactions of the water-displacing moiety. For accurate computation of the effects of ligand modifications, a complete thermodynamic analysis is shown to be needed. It requires identification of the location of water molecules in the protein−ligand interface and evaluation of the free-energy changes associated with their removal and with the introduction of the ligand modification. Direct modification of the ligand in free-energy calculations is likely to trap the ordered molecule and provide misleading guidance for lead optimization."
1855,"scratchpads a datapublishing framework to build share and manage information on the diversity of life",6095667,"Scratchpads: a data-publishing framework to build, share and manage information on the diversity of life.","BACKGROUND: Natural History science is characterised by a single immense goal (to document, describe and synthesise all facets pertaining to the diversity of life) that can only be addressed through a seemingly infinite series of smaller studies. The discipline's failure to meaningfully connect these small studies with natural history's goal has made it hard to demonstrate the value of natural history to a wider scientific community. Digital technologies provide the means to bridge this gap. RESULTS: We describe the system architecture and template design of ""Scratchpads"", a data-publishing framework for groups of people to create their own social networks supporting natural history science. Scratchpads cater to the particular needs of individual research communities through a common database and system architecture. This is flexible and scalable enough to support multiple networks, each with its own choice of features, visual design, and constituent data. Our data model supports web services on standardised data elements that might be used by related initiatives such as GBIF and the Encyclopedia of Life. A Scratchpad allows users to organise data around user-defined or imported ontologies, including biological classifications. Automated semantic annotation and indexing is applied to all content, allowing users to navigate intuitively and curate diverse biological data, including content drawn from third party resources. A system of archiving citable pages allows stable referencing with unique identifiers and provides credit to contributors through normal citation processes. CONCLUSION: Our framework http://scratchpads.eu/ currently serves more than 1,100 registered users across 100 sites, spanning academic, amateur and citizen-science audiences. These users have generated more than 130,000 nodes of content in the first two years of use. The template of our architecture may serve as a model to other research communities developing data publishing frameworks outside biodiversity research."
1856,"bfast an alignment tool for large scale genome resequencing",6097099,"BFAST: An Alignment Tool for Large Scale Genome Resequencing","The new generation of massively parallel DNA sequencers, combined with the challenge of whole human genome resequencing, result in the need for rapid and accurate alignment of billions of short DNA sequence reads to a large reference genome. Speed is obviously of great importance, but equally important is maintaining alignment accuracy of short reads, in the 25â€“100 base range, in the presence of errors and true biological variation."
1857,"revisiting date and party hubs novel approaches to role assignment in protein interaction networks",6109252,"Revisiting Date and Party Hubs: Novel Approaches to Role Assignment in Protein Interaction Networks","The idea of ""date"" and ""party"" hubs has been influential in the study of protein-protein interaction networks. Date hubs display low co-expression with their partners, whilst party hubs have high co-expression. It was proposed that party hubs are local coordinators whereas date hubs are global connectors. Here, we show that the reported importance of date hubs to network connectivity can in fact be attributed to a tiny subset of them. Crucially, these few, extremely central, hubs do not display particularly low expression correlation, undermining the idea of a link between this quantity and hub function. The date/party distinction was originally motivated by an approximately bimodal distribution of hub co-expression; we show that this feature is not always robust to methodological changes. Additionally, topological properties of hubs do not in general correlate with co-expression. However, we find significant correlations between interaction centrality and the functional similarity of the interacting proteins. We suggest that thinking in terms of a date/party dichotomy for hubs in protein interaction networks is not meaningful, and it might be more useful to conceive of roles for protein-protein interactions rather than for individual proteins."
1858,"systemslevel dynamic analyses of fate change in murine embryonic stem cells",6141998,"Systems-level dynamic analyses of fate change in murine embryonic stem cells","Molecular regulation of embryonic stem cell (ESC) fate involves a coordinated interaction between epigenetic1, 2, 3, 4, transcriptional5, 6, 7, 8, 9, 10 and translational11, 12 mechanisms. It is unclear how these different molecular regulatory mechanisms interact to regulate changes in stem cell fate. Here we present a dynamic systems-level study of cell fate change in murine ESCs following a well-defined perturbation. Global changes in histone acetylation, chromatin-bound RNA polymerase II, messenger RNA (mRNA), and nuclear protein levels were measured over 5 days after downregulation of Nanog, a key pluripotency regulator13, 14, 15. Our data demonstrate how a single genetic perturbation leads to progressive widespread changes in several molecular regulatory layers, and provide a dynamic view of information flow in the epigenome, transcriptome and proteome. We observe that a large proportion of changes in nuclear protein levels are not accompanied by concordant changes in the expression of corresponding mRNAs, indicating important roles for translational and post-translational regulation of ESC fate. Gene-ontology analysis across different molecular layers indicates that although chromatin reconfiguration is important for altering cell fate, it is preceded by transcription-factor-mediated regulatory events. The temporal order of gene expression alterations shows the order of the regulatory network reconfiguration and offers further insight into the gene regulatory network. Our studies extend the conventional systems biology approach to include many molecular species, regulatory layers and temporal series, and underscore the complexity of the multilayer regulatory mechanisms responsible for changes in protein expression that determine stem cell fate."
1859,"gene expression atlas at the european bioinformatics institute",6172123,"Gene Expression Atlas at the European Bioinformatics Institute","The Gene Expression Atlas (http://www.ebi.ac.uk/gxa) is an added-value database providing information about gene expression in different cell types, organism parts, developmental stages, disease states, sample treatments and other biological/experimental conditions. The content of this database derives from curation, re-annotation and statistical analysis of selected data from the ArrayExpress Archive of Functional Genomics Data. A simple interface allows the user to query for differential gene expression either (i) by gene names or attributes such as Gene Ontology terms, or (ii) by biological conditions, e.g. diseases, organism parts or cell types. The gene queries return the conditions where expression has been reported, while condition queries return which genes are reported to be expressed in these conditions. A combination of both query types is possible. The query results are ranked using various statistical measures and by how many independent studies in the database show the particular gene-condition association. Currently, the database contains information about more than 200 000 genes from nine species and almost 4500 biological conditions studied in over 30 000 assays from over 1000 independent studies."
1860,"on the diversity of physicochemical environments experienced by identical ligands in binding pockets of unrelated proteins",6174642,"On the diversity of physicochemical environments experienced by identical ligands in binding pockets of unrelated proteins.","Most function prediction methods that identify cognate ligands from binding site analyses work on the assumption of molecular complementarity. These approaches build on the conjectured complementarity of geometrical and physicochemical properties between ligands and binding sites so that similar binding sites will bind similar ligands. We found that this assumption does not generally hold for protein-ligand interactions and observed that it is not the chemical composition of ligand molecules that dictates the complementarity between protein and ligand molecules, but that the ligand's share within the functional mechanism of a protein determines the degree of complementarity. Here, we present for a set of cognate ligands a descriptive analysis and comparison of the physicochemical properties that each ligand experiences in various nonhomologous binding pockets. The comparisons in each ligand set reveal large variations in their experienced physicochemical properties, suggesting that the same ligand can bind to distinct physicochemical environments. In some protein ligand complexes, the variation was found to correlate with the electrochemical characteristic of ligand molecules, whereas in others it was disclosed as a prerequisite for the biochemical function of the protein. To achieve binding, proteins were observed to engage in subtle balancing acts between electrostatic and hydrophobic interactions to generate stabilizing free energies of binding. For the presented analysis, a new method for scoring hydrophobicity from molecular environments was developed showing high correlations with experimental determined desolvation energies. The presented results highlight the complexities of molecular recognition and underline the challenges of computational structural biology in developing methods to detect these important subtleties. Proteins 2010. (c) 2009 Wiley-Liss, Inc."
1861,"a highthroughput screening approach to discovering good forms of biologically inspired visual representation",6221233,"A high-throughput screening approach to discovering good forms of biologically inspired visual representation.","While many models of biological object recognition share a common set of ""broad-stroke"" properties, the performance of any one model depends strongly on the choice of parameters in a particular instantiation of that model--e.g., the number of units per layer, the size of pooling kernels, exponents in normalization operations, etc. Since the number of such parameters (explicit or implicit) is typically large and the computational cost of evaluating one particular parameter set is high, the space of possible model instantiations goes largely unexplored. Thus, when a model fails to approach the abilities of biological visual systems, we are left uncertain whether this failure is because we are missing a fundamental idea or because the correct ""parts"" have not been tuned correctly, assembled at sufficient scale, or provided with enough training. Here, we present a high-throughput approach to the exploration of such parameter sets, leveraging recent advances in stream processing hardware (high-end NVIDIA graphic cards and the PlayStation 3's IBM Cell Processor). In analogy to high-throughput screening approaches in molecular biology and genetics, we explored thousands of potential network architectures and parameter instantiations, screening those that show promising object recognition performance for further analysis. We show that this approach can yield significant, reproducible gains in performance across an array of basic object recognition tasks, consistently outperforming a variety of state-of-the-art purpose-built vision systems from the literature. As the scale of available computational power continues to expand, we argue that this approach has the potential to greatly accelerate progress in both artificial vision and our understanding of the computational underpinning of biological vision."
1862,"transcription factors mediate longrange enhancerpromoter interactions",6259589,"Transcription factors mediate long-range enhancerâpromoter interactions","We examined how remote enhancers establish physical communication with target promoters to activate gene transcription in response to environmental signals. Although the natural IFN-Î² enhancer is located immediately upstream of the core promoter, it also can function as a classical enhancer element conferring virus infection-dependent activation of heterologous promoters, even when it is placed several kilobases away from these promoters. We demonstrated that the remote IFN-Î² enhancer âloops outâ the intervening DNA to reach the target promoter. These chromatin loops depend on sequence-specific transcription factors bound to the enhancer and the promoter and thus can explain the specificity observed in enhancerâpromoter interactions, especially in complex genetic loci. Transcription factor binding sites scattered between an enhancer and a promoter can work as decoys trapping the enhancer in nonproductive loops, thus resembling insulator elements. Finally, replacement of the transcription factor binding sites involved in DNA looping with those of a heterologous prokaryotic protein, the Î» repressor, which is capable of loop formation, rescues enhancer function from a distance by re-establishing enhancerâpromoter loop formation."
1863,"colloquium statistical mechanics of money wealth and income",6285341,"Colloquium: Statistical mechanics of money, wealth, and income","This Colloquium reviews statistical models for money, wealth, and income distributions developed in the econophysics literature since the late 1990s. By analogy with the Boltzmann-Gibbs distribution of energy in physics, it is shown that the probability distribution of money is exponential for certain classes of models with interacting economic agents. Alternative scenarios are also reviewed. Data analysis of the empirical distributions of wealth and income reveals a two-class distribution. The majority of the population belongs to the lower class, characterized by the exponential (""thermal"") distribution, whereas a small fraction of the population in the upper class is characterized by the power-law (""superthermal"") distribution. The lower part is very stable, stationary in time, whereas the upper part is highly dynamical and out of equilibrium."
1864,"to use or not to use web in higher education",6346056,"To use or not to use web 2.0 in higher education?","Web 2.0 has been, during the last years, one of the most fashionable words for a whole range of evolutions regarding the Internet. Although it was identified by the current analysts as the key technology for the next decade, the actors from the educational field do not really know what Web 2.0 means. Since the author started to explore and use Web 2.0 technologies in her own development/improvement, she has been intrigued by their potential and, especially, by the possibility of integrating them in education and in particular in the teaching activity. The purpose of this paper is both to promote scholarly inquiry about the need of a new type a pedagogy (Web 2.0 based) and the development / adoption of best practice in teaching and learning with web 2.0 in higher education (HE). The article main objectives are: • to introduce theoretical aspects of using Web 2.0 technologies in higher education • to present models of integrating Web 2.0 technologies in teaching, learning and assessment • to identify the potential benefits of these technologies as well as to highlight some of the problematic issues / barriers encountered, surrounding the pedagogical use of Web 2.0 in higher education • to propose an agenda for future research, and to develop pedagogy 2.0 scenarios for HE sector."
1865,"a minimal sequence code for switching protein structure and function",6387600,"A minimal sequence code for switching protein structure and function","10.1073/pnas.0906408106 We present here a structural and mechanistic description of how a protein changes its fold and function, mutation by mutation. Our approach was to create 2 proteins that () are stably folded into 2 different folds, () have 2 different functions, and () are very similar in sequence. In this simplified sequence space we explore the mutational path from one fold to another. We show that an IgG-binding, 4β+α fold can be transformed into an albumin-binding, 3-α fold via a mutational pathway in which neither function nor native structure is completely lost. The stabilities of all mutants along the pathway are evaluated, key high-resolution structures are determined by NMR, and an explanation of the switching mechanism is provided. We show that the conformational switch from 4β+α to 3-α structure can occur via a single amino acid substitution. On one side of the switch point, the 4β+α fold is >90% populated (pH 7.2, 20 °C). A single mutation switches the conformation to the 3-α fold, which is >90% populated (pH 7.2, 20 °C). We further show that a bifunctional protein exists at the switch point with affinity for both IgG and albumin."
1866,"threedimensional structure determination from a single view",6390470,"Three-dimensional structure determination from a single view","The ability to determine the structure of matter in three dimensions has profoundly advanced our understanding of nature. Traditionally, the most widely used schemes for three-dimensional (3D) structure determination of an object are implemented by acquiring multiple measurements over various sample orientations, as in the case of crystallography and tomography1, 2, or by scanning a series of thin sections through the sample, as in confocal microscopy3. Here we present a 3D imaging modality, termed ankylography (derived from the Greek words ankylos meaning ‘curved’ and graphein meaning ‘writing’), which under certain circumstances enables complete 3D structure determination from a single exposure using a monochromatic incident beam. We demonstrate that when the diffraction pattern of a finite object is sampled at a sufficiently fine scale on the Ewald sphere, the 3D structure of the object is in principle determined by the 2D spherical pattern. We confirm the theoretical analysis by performing 3D numerical reconstructions of a sodium silicate glass structure at 2 Å resolution, and a single poliovirus at 2–3 nm resolution, from 2D spherical diffraction patterns alone. Using diffraction data from a soft X-ray laser, we also provide a preliminary demonstration that ankylography is experimentally feasible by obtaining a 3D image of a test object from a single 2D diffraction pattern. With further development, this approach of obtaining complete 3D structure information from a single view could find broad applications in the physical and life sciences."
1867,"rna processing and its regulation global insights into biological networks",6401146,"RNA processing and its regulation: global insights into biological networks"," In recent years views of eukaryotic gene expression have been transformed by the finding that enormous diversity can be generated at the RNA level. Advances in technologies for characterizing RNA populations are revealing increasingly complete descriptions of RNA regulation and complexity; for example, through alternative splicing, alternative polyadenylation and RNA editing. New biochemical strategies to map protein–RNA interactions in vivo are yielding transcriptome-wide insights into mechanisms of RNA processing. These advances, combined with bioinformatics and genetic validation, are leading to the generation of functional RNA maps that reveal the rules underlying RNA regulation and networks of biologically coherent transcripts. Together these are providing new insights into molecular cell biology and disease."
1868,"stepwise modification of a modular enhancer underlies adaptation in a drosophila population",6411489,"Stepwise Modification of a Modular Enhancer Underlies Adaptation in a Drosophila Population","The evolution of cis regulatory elements (enhancers) of developmentally regulated genes plays a large role in the evolution of animal morphology. However, the mutational path of enhancer evolution--the number, origin, effect, and order of mutations that alter enhancer function--has not been elucidated. Here, we localized a suite of substitutions in a modular enhancer of the ebony locus responsible for adaptive melanism in a Ugandan Drosophila population. We show that at least five mutations with varied effects arose recently from a combination of standing variation and new mutations and combined to create an allele of large phenotypic effect. We underscore how enhancers are distinct macromolecular entities, subject to fundamentally different, and generally more relaxed, functional constraints relative to protein sequences. 10.1126/science.1178357"
1869,"a quick guide for developing effective bioinformatics programming skills",6434100,"A Quick Guide for Developing Effective Bioinformatics Programming Skills","Bioinformatics programming skills are becoming a necessity across many facets of biology and medicine, owed in part to the continuing explosion of biological data aggregation and the complexity and scale of questions now being addressed through modern bioinformatics. Although many are now receiving formal training in bioinformatics through various university degree and certificate programs, this training is often focused strongly on bioinformatics methodology, leaving many important and practical aspects of bioinformatics to self-education and experience. The following set of guidelines distill several key principals of effective bioinformatics programming, which the authors learned through insights gained across many years of combined experience developing popular bioinformatics software applications and database systems in both academic and commercial settings [1]–[6]. Successful adoption of these principals will serve both beginner and experienced bioinformaticians alike in career development and pursuit of professional and scientific goals."
1870,"learning styles concepts and evidence",6456651,"Learning Styles: Concepts and Evidence","10.1111/j.1539-6053.2009.01038.x The term âlearning stylesâ refers to the concept that individuals differ in regard to what mode of instruction or study is most effective for them. Proponents of learning-style assessment contend that optimal instruction requires diagnosing individuals' learning style and tailoring instruction accordingly. Assessments of learning style typically ask people to evaluate what sort of information presentation they prefer (e.g., words versus pictures versus speech) and/or what kind of mental activity they find most engaging or congenial (e.g., analysis versus listening), although assessment instruments are extremely diverse. The most commonâbut not the onlyâhypothesis about the instructional relevance of learning styles is the meshing hypothesis, according to which instruction is best provided in a format that matches the preferences of the learner (e.g., for a âvisual learner,â emphasizing visual presentation of information).The learning-styles view has acquired great influence within the education field, and is frequently encountered at levels ranging from kindergarten to graduate school. There is a thriving industry devoted to publishing learning-styles tests and guidebooks for teachers, and many organizations offer professional development workshops for teachers and educators built around the concept of learning styles.The authors of the present review were charged with determining whether these practices are supported by scientific evidence. We concluded that any credible validation of learning-styles-based instruction requires robust documentation of a very particular type of experimental finding with several necessary criteria. First, students must be divided into groups on the basis of their learning styles, and then students from each group must be randomly assigned to receive one of multiple instructional methods. Next, students must then sit for a final test that is the same for all students. Finally, in order to demonstrate that optimal learning requires that students receive instruction tailored to their putative learning style, the experiment must reveal a specific type of interaction between learning style and instructional method: Students with one learning style achieve the best educational outcome when given an instructional method that differs from the instructional method producing the best outcome for students with a different learning style. In other words, the instructional method that proves most effective for students with one learning style is not the most effective method for students with a different learning style.Our review of the literature disclosed ample evidence that children and adults will, if asked, express preferences about how they prefer information to be presented to them. There is also plentiful evidence arguing that people differ in the degree to which they have some fairly specific aptitudes for different kinds of thinking and for processing different types of information. However, we found virtually no evidence for the interaction pattern mentioned above, which was judged to be a precondition for validating the educational applications of learning styles. Although the literature on learning styles is enormous, very few studies have even used an experimental methodology capable of testing the validity of learning styles applied to education. Moreover, of those that did use an appropriate method, several found results that flatly contradict the popular meshing hypothesis.We conclude therefore, that at present, there is no adequate evidence base to justify incorporating learning-styles assessments into general educational practice. Thus, limited education resources would better be devoted to adopting other educational practices that have a strong evidence base, of which there are an increasing number. However, given the lack of methodologically sound studies of learning styles, it would be an error to conclude that all possible versions of learning styles have been tested and found wanting; many have simply not been tested at all. Further research on the use of learning-styles assessment in instruction may in some cases be warranted, but such research needs to be performed appropriately."
1871,"the rate and molecular spectrum of spontaneous mutations in arabidopsis thaliana",6463427,"The Rate and Molecular Spectrum of Spontaneous Mutations in Arabidopsis thaliana","To take complete advantage of information on within-species polymorphism and divergence from close relatives, one needs to know the rate and the molecular spectrum of spontaneous mutations. To this end, we have searched for de novo spontaneous mutations in the complete nuclear genomes of five Arabidopsis thaliana mutation accumulation lines that had been maintained by single-seed descent for 30 generations. We identified and validated 99 base substitutions and 17 small and large insertions and deletions. Our results imply a spontaneous mutation rate of 7 x 10-9 base substitutions per site per generation, the majority of which are G:C[-&gt;]A:T transitions. We explain this very biased spectrum of base substitution mutations as a result of two main processes: deamination of methylated cytosines and ultraviolet light-induced mutagenesis. 10.1126/science.1180677"
1872,"netpath a public resource of curated signal transduction pathways",6531882,"NetPath: a public resource of curated signal transduction pathways.","We have developed NetPath as a resource of curated human signaling pathways. As an initial step, NetPath provides detailed maps of a number of immune signaling pathways, which include approximately 1,600 reactions annotated from the literature and more than 2,800 instances of transcriptionally regulated genes - all linked to over 5,500 published articles. We anticipate NetPath to become a consolidated resource for human signaling pathways that should enable systems biology approaches."
1873,"the freeenergy principle a unified brain theory",6569490,"The free-energy principle: a unified brain theory?"," A free-energy principle has been proposed recently that accounts for action, perception and learning. This Review looks at some key brain theories in the biological (for example, neural Darwinism) and physical (for example, information theory and optimal control theory) sciences from the free-energy perspective. Crucially, one key theme runs through each of these theories — optimization. Furthermore, if we look closely at what is optimized, the same quantity keeps emerging, namely value (expected reward, expected utility) or its complement, surprise (prediction error, expected cost). This is the quantity that is optimized under the free-energy principle, which suggests that several global brain theories might be unified within a free-energy framework."
1874,"defining transcribed regions using rnaseq",6580456,"Defining transcribed regions using RNA-seq.","Next-generation sequencing technologies are revolutionizing genomics research. It is now possible to generate gigabase pairs of DNA sequence within a week without time-consuming cloning or massive infrastructure. This technology has recently been applied to the development of 'RNA-seq' techniques for sequencing cDNA from various organisms, with the goal of characterizing entire transcriptomes. These methods provide unprecedented resolution and depth of data, enabling simultaneous quantification of gene expression, discovery of novel transcripts and exons, and measurement of splicing efficiency. We present here a validated protocol for nonstrand-specific transcriptome sequencing via RNA-seq, describing the library preparation process and outlining the bioinformatic analysis procedure. While sample preparation and sequencing take a fairly short period of time (1-2 weeks), the downstream analysis is by far the most challenging and time-consuming aspect and can take weeks to months, depending on the experimental objectives."
1875,"evolutionary mirages selection on binding site composition creates the illusion of conserved grammars in drosophila enhancers",6581403,"Evolutionary Mirages: Selection on Binding Site Composition Creates the Illusion of Conserved Grammars in Drosophila Enhancers","The clustering of transcription factor binding sites in developmental enhancers and the apparent preferential conservation of clustered sites have been widely interpreted as proof that spatially constrained physical interactions between transcription factors are required for regulatory function. However, we show here that selection on the composition of enhancers alone, and not their internal structure, leads to the accumulation of clustered sites with evolutionary dynamics that suggest they are preferentially conserved. We simulated the evolution of idealized enhancers from Drosophila melanogaster constrained to contain only a minimum number of binding sites for one or more factors. Under this constraint, mutations that destroy an existing binding site are tolerated only if a compensating site has emerged elsewhere in the enhancer. Overlapping sites, such as those frequently observed for the activator Bicoid and repressor KrÃ¼ppel, had significantly longer evolutionary half-lives than isolated sites for the same factors. This leads to a substantially higher density of overlapping sites than expected by chance and the appearance that such sites are preferentially conserved. Because D. melanogaster (like many other species) has a bias for deletions over insertions, sites tended to become closer together over time, leading to an overall clustering of sites in the absence of any selection for clustered sites. Since this effect is strongest for the oldest sites, clustered sites also incorrectly appear to be preferentially conserved. Following speciation, sites tend to be closer together in all descendent species than in their common ancestors, violating the common assumption that shared features of species' genomes reflect their ancestral state. Finally, we show that selection on binding site composition alone recapitulates the observed number of overlapping and closely neighboring sites in real D. melanogaster enhancers. Thus, this study calls into question the common practice of inferring â€œ cis -regulatory grammarsâ€� from the organization and evolutionary dynamics of developmental enhancers."
1876,"histone modification levels are predictive for gene expression",6617284,"Histone modification levels are predictive for gene expression","10.1073/pnas.0909344107 Histones are frequently decorated with covalent modifications. These histone modifications are thought to be involved in various chromatin-dependent processes including transcription. To elucidate the relationship between histone modifications and transcription, we derived quantitative models to predict the expression level of genes from histone modification levels. We found that histone modification levels and gene expression are very well correlated. Moreover, we show that only a small number of histone modifications are necessary to accurately predict gene expression. We show that different sets of histone modifications are necessary to predict gene expression driven by high CpG content promoters (HCPs) or low CpG content promoters (LCPs). Quantitative models involving H3K4me3 and H3K79me1 are the most predictive of the expression levels in LCPs, whereas HCPs require H3K27ac and H4K20me1. Finally, we show that the connections between histone modifications and gene expression seem to be general, as we were able to predict gene expression levels of one cell type using a model trained on another one. ER -  "
1877,"gene ontology analysis for rnaseq accounting for selection bias",6628158,"Gene ontology analysis for RNA-seq: accounting for selection bias","We present GOseq, an application for performing Gene Ontology (GO) analysis on RNA-seq data. GO analysis is widely used to reduce complexity and highlight biological processes in genome-wide expression studies, but standard methods give biased results on RNA-seq data due to over-detection of differential expression for long and highly expressed transcripts. Application of GOseq to a prostate cancer data set shows that GOseq dramatically changes the results, highlighting categories more consistent with the known biology."
1878,"ultrahighthroughput screening in dropbased microfluidics for directed evolution",6644901,"Ultrahigh-throughput screening in drop-based microfluidics for directed evolution","The explosive growth in our knowledge of genomes, proteomes, and metabolomes is driving ever-increasing fundamental understanding of the biochemistry of life, enabling qualitatively new studies of complex biological systems and their evolution. This knowledge also drives modern biotechnologies, such as molecular engineering and synthetic biology, which have enormous potential to address urgent problems, including developing potent new drugs and providing environmentally friendly energy. Many of these studies, however, are ultimately limited by their need for even-higher-throughput measurements of biochemical reactions. We present a general ultrahigh-throughput screening platform using drop-based microfluidics that overcomes these limitations and revolutionizes both the scale and speed of screening. We use aqueous drops dispersed in oil as picoliter-volume reaction vessels and screen them at rates of thousands per second. To demonstrate its power, we apply the system to directed evolution, identifying new mutants of the enzyme horseradish peroxidase exhibiting catalytic rates more than 10 times faster than their parent, which is already a very efficient enzyme. We exploit the ultrahigh throughput to use an initial purifying selection that removes inactive mutants; we identify approximately 100 variants comparable in activity to the parent from an initial population of approximately 10(7). After a second generation of mutagenesis and high-stringency screening, we identify several significantly improved mutants, some approaching diffusion-limited efficiency. In total, we screen approximately 10(8) individual enzyme reactions in only 10 h, using < 150 microL of total reagent volume; compared to state-of-the-art robotic screening systems, we perform the entire assay with a 1,000-fold increase in speed and a 1-million-fold reduction in cost."
1879,"x genomes depth does matter",6646501,"2x genomes - depth does matter","BACKGROUND: Given the availability of full genome sequences, mapping gene gains, duplications, and losses during evolution should theoretically be straightforward. However, this endeavor suffers from overemphasis on detecting conserved genome features, which in turn has led to sequencing multiple eutherian genomes with low coverage rather than fewer genomes with high-coverage and more even distribution in the phylogeny. Although limitations associated with analysis of low coverage genomes are recognized, they have not been quantified. RESULTS: Here, using recently developed comparative genomic application systems, we evaluate the impact of low-coverage genomes on inferences pertaining to gene gains and losses when analyzing eukaryote genome evolution through gene duplication. We demonstrate that, when performing inference of genome content evolution, low-coverage genomes generate not only a massive number of false gene losses, but also striking artifacts in gene duplication inference, especially at the most recent common ancestor of low-coverage genomes. We show that the artifactual gains are caused by the low coverage of genome sequence per se rather than by the increased taxon sampling in a biased portion of the species tree. CONCLUSIONS: We argue that it will remain difficult to differentiate artifacts from true changes in modes and tempo of genome evolution until there is better homogeneity in both taxon sampling and high-coverage sequencing. This is important for broadening the utility of full genome data to the community of evolutionary biologists, whose interests go well beyond widely conserved physiologies and developmental patterns as they seek to understand the generative mechanisms underlying biological diversity."
1880,"ancient human genome sequence of an extinct palaeoeskimo",6651681,"Ancient human genome sequence of an extinct Palaeo-Eskimo","We report here the genome sequence of an ancient human. Obtained from ~4,000-year-old permafrost-preserved hair, the genome represents a male individual from the first known culture to settle in Greenland. Sequenced to an average depth of 20×, we recover 79% of the diploid genome, an amount close to the practical limit of current sequencing technologies. We identify 353,151 high-confidence single-nucleotide polymorphisms (SNPs), of which 6.8% have not been reported previously. We estimate raw read contamination to be no higher than 0.8%. We use functional SNP assessment to assign possible phenotypic characteristics of the individual that belonged to a culture whose location has yielded only trace human remains. We compare the high-confidence SNPs to those of contemporary populations to find the populations most closely related to the individual. This provides evidence for a migration from Siberia into the New World some 5,500 years ago, independent of that giving rise to the modern Native Americans and Inuit."
1881,"assigning roles to dna regulatory motifs using comparative genomics",6652871,"Assigning roles to DNA regulatory motifs using comparative genomics","Motivation: Transcription factors (TFs) are crucial during the lifetime of the cell. Their functional roles are defined by the genes they regulate. Uncovering these roles not only sheds light on the TF at hand but puts it into the context of the complete regulatory network.Results: Here, we present an alignment- and threshold-free comparative genomics approach for assigning functional roles to DNA regulatory motifs. We incorporate our approach into the Gomo algorithm, a computational tool for detecting associations between a user-specified DNA regulatory motif [expressed as a position weight matrix (PWM)] and Gene Ontology (GO) terms. Incorporating multiple species into the analysis significantly improves Gomo's ability to identify GO terms associated with the regulatory targets of TFs. Including three comparative species in the process of predicting TF roles in Saccharomyces cerevisiae and Homo sapiens increases the number of significant predictions by 75 and 200%, respectively. The predicted GO terms are also more specific, yielding deeper biological insight into the role of the TF. Adjusting motif (binding) affinity scores for individual sequence composition proves to be essential for avoiding false positive associations. We describe a novel DNA sequence-scoring algorithm that compensates a thermodynamic measure of DNA-binding affinity for individual sequence base composition. Gomo's prediction accuracy proves to be relatively insensitive to how promoters are defined. Because Gomo uses a threshold-free form of gene set analysis, there are no free parameters to tune. Biologists can investigate the potential roles of DNA regulatory motifs of interest using Gomo via the web (http://meme.nbcr.net).Contact: t.bailey@uq.edu.auSupplementary information: Supplementary data are available at Bioinformatics online."
1882,"complete khoisan and bantu genomes from southern africa",6678060,"Complete Khoisan and Bantu genomes from southern Africa","The genetic structure of the indigenous hunter-gatherer peoples of southern Africa, the oldest known lineage of modern human, is important for understanding human diversity. Studies based on mitochondrial1 and small sets of nuclear markers2 have shown that these hunter-gatherers, known as Khoisan, San, or Bushmen, are genetically divergent from other humans1, 3. However, until now, fully sequenced human genomes have been limited to recently diverged populations4, 5, 6, 7, 8. Here we present the complete genome sequences of an indigenous hunter-gatherer from the Kalahari Desert and a Bantu from southern Africa, as well as protein-coding regions from an additional three hunter-gatherers from disparate regions of the Kalahari. We characterize the extent of whole-genome and exome diversity among the five men, reporting 1.3 million novel DNA differences genome-wide, including 13,146 novel amino acid variants. In terms of nucleotide substitutions, the Bushmen seem to be, on average, more different from each other than, for example, a European and an Asian. Observed genomic differences between the hunter-gatherers and others may help to pinpoint genetic adaptations to an agricultural lifestyle. Adding the described variants to current databases will facilitate inclusion of southern Africans in medical research efforts, particularly when family and medical histories can be correlated with genome-wide data.  doi:10.1038/nature08795"
1883,"variability in gene expression underlies incomplete penetrance",6678285,"Variability in gene expression underlies incomplete penetrance","The phenotypic differences between individual organisms can often be ascribed to underlying genetic and environmental variation. However, even genetically identical organisms in homogeneous environments vary, indicating that randomness in developmental processes such as gene expression may also generate diversity. To examine the consequences of gene expression variability in multicellular organisms, we studied intestinal specification in the nematode Caenorhabditis elegans in which wild-type cell fate is invariant and controlled by a small transcriptional network. Mutations in elements of this network can have indeterminate effects: some mutant embryos fail to develop intestinal cells, whereas others produce intestinal precursors. By counting transcripts of the genes in this network in individual embryos, we show that the expression of an otherwise redundant gene becomes highly variable in the mutants and that this variation is subjected to a threshold, producing an ON/OFF expression pattern of the master regulatory gene of intestinal differentiation. Our results demonstrate that mutations in developmental networks can expose otherwise buffered stochastic variability in gene expression, leading to pronounced phenotypic variation."
1884,"gobayes gene ontologybased overrepresentation analysis using a bayesian approach",6714609,"GO-Bayes: Gene Ontology-based overrepresentation analysis using a Bayesian approach.","MOTIVATION: A typical approach for the interpretation of high-throughput experiments, such as gene expression microarrays, is to produce groups of genes based on certain criteria (e.g. genes that are differentially expressed). To gain more mechanistic insights into the underlying biology, overrepresentation analysis (ORA) is often conducted to investigate whether gene sets associated with particular biological functions, for example, as represented by Gene Ontology (GO) annotations, are statistically overrepresented in the identified gene groups. However, the standard ORA, which is based on the hypergeometric test, analyzes each GO term in isolation and does not take into account the dependence structure of the GO-term hierarchy. RESULTS: We have developed a Bayesian approach (GO-Bayes) to measure overrepresentation of GO terms that incorporates the GO dependence structure by taking into account evidence not only from individual GO terms, but also from their related terms (i.e. parents, children, siblings, etc.). The Bayesian framework borrows information across related GO terms to strengthen the detection of overrepresentation signals. As a result, this method tends to identify sets of closely related GO terms rather than individual isolated GO terms. The advantage of the GO-Bayes approach is demonstrated with a simulation study and an application example."
1885,"thirty years of information literacy",6714735,"Thirty years of information literacy (1977â2007)","Over the last three decades, promotion of information literacy has become one of the main goals of librarians and academics. As the emergence of information technologies has raised new challenges and roles for users, information literacy has shifted from the concept of simple training to the provision of the skills and competencies that are critical to the improved use of information. A terminological, conceptual and statistical analysis of the main subjects related to information literacy, as well as its evolution over the last 30 years, is provided with the aim of illustrating how information literacy has been progressively incorporated into the library and academic fields."
1886,"bionet an rpackage for the functional analysis of biological networks",6728249,"BioNet: an R-Package for the functional analysis of biological networks.","MOTIVATION: Increasing quantity and quality of data in transcriptomics and interactomics create the need for integrative approaches to network analysis. Here, we present a comprehensive R-package for the analysis of biological networks including an exact and a heuristic approach to identify functional modules. RESULTS: The BioNet package provides an extensive framework for integrated network analysis in R. This includes the statistics for the integration of transcriptomic and functional data with biological networks, the scoring of nodes as well as methods for network search and visualization. AVAILABILITY: The BioNet package and a tutorial are available from http://bionet.bioapps.biozentrum.uni-wuerzburg.de."
1887,"scriptree scripting phylogenetic graphics",6746962,"ScripTree: scripting phylogenetic graphics","Summary: There is a large amount of tools for interactive display of phylogenetic trees. However, there is a shortage of tools for the automation of tree rendering. Scripting phylogenetic graphics would enable the saving of graphical analyses involving numerous and complex tree handling operations and would allow the automation of repetitive tasks. ScripTree is a tool intended to fill this gap. It is an interpreter to be used in batch mode. Phylogenetic graphics instructions, related to tree rendering as well as tree annotation, are stored in a text file and processed in a sequential way.  Availability: ScripTree can be used online or downloaded at www.scriptree.org, under the GPL license.  Implementation: ScripTree, written in Tcl/Tk, is a cross-platform application available for Windows and Unix-like systems including OS X. It can be used either as a stand-alone package or included in a bioinformatic pipeline and linked to a HTTP server.  Contact: chevenet@ird.fr 10.1093/bioinformatics/btq086"
1888,"a scaling normalization method for differential expression analysis of rnaseq data",6756679,"A scaling normalization method for differential expression analysis of RNA-seq data","The fine detail provided by sequencing-based transcriptome surveys suggests that RNA-seq is likely to become the platform of choice for interrogating steady state RNA. In order to discover biologically important changes in expression, we show that normalization continues to be an essential step in the analysis. We outline a simple and effective method for performing normalization and show dramatically improved results for inferring differential expression in simulated and publicly available data sets."
1889,"statistics and physical origins of pk and ionization state changes upon proteinligand binding",6758508,"Statistics and Physical Origins of pK and Ionization State Changes upon Protein-Ligand Binding","This work investigates statistical prevalence and overall physical origins of changes in charge states of receptor proteins upon ligand binding. These changes are explored as a function of the ligand type (small molecule, protein, and nucleic acid), and distance from the binding region. Standard continuum solvent methodology is used to compute, on an equal footing, pK changes upon ligand binding for a total of 5899 ionizable residues in 20 protein-protein, 20 protein-small molecule, and 20 protein-nucleic acid high-resolution complexes. The size of the data set combined with an extensive error and sensitivity analysis allows us to make statistically justified and conservative conclusions: in 60% of all protein-small molecule, 90% of all protein-protein, and 85% of all protein-nucleic acid complexes there exists at least one ionizable residue that changes its charge state upon ligand binding at physiological conditions (pH = 6.5). Considering the most biologically relevant pH range of 48, the number of ionizable residues that experience substantial pK changes (pK > 1.0) due to ligand binding is appreciable: on average, 6% of all ionizable residues in protein-small molecule complexes, 9% in protein-protein, and 12% in protein-nucleic acid complexes experience a substantial pK change upon ligand binding. These changes are safely above the statistical false-positive noise level. Most of the changes occur in the immediate binding interface region, where approximately one out of five ionizable residues experiences substantial pK change regardless of the ligand type. However, the physical origins of the change differ between the types: in protein-nucleic acid complexes, the pK values of interface residues are predominantly affected by electrostatic effects, whereas in protein-protein and protein-small molecule complexes, structural changes due to the induced-fit effect play an equally important role. In protein-protein and protein-nucleic acid complexes, there is a statistically significant number of substantial pK perturbations, mostly due to the induced-fit structural changes, in regions far from the binding interface."
1890,"mapreduce online",6763715,"MapReduce Online","MapReduce is a popular framework for data-intensive distributed computing of batch jobs. To simplify fault tolerance, many implementations of MapReduce materialize the entire output of each map and reduce task before it can be consumed. In this paper, we propose a modiﬁed MapReduce architecture that allows data to be pipelined between operators. This extends the MapReduce programming model beyond batch processing, and can reduce completion times and improve system utilization for batch jobs as well. We present a modiﬁed version of the Hadoop MapReduce framework that supports on-line aggregation, which allows users to see ""early returns"" from a job as it is being computed. Our Hadoop Online Prototype (HOP) also supports continuous queries, which enable MapReduce programs to be written for applications such as event monitoring and stream processing. HOP retains the fault tolerance properties of Hadoop and can run unmodiﬁed user-deﬁned MapReduce programs."
1891,"rpsph a novel smoothed particle hydrodynamics algorithm",6765697,"rpSPH: a novel Smoothed Particle Hydrodynamics Algorithm","We suggest a novel discretisation of the momentum equation for Smoothed Particle Hydrodynamics (SPH) and show that it significantly improves the accuracy of the obtained solutions. Our new formulation which we refer to as relative pressure SPH, rpSPH, evaluates the pressure force in respect to the local pressure. It respects Newtons first law of motion and applies forces to particles only when there is a net force acting upon them. This is in contrast to standard SPH which explicitly uses Newtons third law of motion continuously applying equal but opposite forces between particles. rpSPH does not show the unphysical particle noise, the clumping or banding instability, unphysical surface tension, and unphysical scattering of different mass particles found for standard SPH. At the same time it uses fewer computational operations. and only changes a single line in existing SPH codes. We demonstrate its performance on isobaric uniform density distributions, uniform density shearing flows, the Kelvin-Helmholtz and Rayleigh-Taylor instabilities, the Sod shock tube, the Sedov-Taylor blast wave and a cosmological integration of the Santa Barbara galaxy cluster formation test. rpSPH is an improvement these cases. The improvements come at the cost of giving up exact momentum conservation of the scheme. Consequently one can also obtain unphysical solutions particularly at low resolutions."
1892,"identification and analysis of unitary pseudogenes historic and contemporary gene losses in humans and other primates",6776856,"Identification and analysis of unitary pseudogenes: historic and contemporary gene losses in humans and other primates.","ABSTRACT: BACKGROUND: Unitary pseudogenes are a class of unprocessed pseudogenes without functioning counterparts in the genome. They constitute only a small fraction of annotated pseudogenes in the human genome. However, as they represent distinct functional losses over time, they shed light on the unique features of humans in primate evolution. RESULTS: We present a pipeline to detect human unitary pseudogenes through analyzing the global inventory of orthologs between the human genome and its mammalian relatives. We focus on gene losses along the human lineage after the divergence from rodents (75 million years ago). In total, we identify 76 unitary pseudogenes, including previously-annotated ones, and many novel ones. By comparing these to their functioning ortholog in other mammals, we can approximately date the creation of each unitary pseudogene, that is, the gene 'death date', and show that for our group of 76 unitary pseudogenes, the functional genes became disabled at a fairly uniform rate throughout primate evolution - not all at once, and correlated, for instance, with the 'Alu burst.' Furthermore, we identify 11 unitary pseudogenes that are polymorphic - they have both nonfunctional and functional alleles currently segregating in the human population. Comparing them with their orthologs in other primates, we find that two of them are in fact pseudogenes in non-human primates, suggesting that they represent cases of a gene being resurrected in the human lineage. CONCLUSION: This analysis of unitary pseudogenes provides insights into the evolutionary constraints faced by different organisms and the timescales of functional gene loss in humans."
1893,"maydayintegrative analytics for expression data",6780233,"Mayday--integrative analytics for expression data.","BACKGROUND: DNA Microarrays have become the standard method for large scale analyses of gene expression and epigenomics. The increasing complexity and inherent noisiness of the generated data makes visual data exploration ever more important. Fast deployment of new methods as well as a combination of predefined, easy to apply methods with programmer's access to the data are important requirements for any analysis framework. Mayday is an open source platform with emphasis on visual data exploration and analysis. Many built-in methods for clustering, machine learning and classification are provided for dissecting complex datasets. Plugins can easily be written to extend Mayday's functionality in a large number of ways. As Java program, Mayday is platform-independent and can be used as Java WebStart application without any installation. Mayday can import data from several file formats, database connectivity is included for efficient data organization. Numerous interactive visualization tools, including box plots, profile plots, principal component plots and a heatmap are available, can be enhanced with metadata and exported as publication quality vector files. RESULTS: We have rewritten large parts of Mayday's core to make it more efficient and ready for future developments. Among the large number of new plugins are an automated processing framework, dynamic filtering, new and efficient clustering methods, a machine learning module and database connectivity. Extensive manual data analysis can be done using an inbuilt R terminal and an integrated SQL querying interface. Our visualization framework has become more powerful, new plot types have been added and existing plots improved. CONCLUSIONS: We present a major extension of Mayday, a very versatile open-source framework for efficient micro array data analysis designed for biologists and bioinformaticians. Most everyday tasks are already covered. The large number of available plugins as well as the extension possibilities using compiled plugins and ad-hoc scripting allow for the rapid adaption of Mayday also to very specialized data exploration. Mayday is available at http://microarray-analysis.org."
1894,"integrative analysis of the melanoma transcriptome",6787348,"Integrative analysis of the melanoma transcriptome","10.1101/gr.103697.109 Global studies of transcript structure and abundance in cancer cells enable the systematic discovery of aberrations that contribute to carcinogenesis, including gene fusions, alternative splice isoforms, and somatic mutations. We developed a systematic approach to characterize the spectrum of cancer-associated mRNA alterations through integration of transcriptomic and structural genomic data, and we applied this approach to generate new insights into melanoma biology. Using paired-end massively parallel sequencing of cDNA (RNA-seq) together with analyses of high-resolution chromosomal copy number data, we identified 11 novel melanoma gene fusions produced by underlying genomic rearrangements, as well as 12 novel readthrough transcripts. We mapped these chimeric transcripts to base-pair resolution and traced them to their genomic origins using matched chromosomal copy number information. We also used these data to discover and validate base-pair mutations that accumulated in these melanomas, revealing a surprisingly high rate of somatic mutation and lending support to the notion that point mutations constitute the major driver of melanoma progression. Taken together, these results may indicate new avenues for target discovery in melanoma, while also providing a template for large-scale transcriptome studies across many tumor types."
1895,"xgap a uniform and extensible data model and software platform for genotype and phenotype experiments",6791333,"XGAP: a uniform and extensible data model and software platform for genotype and phenotype experiments.","We present an extensible software model for the genotype and phenotype community, XGAP. Readers can download a standard XGAP (http://www.xgap.org) or auto-generate a custom version using MOLGENIS with programming interfaces to R-software and web-services or user interfaces for biologists. XGAP has simple load formats for any type of genotype, epigenotype, transcript, protein, metabolite or other phenotype data. Current functionality includes tools ranging from eQTL analysis in mouse to genome-wide association studies in humans."
1896,"analysis of genetic inheritance in a family quartet by wholegenome sequencing",6807250,"Analysis of Genetic Inheritance in a Family Quartet by Whole-Genome Sequencing","We analyzed the whole-genome sequences of a family of four, consisting of two siblings and their parents. Family-based sequencing allowed us to delineate recombination sites precisely, identify 70% of the sequencing errors (resulting in > 99.999% accuracy), and identify very rare single-nucleotide polymorphisms. We also directly estimated a human intergeneration mutation rate of ~1.1 Ã 10â8 per position per haploid genome. Both offspring in this family have two recessive disorders: Miller syndrome, for which the gene was concurrently identified, and primary ciliary dyskinesia, for which causative genes have been previously identified. Family-based genome analysis enabled us to narrow the candidate genes for both of these Mendelian disorders to only four. Our results demonstrate the value of complete genome sequencing in families."
1897,"a guide to web tools to prioritize candidate genes",6889031,"A guide to web tools to prioritize candidate genes","Finding the most promising genes among large lists of candidate genes has been defined as the gene prioritization problem. It is a recurrent problem in genetics in which genetic conditions are reported to be associated with chromosomal regions. In the last decade, several different computational approaches have been developed to tackle this challenging task. In this study, we review 19 computational solutions for human gene prioritization that are freely accessible as web tools and illustrate their differences. We summarize the various biological problems to which they have been successfully applied. Ultimately, we describe several research directions that could increase the quality and applicability of the tools. In addition we developed a website (http://www.esat.kuleuven.be/gpp) containing detailed information about these and other tools, which is regularly updated. This review and the associated website constitute together a guide to help users select a gene prioritization strategy that suits best their needs."
1898,"analogies in theoretical physics",6892271,"Analogies in theoretical physics","Analogies have had and continue to have an important role in the development of theoretical physics. They may start from similarities of physical concepts followed by similarities in the mathematical formalization or it may be a purely mathematical aspect to suggest the development of analogous physical concepts. More often a subtle non obvious interplay between these levels is involved. In this paper I will discuss two cases sufficiently intricate to illustrate some ways of how analogies work. The first topic is the introduction of spontaneous symmetry breaking in particle physics. The second one is the use of the renormalization group in the theory of critical phenomena and its statistical interpretation."
1899,"regulatory divergence in drosophila revealed by mrnaseq",6931396,"Regulatory divergence in Drosophila revealed by mRNA-seq","10.1101/gr.102491.109 The regulation of gene expression is critical for organismal function and is an important source of phenotypic diversity between species. Understanding the genetic and molecular mechanisms responsible for regulatory divergence is therefore expected to provide insight into evolutionary change. Using deep sequencing, we quantified total and allele-specific mRNA expression levels genome-wide in two closely related  species ( and ) and their F hybrids. We show that 78% of expressed genes have divergent expression between species, and that - and -regulatory divergence affects 51% and 66% of expressed genes, respectively, with 35% of genes showing evidence of both. This is a relatively larger contribution of -regulatory divergence than was expected based on prior studies, and may result from the unique demographic history of . Genes with antagonistic - and -regulatory changes were more likely to be misexpressed in hybrids, consistent with the idea that such regulatory changes contribute to hybrid incompatibilities. In addition, -regulatory differences contributed more to divergent expression of genes that showed additive rather than nonadditive inheritance. A correlation between sequence similarity and the conservation of -regulatory activity was also observed that appears to be a general feature of regulatory evolution. Finally, we examined regulatory divergence that may have contributed to the evolution of a specific trait—divergent feeding behavior in . Overall, this study illustrates the power of mRNA sequencing for investigating regulatory evolution, provides novel insight into the evolution of gene expression in , and reveals general trends that are likely to extend to other species."
1900,"genemesh a webbased microarray analysis tool for relating differentially expressed genes to mesh terms",6941278,"GeneMesh: a web-based microarray analysis tool for relating differentially expressed genes to MeSH terms.","BACKGROUND: An important objective of DNA microarray-based gene expression experimentation is determining inter-relationships that exist between differentially expressed genes and biological processes, molecular functions, cellular components, signaling pathways, physiologic processes and diseases. RESULTS: Here we describe GeneMesh, a web-based program that facilitates analysis of DNA microarray gene expression data. GeneMesh relates genes in a query set to categories available in the Medical Subject Headings (MeSH) hierarchical index. The interface enables hypothesis driven relational analysis to a specific MeSH subcategory (e.g., Cardiovascular System, Genetic Processes, Immune System Diseases etc.) or unbiased relational analysis to broader MeSH categories (e.g., Anatomy, Biological Sciences, Disease etc.). Genes found associated with a given MeSH category are dynamically linked to facilitate tabular and graphical depiction of Entrez Gene information, Gene Ontology information, KEGG metabolic pathway diagrams and intermolecular interaction information. Expression intensity values of groups of genes that cluster in relation to a given MeSH category, gene ontology or pathway can be displayed as heat maps of Z score-normalized values. GeneMesh operates on gene expression data derived from a number of commercial microarray platforms including Affymetrix, Agilent and Illumina. CONCLUSIONS: GeneMesh is a versatile web-based tool for testing and developing new hypotheses through relating genes in a query set (e.g., differentially expressed genes from a DNA microarray experiment) to descriptors making up the hierarchical structure of the National Library of Medicine controlled vocabulary thesaurus, MeSH. The system further enhances the discovery process by providing links between sets of genes associated with a given MeSH category to a rich set of html linked tabular and graphic information including Entrez Gene summaries, gene ontologies, intermolecular interactions, overlays of genes onto KEGG pathway diagrams and heatmaps of expression intensity values. GeneMesh is freely available online at http://proteogenomics.musc.edu/genemesh/."
1901,"positive results increase down the hierarchy of the sciences",6976308,"""Positive"" Results Increase Down the Hierarchy of the Sciences","The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the “hardness” of scientific research—i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors—is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a “positive” (full or partial) or “negative” support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in “softer” sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree."
1902,"multiplexed massively parallel selex for characterization of human transcription factor binding specificities",6983695,"Multiplexed massively parallel SELEX for characterization of human transcription factor binding specificities.","The genetic code-the binding specificity of all transfer-RNAs--defines how protein primary structure is determined by DNA sequence. DNA also dictates when and where proteins are expressed, and this information is encoded in a pattern of specific sequence motifs that are recognized by transcription factors. However, the DNA-binding specificity is only known for a small fraction of the approximately 1400 human transcription factors (TFs). We describe here a high-throughput method for analyzing transcription factor binding specificity that is based on systematic evolution of ligands by exponential enrichment (SELEX) and massively parallel sequencing. The method is optimized for analysis of large numbers of TFs in parallel through the use of affinity-tagged proteins, barcoded selection oligonucleotides, and multiplexed sequencing. Data are analyzed by a new bioinformatic platform that uses the hundreds of thousands of sequencing reads obtained to control the quality of the experiments and to generate binding motifs for the TFs. The described technology allows higher throughput and identification of much longer binding profiles than current microarray-based methods. In addition, as our method is based on proteins expressed in mammalian cells, it can also be used to characterize DNA-binding preferences of full-length proteins or proteins requiring post-translational modifications. We validate the method by determining binding specificities of 14 different classes of TFs and by confirming the specificities for NFATC1 and RFX3 using ChIP-seq. Our results reveal unexpected dimeric modes of binding for several factors that were thought to preferentially bind DNA as monomers."
1903,"openflydata an exemplar data web integrating gene expression data on the fruit fly drosophila melanogaster",7010698,"OpenFlyData: an exemplar data web integrating gene expression data on the fruit fly Drosophila melanogaster.","MOTIVATION: Integrating heterogeneous data across distributed sources is a major requirement for in silico bioinformatics supporting translational research. For example, genome-scale data on patterns of gene expression in the fruit fly Drosophila melanogaster are widely used in functional genomic studies in many organisms to inform candidate gene selection and validate experimental results. However, current data integration solutions tend to be heavy weight, and require significant initial and ongoing investment of effort. Development of a common Web-based data integration infrastructure (a.k.a. data web), using Semantic Web standards, promises to alleviate these difficulties, but little is known about the feasibility, costs, risks or practical means of migrating to such an infrastructure. RESULTS: We describe the development of OpenFlyData, a proof-of-concept system integrating gene expression data on D. melanogaster, combining Semantic Web standards with light-weight approaches to Web programming based on Web 2.0 design patterns. To support researchers designing and validating functional genomic studies, OpenFlyData includes user-facing search applications providing intuitive access to and comparison of gene expression data from FlyAtlas, the BDGP in situ database, and FlyTED, using data from FlyBase to expand and disambiguate gene names. OpenFlyData's services are also openly accessible, and are available for reuse by other bioinformaticians and application developers. Semi-automated methods and tools were developed to support labour- and knowledge-intensive tasks involved in deploying SPARQL services. These include methods for generating ontologies and relational-to-RDF mappings for relational databases, which we illustrate using the FlyBase Chado database schema; and methods for mapping gene identifiers between databases. The advantages of using Semantic Web standards for biomedical data integration are discussed, as are open issues. In particular, although the performance of open source SPARQL implementations is sufficient to query gene expression data directly from user-facing applications such as Web-based data fusions (a.k.a. mashups), we found open SPARQL endpoints to be vulnerable to denial-of-service-type problems, which must be mitigated to ensure reliability of services based on this standard. These results are relevant to data integration activities in translational bioinformatics. AVAILABILITY: The gene expression search applications and SPARQL endpoints developed for OpenFlyData are deployed at http://openflydata.org. FlyUI, a library of JavaScript widgets providing re-usable user-interface components for Drosophila gene expression data, is available at http://flyui.googlecode.com. Software and ontologies to support transformation of data from FlyBase, FlyAtlas, BDGP and FlyTED to RDF are available at http://openflydata.googlecode.com. SPARQLite, an implementation of the SPARQL protocol, is available at http://sparqlite.googlecode.com. All software is provided under the GPL version 3 open source license."
1904,"detection and characterization of novel sequence insertions using pairedend nextgeneration sequencing",7013715,"Detection and characterization of novel sequence insertions using paired-end next-generation sequencing","Motivation: In the past few years, human genome structural variation discovery has enjoyed increased attention from the genomics research community. Many studies were published to characterize short insertions, deletions, duplications and inversions, and associate copy number variants (CNVs) with disease. Detection of new sequence insertions requires sequence data, however, the âdetectableâ sequence length with read-pair analysis is limited by the insert size. Thus, longer sequence insertions that contribute to our genetic makeup are not extensively researched.Results: We present NovelSeq: a computational framework to discover the content and location of long novel sequence insertions using paired-end sequencing data generated by the next-generation sequencing platforms. Our framework can be built as part of a general sequence analysis pipeline to discover multiple types of genetic variation (SNPs, structural variation, etc.), thus it requires significantly less-computational resources than de novo sequence assembly. We apply our methods to detect novel sequence insertions in the genome of an anonymous donor and validate our results by comparing with the insertions discovered in the same genome using various sources of sequence data.Availability: The implementation of the NovelSeq pipeline is available at http://compbio.cs.sfu.ca/strvar.htmContact:eee@gs.washington.edu; cenk@cs.sfu.ca"
1905,"genome remodelling in a basallike breast cancer metastasis and xenograft",7020706,"Genome remodelling in a basal-like breast cancer metastasis and xenograft","Massively parallel DNA sequencing technologies provide an unprecedented ability to screen entire genomes for genetic changes associated with tumour progression. Here we describe the genomic analyses of four DNA samples from an African-American patient with basal-like breast cancer: peripheral blood, the primary tumour, a brain metastasis and a xenograft derived from the primary tumour. The metastasis contained two de novo mutations and a large deletion not present in the primary tumour, and was significantly enriched for 20 shared mutations. The xenograft retained all primary tumour mutations and displayed a mutation enrichment pattern that resembled the metastasis. Two overlapping large deletions, encompassing CTNNA1, were present in all three tumour samples. The differential mutation frequencies and structural variation patterns in metastasis and xenograft compared with the primary tumour indicate that secondary tumours may arise from a minority of cells within the primary tumour."
1906,"biotorrents a file sharing service for scientific data",7021637,"BioTorrents: a file sharing service for scientific data.","<p>The transfer of scientific data has emerged as a significant challenge, as datasets continue to grow in size and demand for open access sharing increases. Current methods for file transfer do not scale well for large files and can cause long transfer times. In this study we present BioTorrents, a website that allows open access sharing of scientific data and uses the popular BitTorrent peer-to-peer file sharing technology. BioTorrents allows files to be transferred rapidly due to the sharing of bandwidth across multiple institutions and provides more reliable file transfers due to the built-in error checking of the file sharing technology. BioTorrents contains multiple features, including keyword searching, category browsing, RSS feeds, torrent comments, and a discussion forum. BioTorrents is available at <ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://www.biotorrents.net"" xlink:type=""simple"">http://www.biotorrents.net</ext-link>.</p>"
1907,"putting brain training to the test",7046614,"Putting brain training to the test","‘Brain training’, or the goal of improved cognitive function through the regular use of computerized tests, is a multimillion-pound industry1, yet in our view scientific evidence to support its efficacy is lacking. Modest effects have been reported in some studies of older individuals2, 3 and preschool children4, and video-game players outperform non-players on some tests of visual attention5. However, the widely held belief that commercially available computerized brain-training programs improve general cognitive function in the wider population in our opinion lacks empirical support. The central question is not whether performance on cognitive tests can be improved by training, but rather, whether those benefits transfer to other untrained tasks or lead to any general improvement in the level of cognitive functioning. Here we report the results of a six-week online study in which 11,430 participants trained several times each week on cognitive tasks designed to improve reasoning, memory, planning, visuospatial skills and attention. Although improvements were observed in every one of the cognitive tasks that were trained, no evidence was found for transfer effects to untrained tasks, even when those tasks were cognitively closely related."
1908,"complexity and diversity",7063829,"Complexity and Diversity","The mechanisms for the origin and maintenance of biological diversity are not fully understood. It is known that frequency-dependent selection, generating advantages for rare types, can maintain genetic variation and lead to speciation, but in models with simple phenotypes (that is, low-dimensional phenotype spaces), frequency dependence needs to be strong to generate diversity. However, we show that if the ecological properties of an organism are determined by multiple traits with complex interactions, the conditions needed for frequency-dependent selection to generate diversity are relaxed to the point where they are easily satisfied in high-dimensional phenotype spaces. Mathematically, this phenomenon is reflected in properties of eigenvalues of quadratic forms. Because all living organisms have at least hundreds of phenotypes, this casts the potential importance of frequency dependence for the origin and maintenance of diversity in a new light. 10.1126/science.1187468"
1909,"metaanalysis for pathway enrichment analysis when combining multiple genomic studies",7093894,"Meta-analysis for pathway enrichment analysis when combining multiple genomic studies.","MOTIVATION: Many pathway analysis (or gene set enrichment analysis) methods have been developed to identify enriched pathways under different biological states within a genomic study. As more and more microarray datasets accumulate, meta-analysis methods have also been developed to integrate information among multiple studies. Currently, most meta-analysis methods for combining genomic studies focus on biomarker detection and meta-analysis for pathway analysis has not been systematically pursued. RESULTS: We investigated two approaches of meta-analysis for pathway enrichment (MAPE) by combining statistical significance across studies at the gene level (MAPE_G) or at the pathway level (MAPE_P). Simulation results showed increased statistical power of meta-analysis approaches compared to a single study analysis and showed complementary advantages of MAPE_G and MAPE_P under different scenarios. We also developed an integrated method (MAPE_I) that incorporates advantages of both approaches. Comprehensive simulations and applications to real data on drug response of breast cancer cell lines and lung cancer tissues were evaluated to compare the performance of three MAPE variations. MAPE_P has the advantage of not requiring gene matching across studies. When MAPE_G and MAPE_P show complementary advantages, the hybrid version of MAPE_I is generally recommended. AVAILABILITY: http://www.biostat.pitt.edu/bioinfo/ CONTACT: ctseng@pitt.edu SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online."
1910,"dyslexic children show shortterm memory deficits in phonological storage and serial rehearsal an fmri study",7118071,"Dyslexic Children Show Short-Term Memory Deficits in Phonological Storage and Serial Rehearsal: An fMRI Study","doi: 10.1080/00207450903139671 Dyslexia is primarily associated with a phonological processing deficit. However, the clinical manifestation also includes a reduced verbal working memory (WM) span. It is unclear whether this WM impairment is caused by the phonological deficit or a distinct WM deficit. The main aim of this study was to investigate neuronal activation related to phonological storage and rehearsal of serial order in WM in a sample of 13-year-old dyslexic children compared with age-matched nondyslexic children. A sequential verbal WM task with two tasks was used. In the Letter Probe task, the probe consisted of a single letter and the judgment was for the presence or absence of that letter in the prior sequence of six letters. In the Sequence Probe (SP) task, the probe consisted of all six letters and the judgment was for a match of their serial order with the temporal order in the prior sequence. Group analyses as well as single-subject analysis were performed with the statistical parametric mapping software SPM2. In the Letter Probe task, the dyslexic readers showed reduced activation in the left precentral gyrus (BA6) compared to control group. In the Sequence Probe task, the dyslexic readers showed reduced activation in the prefrontal cortex and the superior parietal cortex (BA7) compared to the control subjects. Our findings suggest that a verbal WM impairment in dyslexia involves an extended neural network including the prefrontal cortex and the superior parietal cortex. Reduced activation in the left BA6 in both the Letter Probe and Sequence Probe tasks may be caused by a deficit in phonological processing. However, reduced bilateral activation in the BA7 in the Sequence Probe task only could indicate a distinct working memory deficit in dyslexia associated with temporal order processing."
1911,"statistical design and analysis of rna sequencing data",7128750,"Statistical Design and Analysis of RNA Sequencing Data","Next-generation sequencing technologies are quickly becoming the preferred approach for characterizing and quantifying entire genomes. Even though data produced from these technologies are proving to be the most informative of any thus far, very little attention has been paid to fundamental design aspects of data collection and analysis, namely sampling, randomization, replication, and blocking. We discuss these concepts in an RNA sequencing framework. Using simulations we demonstrate the benefits of collecting replicated RNA sequencing data according to well known statistical designs that partition the sources of biological and technical variation. Examples of these designs and their corresponding models are presented with the goal of testing differential expression. 10.1534/genetics.110.114983"
1912,"missing heritability and strategies for finding the underlying causes of complex disease",7192460,"Missing heritability and strategies for finding the underlying causes of complex disease"," Although recent genome-wide studies have provided valuable insights into the genetic basis of human disease, they have explained relatively little of the heritability of most complex traits, and the variants identified through these studies have small effect sizes. This has led to the important and hotly debated issue of where the 'missing heritability' of complex diseases might be found. Here, seven leading geneticists offer their opinion about where this heritability is likely to lie, what this could tell us about the underlying genetic architecture of common diseases and how this could inform research strategies for uncovering genetic risk factors."
1913,"gcbiased evolution near human accelerated regions",7203908,"GC-Biased Evolution Near Human Accelerated Regions","Regions of the genome that have been the target of positive selection specifically along the human lineage are of special importance in human biology. We used high throughput sequencing combined with methods to enrich human genomic samples for particular targets to obtain the sequence of 22 chromosomal samples at high depth in 40 kb neighborhoods of 49 previously identified 100â€“400 bp elements that show evidence for human accelerated evolution. In addition to selection, the pattern of nucleotide substitutions in several of these elements suggested an historical bias favoring the conversion of weak (A or T) alleles into strong (G or C) alleles. Here we found strong evidence in the derived allele frequency spectra of many of these 40 kb regions for ongoing weak-to-strong fixation bias. Comparison of the nucleotide composition at polymorphic loci to the composition at sites of fixed substitutions additionally reveals the signature of historical weak-to-strong fixation bias in a subset of these regions. Most of the regions with evidence for historical bias do not also have signatures of ongoing bias, suggesting that the evolutionary forces generating weak-to-strong bias are not constant over time. To investigate the role of selection in shaping these regions, we analyzed the spatial pattern of polymorphism in our samples. We found no significant evidence for selective sweeps, possibly because the signal of such sweeps has decayed beyond the power of our tests to detect them. Together, these results do not rule out functional roles for the observed changes in these regionsâ€”indeed there is good evidence that the first two are functional elements in humansâ€”but they suggest that a fixation process (such as biased gene conversion) that is biased at the nucleotide level, but is otherwise selectively neutral, could be an important evolutionary force at play in them, both historically and at present."
1914,"from computational science to internetics integration of science with computer science",7209028,"From computational science to Internetics: Integration of science with computer science","We describe how our world dominated by science and scientists has been changed and will be revolutionized by technologies moving with internet time. Computers have always been well-used tools but in the beginning only the science counted and little credit or significance was attached to any computing activities associated with scientific research. Some 20 years ago, this started to change and the area of computational science gathered support with the NSF supercomputer centers playing a critical role. However, this vision has stalled over the last 5 years with information technology increasing in importance. The Holy Grail of computational science — scalable parallel computing — is still important but is just one supporting component of the internet revolution. We discuss the emergence of the field of Internetics — bridging computer science and all application areas whether simulation or information based. Internetics is an exciting field, which seems complete and rich enough to be a lasting interdisciplinary area. Physics and other core science and engineering disciplines used to attract the very best minds but now their popularity is declining. We describe curricula initiatives that can reinvigorate these fields. This curricula turmoil must be addressed by our education infrastructure whose professorial staff find it hard to develop courses to satisfy student and employer interests in times of such rapid change. Distance education is very relevant as it can be used to disseminate expertise to students and teachers in these new areas. All of this has implications for our educational institutions, which could be quite profound."
1915,"an esciencebayes strategy for analyzing omics data",7220035,"An eScience-Bayes strategy for analyzing omics data.","BACKGROUND: The omics fields promise to revolutionize our understanding of biology and biomedicine. However, their potential is compromised by the challenge to analyze the huge datasets produced. Analysis of omics data is plagued by the curse of dimensionality, resulting in imprecise estimates of model parameters and performance. Moreover, the integration of omics data with other data sources is difficult to shoehorn into classical statistical models. This has resulted in ad hoc approaches to address specific problems. RESULTS: We present a general approach to omics data analysis that alleviates these problems. By combining eScience and Bayesian methods, we retrieve scientific information and data from multiple sources and coherently incorporate them into large models. These models improve the accuracy of predictions and offer new insights into the underlying mechanisms. This ""eScience-Bayes"" approach is demonstrated in two proof-of-principle applications, one for breast cancer prognosis prediction from transcriptomic data and one for protein-protein interaction studies based on proteomic data. CONCLUSIONS: Bayesian statistics provide the flexibility to tailor statistical models to the complex data structures in omics biology as well as permitting coherent integration of multiple data sources. However, Bayesian methods are in general computationally demanding and require specification of possibly thousands of prior distributions. eScience can help us overcome these difficulties. The eScience-Bayes thus approach permits us to fully leverage on the advantages of Bayesian methods, resulting in models with improved predictive performance that gives more information about the underlying biological system."
1916,"assembly of large genomes using secondgeneration sequencing",7224150,"Assembly of large genomes using second-generation sequencing.","Second-generation sequencing technology can now be used to sequence an entire human genome in a matter of days and at low cost. Sequence read lengths, initially very short, have rapidly increased since the technology first appeared, and we now are seeing a growing number of efforts to sequence large genomes de novo from these short reads. In this Perspective, we describe the issues associated with short-read assembly, the different types of data produced by second-gen sequencers, and the latest assembly algorithms designed for these data. We also review the genomes that have been assembled recently from short reads and make recommendations for sequencing strategies that will yield a high-quality assembly."
1917,"researchers ejournal use and information seeking behaviour",7237686,"Researchers e-journal use and information seeking behaviour","10.1177/0165551510371883  This paper presents the results of the second phase of a Research Information Network study, which sought to establish the impact of e-journals on the scholarly behaviour of researchers in the UK. The first phase of the project was a deep log analysis of the usage and information seeking behaviour of researchers in connection with the ScienceDirect and Oxford Journals databases. This paper reports on the second phase, which sought to explain and provide context for the deep log data by taking the questions raised by the quantitative study to the research community via interview, questionnaire and observation. Nine major research institutions took part, six subjects were covered and the behaviour of about 1400 people was analyzed. Findings show that academic journals have become central to all disciplines and that the e-form is the prime means of access. Most importantly the study demonstrates that computer usage logs provide an accurate picture of online behaviour. High levels of gateway service use point to the re-intermediating of the broken chain between publisher and reader."
1918,"pathtext a text mining integrator for biological pathway visualizations",7262603,"PathText: a text mining integrator for biological pathway visualizations","Motivation: Metabolic and signaling pathways are an increasingly important part of organizing knowledge in systems biology. They serve to integrate collective interpretations of facts scattered throughout literature. Biologists construct a pathway by reading a large number of articles and interpreting them as a consistent network, but most of the models constructed currently lack direct links to those articles. Biologists who want to check the original articles have to spend substantial amounts of time to collect relevant articles and identify the sections relevant to the pathway. Furthermore, with the scientific literature expanding by several thousand papers per week, keeping a model relevant requires a continuous curation effort. In this article, we present a system designed to integrate a pathway visualizer, text mining systems and annotation tools into a seamless environment. This will enable biologists to freely move between parts of a pathway and relevant sections of articles, as well as identify relevant papers from large text bases. The system, PathText, is developed by Systems Biology Institute, Okinawa Institute of Science and Technology, National Centre for Text Mining (University of Manchester) and the University of Tokyo, and is being used by groups of biologists from these locations.  Contact: brian@monrovian.com. 10.1093/bioinformatics/btq221"
1919,"transposable elements have rewired the core regulatory network of human embryonic stem cells",7262609,"Transposable elements have rewired the core regulatory network of human embryonic stem cells","Detection of new genomic control elements is critical in understanding transcriptional regulatory networks in their entirety. We studied the genome-wide binding locations of three key regulatory proteins (POU5F1, also known as OCT4; NANOG; and CTCF) in human and mouse embryonic stem cells. In contrast to CTCF, we found that the binding profiles of OCT4 and NANOG are markedly different, with only ~5% of the regions being homologously occupied. We show that transposable elements contributed up to 25% of the bound sites in humans and mice and have wired new genes into the core regulatory network of embryonic stem cells. These data indicate that species-specific transposable elements have substantially altered the transcriptional circuitry of pluripotent stem cells."
1920,"bioinformatics training a review of challenges actions and support requirements",7343719,"Bioinformatics training: a review of challenges, actions and support requirements","As bioinformatics becomes increasingly central to research in the molecular life sciences, the need to train non-bioinformaticians to make the most of bioinformatics resources is growing. Here, we review the key challenges and pitfalls to providing effective training for users of bioinformatics services, and discuss successful training strategies shared by a diverse set of bioinformatics trainers. We also identify steps that trainers in bioinformatics could take together to advance the state of the art in current training practices. The ideas presented in this article derive from the first Trainer Networking Session held under the auspices of the EU-funded SLING Integrating Activity, which took place in November 2009."
1921,"cito the citation typing ontology",7351537,"CiTO, the Citation Typing Ontology","CiTO, the Citation Typing Ontology, is an ontology for describing the nature of reference citations in scientific research articles and other scholarly works, both to other such publications and also to Web information resources, and for publishing these descriptions on the Semantic Web. Citation are described in terms of the factual and rhetorical relationships between citing publication and cited publication, the in-text and global citation frequencies of each cited work, and the nature of the cited work itself, including its publication and peer review status. This paper describes CiTO and illustrates its usefulness both for the annotation of bibliographic reference lists and for the visualization of citation networks. The latest version of CiTO, which this paper describes, is CiTO Version 1.6, published on 19 March 2010. CiTO is written in the Web Ontology Language OWL, uses the namespace http://purl.org/net/cito/, and is available from http://purl.org/net/cito/. This site uses content negotiation to deliver to the user an OWLDoc Web version of the ontology if accessed via a Web browser, or the OWL ontology itself if accessed from an ontology management tool such as Protege 4 (http://protege.stanford.edu/). Collaborative work is currently under way to harmonize CiTO with other ontologies describing bibliographies and the rhetorical structure of scientific discourse."
1922,"long noncoding rna genes conservation of sequence and brain expression among diverse amniotes",7465908,"Long noncoding RNA genes: conservation of sequence and brain expression among diverse amniotes","BACKGROUND:Long considered to be the building block of life, it is now apparent that protein is only one of many functional products generated by the eukaryotic genome. Indeed, more of the human genome is transcribed into noncoding sequence than into protein-coding sequence. Nevertheless, whilst we have developed a deep understanding of the relationships between evolutionary constraint and function for protein-coding sequence, little is known about these relationships for non-coding transcribed sequence. This dearth of information is partially attributable to a lack of established non-protein-coding (ncRNA) orthologs among birds and mammals within sequence and expression databases.RESULTS:Here, we performed a multi-disciplinary study of four highly conserved and brain-expressed transcripts selected from a list of mouse long intergenic noncoding (lncRNA) loci that generally show pronounced evolutionary constraint within their putative promoter regions and across exon-intron boundaries. We identify some of the first lncRNA orthologs present in birds (chicken), marsupial (opossum), and eutherian mammals (mouse), and investigate whether they exhibit conservation of brain expression. In contrast to conventional protein-coding genes, the sequences, transcriptional start sites, exon structures, and lengths for these non-coding genes are all highly variable.CONCLUSIONS:The biological relevance of lncRNAs would be highly questionable if they were limited to closely-related phyla. Instead, their preservation across diverse amniotes, their residual apparent conservation in exon structure, and similarities in their pattern of brain expression during embryonic and early postnatal stages together indicate that these are functional RNA molecules, of which some have roles in vertebrate brain development."
1923,"highresolution analysis of parentoforigin allelic expression in the mouse brain",7470021,"High-Resolution Analysis of Parent-of-Origin Allelic Expression in the Mouse Brain","Genomic imprinting results in preferential expression of the paternal or maternal allele of certain genes. We have performed a genome-wide characterization of imprinting in the mouse embryonic and adult brain. This approach uncovered parent-of-origin allelic effects of more than 1300 loci. We identified parental bias in the expression of individual genes and of specific transcript isoforms, with differences between brain regions. Many imprinted genes are expressed in neural systems associated with feeding and motivated behaviors, and parental biases preferentially target genetic pathways governing metabolism and cell adhesion. We observed a preferential maternal contribution to gene expression in the developing brain and a major paternal contribution in the adult brain. Thus, parental expression bias emerges as a major mode of epigenetic regulation in the brain."
1924,"quantifying the mechanisms of domain gain in animal proteins",7493416,"Quantifying the mechanisms of domain gain in animal proteins.","BACKGROUND: Protein domains are protein regions that are shared among different proteins and are frequently functionally and structurally independent from the rest of the protein. Novel domain combinations have a major role in evolutionary innovation. However, the relative contributions of the different molecular mechanisms that underlie domain gains in animals are still unknown. By using animal gene phylogenies we were able to identify a set of high confidence domain gain events and by looking at their coding DNA investigate the causative mechanisms. RESULTS: Here we show that the major mechanism for gains of new domains in metazoan proteins is likely to be gene fusion through joining of exons from adjacent genes, possibly mediated by non-allelic homologous recombination. Retroposition and insertion of exons into ancestral introns through intronic recombination are, in contrast to previous expectations, only minor contributors to domain gains and have accounted for less than 1% and 10% of high confidence domain gain events, respectively. Additionally, exonization of previously non-coding regions appears to be an important mechanism for addition of disordered segments to proteins. We observe that gene duplication has preceded domain gain in at least 80% of the gain events. CONCLUSIONS: The interplay of gene duplication and domain gain demonstrates an important mechanism for fast neofunctionalization of genes."
1925,"computational design of an enzyme catalyst for a stereoselective bimolecular dielsalder reaction",7511505,"Computational Design of an Enzyme Catalyst for a Stereoselective Bimolecular Diels-Alder Reaction","The Diels-Alder reaction is a cornerstone in organic synthesis, forming two carbon-carbon bonds and up to four new stereogenic centers in one step. No naturally occurring enzymes have been shown to catalyze bimolecular Diels-Alder reactions. We describe the de novo computational design and experimental characterization of enzymes catalyzing a bimolecular Diels-Alder reaction with high stereoselectivity and substrate specificity. X-ray crystallography confirms that the structure matches the design for the most active of the enzymes, and binding site substitutions reprogram the substrate specificity. Designed stereoselective catalysts for carbon-carbon bond-forming reactions should be broadly useful in synthetic chemistry."
1926,"integration and visualization of systems biology data in context of the genome",7515324,"Integration and visualization of systems biology data in context of the genome","BACKGROUND:High-density tiling arrays and new sequencing technologies are generating rapidly increasing volumes of transcriptome and protein-DNA interaction data. Visualization and exploration of this data is critical to understanding the regulatory logic encoded in the genome by which the cell dynamically affects its physiology and interacts with its environment.RESULTS:The Gaggle Genome Browser is a cross-platform desktop program for interactively visualizing high-throughput data in the context of the genome. Important features include dynamic panning and zooming, keyword search and open interoperability through the Gaggle framework. Users may bookmark locations on the genome with descriptive annotations and share these bookmarks with other users. The program handles large sets of user-generated data using an in-process database and leverages the facilities of SQL and the R environment for importing and manipulating data.A key aspect of the Gaggle Genome Browser is interoperability. By connecting to the Gaggle framework, the genome browser joins a suite of interconnected bioinformatics tools for analysis and visualization with connectivity to major public repositories of sequences, interactions and pathways. To this flexible environment for exploring and combining data, the Gaggle Genome Browser adds the ability to visualize diverse types of data in relation to its coordinates on the genome.CONCLUSIONS:Genomic coordinates function as a common key by which disparate biological data types can be related to one another. In the Gaggle Genome Browser, heterogeneous data are joined by their location on the genome to create information-rich visualizations yielding insight into genome organization, transcription and its regulation and, ultimately, a better understanding of the mechanisms that enable the cell to dynamically respond to its environment."
1927,"optimization of de novo transcriptome assembly from nextgeneration sequencing data",7577931,"Optimization of de novo transcriptome assembly from next-generation sequencing data","Transcriptome analysis has important applications in many biological fields. However, assembling a transcriptome without a known reference remains a challenging task requiring algorithmic improvements. We present two methods for substantially improving transcriptome de novo assembly. The first method relies on the observation that the use of a single k-mer length by current de novo assemblers is suboptimal to assemble transcriptomes where the sequence coverage of transcripts is highly heterogeneous. We present the Multiple-k method in which various k-mer lengths are used for de novo transcriptome assembly. We demonstrate its good performance by assembling de novo a published next-generation transcriptome sequence data set of Aedes aegypti, using the existing genome to check the accuracy of our method. The second method relies on the use of a reference proteome to improve the de novo assembly. We developed the Scaffolding using Translation Mapping (STM) method that uses mapping against the closest available reference proteome for scaffolding contigs that map onto the same protein. In a controlled experiment using simulated data, we show that the STM method considerably improves the assembly, with few errors. We applied these two methods to assemble the transcriptome of the non-model catfish Loricaria gr. cataphracta. Using the Multiple-k and STM methods, the assembly increases in contiguity and in gene identification, showing that our methods clearly improve quality and can be widely used. The new methods were used to assemble successfully the transcripts of the core set of genes regulating tooth development in vertebrates, while classic de novo assembly failed."
1928,"reshaping the gut microbiome with bacterial transplantation and antibiotic intake",7730675,"Reshaping the gut microbiome with bacterial transplantation and antibiotic intake","The intestinal microbiota consists of over 1000 species, which play key roles in gut physiology and homeostasis. Imbalances in the composition of this bacterial community can lead to transient intestinal dysfunctions and chronic disease states. Understanding how to manipulate this ecosystem is thus essential for treating many disorders. In this study, we took advantage of recently developed tools for deep sequencing and phylogenetic clustering to examine the long-term effects of exogenous microbiota transplantation combined with and without an antibiotic pretreatment. In our rat model, deep sequencing revealed an intestinal bacterial diversity exceeding that of the human gut by a factor of two to three. The transplantation produced a marked increase in the microbial diversity of the recipients, which stemmed from both capture of new phylotypes and increase in abundance of others. However, when transplantation was performed after antibiotic intake, the resulting state simply combined the reshaping effects of the individual treatments (including the reduced diversity from antibiotic treatment alone). Therefore, lowering the recipient bacterial load by antibiotic intake prior to transplantation did not increase establishment of the donor phylotypes, although some dominant lineages still transferred successfully. Remarkably, all of these effects were observed after 1 mo of treatment and persisted after 3 mo. Overall, our results indicate that the indigenous gut microbial composition is more plastic that previously anticipated. However, since antibiotic pretreatment counterintuitively interferes with the establishment of an exogenous community, such plasticity is likely conditioned more by the altered microbiome gut homeostasis caused by antibiotics than by the primary bacterial loss."
1929,"the characterization of twenty sequenced human genomes",7811569,"The Characterization of Twenty Sequenced Human Genomes","We present the analysis of twenty human genomes to evaluate the prospects for identifying rare functional variants that contribute to a phenotype of interest. We sequenced at high coverage ten â€œcaseâ€� genomes from individuals with severe hemophilia A and ten â€œcontrolâ€� genomes. We summarize the number of genetic variants emerging from a study of this magnitude, and provide a proof of concept for the identification of rare and highly-penetrant functional variants by confirming that the cause of hemophilia A is easily recognizable in this data set. We also show that the number of novel single nucleotide variants (SNVs) discovered per genome seems to stabilize at about 144,000 new variants per genome, after the first 15 individuals have been sequenced. Finally, we find that, on average, each genome carries 165 homozygous protein-truncating or stop loss variants in genes representing a diverse set of pathways."
